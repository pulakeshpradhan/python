{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Geo Coding Muscle with Python","text":"<p>Welcome to the comprehensive repository of Python learning resources, projects, and geospatial analysis guides.</p>"},{"location":"#repository-structure","title":"\ud83d\udcda Repository Structure","text":"<p>Explore the various sections of this documentation through the interactive tree below. Note that for most tutorials, an Open in Colab button is available at the top of the page.</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 Data Science\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Data Science Bootcamp With Python\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Introduction To Jupyter Notebook\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Jupyter Notebook Tutorial.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Jupyter Notebook Tutorial.Md\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Introduction To Numpy\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Numpy.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Introduction To Numpy.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Numpy Axis.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Numpy Axis.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Numpy Attributes And Methods.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Numpy Attributes And Methods.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Mathematical Operations Using Numpy.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Mathematical Operations Using Numpy.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Advantages Of Using Numpy.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Advantages Of Using Numpy.Md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Random Practice\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Practice.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Introduction To Pandas\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Pandas.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Introduction To Pandas.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pandas Dataframe.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Pandas Dataframe.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Different Ways Of Creating Dataframe.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Different Ways Of Creating Dataframe.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Read Write Excel Csv.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Read Write Excel Csv.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Handle Missing Data.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Handle Missing Data.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Handle Missing Data.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Handle Missing Data.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Group By Method.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Group By Method.Md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Datasets\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Introduction To Matplotlib\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Matplotlib.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Introduction To Matplotlib.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Format Strings In Plot Function.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Format Strings In Plot Function.Md\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Linear Regression\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Simple Linear Regression.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Simple Linear Regression.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Multiple Linear Regression.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Multiple Linear Regression.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cost Function.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Cost Function.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exercises\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Simple Linear Regression Exercise.Ipynb\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Simple Linear Regression Exercise.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Simple Logistic Regression.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Simple Logistic Regression.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Untitled.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Untitled.Md\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Decision Tree\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Decision Tree Classifier.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Decision Tree Classifier.Md\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Datasets\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Projects\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Predicting Life Expectancy Using Linear Regression.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Predicting Life Expectancy Using Linear Regression.Md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Predicting Aqi Using Regression.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Predicting Aqi Using Regression.Md\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Random Practices\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Practice.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcc4 Practice.Md\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Untitled.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcc4 Untitled.Md\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Untitled1.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcc4 Untitled1.Md\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Untitled2.Ipynb\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcc4 Untitled2.Md\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Data Wrangling With Xarray\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Fundamentals\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Data Structures.Ipynb\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 End To End Machine Learning\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Data Gathering\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Working With Csv Files.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Working With Json And Sql.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Fetching Data From An Api.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Fetching Data Using Web Scrapping.Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Understanding Your Data.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Exploratory Data Analysis (Eda)\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Eda Using Univariate Analysis.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Eda Using Bivariate And Multivariate Analysis.Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Pandas Profiling.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Feature Engineering\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Standardization.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Normalization.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Encoding Categorical Data.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 One Hot Encoding.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Column Transformer.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Ml Without Pipeline 1.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Ml Without Pipeline 2.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Ml With Pipeline 1.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Ml With Pipeline 2.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Function Transformer.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Power Transformer.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Discretization.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pca Step By Step.Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Pca Implementation On Mnist Data.Ipynb\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 Machine Learning Algorithms\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Linear Regression\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Simple Linear Regression.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Mathematical Formulation Of Slr.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Regression Metrics.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Multiple Linear Regression.Ipynb\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Mathematical Formulation Of Mlr.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Gradient Descent\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Gradient Descent Step By Step 1.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Gradient Descent Step By Step 2.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Batch Gradient Descent.Ipynb\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Stochastic Gradient Descent.Ipynb\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Mini Batch Gradient Descent.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Polynomial Regression\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Polynomial Regression.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Logistic Regression\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Perceptron Trick.Ipynb\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Sigmoid Function.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Support Vector Machine\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Support Vector Machine.Ipynb\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Kernels In Svm.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc1 Decision Tree\n\u2502           \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Overfitting And Underfitting In Decision Trees.Ipynb\n\u2502           \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Regression Tree.Ipynb\n\u2502           \u2514\u2500\u2500 \ud83d\udcc1 Adaboost\n\u2502               \u2514\u2500\u2500 \ud83d\udcd3 Adaboost Step By Step.Ipynb\n\u251c\u2500\u2500 \ud83d\udcc1 Deep Learning\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 End To End Deep Learning\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Ann\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Perceptron.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Perceptron Tricks.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Problem With Perceptron.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Customer Churn Prediction Using Ann.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Handwritten Digit Classification.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Graduate Admission Prediction Using Ann.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Backpropagation On Regression Data.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Cnn\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Cnn.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cnn Vs Visual Cortex.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Convolution Operation.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Padding And Strides In Cnn.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pooling Layer In Cnn.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cnn Architecture And Lenet 5.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Cnn Vs Ann.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Activation Functions In Deep Learning.Ipynb\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Deep Learning From Scratch\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 [2] Srgan\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Model.Py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Test.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 [3] Transformers\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Simplified Attention Mechanism Without Trainable Weights.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Self Attention With Trainable Weights.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Causal Attention.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Multi Head Attention.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 [4] Vision Transformer (Vit)\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Coding Vit From Scratch.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 [5] Nanovlm\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Build Nanovlm From Scratch.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 [6] Variational Autoencoders\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Coding Vae From Scratch.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 [7] Diffusion Models\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Ddpm Forward Diffusion.Ipynb\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Ddpm Reverse Transition Kernel.Ipynb\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 Pytorch For Deep Learning And Machine Learning\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Fundamentals.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Workflow.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Classification.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Computer Vision.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Cnn\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Cnn.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cnn Vs Visual Cortex.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Convolution Operation.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Padding And Strides In Cnn.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pooling Layer In Cnn.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cnn Architecture And Lenet 5.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cnn Vs Ann.Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Cat Vs Dogs Image Classification.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Pytorch Campusx\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Autograd.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Training Pipeline.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Pytorch Nn Module.Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Dataset And Dataloader.Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Buidling Ann Using Pytorch.Ipynb\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 Projects\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Simple Linear Regression Using Pytorch.Ipynb\n\u2502           \u2514\u2500\u2500 \ud83d\udcd3 Intel Image Classification Using Tinyvgg.Ipynb\n\u251c\u2500\u2500 \ud83d\udcc1 Fundamentals\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Python Basics Learn To Code From Scratch\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Introduction To Python\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Python.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Python Comments.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Variables And Datatypes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Variables.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Introduction To Datatypes.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Basic Input And Output\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Print Function.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Taking Input.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Operators In Python\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Arithemetic Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Assignment Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Comparison Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Logical Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Identity Operators.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Membership Operators.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Bitwise Operators.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Conditional Statements And Loop\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Conditional Statement.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Loops.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Break Continue Pass Return.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Function In Python\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Function In Python.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Types Of Functions And Function Overloading.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Scope Of Variables.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Concept Of Args And Kwargs.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Python Default Parameters.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 List In Python\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Two Dimensional List.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 List Methods.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 List Slicing.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 List Comprehension.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 String\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Strings.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 String Concatenation.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 String Methods.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Iterating On Strings.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Object Oriented Programming\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Oop.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Constructor.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Types Of Attributes.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Inheritance.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Access Modifiers.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Access Modifiers With Inheritance.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Types Of Methods.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Polymorphism.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exception Handling\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Exception Handling.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Try Except Statement.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Try Except Else Statement.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Finally Statement.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Class Code\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Access Modifiers.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Access Modifiers With Inheritence.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Arithmetic Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Assignment Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Bitwise Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Break Continue Pass.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Comparison Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Concept Of Args And Kwargs.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Conditional Statement.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Constructor.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Exercise Solution.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Function In Python.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Identity Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Inheritance.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Jupyter Interface.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Markdown.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Datatypes.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Library And Module.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Oop.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Strings.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Variables.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Iterating On Strings.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 List Comprehension.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 List Methods.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 List Slicing.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Logical Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Loops.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Membership Operators.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Polymorphism.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Python Default Parameters.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Python Comments.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Python Input Function.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Python Print Function.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Scope Of Variables.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 String Concatenation.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 String Methods.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Two Dimensional List.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Types Of Attributes.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Types Of Function.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Types Of Methods.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Untitled.Ipynb\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcc4 Test.Py\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 Python For Beginners To Pros\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 Exercises\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (11 20).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (21 30).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (31 40).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (41 50).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (51 60).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (61 70).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (71 80).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (81 90).Ipynb\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Python Exercise (91 100).Ipynb\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Python Exercise (1 10).Ipynb\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 Notebooks\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Python Strings.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Python Strings Solutions.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Time Complexity.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Functions In Python.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Object Oriented Programming 1.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Object Oriented Programming 2.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Object Oriented Programming 3.Ipynb\n\u2502           \u2514\u2500\u2500 \ud83d\udcd3 File Handling In Python.Ipynb\n\u251c\u2500\u2500 \ud83d\udcc1 Geospatial\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Geospatial Data Science With Python\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Working With Projections\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Working With Gcs And Pcs.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exploring Geospatial Packages\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Geopandas.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Spatial Data Structures.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Spatial Data Manipulation.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Geocoding.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Shapely.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Shapely Properties And Methods.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Rasterio.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Ipyleaflet.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exploratory Spatial Data Analysis\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Exploratory Data Visualization.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Creating Choropleth Map From Points.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Datasets\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcc1 Csvs\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcc1 Shapefiles\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Image Analysis In Remote Sensing With Python\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Images Arrays Matrices\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Algebra Of Vectors And Matrices.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Square Matrices.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Reading And Displaying Image Band.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Integration Of Gee With Numpy.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Machine Learning With Scikit Learn\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Download Landsat 8 Data.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Satellite Image Classification Using Scikit Learn.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Untitled.Ipynb\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Mastering Machine Learning And Gee For Earth Science\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Geemap\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Intro To Geemap.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Cloud Masking.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Band Arithmetic.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Download Image And Image Tiles.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Data Gathering\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Working With Csv Files.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Understanding The Data.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exploratory Data Analysis\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Eda Using Univariate Analysis.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Eda Using Bivariate And Multivariate Analysis.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Feature Engineering\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Standardization.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Normalization.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Encoding Categorical Data.Ipynb\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 One Hot Encoding.Ipynb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Column Transformer.Ipynb\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Machine Learning Algorithms\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Simple Linear Regression.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Regression Metrics.Ipynb\n\u2502   \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Multiple Linear Regression.Ipynb\n\u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Perceptron Trick.Ipynb\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 Climate Data Downscaling\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Config.Py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Dataset.Py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Loss.Py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Model.Py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Train.Ipynb\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Train.Md\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Train.Py\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Utils.Py\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 Geonext Handbook\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Conduct.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 Conduct.Md\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Contributing.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 Contributing.Md\n\u2502       \u251c\u2500\u2500 \ud83d\udcd3 Readme.Ipynb\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 Readme.Md\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 Book\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Bibliography.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc4 Bibliography.Md\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Maize Leaf Disease Classification Using Vit.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcd3 Markdown.Ipynb\n\u2502           \u251c\u2500\u2500 \ud83d\udcc4 Markdown.Md\n\u2502           \u2514\u2500\u2500 \ud83d\udcd3 Notebooks.Ipynb\n\u2514\u2500\u2500 \ud83d\udcc1 Projects\n    \u251c\u2500\u2500 \ud83d\udcc1 Crop Yield Forecasting Germany\n    \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Yield Data Preparation.Ipynb\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Yield Data Preparation.Md\n    \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Data Preparation For Pbms.Ipynb\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Data Preparation For Pbms.Md\n    \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Data Preparation For Pbms.Ipynb\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Data Preparation For Pbms.Md\n    \u251c\u2500\u2500 \ud83d\udcc1 Flood Susceptibility Zonation Of Malda District\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Gee Codes\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Flood Zone Detection Using Sar.Js\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 Flooded And Non Flooded Point Generation.Js\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 Training And Testing Data Preparation.Js\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Model\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Notebooks\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Classify The Image.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Assessment Of Validation Metrics.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Training And Testing Data Preparation.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Application Of Random Forest For Flood Susceptibility Zonation.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Application Of Xgboost For Flood Susceptibility Zonation.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Application Of Logistic Regression For Flood Susceptibility Zonation.Ipynb\n    \u2502       \u251c\u2500\u2500 \ud83d\udcd3 Application Of Svm For Flood Susceptibility Zonation.Ipynb\n    \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Convert The Image Into Csv.Ipynb\n    \u251c\u2500\u2500 \ud83d\udcc1 Indi Res\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Notebooks\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Exploration\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcd3 Experimenting With Sam3.Ipynb\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Data Extraction From Stac.Ipynb\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 Preprocessing\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcd3 Extract Indian Reservoirs.Ipynb\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Modeling\n    \u2502   \u2502       \u2514\u2500\u2500 \ud83d\udcd3 Ganfilling.Ipynb\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1 Src\n    \u2502       \u2514\u2500\u2500 \ud83d\udcc1 Indi Res\n    \u2502           \u2514\u2500\u2500 \ud83d\udcc1 Models\n    \u2502               \u251c\u2500\u2500 \ud83d\udcc4 Convlstm.Py\n    \u2502               \u251c\u2500\u2500 \ud83d\udcc4 Discriminator.Py\n    \u2502               \u2514\u2500\u2500 \ud83d\udcc4 Generator.Py\n    \u2514\u2500\u2500 \ud83d\udcc1 Pyimgproc Image Processing Using Python\n        \u251c\u2500\u2500 \ud83d\udcd3 Image Processing Using Pillow In Python.Ipynb\n        \u251c\u2500\u2500 \ud83d\udcd3 Image Processing Using Scipy.Ipynb\n        \u251c\u2500\u2500 \ud83d\udcd3 Introduction To Opencv In Python.Ipynb\n        \u2514\u2500\u2500 \ud83d\udcd3 Waterbodies Extraction Using Entropy And Otsu'S Threshold.Ipynb\n</code></pre>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Use the navigation tabs above to dive into specific domains: - Fundamentals: Core Python concepts and basics. - Data Science: Pandas, NumPy, and data manipulation. - Deep Learning: PyTorch and neural network projects. - Geospatial: Remote sensing, GEE, and spatial analysis. - Projects: Real-world applications and forecasting models.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/","title":"Data-Science-with-Python","text":"<p>This comprehensive course is designed to take you from a beginner level to a proficient level in data science with Python. The course covers the essentials of data science, including data exploration and visualization, data cleaning and wrangling, statistical analysis, and machine learning.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/","title":"Jupyter Notebook Tutorial","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#jupyter-notebook-tutorial","title":"Jupyter Notebook Tutorial","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#01-working-with-codes","title":"01. Working with Codes","text":"<pre><code># Import the pandas library and give it an alias 'pd'\nimport pandas as pd\n\n# Define the URL where the Iris dataset is hosted as a csv\nurl = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n\n# Use pandas to read the CSV file located at the specified URL into a DataFrame\ndf = pd.read_csv(url)\n\n# Print the first 5 rows of the DataFrame to verify that the data was loaded correctly\ndf.head()\n</code></pre> sepal.length sepal.width petal.length petal.width variety 0 5.1 3.5 1.4 0.2 Setosa 1 4.9 3.0 1.4 0.2 Setosa 2 4.7 3.2 1.3 0.2 Setosa 3 4.6 3.1 1.5 0.2 Setosa 4 5.0 3.6 1.4 0.2 Setosa"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#02-working-with-plots","title":"02. Working with Plots","text":"<pre><code>%matplotlib inline\ndf.plot(y=\"petal.length\", color=\"green\")\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>df.plot.bar(x=\"variety\", y=\"petal.length\")\n</code></pre> <pre><code>&lt;Axes: xlabel='variety'&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#03-working-with-magic-commands","title":"03. Working with Magic Commands","text":"<pre><code>%lsmagic\n</code></pre> <pre><code>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n</code></pre> <pre><code>%time for i in range(1, 10000): i*i\n</code></pre> <pre><code>CPU times: total: 0 ns\nWall time: 1.05 ms\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#04-adding-youtube-videos","title":"04. Adding YouTube Videos","text":"<pre><code>%%HTML\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6_jEgQjwfok\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#05-adding-markdown","title":"05. Adding Markdown","text":"<p>This is an example of a Markdown document. Markdown is a lightweight markup language that is used to format plain text documents. It's easy to learn and use, and it's supported by many tools and platforms.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#headers","title":"Headers","text":"<p>You can use hash symbols (<code>#</code>) to create headers of different levels. For example:</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-1","title":"Heading 1","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-2","title":"Heading 2","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-3","title":"Heading 3","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-4","title":"Heading 4","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-5","title":"Heading 5","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-6","title":"Heading 6","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#text-formatting","title":"Text Formatting","text":"<p>You can use asterisks (<code>*</code>) or underscores (<code>_</code>) to add emphasis to text. For example:</p> <p>This text will be italicized. This text will also be italicized.</p> <p>This text will be bold. This text will also be bold.</p> <p>You can combine these characters to create different effects, like so:</p> <p>This text will be bold and italicized.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#lists","title":"Lists","text":"<p>You can create ordered and unordered lists using hyphens (<code>-</code>) or numbers (<code>1.</code>). For example:</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#unordered-list","title":"Unordered List","text":"<ul> <li>Item 1</li> <li>Item 2</li> <li>Item 3</li> </ul>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#ordered-list","title":"Ordered List","text":"<ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#links","title":"Links","text":"<p>You can create links to other websites or pages using square brackets (<code>[]</code>) and parentheses (<code>()</code>). For example:</p> <p>Click here to visit GeoNext</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#images","title":"Images","text":"<p>You can include images in your Markdown document using the same syntax as links, but with an exclamation mark (<code>!</code>) at the beginning. For example:</p> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#code","title":"Code","text":"<p>You can include code snippets in your Markdown document by wrapping the code in backticks ( `) or by using a code block. For example:</p> <pre><code>print(\"Hello World!\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/","title":"Introduction To Numpy","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#introduction-to-numpy","title":"Introduction to NumPy","text":"<p>NumPy is a library for the Python programming language that provides support for arrays, matrices, and other numerical operations. It is an essential library for scientific computing in Python. In this module, you will learn about the basics of NumPy and how to use it for numerical computations.</p> <p>Prerequisites: Before starting with NumPy, it is recommended to have a basic understanding of Python programming language, especially control statements, functions, and data types.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install NumPy, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install numpy\n</code></pre> <pre><code>import numpy as np\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#02-creation-of-one-dimensional-arrays","title":"02. Creation of One-Dimensional Arrays","text":"<p>In NumPy, you can create a one-dimensional array using the numpy.array() method.</p> <pre><code>myArr = np.array([1, 2, 3, 4, 5], dtype=\"int8\")\nmyArr\n</code></pre> <pre><code>array([1, 2, 3, 4, 5], dtype=int8)\n</code></pre> <pre><code># Accessing elements of an one-dimensional array\nprint(myArr[0], myArr[3])\n</code></pre> <pre><code>1 4\n</code></pre> <pre><code># Print the data type\nmyArr.dtype\n</code></pre> <pre><code>dtype('int8')\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#03-creation-of-two-dimensional-arrays","title":"03. Creation of Two-Dimensional Arrays","text":"<p>In NumPy, you can create a two-dimensional array using the numpy.array() method and passing a list of lists as an argument.</p> <pre><code>myArr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"int8\")\nmyArr2\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]], dtype=int8)\n</code></pre> <pre><code># Accessing elements of two-dimensional array\nprint(myArr2[0])\nprint(myArr2[2][0])\nprint(myArr2[0, 2])\n</code></pre> <pre><code>[1 2 3]\n7\n3\n</code></pre> <pre><code># Print the data type\nmyArr2.dtype\n</code></pre> <pre><code>dtype('int8')\n</code></pre> <pre><code># Print the shape of the array\nmyArr2.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code># Changing the element of a two-dimensional array\nmyArr2[0, 2] = 10\nmyArr2\n</code></pre> <pre><code>array([[ 1,  2, 10],\n       [ 4,  5,  6],\n       [ 7,  8,  9]], dtype=int8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#04-other-ways-of-array-creation","title":"04. Other Ways of Array Creation","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#01-array-creation-from-other-python-structures","title":"01. Array Creation from Other Python Structures","text":"<pre><code># Array creation from List\nmyList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlistArray = np.array(myList)\nlistArray\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>listArray.dtype\n</code></pre> <pre><code>dtype('int32')\n</code></pre> <pre><code>listArray.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code># Array creation from tuple\nmyTuple = ((1, 2, 3), (4, 5, 6), (7, 8, 9))\ntupleArray = np.array(myTuple)\ntupleArray\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>tupleArray.dtype\n</code></pre> <pre><code>dtype('int32')\n</code></pre> <pre><code># Array cration from set\nmySet = {1, 2, 3, 4, 5, 6}\nsetArray = np.array(mySet)\nsetArray\n</code></pre> <pre><code>array({1, 2, 3, 4, 5, 6}, dtype=object)\n</code></pre> <pre><code># dtype object is not efficient for numeric calculations\nsetArray.dtype\n</code></pre> <pre><code>dtype('O')\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#02-intrinsic-numpy-array-creation-objects","title":"02. Intrinsic NumPy Array Creation Objects","text":"<pre><code># Array creation using zeros function\nzeros = np.zeros((3, 3))\nzeros\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code># Array creation using range function\nrngArray = np.arange(0, 11, 2)\nrngArray\n</code></pre> <pre><code>array([ 0,  2,  4,  6,  8, 10])\n</code></pre> <pre><code># Array creation using linspace function\n# linspace function is used to create a linearly spaced array\nlspace = np.linspace(start=0, stop=50, num=5)\nlspace\n</code></pre> <pre><code>array([ 0. , 12.5, 25. , 37.5, 50. ])\n</code></pre> <pre><code>lspace2 = np.linspace(1, 2, 5)\nlspace2\n</code></pre> <pre><code>array([1.  , 1.25, 1.5 , 1.75, 2.  ])\n</code></pre> <pre><code># Array creation using empty function\n# empty function is used to create an empty array\nempArray = np.empty((4, 6)) # Elements will be random in this case\nempArray\n</code></pre> <pre><code>array([[6.23042070e-307, 4.67296746e-307, 1.69121096e-306,\n        2.78148153e-307, 1.29060531e-306, 8.45599366e-307],\n       [7.56593017e-307, 1.33511290e-306, 1.02359645e-306,\n        1.24610383e-306, 1.69118108e-306, 8.06632139e-308],\n       [1.20160711e-306, 1.69119330e-306, 1.29062229e-306,\n        1.60217812e-306, 1.37961370e-306, 1.69118515e-306],\n       [1.11258277e-307, 1.05700515e-307, 1.11261774e-306,\n        1.29060871e-306, 8.34424766e-308, 2.12203497e-312]])\n</code></pre> <pre><code># Array creation using empty_like function\n# empty_like function is used to generate a copy of previously created array\nempArray2 = np.empty_like(lspace)\nempArray2\n</code></pre> <pre><code>array([1.  , 1.25, 1.5 , 1.75, 2.  ])\n</code></pre> <pre><code># Array creation using identity function\n# identity function is used to create an identity matrix\nidentityMatrix = np.identity(4)\nidentityMatrix\n</code></pre> <pre><code>array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n</code></pre> <pre><code>identityMatrix.shape\n</code></pre> <pre><code>(4, 4)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#05-reshaping-numpy-1-d-array-to-2-d-array","title":"05. Reshaping NumPy 1-D Array to 2-D Array","text":"<pre><code># Creating NumPy one-dimensional array using range function\narr = np.arange(50)\narr\n</code></pre> <pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n</code></pre> <pre><code># Reshaping 1-D array to 2-D array using range function\narr = arr.reshape((5, 10))\narr\n</code></pre> <pre><code>array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n</code></pre> <pre><code>arr.shape\n</code></pre> <pre><code>(5, 10)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#06-converting-numpy-2-d-array-to-1-d-array","title":"06. Converting NumPy 2-D Array to 1-D Array","text":"<pre><code># Two-dimensional array\narr\n</code></pre> <pre><code>array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n</code></pre> <pre><code># Converting 2-D array into 1-D array\narr = arr.ravel()\narr\narr\n</code></pre> <pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/","title":"Numpy Axis","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/#numpy-axis","title":"Numpy Axis","text":"<p>In NumPy, the axis parameter is used to specify the dimension of an array along which a particular operation should be performed. It is an important parameter in many NumPy functions that deal with multi-dimensional arrays. Understanding how to use the axis parameter is crucial in performing complex operations on multi-dimensional arrays.</p> <p>The axis parameter can take values of 0, 1, 2, and so on, where 0 represents the first dimension (rows), 1 represents the second dimension (columns), and so on.</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating a 2-D list\nmyList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n</code></pre> <pre><code># Creating a 2-D NumPy array from the list\nmyArr = np.array(myList)\nmyArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/#01-summing-elements-along-a-specific-axis","title":"01. Summing Elements along a Specific Axis","text":"<pre><code># Summing elements along axis 0 (Row)\nrow_sum = myArr.sum(axis=0)\nrow_sum\n</code></pre> <pre><code>array([12, 15, 18])\n</code></pre> <pre><code># Summing elements along axis 1 (Column)\ncolumn_sum = myArr.sum(axis=1)\ncolumn_sum\n</code></pre> <pre><code>array([ 6, 15, 24])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/#02-finding-the-maximum-element-along-a-specific-axis","title":"02. Finding the Maximum Element along a Specific Axis","text":"<pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code># Finding the maximum element along axis 0 (Row)\nmax_axis_0 = myArr.max(axis=0)\nmax_axis_0\n</code></pre> <pre><code>array([7, 8, 9])\n</code></pre> <pre><code># Finding the maximum element along axis 1 (Column)\nmax_axis_1 = myArr.max(axis=1)\nmax_axis_1\n</code></pre> <pre><code>array([3, 6, 9])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/","title":"Numpy Attributes And Methods","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#numpy-attributes-and-methods","title":"Numpy Attributes and Methods","text":"<p>NumPy is a powerful library for working with arrays and matrices in Python. It provides various attributes and methods that can be used to manipulate and analyze arrays. In this section, we will discuss some of the important NumPy attributes and methods.</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating a NumPy 2-D array\nmyArr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-numpy-attributes","title":"01. NumPy Attributes","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-t-attribute","title":"01. 'T' Attribute","text":"<p>In NumPy, the T attribute is used to get the transpose of a matrix. The transpose of a matrix is obtained by interchanging the rows and columns of the matrix.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code># Creating transpose of the matrix\nmyArr_transpose = myArr.T\nmyArr_transpose\n</code></pre> <pre><code>array([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-flat-attribute","title":"02. 'flat' Attribute","text":"<p>In NumPy, the flat attribute is used to get a 1-dimensional iterator over a multi-dimensional array. The flat attribute returns a flat iterator that traverses the array in row-major (C-style) order, which means that it first traverses the rows of the array, and then the columns.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.flat\n</code></pre> <pre><code>&lt;numpy.flatiter at 0x160f8c39a20&gt;\n</code></pre> <pre><code>for item in myArr.flat:\n    print(item)\n</code></pre> <pre><code>1\n2\n3\n4\n5\n6\n7\n8\n9\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#03-ndim-attribute","title":"03. 'ndim' Attribute","text":"<p>This attribute returns the number of dimensions of the array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.ndim\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#04-size-attribute","title":"04. 'size' Attribute","text":"<p>This attribute returns the total number of elements in the array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.size\n</code></pre> <pre><code>9\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#05-nbytes-attribute","title":"05. 'nbytes' Attribute","text":"<p>In NumPy, the nbytes attribute is used to get the total number of bytes occupied by the array data in memory. </p> <pre><code>myArr.nbytes\n</code></pre> <pre><code>36\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-numpy-methods","title":"02. NumPy Methods","text":"<pre><code># Creating a NumPy 1-D array\nmyArr2 = np.array([45, 48, 25, 87, 16])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-argmax-method","title":"01. 'argmax' Method","text":"<p>np.argmax(arr, axis=None, out=None) returns the indices of the maximum values along an axis.</p> <pre><code># argmax in 1-D array\nmyArr2.argmax()\n</code></pre> <pre><code>3\n</code></pre> <pre><code># argmax in 2-D array\nmyArr.argmax()\n</code></pre> <pre><code>8\n</code></pre> <pre><code># Finding maximum values along axis\nmyArr.argmax(axis=0)\n</code></pre> <pre><code>array([2, 2, 2], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-argmin-method","title":"02. 'argmin' Method","text":"<p>np.argmin(arr, axis=None, out=None) returns the indices of the minimum values along an axis.</p> <pre><code># argmin in 1-D array\nmyArr2.argmin()\n</code></pre> <pre><code>4\n</code></pre> <pre><code># argmin in 2-D array\nmyArr.argmin()\n</code></pre> <pre><code>0\n</code></pre> <pre><code># Finding minimum values along axis\nmyArr.argmin(axis=0)\n</code></pre> <pre><code>array([0, 0, 0], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#03-argsort-method","title":"03. 'argsort' Method","text":"<p>argsort() is a method provided by NumPy that returns the indices that would sort an array in ascending or descending order.</p> <pre><code># argsort in 1-D array\nmyArr2.argsort()\n</code></pre> <pre><code>array([4, 2, 0, 1, 3], dtype=int64)\n</code></pre> <pre><code># argsort in 2-D array\nmyArr.argsort()\n</code></pre> <pre><code>array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int64)\n</code></pre> <pre><code># Sorting values along axis\nmyArr.argsort(axis=0)\n</code></pre> <pre><code>array([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2]], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#04-ravel-method","title":"04. 'ravel' Method","text":"<p>ravel() is used to flatten an array into a 1D array. </p> <pre><code>myArr.ravel()\n</code></pre> <pre><code>array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#05-reshape-method","title":"05. 'reshape' Method","text":"<p>reshape() is used to change the shape of an array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.reshape((9, 1))\n</code></pre> <pre><code>array([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8],\n       [9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/","title":"Mathematical Operations Using Numpy","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#mathematical-operations-using-numpy","title":"Mathematical Operations Using NumPy","text":"<p>NumPy provides a wide range of mathematical operations that can be performed on arrays. These operations include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more advanced mathematical functions (trigonometric functions, logarithmic functions, etc.).</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating our first array\narr1 = np.array([[1, 3, 5], [4, 7, 4], [3, 6, 1]])\narr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 4],\n       [3, 6, 1]])\n</code></pre> <pre><code># Creating our second array\narr2 = np.array([[2, 7, 6], [3, 4, 8], [1, 7, 3]])\narr2\n</code></pre> <pre><code>array([[2, 7, 6],\n       [3, 4, 8],\n       [1, 7, 3]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-basic-matrix-operations","title":"01. Basic Matrix Operations","text":"<p>A wide range of matrix operations can be performed on arrays by using NumPy. These operations include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more advanced matrix operations (determinants, inverses, eigenvalues, etc.).</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-matrix-addition","title":"01. Matrix Addition","text":"<pre><code># Addition\narr1 + arr2\n</code></pre> <pre><code>array([[ 3, 10, 11],\n       [ 7, 11, 12],\n       [ 4, 13,  4]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-matrix-subtraction","title":"02. Matrix Subtraction","text":"<pre><code># Subtraction\narr1 - arr2\n</code></pre> <pre><code>array([[-1, -4, -1],\n       [ 1,  3, -4],\n       [ 2, -1, -2]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-matrix-multiplication","title":"03. Matrix Multiplication","text":"<pre><code># Multiplication\narr1 * arr2\n</code></pre> <pre><code>array([[ 2, 21, 30],\n       [12, 28, 32],\n       [ 3, 42,  3]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#04-matrix-division","title":"04. Matrix Division","text":"<pre><code># Division\narr1 / arr2\n</code></pre> <pre><code>array([[0.5       , 0.42857143, 0.83333333],\n       [1.33333333, 1.75      , 0.5       ],\n       [3.        , 0.85714286, 0.33333333]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-basic-statistical-operations","title":"02. Basic Statistical Operations","text":"<p>NumPy provides several methods to perform basic statistical operations on arrays such as sqrt(), sum(), min(), and max().</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-sqrt-method","title":"01. 'sqrt' Method","text":"<p>The numpy.sqrt() method is used to calculate the square root of each element in a NumPy array.</p> <pre><code>np.sqrt(arr1)\n</code></pre> <pre><code>array([[1.        , 1.73205081, 2.23606798],\n       [2.        , 2.64575131, 2.        ],\n       [1.73205081, 2.44948974, 1.        ]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-sum-method","title":"02. 'sum' Method","text":"<p>The sum() method returns the sum of all elements in the array or along a specified axis.</p> <pre><code>np.sum(arr1)\n</code></pre> <pre><code>34\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-min-method","title":"03. 'min' Method","text":"<p>The min() method returns the minimum value in the array or along a specified axis.</p> <pre><code>np.min(arr1, axis=0)\n</code></pre> <pre><code>array([1, 3, 1])\n</code></pre> <pre><code># Return minimum value of an array\narr1.min()\n</code></pre> <pre><code>1\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#04-max-method","title":"04. 'max' Method","text":"<p>The max() method returns the maximum value in the array or along a specified axis.</p> <pre><code>np.max(arr1, axis=0)\n</code></pre> <pre><code>array([4, 7, 5])\n</code></pre> <pre><code># Return maximum value of an array\narr1.max()\n</code></pre> <pre><code>7\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-other-useful-methods","title":"03. Other Useful Methods","text":"<p>NumPy provides several methods to work with Boolean arrays, including where(), count_nonzero(), and nonzero(). </p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-where-method","title":"01. 'where' Method","text":"<p>The where() method returns an array of the same shape as the input array, where each element is replaced by either the value x if the corresponding element in the Boolean mask is True, or the value y if the corresponding element in the Boolean mask is False. You can also use the where() method to extract the indices of the elements that meet a certain condition.</p> <pre><code>arr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 4],\n       [3, 6, 1]])\n</code></pre> <pre><code>np.where(arr1&gt;5, 1, 0)\n</code></pre> <pre><code>array([[0, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-count_nonzero-method","title":"02. 'count_nonzero' Method","text":"<p>The count_nonzero() method returns the number of non-zero elements in the input array or along a specified axis.</p> <pre><code>np.count_nonzero(arr1)\n</code></pre> <pre><code>9\n</code></pre> <pre><code># Changing the element of arr1 to 0 at the index position off [1, 2]\narr1[1, 2] = 0\narr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 0],\n       [3, 6, 1]])\n</code></pre> <pre><code>np.count_nonzero(arr1)\n</code></pre> <pre><code>8\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-nonzero-method","title":"03. 'nonzero' Method","text":"<p>The nonzero() method returns a tuple of arrays, one for each dimension of the input array, containing the indices of the non-zero elements in that dimension. You can also use the nonzero() method to extract the non-zero elements of an array.</p> <pre><code>arr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 0],\n       [3, 6, 1]])\n</code></pre> <pre><code>arr1[1, 2]\n</code></pre> <pre><code>0\n</code></pre> <pre><code>np.nonzero(arr1)\n</code></pre> <pre><code>(array([0, 0, 0, 1, 1, 2, 2, 2], dtype=int64),\n array([0, 1, 2, 0, 1, 0, 1, 2], dtype=int64))\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/05_Advantages_of_Using_NumPy/","title":"Advantages Of Using Numpy","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/05_Advantages_of_Using_NumPy/#advantages-of-using-numpy","title":"Advantages of Using NumPy","text":"<p>NumPy arrays are more memory-efficient than Python lists because they are homogeneous, which means that all elements in the array are of the same data type. This allows NumPy to store the data more efficiently in memory.</p> <p>When you create a Python list, Python has to allocate memory for each element of the list, as well as for the list object itself. This means that lists can use more memory than necessary if the elements have different data types.</p> <p>In contrast, NumPy arrays are designed to be memory-efficient. When you create a NumPy array, NumPy allocates a block of memory for the entire array, based on the data type and size of the array. This means that NumPy arrays can use less memory than equivalent lists, especially for large datasets.</p> <pre><code>import sys\nimport numpy as np\n</code></pre> <pre><code># Creating a list\nmyList = [1, 2, 3, 4, 5]\nmyList\n</code></pre> <pre><code>[1, 2, 3, 4, 5]\n</code></pre> <pre><code># Creating a NumPy array from the list\nmyArr = np.array(myList)\nmyArr\n</code></pre> <pre><code>array([1, 2, 3, 4, 5])\n</code></pre> <pre><code># Print the size of the python list\n# Memory in Bytes\nsys.getsizeof(myList) * len(myList)\n</code></pre> <pre><code>600\n</code></pre> <pre><code># Print the size of the NumPy array\n# Memory in Bytes\nmyArr.itemsize * myArr.size\n</code></pre> <pre><code>20\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/05_Advantages_of_Using_NumPy/#convert-numpy-array-to-list","title":"Convert NumPy Array to List","text":"<pre><code>arr_to_lst = myArr.tolist()\narr_to_lst\n</code></pre> <pre><code>[1, 2, 3, 4, 5]\n</code></pre> <pre><code>sys.getsizeof(arr_to_lst)\n</code></pre> <pre><code>96\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/Random_Practice/01_Practice/","title":"Practice","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p> <pre><code>import numpy as np\n</code></pre> <pre><code>myArr1D = np.array([1, 2, 3, 4])\nmyArr1D\n</code></pre> <pre><code>myArr2D = np.array([[1, 2, 3], [4, 5, 6]])\nmyArr2D\n</code></pre> <pre><code>myArr1D.ndim\n</code></pre> <pre><code>myArr2D.ndim\n</code></pre> <pre><code>myArr1D.itemsize\n</code></pre> <pre><code>myArr1D.dtype\n</code></pre> <pre><code>myArr2D = np.array([[1, 2, 3], [4, 5, 6]], dtype=\"float64\")\nmyArr2D\n</code></pre> <pre><code>myArr2D.itemsize\n</code></pre> <pre><code>myArr2D.size\n</code></pre> <pre><code>myArr2D.shape\n</code></pre> <pre><code>complex_array = np.array([[1, 2], [3, 4], [5, 6]], dtype=complex)\ncomplex_array\n</code></pre> <pre><code>np.zeros((3, 3))\n</code></pre> <pre><code>np.ones((3, 3))\n</code></pre> <pre><code>np.arange(1, 20, 2)\n</code></pre> <pre><code>np.linspace(1, 10, 5)\n</code></pre> <pre><code>np.reshape(myArr2D, (3, 2))\n</code></pre> <pre><code>myArr2D.ravel()\n</code></pre> <pre><code>myArr2D.min()\n</code></pre> <pre><code>myArr2D.max()\n</code></pre> <pre><code>myArr2D.sum()\n</code></pre> <pre><code>myArr2D\n</code></pre> <pre><code>myArr2D.sum(axis=0)\n</code></pre> <pre><code>myArr2D.sum(axis=1)\n</code></pre> <pre><code>np.sqrt(myArr1D)\n</code></pre> <pre><code>np.std(myArr2D)\n</code></pre> <pre><code>a = np.array([[1, 2], [3, 4]])\na\n</code></pre> <pre><code>b = np.array([[5, 6], [7, 8]])\nb\n</code></pre> <pre><code>a + b\n</code></pre> <pre><code>a - b\n</code></pre> <pre><code>a * b\n</code></pre> <pre><code>a / b\n</code></pre> <pre><code>a.dot(b)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/","title":"Introduction To Pandas","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#introduction-to-pandas","title":"Introduction to Pandas","text":"<p>Pandas is a Python library that provides powerful data manipulation capabilities. It is built on top of NumPy and provides easy-to-use data structures and data analysis tools for data processing and analysis.</p> <p>In this module, we will cover the basics of using Pandas for data analysis. We will start with an introduction to the Pandas library and then move on to topics such as data structures, data cleaning, data visualization, and statistical analysis.</p> <p>Prerequisites: Before starting with Pandas, you should have a basic understanding of Python programming and NumPy. If you are new to Python, we recommend taking an introductory Python course before starting with this course.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install Pandas, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install pandas\n</code></pre> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#02-import-a-csv-data","title":"02. Import a CSV Data","text":"<pre><code># Creating a dataframe\npath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\ndf = pd.read_csv(path)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#03-basic-operations","title":"03. Basic Operations","text":"<pre><code># Print the first five row of csv\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code># Print the shape of the csv\ndf.shape\n</code></pre> <pre><code>(1206, 9)\n</code></pre> <pre><code># Fill th NaN values with 0\ndf.fillna(0, inplace=True)\n</code></pre> <pre><code># Print the maximum earthquake magnitude\ndf[\"Mag\"].max()\n</code></pre> <pre><code>9.1\n</code></pre> <pre><code># Print the Location Name of the earthquakes where total deaths were greater than 50000\ndf[\"Location Name\"][df[\"Total Deaths\"] &gt; 50000]\n</code></pre> <pre><code>272         INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n320    PAKISTAN:  MUZAFFARABAD, URI, ANANTNAG, BARAMULA\n490                            CHINA:  SICHUAN PROVINCE\n607                              HAITI:  PORT-AU-PRINCE\nName: Location Name, dtype: object\n</code></pre> <pre><code># Print the Location Name where the magnitude of the earthquake crossed 8.5 \ndf[\"Location Name\"][df[\"Mag\"] &gt;= 8.5]\n</code></pre> <pre><code>272    INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n294                      INDONESIA:  SUMATERA:  SW\n614          CHILE:  MAULE, CONCEPCION, TALCAHUANO\n674                                 JAPAN:  HONSHU\n736         INDONESIA:  N SUMATRA:  OFF WEST COAST\nName: Location Name, dtype: object\n</code></pre> <pre><code># Print the average focal depth (km) of the earthquakes\ndf[\"Focal Depth (km)\"].mean()\n</code></pre> <pre><code>30.892205638474294\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/","title":"Pandas Dataframe","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#pandas-dataframe","title":"Pandas Dataframe","text":"<p>Pandas DataFrame is a two-dimensional labeled data structure, where the columns can have different data types, such as integers, floats, and strings. It is one of the most popular data structures used in data analysis and machine learning tasks.</p> <p>In this course, we will cover the basics of using Pandas DataFrame for data analysis. We will start with an introduction to Pandas DataFrame and then move on to topics such as data manipulation, data cleaning, and data visualization.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-creating-a-pandas-dataframe","title":"01. Creating a Pandas DataFrame","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-creating-dataframe-from-dictionary","title":"01. Creating DataFrame from Dictionary","text":"<pre><code>weather_dict = {\n    \"day\": [\"1/1/2020\", \"1/2/2020\", \"1/3/2020\", \"1/4/2020\", \"1/5/2020\", \"1/6/2020\"],\n    \"temperature\": [32, 35, 28, 24, 32, 31],\n    \"windspeed\": [6, 7, 2, 7, 4, 2],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\", \"Snow\", \"Rain\", \"Sunny\"]\n}\n</code></pre> <pre><code># Creating dataframe from dictionary\ndf1 = pd.DataFrame(weather_dict)\ndf1\n</code></pre> day temperature windspeed event 0 1/1/2020 32 6 Rain 1 1/2/2020 35 7 Sunny 2 1/3/2020 28 2 Snow 3 1/4/2020 24 7 Snow 4 1/5/2020 32 4 Rain 5 1/6/2020 31 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-creating-dataframe-from-csv","title":"02. Creating DataFrame from CSV","text":"<pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.csv\"\ndf2 = pd.read_csv(path)\ndf2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the type of the df2 variable\ntype(df2)\n</code></pre> <pre><code>pandas.core.frame.DataFrame\n</code></pre> <pre><code># Printing the shape of the dataframe\nrows, colums = df2.shape\nrows\n</code></pre> <pre><code>6\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-head-method","title":"02. head() Method","text":"<p>In Pandas, the head() method is used to view the first few rows of a DataFrame. By default, it displays the first 5 rows of the DataFrame. This method is useful to get a quick overview of the data in the DataFrame.</p> <pre><code># Print first five rows of the dataframe\ndf2.head()\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain <pre><code># Print the first two rows of the dataframe\ndf2.head(2)\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#03-tail-method","title":"03. tail() Method","text":"<p>In Pandas, the tail() method is used to view the last few rows of a DataFrame. By default, it displays the last 5 rows of the DataFrame. This method is useful to get a quick overview of the data in the DataFrame.</p> <pre><code># Print last five rows of dataframe\ndf2.tail()\n</code></pre> day temperature windspeed event 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the last two rows of the dataframe\ndf2.tail(2)\n</code></pre> day temperature windspeed event 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#04-indexing-and-slicing-in-dataframe","title":"04. Indexing and Slicing in DataFrame","text":"<pre><code># Print row number 2 to 4\ndf2[2:5]\n</code></pre> day temperature windspeed event 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain <pre><code># Print the names of the columns\ndf2.columns\n</code></pre> <pre><code>Index(['day', 'temperature', 'windspeed', 'event'], dtype='object')\n</code></pre> <pre><code># Print individual column of the dataframe\ndf2.day\n</code></pre> <pre><code>0    01-01-2020\n1    01-02-2020\n2    01-03-2020\n3    01-04-2020\n4    01-05-2020\n5    01-06-2020\nName: day, dtype: object\n</code></pre> <pre><code># Print the type of a column\ntype(df2[\"event\"])\n</code></pre> <pre><code>pandas.core.series.Series\n</code></pre> <pre><code># Print specific columns from dataframe\ndf2[[\"day\", \"temperature\", \"event\"]]\n</code></pre> day temperature event 0 01-01-2020 32 Rain 1 01-02-2020 35 Sunny 2 01-03-2020 28 Snow 3 01-04-2020 24 Snow 4 01-05-2020 32 Rain 5 01-06-2020 32 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#05-operations-with-dataframe","title":"05. Operations with DataFrame","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-max-method","title":"01. max() Method","text":"<pre><code># Print the maximum temperature\ndf2[\"temperature\"].max()\n</code></pre> <pre><code>35\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-min-method","title":"02. min() Method","text":"<pre><code># Print the minimum temperature\ndf2[\"temperature\"].min()\n</code></pre> <pre><code>24\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#03-mean-method","title":"03. mean() Method","text":"<pre><code># Print the mean (average) of the temperature\ndf2[\"temperature\"].mean()\n</code></pre> <pre><code>30.5\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#04-std-method","title":"04. std() Method","text":"<pre><code># Print the standard deviation of the temperature\ndf2[\"temperature\"].std()\n</code></pre> <pre><code>3.8858718455450894\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#05-describe-method","title":"05. describe() Method","text":"<pre><code># Print the statistics of the whole dataframe\ndf2.describe()\n</code></pre> temperature windspeed count 6.000000 6.000000 mean 30.500000 4.666667 std 3.885872 2.338090 min 24.000000 2.000000 25% 29.000000 2.500000 50% 32.000000 5.000000 75% 32.000000 6.750000 max 35.000000 7.000000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#06-conditional-selection-in-dataframe","title":"06. Conditional Selection in DataFrame","text":"<pre><code># Print all the rows where temperature greater than or equal to 30\ndf2[df2.temperature &gt;= 30]\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the row where temperature is maximum\ndf2[df2.temperature == df2[\"temperature\"].max()]\n</code></pre> day temperature windspeed event 1 01-02-2020 35 7 Sunny <pre><code># Print only the day and temperature column where the temperature is maximum\ndf2[[\"day\", \"temperature\"]][df2.temperature == df2[\"temperature\"].max()]\n</code></pre> day temperature 1 01-02-2020 35"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#07-set_index-method","title":"07. set_index() Method","text":"<p>In Pandas, the set_index() method is used to set one or more columns as the index of a DataFrame. This method returns a new DataFrame with the specified column(s) set as the index.</p> <pre><code>df2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code>df2.index\n</code></pre> <pre><code>RangeIndex(start=0, stop=6, step=1)\n</code></pre> <pre><code># Set the 'day' column as the index of the dataframe\ndf2.set_index(\"day\", inplace=True)\n</code></pre> <pre><code>df2\n</code></pre> temperature windspeed event day 01-01-2020 32 6 Rain 01-02-2020 35 7 Sunny 01-03-2020 28 2 Snow 01-04-2020 24 7 Snow 01-05-2020 32 4 Rain 01-06-2020 32 2 Sunny <pre><code># The 'loc' function is used to access a group of rows and columns by label(s) or a boolean array.\ndf2.loc[\"01-01-2020\"]\n</code></pre> <pre><code>temperature      32\nwindspeed         6\nevent          Rain\nName: 01-01-2020, dtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#08-reset_index-method","title":"08. reset_index() Method","text":"<p>In Pandas, the reset_index() method is used to reset the index of a DataFrame to a default numbered index. It is often used to reset the index after setting it to a column or multiple columns using the set_index() method.</p> <pre><code>df2.reset_index(inplace=True)\n</code></pre> <pre><code>df2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/","title":"Different Ways Of Creating Dataframe","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#different-ways-of-creating-dataframe","title":"Different Ways of Creating DataFrame","text":"<p>Pandas is a powerful library for data manipulation and analysis in Python. It provides the DataFrame object, which is a two-dimensional table-like data structure with rows and columns. In this tutorial, we will cover different ways of creating a DataFrame in Pandas.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#01-creating-dataframe-using-read_csv-method","title":"01. Creating DataFrame Using read_csv() Method","text":"<p>Pandas provides a read_csv() method which allows us to create a DataFrame by reading a CSV file. This is one of the most common ways of creating a DataFrame in Pandas, especially when working with large datasets.</p> <pre><code>csv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.csv\"\ndf1 = pd.read_csv(csv_path)\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#02-creating-dataframe-using-read_excel-method","title":"02. Creating DataFrame Using read_excel() Method","text":"<p>In addition to read_csv(), Pandas also provides a read_excel() method which allows us to create a DataFrame by reading an Excel file.</p> <pre><code>xls_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.xlsx\"\ndf2 = pd.read_excel(xls_path)\ndf2\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#03-creating-dataframe-from-dictionary","title":"03. Creating DataFrame from Dictionary","text":"<p>Another way to create a DataFrame in Pandas is by using a Python dictionary. The keys of the dictionary represent the column names of the DataFrame, while the values represent the data for each column. The values can be of any data type that can be represented in a Pandas DataFrame (such as lists, NumPy arrays, or Pandas Series).</p> <pre><code>weather_dict = {\n    \"day\": [\"2020-01-01\", \"2020-02-01\", \"2020-03-01\", \"2020-04-01\", \"2020-05-01\", \"2020-06-01\"],\n    \"temperature\": [32, 35, 28, 24, 32, 32],\n    \"windspeed\": [6, 7, 2, 7, 4, 2],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\", \"Snow\", \"Rain\", \"Sunny\"]\n}\n</code></pre> <pre><code>df3 = pd.DataFrame(weather_dict)\ndf3\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#04-creating-dataframe-from-a-list-of-tuples","title":"04. Creating DataFrame from a List of Tuples","text":"<p>Another way to create a DataFrame in Pandas is by using a list of tuples. In this case, you need to provide the column names. The column names are optional, but if you don't specify them, Pandas will assign default column names (0, 1, 2, etc.) to the DataFrame.</p> <pre><code>weather_data = [\n    (\"2020-01-01\", 32, 6, \"Rain\"),\n    (\"2020-02-01\", 35, 7, \"Sunny\"),\n    (\"2020-03-01\", 28, 2, \"Snow\"),\n    (\"2020-04-01\", 24, 7, \"Snow\"),\n    (\"2020-05-01\", 32, 4, \"Rain\"),\n    (\"2020-06-01\", 32, 2, \"Sunny\")\n]\n</code></pre> <pre><code>df4 = pd.DataFrame(weather_data, columns=[\"day\", \"temperature\", \"windspeed\", \"event\"])\ndf4\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#05-creating-dataframe-using-list-of-dictionaries","title":"05. Creating DataFrame Using List of Dictionaries","text":"<p>Another way to create a DataFrame in Pandas is by using a list of dictionaries. In this case, the keys of each dictionary are used as column names in the resulting DataFrame. The order of the keys in the first dictionary determines the order of the columns in the DataFrame.</p> <pre><code>weather_dict_list = [\n    {\"day\": \"2020-01-01\", \"temperature\": 32, \"windspeed\": 6, \"event\": \"Rain\"},\n    {\"day\": \"2020-02-01\", \"temperature\": 35, \"windspeed\": 7, \"event\": \"Sunny\"},\n    {\"day\": \"2020-03-01\", \"temperature\": 28, \"windspeed\": 2, \"event\": \"Snow\"},\n    {\"day\": \"2020-04-01\", \"temperature\": 24, \"windspeed\": 7, \"event\": \"Snow\"},\n    {\"day\": \"2020-05-01\", \"temperature\": 32, \"windspeed\": 4, \"event\": \"Rain\"},\n    {\"day\": \"2020-06-01\", \"temperature\": 32, \"windspeed\": 2, \"event\": \"Sunny\"},\n]\n</code></pre> <pre><code>df5 = pd.DataFrame(weather_dict_list)\ndf5\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/","title":"Read Write Excel Csv","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#read-and-write-excel-csv","title":"Read and Write Excel CSV","text":"<p>Pandas is a powerful tool for reading and writing data in various formats including Excel and CSV. In this module, we will explore how to read and write Excel and CSV files using Pandas.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-read-csv-file-using-read_csv-method","title":"01. Read CSV File Using read_csv() Method","text":"<p>To read a CSV file using Pandas, you can use the read_csv() function. This function takes the filename as an argument and returns a Pandas DataFrame object.</p> <pre><code>filepath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\stock_data.csv\"\ndf = pd.read_csv(filepath)\ndf\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-skip-rows-in-dataframe-using-skiprows-method","title":"02. Skip Rows in DataFrame Using skiprows() Method","text":"<p>In Pandas, you can use the skiprows() method to skip rows in a DataFrame while reading a CSV or Excel file. This can be useful when you have header rows, comment lines, or other non-data rows that you want to exclude from the DataFrame.</p> <pre><code>df1 = pd.read_csv(filepath, skiprows=1)\ndf1\n</code></pre> GOOGL 27.82 87 845 larry page 0 WMT 4.61 484 65 n.a. 1 MSFT -1 85 64 bill gates 2 RIL not available 50 1023 mukesh ambani 3 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#03-import-data-from-csv-with-null-header","title":"03. Import Data from CSV with \"null header\"","text":"<p>Sometimes you may encounter CSV files that do not have a header row or have a header row with blank or null values. In Pandas, you can still import such CSV files and specify column names later using the header parameter in the read_csv() function.</p> <pre><code>df2 = pd.read_csv(filepath, skiprows=1, header=None)\ndf2\n</code></pre> 0 1 2 3 4 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata <pre><code>df3 = pd.read_csv(filepath, skiprows=1, header=None, names=[\"tickets\", \"eps\", \"revenue\", \"price\", \"people\"])\ndf3\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#04-reading-limited-data-from-csv","title":"04. Reading Limited Data from CSV","text":"<p>In Pandas, you can read a limited number of rows from a CSV file using the nrows parameter in the read_csv() function.</p> <pre><code>df4 = pd.read_csv(filepath, nrows=4)\ndf4\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#05-clean-up-messy-data-from-csv-using-na_values-method","title":"05. Clean Up Messy Data from CSV using na_values() Method","text":"<p>When working with CSV files, you may encounter missing or null values that can make your data messy and difficult to work with. In Pandas, you can use the na_values() method to clean up messy data by specifying which values should be treated as null values.</p> <pre><code>df5 = pd.read_csv(filepath, na_values=[\"not available\", \"n.a.\"])\ndf5\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845.0 larry page 1 WMT 4.61 484 65.0 NaN 2 MSFT -1.00 85 64.0 bill gates 3 RIL NaN 50 1023.0 mukesh ambani 4 TATA 5.60 -1 NaN ratan tata <p>In addition to using a list to specify which values should be treated as null values when reading a CSV file in Pandas, you can also use a dictionary to map specific null values to specific columns. This can be helpful when you need to treat different columns differently based on the null values they contain.</p> <pre><code>df6 = pd.read_csv(filepath, na_values={\n    \"revenue\": [-1, \"n.a.\", \"not applicable\"],\n    \"eps\": [\"n.a.\", 'not available'],\n    \"people\": [\"n.a.\"],\n    \"price\": [\"n.a.\", \"not available\"]\n})\ndf6\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87.0 845.0 larry page 1 WMT 4.61 484.0 65.0 NaN 2 MSFT -1.00 85.0 64.0 bill gates 3 RIL NaN 50.0 1023.0 mukesh ambani 4 TATA 5.60 NaN NaN ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#06-write-dataframe-into-csv-using-to_csv-method","title":"06. Write DataFrame into CSV Using to_csv() Method","text":"<p>Once you have cleaned up and processed your data in a Pandas DataFrame, you may want to save it to a CSV file for further analysis or sharing with others. You can easily do this using the to_csv() method in Pandas.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-index-parameter","title":"01. index Parameter","text":"<p>In the above code, we use the to_csv() method to write the DataFrame to a CSV file called 'new_stock_data.csv'. We can pass index=False to exclude the DataFrame index from being written to the file.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-columns-parameter","title":"02. columns Parameter","text":"<p>The to_csv() method in Pandas allows you to customize the output of your DataFrame to a CSV file. One of the options you can specify is the columns parameter, which allows you to write only specific columns from your DataFrame to the CSV file.</p> <pre><code>df6.columns\n</code></pre> <pre><code>Index(['tickets', 'eps', 'revenue', 'price', 'people'], dtype='object')\n</code></pre> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False, columns=[\"tickets\", \"eps\"])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#03-header-parameter","title":"03. header Parameter","text":"<p>The header parameter allows you to include or exclude the column names as the first row in the CSV file.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False, header=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#07-read-excel-file-using-read_excel-method","title":"07. Read Excel File Using read_excel() Method","text":"<p>To read an excel file using Pandas, you can use the read_excel() function. This function takes the filename as an argument and returns a Pandas DataFrame object.</p> <pre><code>filepath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\stock_data.xlsx\"\ndf7 = pd.read_excel(filepath, sheet_name=\"stock_data\")\ndf7\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#08-converters-argument-in-read_excel","title":"08. Converters Argument in read_excel()","text":"<p>The read_excel() function in Pandas allows you to read data from an Excel file into a Pandas DataFrame. One of the arguments you can use to customize the import process is the converters parameter.</p> <p>The converters parameter is used to specify a dictionary of functions that should be applied to specific columns during the import process. The keys of the dictionary represent the column names or indices, and the values are the functions to apply to the corresponding columns.</p> <p>In this example, we define a custom function 'convert_people_cell()' that converts any 'n.a.' input to a string which is 'bill gates'. We then read an Excel file called data.xlsx using the read_excel() function and pass a dictionary to the converters parameter. The dictionary has one key-value pair, where the key is the name of the column to apply the function to (people), and the value is the function to apply (convert_people_cell).</p> <pre><code>def convert_people_cell(cell):\n    if cell == \"n.a.\":\n        return \"jeff bezos\"\n    else:\n        return cell\n</code></pre> <pre><code>df8 = pd.read_excel(filepath, converters={\n    \"people\": convert_people_cell\n})\ndf8\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 jeff bezos 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata <pre><code>def convert_eps_cell(cell):\n    if cell == \"not available\":\n        return None\n    else:\n        return cell\n</code></pre> <pre><code>df9 = pd.read_excel(filepath, converters={\n    \"eps\": convert_eps_cell,\n    \"people\": convert_people_cell\n})\ndf9\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 jeff bezos 2 MSFT -1.00 85 64 bill gates 3 RIL NaN 50 1023 mukesh ambani 4 TATA 5.60 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#09-write-dataframe-into-excel-file-using-to_excel-method","title":"09. Write DataFrame into 'excel' File using to_excel() Method","text":"<p>To write a Pandas DataFrame to an Excel file, you can use the to_excel() method.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-index-parameter_1","title":"01. index Parameter","text":"<p>The to_excel() method in Pandas allows you to write a DataFrame to an Excel file with various options to customize the output. One of these options is the index parameter, which controls whether or not to include the DataFrame's index in the Excel file.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-startrow-and-startcol-parameter","title":"02. startrow and startcol Parameter","text":"<p>The startrow and startcol parameters in the to_excel() method of Pandas allow you to specify the starting row and column for writing data to an Excel file.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\", index=False, startrow=1, startcol=1)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#10-use-excelwritter-class","title":"10. Use ExcelWritter() Class","text":"<p>The ExcelWriter class in Pandas is a powerful tool for writing data frames to one or more sheets in an Excel file. This class provides a lot of flexibility and options for formatting the output, such as specifying the sheet name, adding headers and footers, setting column widths and row heights, and so on.</p> <pre><code># Creating two separate dataframe\ndf_stocs = pd.DataFrame({\n    \"tickets\": [\"GOOGLE\", \"WMT\", \"MSFT\"],\n    \"price\": [845, 65, 64],\n    \"Pe\": [30.37, 14.26, 30.97],\n    \"eps\": [27.82, 4.61, 2.12]\n})\n\ndf_weather = pd.DataFrame({\n    \"day\": [\"1/1/2020\", \"1/2/2020\", \"1/3/2020\"],\n    \"temperature\": [32, 35, 28],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\"]\n})\n</code></pre> <pre><code>with pd.ExcelWriter(\"stocks_and_weather.xlsx\") as writer:\n    df_stocs.to_excel(writer, sheet_name=\"Stock\", index=False)\n    df_weather.to_excel(writer, sheet_name=\"Weather\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/","title":"Handle Missing Data","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#handle-missing-data","title":"Handle Missing Data","text":"<p>Handling missing data is an important part of data analysis, and Pandas provides a number of methods for dealing with missing values. In this notebook, we will cover some common techniques for handling missing data using Pandas.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Creating a dataframe from CSV\ndf1 = pd.read_csv(\"weather_data.csv\")\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2020 32.0 6.0 Rain 1 01-04-2020 NaN 7.0 Sunny 2 01-05-2020 NaN NaN NaN 3 01-06-2020 24.0 NaN Snow 4 01-07-2020 NaN 4.0 Rain 5 01-08-2020 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-convert-string-column-into-date-type","title":"01. Convert String Column into Date Type","text":"<p>In Pandas, it is common to work with data that includes dates. However, sometimes the dates are stored as strings, which makes it difficult to perform any operations on them. In this case, it is necessary to convert the string column into a date type.</p> <p>Pandas provides the to_datetime() method for converting a string column into a date type. This method is very powerful and flexible, allowing you to convert many different string formats into dates.</p> <pre><code># Print the datatype of values in 'day' column\ntype(df1.day[0])\n</code></pre> <pre><code>str\n</code></pre> <pre><code>df2 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf2\n</code></pre> day temperature windspeed event 0 2020-01-01 32.0 6.0 Rain 1 2020-01-04 NaN 7.0 Sunny 2 2020-01-05 NaN NaN NaN 3 2020-01-06 24.0 NaN Snow 4 2020-01-07 NaN 4.0 Rain 5 2020-01-08 32.0 NaN Sunny <pre><code># Print the datatype of values in 'day' column\ntype(df2.day[0])\n</code></pre> <pre><code>pandas._libs.tslibs.timestamps.Timestamp\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-use-date-as-index-of-dataframe","title":"02. Use Date as Index of DataFrame","text":"<p>To use a date column as the index of a DataFrame, we can use the set_index() method of the DataFrame object, and pass the name of the date column as an argument. The set_index() method will return a new DataFrame with the specified column as the index.</p> <p>The inplace=True argument is used to modify the DataFrame in place, rather than creating a new one.</p> <pre><code>df3 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf3.set_index(\"day\", inplace=True)\ndf3\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#03-use-fillna-method","title":"03. Use fillna() Method","text":"<p>In Pandas, fillna() is a method used to fill missing or null values in a DataFrame with a specified value or technique. This method can be used to clean up the data before further processing.</p> <pre><code>df4 = df3.fillna(0)\ndf4\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 0.0 7.0 Sunny 2020-01-05 0.0 0.0 0 2020-01-06 24.0 0.0 Snow 2020-01-07 0.0 4.0 Rain 2020-01-08 32.0 0.0 Sunny <pre><code>df5 = df3.fillna({\n    \"temperature\": 0,\n    \"windspeed\": 0,\n    \"event\": \"no event\"\n})\ndf5\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 0.0 7.0 Sunny 2020-01-05 0.0 0.0 no event 2020-01-06 24.0 0.0 Snow 2020-01-07 0.0 4.0 Rain 2020-01-08 32.0 0.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#04-use-fillnamethodffillbfill-method","title":"04. Use fillna(method=\"ffill\"/\"bfill\") Method","text":"<p>The fillna() method in pandas is used to fill the missing or NaN values in a DataFrame with a specified value or method. The method parameter can be used to fill the missing values using forward or backward filling method. When method='ffill', it fills the missing values with the previous non-missing value along each column. When method='bfill', it fills the missing values with the next non-missing value along each column.</p> <pre><code># Using fillna(method=\"ffill\") Method\ndf6 = df3.fillna(method=\"ffill\")\ndf6\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 32.0 7.0 Sunny 2020-01-05 32.0 7.0 Sunny 2020-01-06 24.0 7.0 Snow 2020-01-07 24.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny <pre><code># Using fillna(method=\"bfill\") Method\ndf7 = df3.fillna(method=\"bfill\")\ndf7\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 24.0 7.0 Sunny 2020-01-05 24.0 4.0 Snow 2020-01-06 24.0 4.0 Snow 2020-01-07 32.0 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-axis-parameter-in-fillna-method","title":"01. 'axis' Parameter in fillna() Method","text":"<p>The fillna() method in pandas is used to fill the missing or NaN values in a DataFrame with a specified value or method. The axis parameter is used to specify the direction in which the fill operation should be applied.</p> <p>The axis parameter can be set to 0 or 'index' to apply the fill operation along the rows, and to 1 or 'columns' to apply the fill operation along the columns.</p> <pre><code>df8 = df3.fillna(method=\"ffill\", axis=\"columns\")\ndf8\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 24.0 Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 32.0 Sunny <pre><code>df9 = df3.fillna(method=\"bfill\", axis=1)\ndf9\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 7.0 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 Snow Snow 2020-01-07 4.0 4.0 Rain 2020-01-08 32.0 Sunny Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-limit-parameter-in-fillna-method","title":"02. 'limit' Parameter in fillna() Method","text":"<p>The limit parameter in the fillna() method is used to specify the maximum number of consecutive NaN values to be filled. This parameter takes an integer value.</p> <pre><code>df10 = df3.fillna(method=\"ffill\", limit=1)\ndf10\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 32.0 7.0 Sunny 2020-01-05 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 24.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#05-use-interpolate-method-to-do-interpolation","title":"05. Use interpolate() Method to do Interpolation","text":"<p>The interpolate() method in Pandas is used to fill the missing values (NaN) in a DataFrame or a Series by using various interpolation techniques. The interpolate() method supports several interpolation techniques, including linear, quadratic, cubic, and more. You can specify the interpolation technique by passing a value for the method parameter. For example, to use quadratic interpolation, you can set method='quadratic'</p> <pre><code>df3\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code>df11 = df3.interpolate()\ndf11\n</code></pre> temperature windspeed event day 2020-01-01 32.000000 6.0 Rain 2020-01-04 29.333333 7.0 Sunny 2020-01-05 26.666667 6.0 NaN 2020-01-06 24.000000 5.0 Snow 2020-01-07 28.000000 4.0 Rain 2020-01-08 32.000000 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-interpolate-method-time","title":"01. interpolate() Method 'time'","text":"<p>The interpolate() method in Pandas can perform different types of interpolation, including linear, polynomial, and time-based interpolation. Time-based interpolation is used when dealing with time series data, where missing values are filled based on the time difference between observations.</p> <p>To perform time-based interpolation, the interpolate() method needs to be called with the method parameter set to 'time'. This method uses the time stamps of the available data points to estimate the values of missing data points.</p> <pre><code>df12 = df3.interpolate(method=\"time\")\ndf12\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 27.2 7.0 Sunny 2020-01-05 25.6 6.0 NaN 2020-01-06 24.0 5.0 Snow 2020-01-07 28.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#06-use-dropna-method-to-drop-all-the-rows-with-nan-values","title":"06. Use dropna() Method to Drop all the rows With 'NaN' Values","text":"<p>In Pandas, the dropna() method is used to remove the rows or columns containing NaN or missing data. This method is useful when we have missing values in our dataset and want to remove them.</p> <pre><code>df13 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf13.set_index(\"day\", inplace=True)\ndf13\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Dropping all the rows With 'NaN' Values\ndf14 = df13.dropna()\ndf14\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-how-parameter-in-dropna-method","title":"01. 'how' Parameter in dropna() Method","text":"<p>The how parameter in the dropna() method of pandas is used to determine the condition for dropping the rows or columns containing the missing values (NaN values). * how='any': This is the default option. If any missing value is present in a row or column, the entire row or column will be dropped. * how='all': Only the rows or columns containing all missing values will be dropped. * how='thresh': Only the rows or columns containing a minimum number of non-missing values, specified by the thresh parameter, will be kept.</p> <pre><code># Using dropna(how=\"all\")\ndf15 = df13.dropna(how=\"all\")\ndf15\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-thresh-parameter-in-dropna-method","title":"02. 'thresh' Parameter in dropna() Method","text":"<p>The thresh parameter in the dropna() method of pandas is used in combination with the how='thresh' option to specify the minimum number of non-missing values required to keep a row or column.</p> <p>For example, if thresh=2, only the rows or columns containing at least two non-missing values will be kept, and all others will be dropped.</p> <pre><code>df13\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code>df16 = df13.dropna(thresh=2)\ndf16\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#07-inserting-missing-dates","title":"07. Inserting Missing Dates","text":"<p>In Pandas, missing dates can be inserted into a DataFrame using the reindex() method. This method returns a new DataFrame with the specified index.</p> <p>To insert missing dates, we first need to create a range of dates that includes all the missing dates we want to add. We can do this using the pd.date_range() function. This function takes a start and end date, and returns a range of dates in between.</p> <pre><code>df17 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf17.set_index(\"day\", inplace=True)\ndf17\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Inserting missing dates\ndate_range = pd.date_range(\"01-01-2020\", \"01-08-2020\")\ndate_id = pd.DatetimeIndex(date_range)\ndf18 = df17.reindex(date_id)\ndf18\n</code></pre> temperature windspeed event 2020-01-01 32.0 6.0 Rain 2020-01-02 NaN NaN NaN 2020-01-03 NaN NaN NaN 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Filling missing values\n# Filling numeric values using interpolate method\ndf18.interpolate(inplace=True)\n# Filling non-numeric values using fillna method\ndf18.fillna(method=\"ffill\", inplace=True)\ndf18\n</code></pre> temperature windspeed event 2020-01-01 32.0 6.000000 Rain 2020-01-02 30.4 6.333333 Rain 2020-01-03 28.8 6.666667 Rain 2020-01-04 27.2 7.000000 Sunny 2020-01-05 25.6 6.000000 Sunny 2020-01-06 24.0 5.000000 Snow 2020-01-07 28.0 4.000000 Rain 2020-01-08 32.0 4.000000 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/","title":"Handle Missing Data","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#handle-missing-data","title":"Handle Missing Data","text":"<p>In data analysis, missing data can be a common occurrence, and handling it properly is important to ensure accurate analysis. Pandas is a powerful library in Python that provides several methods for handling missing data. One of the methods to handle missing data is by using the replace() method.</p> <p>The replace() method in Pandas can be used to replace a specific value with another value, including replacing missing or null values.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>df1 = pd.read_csv(\"weather_data2.csv\")\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#01-use-replace-method-to-replace-values","title":"01. Use replace() Method to Replace Values","text":"<p>Pandas provides the replace() method to replace values in a DataFrame. The replace() method can be used to replace a single value or multiple values at once.</p> <pre><code># Replacing single value\ndf2 = df1.replace(-99999, \"NaN\")\ndf2\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0 <pre><code># Replacing multiple values\ndf3 = df1.replace([-99999, -88888], \"NaN\")\ndf3\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0 <pre><code># Replacing values based on specific columns\ndf4 = df1.replace({\n    \"temperature\": -99999,\n    \"windspeed\": [-99999, -88888],\n    \"event\": \"0\"\n}, \"NaN\")\ndf4\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 NaN <pre><code># Mapping specific values\ndf5 = df1.replace({\n    -99999: \"NaN\",\n    \"No Event\": \"Sunny\"\n})\ndf5\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 Sunny 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#02-regex-regular-expression-parameter","title":"02. regex() (Regular Expression) Parameter","text":"<p>The regex parameter is a boolean parameter used in Pandas to determine whether to interpret the to_replace parameter in the replace() method as a regular expression. When regex is set to True, to_replace is treated as a regular expression pattern, and any matches are replaced with the value parameter.</p> <p>Regular expressions are a powerful tool for pattern matching in text data. They allow us to search for patterns in text, and can be used to extract specific parts of a string or replace certain parts of a string with another value.</p> <p>In the context of Pandas, using regular expressions can be very useful for cleaning and transforming data. For example, we could use regular expressions to extract email addresses from a column of text data or to replace certain characters with others.</p> <pre><code># Using regex\ndf6 = df1.replace(\"[A-Za-z]\", \"\", regex=True)\ndf6\n</code></pre> day temperature windspeed event 0 01-01-2017 32 6 1 01-02-2017 -99999 7 2 01-03-2017 28 -99999 3 01-04-2017 -99999 7 4 01-05-2017 32 -88888 5 01-06-2017 31 2 6 01-06-2017 34 5 0 <pre><code># Using regex based on specific columns\ndf7 = df1.replace({\n    \"temperature\": \"[A-Za-z]\",\n    \"windspeed\": \"[A-Za-z]\"\n}, \"\", regex=True)\ndf7\n</code></pre> day temperature windspeed event 0 01-01-2017 32 6 Rain 1 01-02-2017 -99999 7 Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#03-replace-the-list-of-values-with-another-list-of-values","title":"03. Replace the List of Values with Another List of Values","text":"<p>In Pandas, we can use the replace() method to replace a list of values in a DataFrame column with another list of values. This can be useful when we want to replace multiple values with a single value or when we want to replace values with different values depending on their original value.</p> <pre><code># Creatin a new dataframe\ndf = pd.DataFrame({\n    \"score\": [\"exceptional\", \"average\", \"good\", \"poor\", \"average\", \"exceptional\"],\n    \"student\": [\"rob\", \"maya\", \"parthiv\", \"tom\", \"julian\", \"erica\"]\n})\ndf\n</code></pre> score student 0 exceptional rob 1 average maya 2 good parthiv 3 poor tom 4 average julian 5 exceptional erica <pre><code>new_df = df.replace([\"poor\", \"average\", \"good\", \"exceptional\"], [1, 2, 3, 4])\nnew_df\n</code></pre> score student 0 4 rob 1 2 maya 2 3 parthiv 3 1 tom 4 2 julian 5 4 erica"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/","title":"Group By Method","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#group-by-split-apply-combine","title":"Group By (Split, Apply, Combine)","text":"<p>One of the most powerful features of Pandas is its ability to group data and perform aggregations on the grouped data. The groupby() method is used to group data in Pandas based on one or more columns.</p> <p>Split-Apply-Combine is a common pattern in data analysis where the data is first split into groups based on one or more criteria, then a function is applied to each group individually, and finally the results are combined into a single data structure.</p> <p>Here's a breakdown of each step in the Split-Apply-Combine process:</p> <ul> <li> <p>Split: The data is split into smaller groups based on one or more criteria. For example, we might split data based on the values in a particular column or based on a time period.</p> </li> <li> <p>Apply: A function is applied to each group individually. This function could be an aggregation function like sum() or mean(), or it could be a transformation function that modifies the data within each group.</p> </li> <li> <p>Combine: The results from each group are combined back together into a single data structure. This could be a new DataFrame or Series, or it could be a summary statistic like a mean or a standard deviation.</p> </li> </ul> <pre><code>import pandas as pd\n</code></pre> <pre><code># Creating a dataframe\nfilepath = r\"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\04_Introduction_to_Pandas\\Datasets\\weather_by_cities.csv\"\ndf1 = pd.read_csv(filepath)\ndf1\n</code></pre> day city temperature windspeed event 0 01-01-2017 new york 32 6 Rain 1 01-02-2017 new york 36 7 Sunny 2 01-03-2017 new york 28 12 Snow 3 01-04-2017 new york 33 7 Sunny 4 01-01-2017 mumbai 90 5 Sunny 5 01-02-2017 mumbai 85 12 Fog 6 01-03-2017 mumbai 87 15 Fog 7 01-04-2017 mumbai 92 5 Rain 8 01-01-2017 paris 45 20 Sunny 9 01-02-2017 paris 50 13 Cloudy 10 01-03-2017 paris 54 8 Cloudy 11 01-04-2017 paris 42 10 Cloudy"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#01-use-groupby-method","title":"01. Use groupby() Method","text":"<p>The groupby() method is used to group data in Pandas. It takes one or more column names as arguments and returns a GroupBy object.</p> <pre><code>groups = df1.groupby(\"city\")\ngroups\n</code></pre> <pre><code>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000021627575EE0&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#02-groupby-representation-internally","title":"02. groupby() Representation Internally","text":"<pre><code>for city, city_df in groups:\n    print(city)\n    print(city_df)\n</code></pre> <pre><code>mumbai\n          day    city  temperature  windspeed  event\n4  01-01-2017  mumbai           90          5  Sunny\n5  01-02-2017  mumbai           85         12    Fog\n6  01-03-2017  mumbai           87         15    Fog\n7  01-04-2017  mumbai           92          5   Rain\nnew york\n          day      city  temperature  windspeed  event\n0  01-01-2017  new york           32          6   Rain\n1  01-02-2017  new york           36          7  Sunny\n2  01-03-2017  new york           28         12   Snow\n3  01-04-2017  new york           33          7  Sunny\nparis\n           day   city  temperature  windspeed   event\n8   01-01-2017  paris           45         20   Sunny\n9   01-02-2017  paris           50         13  Cloudy\n10  01-03-2017  paris           54          8  Cloudy\n11  01-04-2017  paris           42         10  Cloudy\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#03-aggregating-data-using-groupby","title":"03. Aggregating Data using groupby()","text":"<p>Once the data is grouped, we can perform various aggregation functions on the data. Some of the commonly used aggregation functions are:</p> <ul> <li>sum(): returns the sum of values in each group</li> <li>mean(): returns the mean of values in each group</li> <li>median(): returns the median of values in each group</li> <li>min(): returns the minimum value in each group</li> <li>max(): returns the maximum value in each group</li> <li>count(): returns the number of values in each group</li> </ul> <pre><code>groups.max()\n</code></pre> day temperature windspeed event city mumbai 01-04-2017 92 15 Sunny new york 01-04-2017 36 12 Sunny paris 01-04-2017 54 20 Sunny <pre><code>groups.mean()\n</code></pre> <pre><code>C:\\Users\\KRISH\\AppData\\Local\\Temp\\ipykernel_3440\\642604181.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  groups.mean()\n</code></pre> temperature windspeed city mumbai 88.50 9.25 new york 32.25 8.00 paris 47.75 12.75 <pre><code>groups.describe()\n</code></pre> temperature windspeed count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max city mumbai 4.0 88.50 3.109126 85.0 86.50 88.5 90.50 92.0 4.0 9.25 5.057997 5.0 5.00 8.5 12.75 15.0 new york 4.0 32.25 3.304038 28.0 31.00 32.5 33.75 36.0 4.0 8.00 2.708013 6.0 6.75 7.0 8.25 12.0 paris 4.0 47.75 5.315073 42.0 44.25 47.5 51.00 54.0 4.0 12.75 5.251984 8.0 9.50 11.5 14.75 20.0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#04-plotting-groupby-data","title":"04. Plotting groupby() Data","text":"<pre><code>%matplotlib inline\ngroups.plot()\n</code></pre> <pre><code>city\nmumbai      Axes(0.125,0.11;0.775x0.77)\nnew york    Axes(0.125,0.11;0.775x0.77)\nparis       Axes(0.125,0.11;0.775x0.77)\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/","title":"Introduction To Matplotlib","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#introduction-to-matplotlib","title":"Introduction to Matplotlib","text":"<p>Matplotlib is a widely used Python library for data visualization. It provides a variety of functions for creating different types of graphs and charts. Matplotlib can be used to create simple line plots, scatter plots, histograms, bar charts, 3D plots, and more. In this module, we will cover the basics of Matplotlib and how to use it for data visualization.</p> <p>Prerequisites: * Basic understanding of Python * Familiarity with NumPy library is recommended but not required.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install Matplotlib, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install matplotlib\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#02-plotting-simple-data","title":"02. Plotting Simple Data","text":"<p>Let's start by creating a simple line plot.</p> <pre><code># Creating two variable\nx = [1, 2, 3, 4, 5, 6, 7]\ny = [40, 38, 43, 45, 42, 40, 39]\n</code></pre> <pre><code>plt.plot(x, y)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x20719040370&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#03-api-for-plot","title":"03. API for Plot","text":"<p>In Matplotlib, you can customize the appearance of lines in a plot using the color, linewidth, and linestyle arguments in the plot() function.</p> <ul> <li> <p>The color argument sets the color of the line. It can be specified using a string such as \"red\" or \"blue\", a hex code such as \"#FF5733\", or an RGB tuple such as (0.5, 0.5, 0.5).</p> </li> <li> <p>The linewidth argument sets the width of the line. It can be specified using a floating-point number.</p> </li> <li> <p>The linestyle argument sets the style of the line. It can be specified using a string such as \"solid\", \"dashed\", or \"dotted\", or a combination of those using a \":\" for dots and a \"--\" for dashes.</p> </li> </ul> <pre><code># Creating a new plot\nplt.plot(x, y, color=\"red\", linewidth=4, linestyle=\"dotted\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x2071908e8e0&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#04-adding-title-and-labels-to-the-chart","title":"04. Adding Title and Labels to the Chart","text":"<p>In Matplotlib, you can add titles and labels to your plot to provide context and improve its readability.</p> <ul> <li> <p>To add a title to your plot, you can use the title() method of the axes object. This method takes a string argument that specifies the title of the plot.</p> </li> <li> <p>To add labels to the x-axis and y-axis, you can use the xlabel() and ylabel() methods of the axes object, respectively. These methods take a string argument that specifies the label of the axis. </p> </li> </ul> <pre><code># Creating another plot\nplt.plot(x, y, color=\"green\", linewidth=4, linestyle=\"dashed\")\nplt.title(\"Weather\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Temperature\")\n</code></pre> <pre><code>Text(0, 0.5, 'Temperature')\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/","title":"Format Strings In Plot Function","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/#format-strings-in-plot-function","title":"Format Strings in plot() Function","text":"<p>In Matplotlib, you can customize the appearance of data points in a plot using format strings in the plot() function. A format string is a shorthand way of specifying the color, marker, and line style of the plot.</p> <p>A format string consists of one or more characters that represent the color, marker, and line style of the plot. The characters are specified in a specific order: first the color, then the marker, and finally the line style. Each component is represented by a single character code.</p> <p>Click Here to Check the Documentation of plot() Function </p> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code># Creating two variables\ndays = [1, 2, 3, 4, 5, 6, 7]\ntemperature = [50, 51, 52, 48, 47, 49, 46]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/#01-plotting-using-format-string","title":"01. Plotting Using Format String","text":"<p>Here are the most commonly used codes: * Color codes: b (blue), g (green), r (red), c (cyan), m (magenta), y (yellow), k (black), w (white) * Marker codes: . (point), o (circle), v (downward-pointing triangle), ^ (upward-pointing triangle), s (square), + (plus), x (cross), * (star), p (pentagon), h (hexagon) * Line style codes: - (solid), -- (dashed), : (dotted), -. (dash-dot)</p> <pre><code>plt.plot(days, temperature, \"r.--\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2648070&gt;]\n</code></pre> <p></p> <pre><code># Plotting with keyword arguments\nplt.plot(days, temperature, color=\"r\", marker=\".\", linestyle=\"--\", markersize=10)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c268da00&gt;]\n</code></pre> <p></p> <pre><code>plt.plot(days, temperature, color=\"#58508d\", marker=\"D\", linestyle=\"\", markersize=5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2712370&gt;]\n</code></pre> <p></p> <pre><code>plt.plot(days, temperature, color=\"#58508d\", marker=\"D\", linestyle=\"\", markersize=5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2796220&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/#02-alpha-property-for-plot-api","title":"02. alpha Property for plot API","text":"<p>In Matplotlib, the alpha property of the plot() function can be used to adjust the transparency of data points, lines, and other plot elements.</p> <p>The alpha property takes a value between 0 and 1, where 0 is completely transparent and 1 is completely opaque. By default, the value of alpha is 1, which means that plot elements are fully opaque.</p> <pre><code>plt.plot(days, temperature, color=\"#367635\",marker=\".\", markersize=10, linestyle=\"--\", alpha=0.5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2890400&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/","title":"Simple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#simple-linear-regression","title":"Simple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#description","title":"Description:","text":"<p>Simple linear regression is a statistical technique used to establish a relationship between two variables - one independent and one dependent. The purpose of this technique is to determine whether there is a linear relationship between the two variables, and if so, to develop a mathematical model that can be used to predict the value of the dependent variable based on the value of the independent variable.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#assumptions","title":"Assumptions:","text":"<p>Simple linear regression is a parametric test, meaning that it makes certain assumptions about the data. These assumptions are: 1. Homogeneity of variance: the size of the error in our prediction doesn\u2019t change significantly across the values of the independent variable. 2. Independence of observations: the observations in the dataset were collected using statistically valid sampling methods, and there are no hidden relationships among observations. 3. Normality: The data follows a normal distribution.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#how-to-perform-a-simple-linear-regression","title":"How to perform a Simple Linear Regression:","text":"<p>Simple linear regression Formula: $$ y = {\\beta_0} + {\\beta_1{X}} + {\\epsilon} $$ * y is the predicted value of the dependent variable (y) for any given value of the independent variable (x). * B0 is the intercept, the predicted value of y when the x is 0. * B1 is the regression coefficient or slope \u2013 how much we expect y to change as x increases. * x is the independent variable ( the variable we expect is influencing y). * e is the error of the estimate, or how much variation there is in our estimate of the regression coefficient.</p> <p>Linear regression finds the line of best fit line through your data by searching for the regression coefficient (B1) that minimizes the total error (e) of the model.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#simple-linear-regression-project","title":"Simple Linear Regression Project:","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#predicting-average-temperature-in-northern-hemisphere-based-on-latitude-using-simple-linear-regression","title":"Predicting Average Temperature in Northern Hemisphere based on Latitude using Simple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#project-description","title":"Project Description:","text":"<p>The relationship between latitude and temperature is a well-established phenomenon in climatology. In general, temperature decreases as we move away from the equator towards the poles. This is because the Earth's surface receives more direct sunlight near the equator than at the poles, and therefore the equator is warmer.</p> <p>The project of predicting average temperature based on latitude using simple linear regression is an exciting machine learning endeavor that has many real-world applications. The goal of this project is to build a model that can accurately estimate the average temperature of a particular location based on its latitude information.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#02-reading-the-csv-file-with-pandas","title":"02. Reading the CSV File with Pandas","text":"<pre><code># Defining the path of the csv\ncsv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\GlobalLandTemperaturesByCity.csv\"\n</code></pre> <pre><code># Reading the csv file with pandas library\ndf = pd.read_csv(csv_path)\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13N 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania 0.80S 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45N 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil 7.23S 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48N 91.27W <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(20000, 8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#03-data-cleaning","title":"03. Data Cleaning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#301-removing-the-rows-with-missing-values","title":"3.01 Removing the Rows with Missing Values","text":"<pre><code># Counting the number of missing values (i.e., NaN values) in each column of the pandas DataFrame\ndf.isnull().sum()\n</code></pre> <pre><code>Unnamed: 0                       0\ndt                               0\nAverageTemperature               0\nAverageTemperatureUncertainty    0\nCity                             0\nCountry                          0\nLatitude                         0\nLongitude                        0\ndtype: int64\n</code></pre> <pre><code># Dropping the rows with missing values\ndf.dropna(inplace=True)\n# Checking the shape of the dataframe after dropping rows with missing values\ndf.shape\n</code></pre> <pre><code>(20000, 8)\n</code></pre> <pre><code># Checking the dataframe after dropping the rows with missing values\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13N 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania 0.80S 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45N 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil 7.23S 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48N 91.27W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#302-manipulating-the-latitude-column","title":"3.02 Manipulating the 'Latitude' Column","text":"<pre><code># Adding a \"-\" (minus) before the latitudes of the sothern hemisphere\ndf[\"Latitude\"] = df[\"Latitude\"].apply(lambda x: \"-\"+x if x.endswith(\"S\") else x)\n</code></pre> <pre><code># Removing the 'N' and 'S' from 'Latitude' column\ndf[\"Latitude\"] = df[\"Latitude\"].str.replace(\"N\", \"\")\ndf[\"Latitude\"] = df[\"Latitude\"].str.replace(\"S\", \"\")\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania -0.80 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil -7.23 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48 91.27W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#303-changing-the-datatype-of-the-latitude-column","title":"3.03 Changing the Datatype of the 'Latitude' Column","text":"<pre><code># Changing the datatype of the 'Latitude' column from 'str' to 'float'\nconvert_dict = {\"Latitude\": float}\ndf = df.astype(convert_dict)\ndf.dtypes\n</code></pre> <pre><code>Unnamed: 0                         int64\ndt                                object\nAverageTemperature               float64\nAverageTemperatureUncertainty    float64\nCity                              object\nCountry                           object\nLatitude                         float64\nLongitude                         object\ndtype: object\n</code></pre> <pre><code># Checking the latitudes of the southern hemisphere\ndf[df[\"Latitude\"] &lt; 0].head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 1 1 1962-09-01 21.115 0.309 Musoma Tanzania -0.80 34.55E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil -7.23 34.86W 8 8 1965-07-01 20.737 0.402 Bukavu Congo (Democratic Republic Of The) -2.41 28.13E 14 14 1943-06-01 22.852 0.513 Toliary Madagascar -23.31 42.82E 19 19 1985-06-01 15.466 0.427 Ferraz De Vasconcelos Brazil -23.31 46.31W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#304-selecting-the-northern-latitudes-only","title":"3.04 Selecting the Northern Latitudes Only","text":"<pre><code># Selecting the latitudes of the Northern Hemisphere only\ndf = df[df[\"Latitude\"] &gt;= 0]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#305-selecting-a-random-sample-from-the-dataframe","title":"3.05 Selecting a Random Sample from the DataFrame","text":"<pre><code># Selecting a random sample from the dataframe\ndf = df.sample(10000, random_state=0)\n</code></pre> <pre><code># Selecting only two columns 'Latitude' and 'AverageTemperature' from the dataframe\ndf = df[[\"Latitude\", \"AverageTemperature\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#306-removing-the-outliers","title":"3.06 Removing the Outliers","text":"<pre><code># Visualizing the boxplot of the dataframe\nsns.boxplot(df)\nplt.title(\"Boxplot before Outliers Removal\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Boxplot before Outliers Removal')\n</code></pre> <pre><code># Removing the outliers from the 'AverageTemperature' column\n# Getting the value of First and Third Quartile (Q1 &amp; Q3) of the 'AverageTemperature'\nQ1 = df[\"AverageTemperature\"].quantile(0.25)\nQ3 = df[\"AverageTemperature\"].quantile(0.75)\nprint(\"Quartile1:\", Q1)\nprint(\"Quartile2:\", Q3)\n</code></pre> <pre><code>Quartile1: 8.628\nQuartile2: 25.2815\n</code></pre> <pre><code># Calulating the Inter Quartile Range (IQR)\nIQR = Q3 - Q1\nprint(\"IQR:\", IQR)\n</code></pre> <pre><code>IQR: 16.6535\n</code></pre> <pre><code># Calculating the Higher Fence and Lower Fence\nlower_fence =  Q1 - (1.5 * IQR)\nhigher_fence = Q3 + (1.5 * IQR)\nprint(\"Lower Fence:\", lower_fence)\nprint(\"Higher Fence:\", higher_fence)\n</code></pre> <pre><code>Lower Fence: -16.35225\nHigher Fence: 50.261750000000006\n</code></pre> <pre><code># Removing the Outliers\ndf = df[~((df[\"AverageTemperature\"] &lt; lower_fence) | (df[\"AverageTemperature\"] &gt; higher_fence))]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#307-checking-the-final-dataframe","title":"3.07 Checking the Final DataFrame","text":"<pre><code>df.shape\n</code></pre> <pre><code>(9927, 2)\n</code></pre> <pre><code># Resetting the index of the dataframe\ndf.reset_index(drop=True, inplace=True)\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Latitude AverageTemperature 0 32.95 20.134 1 36.17 12.346 2 47.42 -0.779 3 36.17 15.944 4 40.99 0.793 <pre><code># Describing the univariate statistics of the dataframe\ndf.describe()\n</code></pre> Latitude AverageTemperature count 9927.000000 9927.000000 mean 33.014089 16.345360 std 14.580933 10.448532 min 0.800000 -16.335000 25% 23.310000 8.856500 50% 34.560000 18.094000 75% 44.200000 25.317500 max 69.920000 37.283000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#04-data-visualization","title":"04. Data Visualization","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#401-histogram-of-average-temperature-with-kernel-density-estimation-kde","title":"4.01 Histogram of Average Temperature with Kernel Density Estimation (KDE))","text":"<pre><code># Visualizing Histogram of 'AverageTemperature' with Probability Density Function\nsns.histplot(df[\"AverageTemperature\"], kde=True)\nplt.title(\"Historam of Average Temperature\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#402-boxplot-of-average-temperature","title":"4.02 Boxplot of Average Temperature","text":"<pre><code># Visualizing 5 number summary of the 'AverageTemperature'\nsns.boxplot(x=df[\"AverageTemperature\"], width=0.5)\nplt.title(\"Boxplot of Average Temperature after Outliers Removal\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#403-scatterplot-between-latitude-and-average-temperature","title":"4.03 Scatterplot between Latitude and Average Temperature","text":"<pre><code>sns.scatterplot(x=df[\"Latitude\"], y=df[\"AverageTemperature\"], marker=\".\")\nplt.title(\"Scatterplot between North Latitude and Average Temperature\")\nplt.xlabel(\"North Latitude\")\nplt.ylabel(\"Average Temperature (in \u00b0C)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#05-dividing-the-data-into-trainining-and-testing-set","title":"05. Dividing the Data into Trainining and Testing Set","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#501-defining-the-dependent-and-independent-variable","title":"5.01 Defining the Dependent and Independent Variable","text":"<pre><code># Dependent Variable (y) = 'AverageTemperature'\n# Independent Variable (x) = 'Latitude'\nx = df[[\"Latitude\"]]\ny = df[[\"AverageTemperature\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#502-splitting-the-data-into-training-and-testing-set","title":"5.02 Splitting the Data into Training and Testing Set","text":"<pre><code># Importing the tarin_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Training Data = 70% and Testing Data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#06-instantiating-the-simple-linear-regression-model","title":"06. Instantiating the Simple Linear Regression Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#601-importing-linearregression-model-from-sklearn-library","title":"6.01 Importing LinearRegression Model from sklearn Library","text":"<pre><code># Importing the Linear Regression Model from sklearn library\nfrom sklearn.linear_model import LinearRegression\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#602-generating-a-linearregression-object","title":"6.02 Generating a LinearRegression Object","text":"<pre><code># Creating a linear regression object\nlin_reg = LinearRegression()\n# Feeding the training data to the model\nlin_reg.fit(x_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#603-getting-the-coefficients-of-the-linear-regression-model","title":"6.03 Getting the Coefficients of the Linear Regression Model","text":"<pre><code># Getting the slope of the model\nlin_reg.coef_\n</code></pre> <pre><code>array([[-0.48602283]])\n</code></pre> <pre><code># Getting the y-intercept of the model\nlin_reg.intercept_\n</code></pre> <pre><code>array([32.37125607])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#07-validation-of-the-model","title":"07. Validation of the Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#701-importing-some-validation-metrics","title":"7.01 Importing Some Validation Metrics","text":"<pre><code># Importing some validation metrics from sklearn library\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#702-validating-the-linear-regression-model","title":"7.02 Validating the Linear Regression Model","text":"<pre><code># Predicting the AverageTemperature of the x_test (Latitude) data\ny_predicted = lin_reg.predict(x_test)\n</code></pre> <pre><code># Defining the actual 'Average Temperature' data\ny_actual = y_test\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#703-calculating-the-mean-absolute-error-mean-squared-error-of-the-model","title":"7.03 Calculating the Mean Absolute Error, Mean Squared Error of the Model","text":"<pre><code># Calculating the Mean Absolute Error (MAE)\nMAE = mean_absolute_error(y_actual, y_predicted)\nprint(\"Mean Absolute Error (MAE):\", MAE.round(4))\n</code></pre> <pre><code>Mean Absolute Error (MAE): 6.0816\n</code></pre> <pre><code># Calulating the Mean Squared Error (MSE)\nMSE = mean_squared_error(y_actual, y_predicted)\nprint(\"Mean Squared Error (MSE):\", MSE.round(4))\n</code></pre> <pre><code>Mean Squared Error (MSE): 57.4893\n</code></pre> <pre><code># Calulating the Root Mean Squared Error (MSE)\nRMSE = np.sqrt(MSE)\nprint(\"Root Mean Squared Error (RMSE):\", RMSE.round(4))\n</code></pre> <pre><code>Root Mean Squared Error (RMSE): 7.5822\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#704-plotting-the-linear-regression-line","title":"7.04 Plotting the Linear Regression Line","text":"<pre><code>sns.scatterplot(x=df[\"Latitude\"], y=df[\"AverageTemperature\"], marker=\".\")\nplt.plot(x, lin_reg.predict(x), color=\"red\", label=\"Regression Line\")\nplt.title(\"Scatterplot between North Latitude and Average Temperature\")\nplt.xlabel(\"North Latitude\")\nplt.ylabel(\"Average Temperature (in \u00b0C)\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/","title":"Multiple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#multiple-linear-regression","title":"Multiple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#01-import-the-essential-libraries","title":"01. Import the Essential Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn import linear_model\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#02-creating-a-dataframe","title":"02. Creating a DataFrame","text":"<pre><code>df = pd.read_csv(\"Housing.csv\")\ndf.head()\n</code></pre> price area bedrooms bathrooms stories mainroad guestroom basement hotwaterheating airconditioning parking prefarea furnishingstatus 0 13300000 7420 4 2 3 yes no no no yes 2 yes furnished 1 12250000 8960 4 4 4 yes no no no yes 3 no furnished 2 12250000 9960 3 2 2 yes no yes no no 2 yes semi-furnished 3 12215000 7500 4 2 2 yes no yes no yes 3 yes furnished 4 11410000 7420 4 1 2 yes yes yes no yes 2 no furnished <pre><code># Slecting only the fields which have numerical values\nnew_df = df[[\"price\", \"area\", \"bedrooms\", \"bathrooms\"]]\nnew_df.head()\n</code></pre> price area bedrooms bathrooms 0 13300000 7420 4 2 1 12250000 8960 4 4 2 12250000 9960 3 2 3 12215000 7500 4 2 4 11410000 7420 4 1"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#03-creating-linear-regression-object","title":"03. Creating Linear Regression Object","text":"<pre><code># Defining independent(x) and dependent(y) variables\n# In this case, independent variable(x) will be 'area', 'bedrooms', 'bathrooms', 'stories', 'parking'\n# and dependent variable will be 'price'\nx = new_df[[\"area\", \"bedrooms\", \"bathrooms\"]]\ny = new_df[[\"price\"]]\n</code></pre> <pre><code># Creating a Linear Regression model\nreg = linear_model.LinearRegression()\n# Training the model using the x and y\nreg.fit(x, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#04-getting-the-coefficients","title":"04. Getting the Coefficients","text":"<p>Linear Equation: y = m1x1 + m2x2 + m3x3 + ... + mnxn + c where, y = dependent variable m = slope x = independent variables c = y-intercept</p> <pre><code># Getting the slopes for all the independent variables\nreg.coef_\n</code></pre> <pre><code>array([[3.78762754e+02, 4.06820034e+05, 1.38604950e+06]])\n</code></pre> <pre><code># Getting the y-intercept\nreg.intercept_\n</code></pre> <pre><code>array([-173171.60763263])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#05-predicting-price-of-houses","title":"05. Predicting Price of Houses","text":"<pre><code>reg.predict([[0, 4, 3]])\n</code></pre> <pre><code>C:\\Users\\KRISH\\miniconda3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\narray([[43488532.37932754]])\n</code></pre> <pre><code>\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/03_Cost_Function/","title":"Cost Function","text":"<pre><code>import numpy as np\n</code></pre> <p>y = 2x + 3</p> <pre><code>x = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 7, 9, 11, 13])\n</code></pre> <pre><code>def gradient_descent(x, y):\n    m_curr = b_curr = 0\n    iterations = 1000\n    learning_rate = 0.05\n    n = len(x)\n\n    for i in range(iterations):\n        y_predicted = m_curr * x + b_curr\n        cost = (1 / n) * sum([val**2 for val in (y - y_predicted)])\n        print(f\"m {m_curr}, b {b_curr}, cost {cost}, iteration {i}\")\n        md = -(2 / n) * sum(x * (y - y_predicted))\n        bd = -(2 / n) * sum(y - y_predicted)\n        m_curr = m_curr - learning_rate * md\n        b_curr = b_curr - learning_rate * bd\n</code></pre> <pre><code>gradient_descent(x, y)\n</code></pre> <pre><code>m 0, b 0, cost 89.0, iteration 0\nm 3.1, b 0.9, cost 3.8599999999999994, iteration 1\nm 2.52, b 0.78, cost 0.9763999999999999, iteration 2\nm 2.6140000000000003, b 0.8460000000000001, cost 0.8513360000000004, iteration 3\nm 2.5848, b 0.8772, cost 0.81970064, iteration 4\nm 2.57836, b 0.91404, cost 0.7921173536, iteration 5\nm 2.567952, b 0.949128, cost 0.7655590528640001, iteration 6\nm 2.5584664, b 0.9838296, cost 0.7398944506073604, iteration 7\nm 2.54900448, b 1.01790672, cost 0.7150903372945663, iteration 8\nm 2.539727536, b 1.0514147040000001, cost 0.6911177570948888, iteration 9\nm 2.5306028352, b 1.0843549728000001, cost 0.667948830166544, iteration 10\nm 2.52163322464, b 1.1167386249600002, cost 0.6455566148366696, iteration 11\nm 2.512815090048, b 1.1485747950720002, cost 0.6239150727392739, iteration 12\nm 2.5041460524735997, b 1.1798727885504001, cost 0.602999038418565, iteration 13\nm 2.49562355818752, b 1.2106416939532803, cost 0.5827841900618126, iteration 14\nm 2.487245135995264, b 1.2408904571016963, cost 0.5632470212170514, iteration 15\nm 2.479008349269965, b 1.2706278705929475, cost 0.5443648134590486, iteration 16\nm 2.4709108038951193, b 1.2998625787526632, cost 0.5261156099716162, iteration 17\nm 2.4629501459846894, b 1.328603079708861, cost 0.508478190015543, iteration 18\nm 2.4551240614888727, b 1.3568577279425682, cost 0.4914320442524625, iteration 19\nm 2.447430275468342, b 1.3846347367016496, cost 0.4749573508959591, iteration 20\nm 2.439866551442671, b 1.411942180390982, cost 0.4590349526621794, iteration 21\nm 2.432430690738438, b 1.4387879969190824, cost 0.4436463344931504, iteration 22\nm 2.4251205318504314, b 1.4651799900056428, cost 0.42877360202689574, iteration 23\nm 2.417933949813264, b 1.4911258314499491, cost 0.41439946078931755, iteration 24\nm 2.410868855583689, b 1.516633063360975, cost 0.40050719608364665, iteration 25\nm 2.403923195433338, b 1.5417091003497707, cost 0.38708065355407184, iteration 26\nm 2.397094950351735, b 1.5663612316847921, cost 0.3741042204009627, iteration 27\nm 2.390382135459389, b 1.5905966234107924, cost 0.36156280722581025, iteration 28\nm 2.3837827994308234, b 1.6144223204318966, cost 0.3494418304848192, iteration 29\nm 2.3772950239273487, b 1.63784524855946, cost 0.33772719553070274, iteration 30\nm 2.3709169230394274, b 1.6608722165253094, cost 0.32640528022299464, iteration 31\nm 2.3646466427384647, b 1.6835099179609503, cost 0.31546291908779983, iteration 32\nm 2.3584823603378684, b 1.7057649333433158, cost 0.30488738800857484, iteration 33\nm 2.3524222839632185, b 1.7276437319076237, cost 0.29466638943012996, iteration 34\nm 2.346464652031391, b 1.7491526735278957, cost 0.2847880380586522, iteration 35\nm 2.3406077327384924, b 1.7702980105656887, cost 0.275240847041116, iteration 36\nm 2.334849823556444, b 1.791085889687572, cost 0.26601371460801604, iteration 37\nm 2.329189250738084, b 1.8115223536518816, cost 0.2570959111638838, iteration 38\nm 2.323624368830627, b 1.8316133430652684, cost 0.2484770668105837, iteration 39\nm 2.318153560197357, b 1.8513646981095535, cost 0.2401471592888743, iteration 40\nm 2.312775234547398, b 1.870782160239391, cost 0.23209650232421133, iteration 41\nm 2.307487828473443, b 1.8898713738512325, cost 0.2243157343632518, iteration 42\nm 2.302289804997286, b 1.9086378879240764, cost 0.21679580768794726, iteration 43\nm 2.2971796531230484, b 1.9270871576324828, cost 0.20952797789457814, iteration 44\nm 2.2921558873979504, b 1.94522454593232, cost 0.2025037937254882, iteration 45\nm 2.287217047480509, b 1.963055325119703, cost 0.19571508724170383, iteration 46\nm 2.282361697716038, b 1.9805846783635799, cost 0.1891539643249979, iteration 47\nm 2.2775884267193223, b 1.9978177012124105, cost 0.18281279549836707, iteration 48\nm 2.2728958469643445, b 2.014759403075373, cost 0.1766842070542369, iteration 49\nm 2.2682825943809535, b 2.031414708678532, cost 0.17076107248009, iteration 50\nm 2.263747327958345, b 2.047788459496393, cost 0.16503650417153282, iteration 51\nm 2.2592887293552475, b 2.0638854151592505, cost 0.1595038454231776, iteration 52\nm 2.2549055025167, b 2.079710254836751, cost 0.1541566626880198, iteration 53\nm 2.2505963732973044, b 2.095267578598066, cost 0.14898873809630875, iteration 54\nm 2.24636008909085, b 2.110561908749068, cost 0.14399406222521696, iteration 55\nm 2.2421954184661947, b 2.1255976911469063, cost 0.13916682711089676, iteration 56\nm 2.2381011508093085, b 2.140379296492357, cost 0.13450141949479977, iteration 57\nm 2.234076095971362, b 2.154911021600329, cost 0.12999241429640668, iteration 58\nm 2.230119083922765, b 2.1691970906488875, cost 0.12563456830477537, iteration 59\nm 2.2262289644130573, b 2.1832416564071693, cost 0.12142281408157163, iteration 60\nm 2.2224046066365437, b 2.197048801442535, cost 0.11735225406849686, iteration 61\nm 2.218644898903585, b 2.2106225393073187, cost 0.11341815489225414, iteration 62\nm 2.214948748317446, b 2.223966815705511, cost 0.10961594186043476, iteration 63\nm 2.2113150804566017, b 2.237085509639726, cost 0.10594119364192585, iteration 64\nm 2.207742839062422, b 2.2499824345387727, cost 0.1023896371256476, iteration 65\nm 2.204230985732126, b 2.262661339366169, cost 0.09895714245164931, iteration 66\nm 2.200778499616937, b 2.275125909709914, cost 0.09563971820877826, iteration 67\nm 2.197384377125332, b 2.2873797688538415, cost 0.09243350679334461, iteration 68\nm 2.1940476316313147, b 2.299426478830858, cost 0.08933477992338035, iteration 69\nm 2.190767293187611, b 2.311269541458378, cost 0.08633993430327605, iteration 70\nm 2.187542408243725, b 2.3229123993562566, cost 0.08344548743375868, iteration 71\nm 2.1843720393687502, b 2.334358436947513, cost 0.08064807356233224, iteration 72\nm 2.181255264978871, b 2.3456109814421366, cost 0.07794443976947857, iteration 73\nm 2.1781911790694717, b 2.3566733038042615, cost 0.0753314421860593, iteration 74\nm 2.175178890951774, b 2.367548619702994, cost 0.07280604233752795, iteration 75\nm 2.1722175249939246, b 2.3782400904471626, cost 0.07036530361069966, iteration 76\nm 2.1693062203664586, b 2.3887508239042687, cost 0.06800638783896373, iteration 77\nm 2.1664441307920734, b 2.399083875403904, cost 0.0657265520019769, iteration 78\nm 2.1636304242996216, b 2.4092422486258918, cost 0.06352314503599484, iteration 79\nm 2.16086428298227, b 2.419228896473416, cost 0.0613936047511307, iteration 80\nm 2.1581449027597484, b 2.4290467219313934, cost 0.05933545485196432, iteration 81\nm 2.155471493144607, b 2.4386985789103295, cost 0.057346302058027765, iteration 82\nm 2.1528432770124404, b 2.4481872730759147, cost 0.05542383332082406, iteration 83\nm 2.1502594903759817, b 2.4575155626645913, cost 0.053565813134143936, iteration 84\nm 2.1477193821630243, b 2.4666861592853375, cost 0.05177008093454941, iteration 85\nm 2.145222213998096, b 2.4757017287078966, cost 0.05003454858900295, iteration 86\nm 2.1427672599878216, b 2.484564891637678, cost 0.04835719796672347, iteration 87\nm 2.1403538065099146, b 2.4932782244775638, cost 0.04673607859243991, iteration 88\nm 2.1379811520057395, b 2.501844260076833, cost 0.04516930537831803, iteration 89\nm 2.135648606776376, b 2.510265488467428, cost 0.043655056431923134, iteration 90\nm 2.133355492782134, b 2.518544357587772, cost 0.04219157093766536, iteration 91\nm 2.131101143445455, b 2.526683273994355, cost 0.040777147109271154, iteration 92\nm 2.128884903457148, b 2.534684603561283, cost 0.039410140210890766, iteration 93\nm 2.1267061285859, b 2.5425506721680105, cost 0.0380889606445504, iteration 94\nm 2.124564185491007, b 2.5502837663754394, cost 0.03681207210171745, iteration 95\nm 2.122458451538267, b 2.557886134090593, cost 0.035577989776834094, iteration 96\nm 2.1203883146189955, b 2.5653599852200535, cost 0.03438527864073859, iteration 97\nm 2.118353172972084, b 2.5727074923123494, cost 0.03323255177196899, iteration 98\nm 2.1163524350090865, b 2.579930791189489, cost 0.03211846874400929, iteration 99\nm 2.1143855191422443, b 2.587031981567814, cost 0.031041734066597326, iteration 100\nm 2.1124518536154313, b 2.5940131276683593, cost 0.030001095679291486, iteration 101\nm 2.110550876337949, b 2.600876258816894, cost 0.028995343495533815, iteration 102\nm 2.1086820347211366, b 2.60762337003382, cost 0.028023307995524594, iteration 103\nm 2.1068447855177403, b 2.6142564226140967, cost 0.02708385886626944, iteration 104\nm 2.1050385946639967, b 2.620777344697365, cost 0.026175903687214466, iteration 105\nm 2.103262937124391, b 2.6271880318284295, cost 0.02529838665994734, iteration 106\nm 2.101517296739032, b 2.6334903475082694, cost 0.024450287380481515, iteration 107\nm 2.099801166073616, b 2.639686123735733, cost 0.023630619652699258, iteration 108\nm 2.0981140462719186, b 2.645777161540075, cost 0.02283843034157164, iteration 109\nm 2.096455446910786, b 2.6517652315044917, cost 0.02207279826482414, iteration 110\nm 2.094824885857574, b 2.657652074280807, cost 0.021332833121757225, iteration 111\nm 2.0932218891300005, b 2.663439401095454, cost 0.020617674457976888, iteration 112\nm 2.091645990758364, b 2.6691288942469087, cost 0.019926490664832017, iteration 113\nm 2.090096732650091, b 2.6747222075947086, cost 0.01925847801239354, iteration 114\nm 2.0885736644565784, b 2.6802209670402104, cost 0.01861285971485296, iteration 115\nm 2.087076343442279, b 2.6856267709992157, cost 0.017988885027251458, iteration 116\nm 2.0856043343560073, b 2.6909411908666105, cost 0.01738582837249017, iteration 117\nm 2.0841572093044163, b 2.6961657714731473, cost 0.01680298849760723, iteration 118\nm 2.0827345476276142, b 2.7013020315345075, cost 0.016239687658338633, iteration 119\nm 2.0813359357768864, b 2.7063514640927724, cost 0.01569527083101607, iteration 120\nm 2.0799609671944794, b 2.7113155369504294, cost 0.015169104950885935, iteration 121\nm 2.078609242195423, b 2.716195693097043, cost 0.014660578175960946, iteration 122\nm 2.077280367851345, b 2.720993351128712, cost 0.014169099175552197, iteration 123\nm 2.075973957876252, b 2.725709905660437, cost 0.0136940964426512, iteration 124\nm 2.0746896325142434, b 2.7303467277315177, cost 0.01323501762936315, iteration 125\nm 2.0734270184291206, b 2.734905165204093, cost 0.01279132890462135, iteration 126\nm 2.07218574859586, b 2.7393865431549473, cost 0.012362514333429959, iteration 127\nm 2.07096546219393, b 2.743792164260695, cost 0.011948075276920229, iteration 128\nm 2.0697658045023983, b 2.7481233091764463, cost 0.011547529812517096, iteration 129\nm 2.068586426796826, b 2.7523812369080822, cost 0.011160412173544932, iteration 130\nm 2.067426986247893, b 2.756567185178226, cost 0.010786272207619406, iteration 131\nm 2.0662871458217427, b 2.7606823707860357, cost 0.010424674853196461, iteration 132\nm 2.065166574182015, b 2.7647279899609094, cost 0.010075199633669624, iteration 133\nm 2.064064945593526, b 2.768705218710214, cost 0.009737440168425999, iteration 134\nm 2.062981939827583, b 2.772615213161135, cost 0.009411003700295115, iteration 135\nm 2.061917242068901, b 2.7764591098967464, cost 0.009095510638838102, iteration 136\nm 2.0608705428240857, b 2.7802380262864013, cost 0.008790594118948577, iteration 137\nm 2.0598415378316712, b 2.7839530608105356, cost 0.008495899574250211, iteration 138\nm 2.0588299279736724, b 2.787605293379981, cost 0.008211084324796268, iteration 139\nm 2.0578354191886383, b 2.7911957856498812, cost 0.007935817178590546, iteration 140\nm 2.0568577223861717, b 2.7947255813283016, cost 0.007669778046467093, iteration 141\nm 2.0558965533628926, b 2.79819570647962, cost 0.007412657569880772, iteration 142\nm 2.054951632719825, b 2.8016071698227902, cost 0.007164156761175289, iteration 143\nm 2.0540226857811805, b 2.8049609630245635, cost 0.006923986655911223, iteration 144\nm 2.0531094425145127, b 2.808258060987753, cost 0.006691867976849126, iteration 145\nm 2.052211637452223, b 2.8114994221346237, cost 0.0064675308091975735, iteration 146\nm 2.0513290096143906, b 2.8146859886854947, cost 0.00625071428674765, iteration 147\nm 2.0504613024329124, b 2.817818686932628, cost 0.006041166288529687, iteration 148\nm 2.0496082636769204, b 2.8208984275094915, cost 0.005838643145638964, iteration 149\nm 2.0487696453794606, b 2.8239261056554663, cost 0.0056429093578906605, iteration 150\nm 2.047945203765414, b 2.8269026014760814, cost 0.005453737319971924, iteration 151\nm 2.047134699180634, b 2.829828780198849, cost 0.005270907056776289, iteration 152\nm 2.046337896022282, b 2.832705492424774, cost 0.005094205967609158, iteration 153\nm 2.0455545626703397, b 2.8355335743756123, cost 0.0049234285789696635, iteration 154\nm 2.0447844714202823, b 2.838313848136949, cost 0.004758376305619132, iteration 155\nm 2.0440273984168873, b 2.8410471218971693, cost 0.004598857219660502, iteration 156\nm 2.0432831235891604, b 2.8437341901823863, cost 0.004444685827358471, iteration 157\nm 2.042551430586368, b 2.8463758340873997, cost 0.0042956828534413995, iteration 158\nm 2.0418321067151433, b 2.848972821502749, cost 0.004151675032634976, iteration 159\nm 2.0411249428776608, b 2.8515259073379315, cost 0.004012494908183505, iteration 160\nm 2.0404297335108543, b 2.85403583374084, cost 0.0038779806371261267, iteration 161\nm 2.0397462765266625, b 2.8565033303134997, cost 0.003747975802100954, iteration 162\nm 2.039074373253284, b 2.858929114324151, cost 0.0036223292294580824, iteration 163\nm 2.0384138283774265, b 2.8613138909157505, cost 0.003500894813470044, iteration 164\nm 2.0377644498875322, b 2.8636583533109476, cost 0.003383531346435587, iteration 165\nm 2.0371260490179623, b 2.865963183013593, cost 0.003270102354479139, iteration 166\nm 2.036498440194126, b 2.868229050006845, cost 0.003160475938854661, iteration 167\nm 2.035881440978534, b 2.8704566129479225, cost 0.0030545246225695453, iteration 168\nm 2.03527487201777, b 2.87264651935957, cost 0.002952125202150655, iteration 169\nm 2.034678556990352, b 2.874799405818282, cost 0.002853158604379423, iteration 170\nm 2.0340923225554803, b 2.876915898139348, cost 0.002757509747829726, iteration 171\nm 2.0335159983026476, b 2.8789966115587693, cost 0.0026650674090478023, iteration 172\nm 2.0329494167021047, b 2.8810421509120983, cost 0.0025757240932181157, iteration 173\nm 2.03239241305616, b 2.883053110810257, cost 0.002489375909165006, iteration 174\nm 2.031844825451307, b 2.885030075812383, cost 0.0024059224485447534, iteration 175\nm 2.0313064947111545, b 2.886973620595753, cost 0.0023252666690877825, iteration 176\nm 2.0307772643501587, b 2.888884310122831, cost 0.0022473147817549484, iteration 177\nm 2.0302569805281347, b 2.8907626998055003, cost 0.0021719761416765025, iteration 178\nm 2.0297454920055364, b 2.89260933566651, cost 0.002099163142747632, iteration 179\nm 2.0292426500994933, b 2.894424754498198, cost 0.002028791115757323, iteration 180\nm 2.028748308640591, b 2.8962094840185304, cost 0.0019607782299322874, iteration 181\nm 2.028262323930382, b 2.8979640430245, cost 0.0018950453977817196, iteration 182\nm 2.027784554699612, b 2.8996889415429354, cost 0.0018315161831319372, iteration 183\nm 2.027314862067158, b 2.9013846809787585, cost 0.0017701167122437873, iteration 184\nm 2.026853109499657, b 2.9030517542607353, cost 0.001710775587910338, iteration 185\nm 2.026399162771814, b 2.904690645984765, cost 0.0016534238064336356, iteration 186\nm 2.0259528899273893, b 2.9063018325547443, cost 0.0015979946773853376, iteration 187\nm 2.025514161240838, b 2.907885782321053, cost 0.0015444237460568813, iteration 188\nm 2.0250828491796002, b 2.9094429557166963, cost 0.0014926487185096338, iteration 189\nm 2.024658828367031, b 2.9109738053911465, cost 0.0014426093891373427, iteration 190\nm 2.024241975545953, b 2.9124787763419224, cost 0.0013942475706576515, iteration 191\nm 2.023832169542828, b 2.9139583060439445, cost 0.0013475070264496187, iteration 192\nm 2.023429291232534, b 2.9154128245767015, cost 0.0013023334051602985, iteration 193\nm 2.0230332235037363, b 2.916842754749271, cost 0.0012586741775033304, iteration 194\nm 2.0226438512248452, b 2.918248512223223, cost 0.0012164785751761667, iteration 195\nm 2.0222610612105485, b 2.9196305056334473, cost 0.0011756975318250324, iteration 196\nm 2.021884742188911, b 2.920989136706938, cost 0.0011362836259893234, iteration 197\nm 2.0215147847690274, b 2.9223248003795708, cost 0.001098191025958192, iteration 198\nm 2.021151081409226, b 2.9236378849109053, cost 0.001061375436476129, iteration 199\nm 2.020793526385806, b 2.9249287719970467, cost 0.001025794047235109, iteration 200\nm 2.0204420157623053, b 2.9261978368816, cost 0.000991405483093303, iteration 201\nm 2.0200964473592893, b 2.9274454484647485, cost 0.0009581697559628608, iteration 202\nm 2.0197567207246463, b 2.928671969410487, cost 0.0009260482183106579, iteration 203\nm 2.019422737104389, b 2.9298777562520444, cost 0.000895003518217432, iteration 204\nm 2.0190943994139476, b 2.931063159495523, cost 0.0008649995559441768, iteration 205\nm 2.0187716122099486, b 2.932228523721786, cost 0.0008360014419539533, iteration 206\nm 2.018454281662469, b 2.933374187686623, cost 0.000807975456341349, iteration 207\nm 2.0181423155277662, b 2.9345004844192197, cost 0.0007808890096220431, iteration 208\nm 2.0178356231214574, b 2.935607741318968, cost 0.0007547106048365685, iteration 209\nm 2.0175341152921638, b 2.9366962802506342, cost 0.0007294098009248421, iteration 210\nm 2.017237704395593, b 2.9377664176379215, cost 0.0007049571773281187, iteration 211\nm 2.0169463042690645, b 2.9388184645554514, cost 0.0006813242997781602, iteration 212\nm 2.016659830206458, b 2.9398527268191867, cost 0.0006584836872327109, iteration 213\nm 2.0163781989335985, b 2.9408695050753306, cost 0.0006364087799199053, iteration 214\nm 2.0161013285840412, b 2.941869094887718, cost 0.0006150739084535631, iteration 215\nm 2.0158291386752807, b 2.942851786823734, cost 0.0005944542639841898, iteration 216\nm 2.015561550085352, b 2.9438178665387764, cost 0.0005745258693502642, iteration 217\nm 2.015298485029832, b 2.9447676148592934, cost 0.0005552655511971699, iteration 218\nm 2.015039867039229, b 2.9457013078644145, cost 0.000536650913030237, iteration 219\nm 2.0147856209367525, b 2.946619216966204, cost 0.0005186603091714572, iteration 220\nm 2.0145356728164634, b 2.947521608988558, cost 0.0005012728195892724, iteration 221\nm 2.0142899500217863, b 2.948408746244763, cost 0.00048446822557211844, iteration 222\nm 2.0140483811243923, b 2.9492808866137508, cost 0.00046822698621744297, iteration 223\nm 2.0138108959034353, b 2.950138283615058, cost 0.0004525302157088875, iteration 224\nm 2.013577425325139, b 2.9509811864825215, cost 0.000437359661355429, iteration 225\nm 2.01334790152273, b 2.9518098402367277, cost 0.00042269768236648616, iteration 226\nm 2.013122257776709, b 2.952624485756236, cost 0.0004085272293385638, iteration 227\nm 2.0129004284954584, b 2.9534253598475995, cost 0.0003948318244298947, iteration 228\nm 2.012682349196174, b 2.954212695314202, cost 0.00038159554219932676, iteration 229\nm 2.012467956486122, b 2.9549867210239293, cost 0.0003688029910877851, iteration 230\nm 2.012257188044209, b 2.9557476619756997, cost 0.00035643929552053974, iteration 231\nm 2.012049982602869, b 2.956495739364867, cost 0.0003444900786092948, iteration 232\nm 2.011846279930253, b 2.9572311706475194, cost 0.0003329414454344322, iteration 233\nm 2.011646020812719, b 2.9579541696036915, cost 0.0003217799668875029, iteration 234\nm 2.0114491470376206, b 2.9586649463995065, cost 0.00031099266405542106, iteration 235\nm 2.011255601376386, b 2.9593637076482695, cost 0.0003005669931282556, iteration 236\nm 2.0110653275678807, b 2.9600506564705267, cost 0.00029049083081287305, iteration 237\nm 2.010878270302054, b 2.96072599255311, cost 0.0002807524602355139, iteration 238\nm 2.0106943752038617, b 2.9613899122071827, cost 0.0002713405573171734, iteration 239\nm 2.010513588817459, b 2.962042608425306, cost 0.00026224417760553994, iteration 240\nm 2.010335858590662, b 2.9626842709375376, cost 0.00025345274354845977, iteration 241\nm 2.0101611328596727, b 2.963315086266585, cost 0.00024495603219404225, iteration 242\nm 2.0099893608340573, b 2.963935237782025, cost 0.00023674416330307318, iteration 243\nm 2.0098204925819867, b 2.964544905753605, cost 0.00022880758786000452, iteration 244\nm 2.00965447901572, b 2.9651442674036486, cost 0.0002211370769689657, iteration 245\nm 2.0094912718773337, b 2.965733496958568, cost 0.0002137237111223087, iteration 246\nm 2.0093308237246963, b 2.966312765699511, cost 0.00020655886982851574, iteration 247\nm 2.0091730879176772, b 2.9668822420121512, cost 0.00019963422158815705, iteration 248\nm 2.009018018604587, b 2.967442091435633, cost 0.0001929417142057066, iteration 249\nm 2.0088655707088514, b 2.9679924767106938, cost 0.0001864735654262438, iteration 250\nm 2.0087156999159066, b 2.968533557826969, cost 0.00018022225388599245, iteration 251\nm 2.0085683626603186, b 2.9690654920695, cost 0.00017418051036617734, iteration 252\nm 2.008423516113118, b 2.9695884340644545, cost 0.00016834130934027756, iteration 253\nm 2.0082811181693514, b 2.9701025358240734, cost 0.0001626978608044125, iteration 254\nm 2.008141127435843, b 2.9706079467908606, cost 0.00015724360238177987, iteration 255\nm 2.0080035032191574, b 2.9711048138810217, cost 0.00015197219169171806, iteration 256\nm 2.0078682055137778, b 2.971593281527172, cost 0.00014687749897454057, iteration 257\nm 2.0077351949904707, b 2.9720734917203218, cost 0.00014195359996372998, iteration 258\nm 2.0076044329848566, b 2.9725455840511485, cost 0.00013719476899696085, iteration 259\nm 2.0074758814861697, b 2.973009695750577, cost 0.0001325954723581247, iteration 260\nm 2.00734950312621, b 2.9734659617296684, cost 0.00012815036184263433, iteration 261\nm 2.007225261168479, b 2.9739145146188384, cost 0.0001238542685382261, iteration 262\nm 2.0071031194975006, b 2.9743554848064107, cost 0.00011970219681452683, iteration 263\nm 2.0069830426083266, b 2.974789000476519, cost 0.00011568931851389433, iteration 264\nm 2.0068649955962115, b 2.9752151876463695, cost 0.00011181096733710637, iteration 265\nm 2.006748944146468, b 2.975634170202869, cost 0.0001080626334172468, iteration 266\nm 2.0066348545244925, b 2.9760460699386417, cost 0.0001044399580755298, iteration 267\nm 2.0065226935659584, b 2.9764510065874297, cost 0.00010093872875281702, iteration 268\nm 2.006412428667175, b 2.9768490978588993, cost 9.75548741111627e-05, iteration 269\nm 2.0063040277756126, b 2.9772404594728568, cost 9.428445929957153e-05, iteration 270\nm 2.006197459380582, b 2.9776252051928873, cost 9.112368137837175e-05, iteration 271\nm 2.0060926925040756, b 2.978003446859424, cost 8.806886489706522e-05, iteration 272\nm 2.005989696691765, b 2.978375294422259, cost 8.511645762040662e-05, iteration 273\nm 2.005888442004146, b 2.9787408559725033, cost 8.226302639773913e-05, iteration 274\nm 2.0057888990078343, b 2.9791002377740092, cost 7.950525317084055e-05, iteration 275\nm 2.0056910387670137, b 2.979453544294258, cost 7.683993111554967e-05, iteration 276\nm 2.0055948328350213, b 2.979800878234728, cost 7.426396091281949e-05, iteration 277\nm 2.0055002532460793, b 2.980142340560749, cost 7.177434714467426e-05, iteration 278\nm 2.0054072725071674, b 2.98047803053085, cost 6.936819481110102e-05, iteration 279\nm 2.0053158635900283, b 2.980808045725615, cost 6.704270596361031e-05, iteration 280\nm 2.005225999923313, b 2.981132482076045, cost 6.479517645173692e-05, iteration 281\nm 2.005137655384855, b 2.9814514338914466, cost 6.262299277850866e-05, iteration 282\nm 2.0050508042940804, b 2.9817649938868453, cost 6.052362906147483e-05, iteration 283\nm 2.0049654214045383, b 2.9820732532099368, cost 5.849464409545375e-05, iteration 284\nm 2.0048814818965655, b 2.9823763014675815, cost 5.653367851386111e-05, iteration 285\nm 2.004798961370069, b 2.9826742267518536, cost 5.463845204516603e-05, iteration 286\nm 2.004717835837437, b 2.9829671156656477, cost 5.280676086131905e-05, iteration 287\nm 2.004638081716562, b 2.9832550533478517, cost 5.10364750150596e-05, iteration 288\nm 2.0045596758239883, b 2.9835381234980978, cost 4.93255359631504e-05, iteration 289\nm 2.004482595368172, b 2.9838164084010916, cost 4.767195417266154e-05, iteration 290\nm 2.004406817942855, b 2.984089988950531, cost 4.6073806807456196e-05, iteration 291\nm 2.0043323215205553, b 2.9843589446726213, cost 4.452923549226037e-05, iteration 292\nm 2.004259084446158, b 2.9846233537491926, cost 4.303644415169875e-05, iteration 293\nm 2.0041870854306265, b 2.984883293040426, cost 4.1593696921745634e-05, iteration 294\nm 2.0041163035448095, b 2.9851388381071957, cost 4.019931613122875e-05, iteration 295\nm 2.0040467182133606, b 2.9853900632330332, cost 3.8851680350969584e-05, iteration 296\nm 2.003978309208754, b 2.9856370414457216, cost 3.754922250832148e-05, iteration 297\nm 2.003911056645408, b 2.9858798445385233, cost 3.6290428064943247e-05, iteration 298\nm 2.0038449409739023, b 2.9861185430910484, cost 3.5073833255675404e-05, iteration 299\nm 2.003779942975295, b 2.9863532064897726, cost 3.389802338637342e-05, iteration 300\nm 2.0037160437555386, b 2.986583902948207, cost 3.276163118889191e-05, iteration 301\nm 2.0036532247399843, b 2.9868106995267247, cost 3.166333523117067e-05, iteration 302\nm 2.003591467667984, b 2.987033662152057, cost 3.060185838065213e-05, iteration 303\nm 2.003530754587585, b 2.987252855636456, cost 2.957596631916295e-05, iteration 304\nm 2.0034710678503047, b 2.987468343696535, cost 2.858446610762486e-05, iteration 305\nm 2.0034123901060092, b 2.9876801889717903, cost 2.7626204798887063e-05, iteration 306\nm 2.003354704297862, b 2.9878884530428085, cost 2.6700068096999528e-05, iteration 307\nm 2.0032979936573714, b 2.988093196449169, cost 2.5804979061515924e-05, iteration 308\nm 2.003242241699512, b 2.988294478707041, cost 2.493989685517287e-05, iteration 309\nm 2.0031874322179366, b 2.988492358326483, cost 2.4103815533580215e-05, iteration 310\nm 2.0031335492802613, b 2.988686892828454, cost 2.3295762875472415e-05, iteration 311\nm 2.0030805772234377, b 2.98887813876153, cost 2.2514799252184216e-05, iteration 312\nm 2.0030285006491972, b 2.9890661517183457, cost 2.1760016535018917e-05, iteration 313\nm 2.0029773044195767, b 2.989250986351752, cost 2.1030537039250442e-05, iteration 314\nm 2.002926973652517, b 2.9894326963907036, cost 2.032551250351652e-05, iteration 315\nm 2.0028774937175373, b 2.989611334655878, cost 1.9644123103442462e-05, iteration 316\nm 2.0028288502314826, b 2.989786953075029, cost 1.898557649832854e-05, iteration 317\nm 2.002781029054343, b 2.9899596026980815, cost 1.8349106909778214e-05, iteration 318\nm 2.0027340162851415, b 2.9901293337119705, cost 1.7733974231230625e-05, iteration 319\nm 2.002687798257895, b 2.990296195455231, cost 1.7139463167356884e-05, iteration 320\nm 2.002642361537641, b 2.9904602364323396, cost 1.6564882402259655e-05, iteration 321\nm 2.002597692916534, b 2.9906215043278133, cost 1.60095637956432e-05, iteration 322\nm 2.0025537794100026, b 2.9907800460200717, cost 1.5472861605806873e-05, iteration 323\nm 2.002510608252978, b 2.990935907595064, cost 1.4954151738827869e-05, iteration 324\nm 2.002468166896183, b 2.991089134359664, cost 1.445283102279757e-05, iteration 325\nm 2.0024264430024825, b 2.991239770854843, cost 1.3968316506459802e-05, iteration 326\nm 2.0023854244432986, b 2.9913878608686137, cost 1.3500044781323599e-05, iteration 327\nm 2.002345099295086, b 2.991533447448763, cost 1.3047471326512552e-05, iteration 328\nm 2.0023054558358626, b 2.991676572915361, cost 1.26100698755995e-05, iteration 329\nm 2.0022664825418053, b 2.991817278873066, cost 1.2187331804618788e-05, iteration 330\nm 2.0022281680838994, b 2.9919556062232178, cost 1.1778765540647888e-05, iteration 331\nm 2.0021905013246446, b 2.9920915951757263, cost 1.1383895990180191e-05, iteration 332\nm 2.0021534713148177, b 2.9922252852607603, cost 1.1002263986661902e-05, iteration 333\nm 2.00211706729029, b 2.992356715340239, cost 1.0633425756580058e-05, iteration 334\nm 2.0020812786688995, b 2.992485923619128, cost 1.0276952403412037e-05, iteration 335\nm 2.0020460950473717, b 2.9926129476565455, cost 9.932429408901351e-06, iteration 336\nm 2.002011506198299, b 2.9927378243766793, cost 9.599456151029557e-06, iteration 337\nm 2.0019775020671666, b 2.992860590079522, cost 9.277645438180883e-06, iteration 338\nm 2.001944072769427, b 2.9929812804514198, cost 8.966623058886961e-06, iteration 339\nm 2.0019112085876314, b 2.9930999305754495, cost 8.666027346692225e-06, iteration 340\nm 2.001878899968602, b 2.993216574941615, cost 8.375508759582018e-06, iteration 341\nm 2.0018471375206555, b 2.993331247456873, cost 8.094729473551503e-06, iteration 342\nm 2.0018159120108723, b 2.9934439814549894, cost 7.823362989738142e-06, iteration 343\nm 2.0017852143624157, b 2.993554809706229, cost 7.561093754795127e-06, iteration 344\nm 2.0017550356518896, b 2.993663764426881, cost 7.307616793928649e-06, iteration 345\nm 2.0017253671067468, b 2.9937708772886262, cost 7.062637356274208e-06, iteration 346\nm 2.0016962001027374, b 2.9938761794277395, cost 6.82587057214774e-06, iteration 347\nm 2.0016675261614045, b 2.9939797014541445, cost 6.597041121805301e-06, iteration 348\nm 2.001639336947616, b 2.9940814734603087, cost 6.3758829152678295e-06, iteration 349\nm 2.001611624267146, b 2.994181525029993, cost 6.162138782921593e-06, iteration 350\nm 2.0015843800642874, b 2.9942798852468497, cost 5.955560176465032e-06, iteration 351\nm 2.0015575964195165, b 2.9943765827028788, cost 5.755906879899972e-06, iteration 352\nm 2.0015312655471846, b 2.994471645506736, cost 5.562946730180943e-06, iteration 353\nm 2.0015053797932607, b 2.994565101291907, cost 5.37645534727524e-06, iteration 354\nm 2.0014799316331016, b 2.994656977224738, cost 5.196215873214411e-06, iteration 355\nm 2.0014549136692685, b 2.994747300012334, cost 5.022018719963531e-06, iteration 356\nm 2.001430318629373, b 2.99483609591032, cost 4.85366132567027e-06, iteration 357\nm 2.001406139363967, b 2.994923390730476, cost 4.690947919145713e-06, iteration 358\nm 2.0013823688444607, b 2.9950092098482384, cost 4.533689292192039e-06, iteration 359\nm 2.0013590001610826, b 2.9950935782100765, cost 4.381702579613439e-06, iteration 360\nm 2.001336026520869, b 2.995176520340744, cost 4.234811046545413e-06, iteration 361\nm 2.00131344124569, b 2.995258060350409, cost 4.09284388296629e-06, iteration 362\nm 2.0012912377703085, b 2.995338221941661, cost 3.955636005058435e-06, iteration 363\nm 2.001269409640471, b 2.9954170284164023, cost 3.823027863251998e-06, iteration 364\nm 2.001247950511032, b 2.9954945026826207, cost 3.6948652566878756e-06, iteration 365\nm 2.0012268541441105, b 2.995570667261049, cost 3.5709991539172554e-06, iteration 366\nm 2.001206114407274, b 2.995645544291711, cost 3.451285519601106e-06, iteration 367\nm 2.0011857252717595, b 2.9957191555403577, cost 3.3355851470134313e-06, iteration 368\nm 2.001165680810717, b 2.995791522404794, cost 3.223763496177528e-06, iteration 369\nm 2.00114597519749, b 2.9958626659211, cost 3.1156905374144083e-06, iteration 370\nm 2.0011266027039207, b 2.9959326067697427, cost 3.0112406001376504e-06, iteration 371\nm 2.001107557698685, b 2.996001365281592, cost 2.9102922267253358e-06, iteration 372\nm 2.001088834645654, b 2.9960689614438274, cost 2.8127280312811814e-06, iteration 373\nm 2.0010704281022864, b 2.9961354149057486, cost 2.7184345631362976e-06, iteration 374\nm 2.0010523327180465, b 2.996200744984488, cost 2.6273021749219444e-06, iteration 375\nm 2.0010345432328487, b 2.9962649706706252, cost 2.5392248950754956e-06, iteration 376\nm 2.0010170544755277, b 2.996328110633708, cost 2.4541003046093913e-06, iteration 377\nm 2.0009998613623345, b 2.996390183227679, cost 2.371829418008042e-06, iteration 378\nm 2.000982958895463, b 2.9964512064962108, cost 2.292316568139323e-06, iteration 379\nm 2.0009663421615906, b 2.996511198177951, cost 2.215469294994737e-06, iteration 380\nm 2.0009500063304557, b 2.9965701757116787, cost 2.141198238188458e-06, iteration 381\nm 2.000933946653451, b 2.9966281562413744, cost 2.0694170330300297e-06, iteration 382\nm 2.0009181584622424, b 2.9966851566212016, cost 2.0000422101131872e-06, iteration 383\nm 2.0009026371674152, b 2.9967411934204087, cost 1.9329930982423333e-06, iteration 384\nm 2.000887378257136, b 2.9967962829281434, cost 1.8681917306345287e-06, iteration 385\nm 2.0008723772958437, b 2.9968504411581884, cost 1.8055627542516115e-06, iteration 386\nm 2.000857629922959, b 2.9969036838536165, cost 1.7450333421789989e-06, iteration 387\nm 2.000843131851619, b 2.9969560264913673, cost 1.686533108940892e-06, iteration 388\nm 2.000828878867428, b 2.997007484286745, cost 1.6299940286534194e-06, iteration 389\nm 2.000814866827234, b 2.997058072197842, cost 1.5753503559252941e-06, iteration 390\nm 2.000801091657924, b 2.9971078049298874, cost 1.5225385493966499e-06, iteration 391\nm 2.0007875493552416, b 2.9971566969395216, cost 1.4714971978646103e-06, iteration 392\nm 2.0007742359826195, b 2.997204762438997, cost 1.4221669488648965e-06, iteration 393\nm 2.000761147670039, b 2.9972520154003113, cost 1.3744904396535516e-06, iteration 394\nm 2.0007482806129024, b 2.9972984695592686, cost 1.328412230510546e-06, iteration 395\nm 2.000735631070929, b 2.997344138419471, cost 1.2838787402658308e-06, iteration 396\nm 2.000723195367066, b 2.997389035256245, cost 1.2408381839964914e-06, iteration 397\nm 2.00071096988642, b 2.9974331731205006, cost 1.1992405128127549e-06, iteration 398\nm 2.000698951075208, b 2.9974765648425246, cost 1.1590373556511853e-06, iteration 399\nm 2.0006871354397218, b 2.9975192230357095, cost 1.1201819630364643e-06, iteration 400\nm 2.000675519545315, b 2.997561160100222, cost 1.0826291527128802e-06, iteration 401\nm 2.0006641000154017, b 2.9976023882266056, cost 1.0463352571101619e-06, iteration 402\nm 2.0006528735304783, b 2.9976429193993246, cost 1.0112580725629653e-06, iteration 403\nm 2.0006418368271546, b 2.9976827654002487, cost 9.773568102324307e-07, iteration 404\nm 2.00063098669721, b 2.9977219378120776, cost 9.445920486806256e-07, iteration 405\nm 2.000620319986656, b 2.997760448021707, cost 9.129256880287831e-07, iteration 406\nm 2.0006098335948224, b 2.9977983072235395, cost 8.823209056522382e-07, iteration 407\nm 2.0005995244734556, b 2.9978355264227385, cost 8.527421133598955e-07, iteration 408\nm 2.0005893896258327, b 2.997872116438428, cost 8.241549160170468e-07, iteration 409\nm 2.000579426105888, b 2.9979080879068354, cost 7.965260715444415e-07, iteration 410\nm 2.0005696310173606, b 2.997943451284385, cost 7.698234522647023e-07, iteration 411\nm 2.0005600015129485, b 2.9979782168507385, cost 7.440160075466379e-07, iteration 412\nm 2.0005505347934838, b 2.9980123947117803, cost 7.19073727693696e-07, iteration 413\nm 2.0005412281071178, b 2.998045994802557, cost 6.949676090489643e-07, iteration 414\nm 2.000532078748521, b 2.998079026890166, cost 6.716696202715433e-07, iteration 415\nm 2.000523084058098, b 2.998111500576593, cost 6.491526697393401e-07, iteration 416\nm 2.0005142414212123, b 2.998143425301504, cost 6.273905740424706e-07, iteration 417\nm 2.0005055482674274, b 2.99817481034499, cost 6.063580275424435e-07, iteration 418\nm 2.0004970020697606, b 2.998205664830263, cost 5.860305729438251e-07, iteration 419\nm 2.000488600343945, b 2.9982359977263084, cost 5.663845728510913e-07, iteration 420\nm 2.0004803406477127, b 2.998265817850494, cost 5.473971822872235e-07, iteration 421\nm 2.0004722205800807, b 2.998295133871131, cost 5.290463221273682e-07, iteration 422\nm 2.000464237780653, b 2.9983239543099933, cost 5.113106534211992e-07, iteration 423\nm 2.0004563899289365, b 2.998352287544798, cost 4.941695525845435e-07, iteration 424\nm 2.000448674743667, b 2.998380141811637, cost 4.77603087413727e-07, iteration 425\nm 2.000441089982142, b 2.9984075252073734, cost 4.615919939099072e-07, iteration 426\nm 2.000433633439574, b 2.9984344456919936, cost 4.461176538779225e-07, iteration 427\nm 2.0004263029484446, b 2.998460911090922, cost 4.3116207327472085e-07, iteration 428\nm 2.000419096377879, b 2.998486929097296, cost 4.167078612893068e-07, iteration 429\nm 2.0004120116330233, b 2.998512507274203, cost 4.0273821011569484e-07, iteration 430\nm 2.000405046654437, b 2.9985376530568755, cost 3.892368754096778e-07, iteration 431\nm 2.0003981994174937, b 2.998562373754857, cost 3.7618815740197184e-07, iteration 432\nm 2.0003914679317933, b 2.9985866765541234, cost 3.635768826385896e-07, iteration 433\nm 2.0003848502405837, b 2.998610568519173, cost 3.513883863387471e-07, iteration 434\nm 2.0003783444201897, b 2.9986340565950806, cost 3.396084953412681e-07, iteration 435\nm 2.0003719485794567, b 2.998657147609516, cost 3.2822351162342296e-07, iteration 436\nm 2.0003656608591998, b 2.9986798482747274, cost 3.1722019637466914e-07, iteration 437\nm 2.000359479431662, b 2.9987021651894947, cost 3.065857545982072e-07, iteration 438\nm 2.0003534024999854, b 2.9987241048410467, cost 2.9630782023581875e-07, iteration 439\nm 2.000347428297687, b 2.998745673606946, cost 2.863744417862908e-07, iteration 440\nm 2.0003415550881476, b 2.9987668777569456, cost 2.7677406841127414e-07, iteration 441\nm 2.0003357811641016, b 2.998787723454807, cost 2.674955364972882e-07, iteration 442\nm 2.000330104847148, b 2.9988082167600956, cost 2.5852805668066325e-07, iteration 443\nm 2.0003245244872563, b 2.9988283636299418, cost 2.4986120129795766e-07, iteration 444\nm 2.0003190384622918, b 2.9988481699207705, cost 2.4148489226178747e-07, iteration 445\nm 2.0003136451775396, b 2.998867641390006, cost 2.3338938933969108e-07, iteration 446\nm 2.000308343065244, b 2.9988867836977438, cost 2.2556527883011955e-07, iteration 447\nm 2.0003031305841525, b 2.998905602408396, cost 2.180034626152422e-07, iteration 448\nm 2.000298006219066, b 2.998924102992311, cost 2.1069514758109576e-07, iteration 449\nm 2.0002929684804003, b 2.99894229082736, cost 2.036318353927086e-07, iteration 450\nm 2.000288015903752, b 2.998960171200504, cost 1.9680531261116642e-07, iteration 451\nm 2.000283147049474, b 2.998977749309328, cost 1.9020764114430228e-07, iteration 452\nm 2.0002783605022545, b 2.998995030263553, cost 1.8383114901549317e-07, iteration 453\nm 2.0002736548707087, b 2.9990120190865213, cost 1.77668421442709e-07, iteration 454\nm 2.0002690287869727, b 2.9990287207166566, cost 1.717122922145975e-07, iteration 455\nm 2.0002644809063055, b 2.9990451400088993, cost 1.6595583535992449e-07, iteration 456\nm 2.0002600099067, b 2.9990612817361177, cost 1.6039235709222388e-07, iteration 457\nm 2.0002556144884944, b 2.999077150590496, cost 1.5501538802659654e-07, iteration 458\nm 2.0002512933740015, b 2.999092751184898, cost 1.4981867565660968e-07, iteration 459\nm 2.0002470453071304, b 2.9991080880542076, cost 1.4479617708467426e-07, iteration 460\nm 2.0002428690530247, b 2.999123165656648, cost 1.3994205199341067e-07, iteration 461\nm 2.0002387633977032, b 2.9991379883750757, cost 1.3525065585564157e-07, iteration 462\nm 2.0002347271477072, b 2.999152560518257, cost 1.3071653337114823e-07, iteration 463\nm 2.000230759129752, b 2.9991668863221195, cost 1.2633441212130913e-07, iteration 464\nm 2.000226858190389, b 2.9991809699509817, cost 1.22099196440329e-07, iteration 465\nm 2.000223023195667, b 2.999194815498767, cost 1.1800596148720155e-07, iteration 466\nm 2.0002192530308034, b 2.99920842699019, cost 1.140499475228059e-07, iteration 467\nm 2.0002155465998626, b 2.99922180838193, cost 1.1022655437075701e-07, iteration 468\nm 2.0002119028254346, b 2.9992349635637785, cost 1.065313360705455e-07, iteration 469\nm 2.000208320648323, b 2.9992478963597704, cost 1.0295999570921204e-07, iteration 470\nm 2.0002047990272365, b 2.9992606105292965, cost 9.950838042043909e-08, iteration 471\nm 2.0002013369384875, b 2.999273109768196, cost 9.61724765597035e-08, iteration 472\nm 2.0001979333756923, b 2.99928539770983, cost 9.294840503374454e-08, iteration 473\nm 2.000194587349482, b 2.9992974779261394, cost 8.983241679335354e-08, iteration 474\nm 2.00019129788721, b 2.999309353928681, cost 8.682088847004657e-08, iteration 475\nm 2.000188064032675, b 2.99932102916965, cost 8.391031816596976e-08, iteration 476\nm 2.0001848848458375, b 2.9993325070428827, cost 8.109732137964312e-08, iteration 477\nm 2.0001817594025515, b 2.999343790884843, cost 7.83786270711003e-08, iteration 478\nm 2.0001786867942917, b 2.9993548839755935, cost 7.57510738583689e-08, iteration 479\nm 2.0001756661278924, b 2.9993657895397465, cost 7.321160634119732e-08, iteration 480\nm 2.000172696525287, b 2.999376510747404, cost 7.075727154813042e-08, iteration 481\nm 2.00016977712325, b 2.9993870507150775, cost 6.83852155027598e-08, iteration 482\nm 2.000166907073152, b 2.9993974125065948, cost 6.609267990444405e-08, iteration 483\nm 2.0001640855407063, b 2.9994075991339897, cost 6.38769989224823e-08, iteration 484\nm 2.000161311705732, b 2.999417613558379, cost 6.17355960932464e-08, iteration 485\nm 2.000158584761913, b 2.9994274586908216, cost 5.966598132812731e-08, iteration 486\nm 2.0001559039165624, b 2.9994371373931656, cost 5.7665748014830105e-08, iteration 487\nm 2.000153268390394, b 2.9994466524788805, cost 5.573257021977345e-08, iteration 488\nm 2.0001506774172966, b 2.9994560067138742, cost 5.3864199984076227e-08, iteration 489\nm 2.000148130244108, b 2.999465202817298, cost 5.2058464708970585e-08, iteration 490\nm 2.0001456261304, b 2.999474243462336, cost 5.0313264629494776e-08, iteration 491\nm 2.000143164348259, b 2.9994831312769823, cost 4.862657037294957e-08, iteration 492\nm 2.0001407441820795, b 2.9994918688448062, cost 4.699642059883101e-08, iteration 493\nm 2.00013836492835, b 2.9995004587057017, cost 4.542091971893986e-08, iteration 494\nm 2.0001360258954546, b 2.9995089033566265, cost 4.389823569126882e-08, iteration 495\nm 2.0001337264034666, b 2.9995172052523276, cost 4.242659789264004e-08, iteration 496\nm 2.000131465783955, b 2.9995253668060546, cost 4.100429505622438e-08, iteration 497\nm 2.000129243379788, b 2.999533390390263, cost 3.962967328457475e-08, iteration 498\nm 2.000127058544942, b 2.9995412783373, cost 3.830113412451502e-08, iteration 499\nm 2.0001249106443155, b 2.9995490329400876, cost 3.7017132710156866e-08, iteration 500\nm 2.000122799053542, b 2.9995566564527842, cost 3.5776175964473736e-08, iteration 501\nm 2.0001207231588105, b 2.9995641510914433, cost 3.4576820864567087e-08, iteration 502\nm 2.000118682356686, b 2.9995715190346557, cost 3.3417672763283335e-08, iteration 503\nm 2.000116676053935, b 2.9995787624241843, cost 3.229738376748904e-08, iteration 504\nm 2.0001147036673514, b 2.9995858833655853, cost 3.1214651169901664e-08, iteration 505\nm 2.0001127646235894, b 2.9995928839288215, cost 3.016821593584658e-08, iteration 506\nm 2.000110858358995, b 2.9995997661488625, cost 2.9156861237819137e-08, iteration 507\nm 2.000108984319442, b 2.9996065320262777, cost 2.817941104162898e-08, iteration 508\nm 2.0001071419601724, b 2.9996131835278175, cost 2.7234728737391134e-08, iteration 509\nm 2.0001053307456376, b 2.999619722586984, cost 2.6321715819770587e-08, iteration 510\nm 2.000103550149341, b 2.9996261511045943, cost 2.5439310608683315e-08, iteration 511\nm 2.0001017996536876, b 2.9996324709493325, cost 2.4586487016206082e-08, iteration 512\nm 2.0001000787498313, b 2.999638683958293, cost 2.3762253352316448e-08, iteration 513\nm 2.0000983869375286, b 2.999644791937514, cost 2.2965651173012342e-08, iteration 514\nm 2.000096723724993, b 2.9996507966625043, cost 2.219575416442355e-08, iteration 515\nm 2.000095088628749, b 2.999656699878756, cost 2.145166706632386e-08, iteration 516\nm 2.0000934811734985, b 2.9996625033022557, cost 2.0732524631486106e-08, iteration 517\nm 2.0000919008919733, b 2.9996682086199806, cost 2.003749061864842e-08, iteration 518\nm 2.000090347324808, b 2.9996738174903905, cost 1.93657568203685e-08, iteration 519\nm 2.000088820020402, b 2.999679331543909, cost 1.8716542124422874e-08, iteration 520\nm 2.000087318534787, b 2.9996847523833976, cost 1.808909160337981e-08, iteration 521\nm 2.000085842431502, b 2.9996900815846215, cost 1.748267563861547e-08, iteration 522\nm 2.0000843912814634, b 2.9996953206967087, cost 1.6896589070632873e-08, iteration 523\nm 2.000082964662841, b 2.999700471242599, cost 1.6330150379929762e-08, iteration 524\nm 2.0000815621609362, b 2.9997055347194865, cost 1.5782700894064276e-08, iteration 525\nm 2.0000801833680604, b 2.999710512599257, cost 1.5253604021850355e-08, iteration 526\nm 2.0000788278834167, b 2.9997154063289133, cost 1.4742244512878204e-08, iteration 527\nm 2.000077495312984, b 2.999720217330997, cost 1.4248027742547621e-08, iteration 528\nm 2.0000761852694025, b 2.9997249470040024, cost 1.3770379020334507e-08, iteration 529\nm 2.0000748973718587, b 2.9997295967227813, cost 1.330874292140984e-08, iteration 530\nm 2.0000736312459795, b 2.9997341678389455, cost 1.286258264105366e-08, iteration 531\nm 2.0000723865237187, b 2.999738661681257, cost 1.2431379370579304e-08, iteration 532\nm 2.0000711628432506, b 2.9997430795560156, cost 1.2014631692934305e-08, iteration 533\nm 2.0000699598488705, b 2.999747422747439, cost 1.161185500130658e-08, iteration 534\nm 2.000068777190881, b 2.999751692518034, cost 1.1222580934471042e-08, iteration 535\nm 2.0000676145255016, b 2.999755890108966, cost 1.0846356832490916e-08, iteration 536\nm 2.00006647151476, b 2.999760016740419, cost 1.0482745210149268e-08, iteration 537\nm 2.000065347826398, b 2.9997640736119493, cost 1.0131323248760569e-08, iteration 538\nm 2.0000642431337754, b 2.999768061902835, cost 9.791682303888903e-09, iteration 539\nm 2.0000631571157723, b 2.999771982772419, cost 9.463427430695598e-09, iteration 540\nm 2.000062089456697, b 2.9997758373604455, cost 9.146176924072041e-09, iteration 541\nm 2.0000610398461967, b 2.999779626787392, cost 8.83956187527404e-09, iteration 542\nm 2.0000600079791626, b 2.9997833521547936, cost 8.543225742870437e-09, iteration 543\nm 2.000058993555646, b 2.9997870145455656, cost 8.256823938140152e-09, iteration 544\nm 2.0000579962807654, b 2.9997906150243154, cost 7.980023424079358e-09, iteration 545\nm 2.0000570158646287, b 2.9997941546376543, cost 7.712502328475834e-09, iteration 546\nm 2.000056052022241, b 2.9997976344145, cost 7.45394956960665e-09, iteration 547\nm 2.0000551044734256, b 2.999801055366378, cost 7.204064494110706e-09, iteration 548\nm 2.000054172942744, b 2.9998044184877126, cost 6.962556528043388e-09, iteration 549\nm 2.000053257159412, b 2.9998077247561183, cost 6.729144838422725e-09, iteration 550\nm 2.000052356857223, b 2.999810975132683, cost 6.503558006898574e-09, iteration 551\nm 2.000051471774473, b 2.999814170562248, cost 6.285533714145493e-09, iteration 552\nm 2.0000506016538786, b 2.9998173119736813, cost 6.074818434735612e-09, iteration 553\nm 2.000049746242508, b 2.9998204002801496, cost 5.8711671424135896e-09, iteration 554\nm 2.0000489052917043, b 2.999823436379382, cost 5.67434302512389e-09, iteration 555\nm 2.000048078557015, b 2.9998264211539327, cost 5.484117209620641e-09, iteration 556\nm 2.000047265798119, b 2.999829355471435, cost 5.300268495716046e-09, iteration 557\nm 2.0000464667787576, b 2.9998322401848556, cost 5.122583098039516e-09, iteration 558\nm 2.0000456812666676, b 2.999835076132743, cost 4.95085439886598e-09, iteration 559\nm 2.00004490903351, b 2.9998378641394683, cost 4.7848827064767044e-09, iteration 560\nm 2.0000441498548085, b 2.9998406050154682, cost 4.624475023950436e-09, iteration 561\nm 2.0000434035098786, b 2.999843299557479, cost 4.4694448242298635e-09, iteration 562\nm 2.0000426697817684, b 2.9998459485487676, cost 4.31961183347053e-09, iteration 563\nm 2.000041948457193, b 2.9998485527593606, cost 4.174801821158383e-09, iteration 564\nm 2.000041239326473, b 2.9998511129462666, cost 4.034846397711926e-09, iteration 565\nm 2.0000405421834726, b 2.999853629853698, cost 3.899582818745409e-09, iteration 566\nm 2.000039856825543, b 2.9998561042132863, cost 3.768853795448162e-09, iteration 567\nm 2.0000391830534596, b 2.999858536744295, cost 3.6425073121103014e-09, iteration 568\nm 2.0000385206713656, b 2.9998609281538275, cost 3.520396449138145e-09, iteration 569\nm 2.0000378694867154, b 2.999863279137035, cost 3.4023792121824403e-09, iteration 570\nm 2.000037229310218, b 2.999865590377317, cost 3.2883183672462488e-09, iteration 571\nm 2.000036599955783, b 2.99986786254652, cost 3.178081280761494e-09, iteration 572\nm 2.0000359812404658, b 2.9998700963051332, cost 3.0715397656458358e-09, iteration 573\nm 2.0000353729844136, b 2.9998722923024803, cost 2.968569932169328e-09, iteration 574\nm 2.0000347750108145, b 2.999874451176908, cost 2.869052043759658e-09, iteration 575\nm 2.0000341871458462, b 2.999876573555973, cost 2.7728703779795896e-09, iteration 576\nm 2.0000336092186233, b 2.9998786600566216, cost 2.6799130917582522e-09, iteration 577\nm 2.0000330410611515, b 2.9998807112853725, cost 2.590072091529344e-09, iteration 578\nm 2.000032482508273, b 2.99988272783849, cost 2.503242907318318e-09, iteration 579\nm 2.0000319333976257, b 2.999884710302159, cost 2.419324571563951e-09, iteration 580\nm 2.0000313935695897, b 2.9998866592526556, cost 2.3382195013768625e-09, iteration 581\nm 2.000030862867244, b 2.999888575256513, cost 2.2598333852794838e-09, iteration 582\nm 2.0000303411363216, b 2.9998904588706883, cost 2.1840750734350684e-09, iteration 583\nm 2.0000298282251614, b 2.999892310642723, cost 2.1108564717349175e-09, iteration 584\nm 2.000029323984667, b 2.9998941311109024, cost 2.040092439356859e-09, iteration 585\nm 2.000028828268263, b 2.9998959208044123, cost 1.971700689688378e-09, iteration 586\nm 2.00002834093185, b 2.999897680243492, cost 1.905601694656687e-09, iteration 587\nm 2.0000278618337672, b 2.9998994099395877, cost 1.8417185922887424e-09, iteration 588\nm 2.000027390834747, b 2.999901110395499, cost 1.7799770973963241e-09, iteration 589\nm 2.000026927797876, b 2.999902782105525, cost 1.720305415006832e-09, iteration 590\nm 2.0000264725885546, b 2.9999044255556093, cost 1.662634157042905e-09, iteration 591\nm 2.0000260250744617, b 2.999906041223482, cost 1.6068962615810037e-09, iteration 592\nm 2.000025585125509, b 2.999907629578795, cost 1.5530269149084542e-09, iteration 593\nm 2.0000251526138104, b 2.999909191083263, cost 1.5009634760165467e-09, iteration 594\nm 2.00002472741364, b 2.9999107261907936, cost 1.4506454040130714e-09, iteration 595\nm 2.000024309401398, b 2.9999122353476224, cost 1.402014187360453e-09, iteration 596\nm 2.0000238984555736, b 2.9999137189924405, cost 1.3550132762944366e-09, iteration 597\nm 2.000023494456711, b 2.9999151775565243, cost 1.30958801662969e-09, iteration 598\nm 2.0000230972873716, b 2.9999166114638585, cost 1.265685586481153e-09, iteration 599\nm 2.0000227068321053, b 2.9999180211312613, cost 1.2232549347260957e-09, iteration 600\nm 2.000022322977411, b 2.9999194069685036, cost 1.1822467217215566e-09, iteration 601\nm 2.000021945611708, b 2.9999207693784298, cost 1.1426132618637828e-09, iteration 602\nm 2.0000215746253, b 2.999922108757074, cost 1.1043084680606733e-09, iteration 603\nm 2.0000212099103476, b 2.9999234254937766, cost 1.067287798365951e-09, iteration 604\nm 2.000020851360832, b 2.9999247199712946, cost 1.0315082040208165e-09, iteration 605\nm 2.0000204988725283, b 2.9999259925659154, cost 9.969280793630289e-10, iteration 606\nm 2.0000201523429726, b 2.9999272436475652, cost 9.635072135623713e-10, iteration 607\nm 2.000019811671433, b 2.9999284735799168, cost 9.312067437805838e-10, iteration 608\nm 2.0000194767588817, b 2.999929682720495, cost 8.999891100628852e-10, iteration 609\nm 2.000019147507963, b 2.9999308714207813, cost 8.698180115458756e-10, iteration 610\nm 2.0000188238229692, b 2.999932040026314, cost 8.406583643912264e-10, iteration 611\nm 2.000018505609809, b 2.999933188876792, cost 8.124762608185818e-10, iteration 612\nm 2.0000181927759817, b 2.99993431830617, cost 7.85238929835247e-10, iteration 613\nm 2.000017885230551, b 2.9999354286427584, cost 7.589146989629108e-10, iteration 614\nm 2.0000175828841176, b 2.9999365202093173, cost 7.334729576404032e-10, iteration 615\nm 2.000017285648793, b 2.9999375933231502, cost 7.088841214061714e-10, iteration 616\nm 2.0000169934381757, b 2.9999386482961974, cost 6.851195975792313e-10, iteration 617\nm 2.0000167061673233, b 2.999939685435125, cost 6.621517520280322e-10, iteration 618\nm 2.0000164237527303, b 2.9999407050414155, cost 6.399538770737996e-10, iteration 619\nm 2.0000161461123023, b 2.999941707411455, cost 6.185001603067773e-10, iteration 620\nm 2.0000158731653332, b 2.9999426928366186, cost 5.977656546816693e-10, iteration 621\nm 2.000015604832481, b 2.9999436616033566, cost 5.777262495015307e-10, iteration 622\nm 2.000015341035745, b 2.999944613993277, cost 5.583586422835158e-10, iteration 623\nm 2.0000150816984426, b 2.9999455502832255, cost 5.396403118019512e-10, iteration 624\nm 2.000014826745188, b 2.9999464707453702, cost 5.215494917903966e-10, iteration 625\nm 2.0000145761018704, b 2.999947375647277, cost 5.040651456802989e-10, iteration 626\nm 2.0000143296956296, b 2.999948265251988, cost 4.871669421386148e-10, iteration 627\nm 2.0000140874548404, b 2.9999491398181006, cost 4.708352314311233e-10, iteration 628\nm 2.0000138493090858, b 2.9999499995998384, cost 4.5505102250203393e-10, iteration 629\nm 2.0000136151891397, b 2.999950844847129, cost 4.397959609975658e-10, iteration 630\nm 2.0000133850269473, b 2.999951675805674, cost 4.250523078737088e-10, iteration 631\nm 2.0000131587556034, b 2.9999524927170222, cost 4.1080291873794877e-10, iteration 632\nm 2.000012936309333, b 2.999953295818639, cost 3.9703122395431053e-10, iteration 633\nm 2.000012717623475, b 2.9999540853439752, cost 3.8372120939969167e-10, iteration 634\nm 2.00001250263446, b 2.9999548615225353, cost 3.7085739773650256e-10, iteration 635\nm 2.000012291279793, b 2.9999556245799437, cost 3.5842483056476314e-10, iteration 636\nm 2.0000120834980377, b 2.9999563747380114, cost 3.4640905088604284e-10, iteration 637\nm 2.0000118792287926, b 2.999957112214799, cost 3.347960863733322e-10, iteration 638\nm 2.000011678412681, b 2.9999578372246813, cost 3.2357243310933865e-10, iteration 639\nm 2.0000114809913274, b 2.999958549978409, cost 3.1272503991257533e-10, iteration 640\nm 2.0000112869073448, b 2.99995925068317, cost 3.02241293075782e-10, iteration 641\nm 2.000011096104315, b 2.9999599395426495, cost 2.921090017767861e-10, iteration 642\nm 2.000010908526774, b 2.99996061675709, cost 2.8231638388374695e-10, iteration 643\nm 2.000010724120196, b 2.999961282523349, cost 2.728520522427942e-10, iteration 644\nm 2.000010542830976, b 2.9999619370349553, cost 2.637050013959183e-10, iteration 645\nm 2.000010364606416, b 2.999962580482167, cost 2.54864594901676e-10, iteration 646\nm 2.0000101893947084, b 2.9999632130520255, cost 2.4632055284927057e-10, iteration 647\nm 2.0000100171449215, b 2.9999638349284106, cost 2.380629399894421e-10, iteration 648\nm 2.000009847806985, b 2.999964446292093, cost 2.3008215408481164e-10, iteration 649\nm 2.0000096813316737, b 2.999965047320788, cost 2.2236891482628946e-10, iteration 650\nm 2.000009517670596, b 2.9999656381892073, cost 2.149142530338792e-10, iteration 651\nm 2.0000093567761783, b 2.999966219069108, cost 2.0770950019189887e-10, iteration 652\nm 2.0000091986016497, b 2.9999667901293434, cost 2.0074627838896963e-10, iteration 653\nm 2.000009043101032, b 2.999967351535914, cost 1.9401649059229706e-10, iteration 654\nm 2.0000088902291226, b 2.999967903452013, cost 1.875123111644275e-10, iteration 655\nm 2.000008739941484, b 2.999968446038075, cost 1.8122617687011795e-10, iteration 656\nm 2.0000085921944293, b 2.9999689794518223, cost 1.7515077798618563e-10, iteration 657\nm 2.0000084469450106, b 2.999969503848311, cost 1.6927904985106842e-10, iteration 658\nm 2.0000083041510055, b 2.999970019379977, cost 1.6360416466050117e-10, iteration 659\nm 2.000008163770906, b 2.9999705261966776, cost 1.581195234264064e-10, iteration 660\nm 2.0000080257639063, b 2.9999710244457383, cost 1.5281874849887374e-10, iteration 661\nm 2.000007890089888, b 2.9999715142719925, cost 1.4769567593980757e-10, iteration 662\nm 2.0000077567094134, b 2.999971995817827, cost 1.4274434848843807e-10, iteration 663\nm 2.0000076255837107, b 2.99997246922322, cost 1.3795900858904403e-10, iteration 664\nm 2.000007496674663, b 2.999972934625785, cost 1.3333409169086087e-10, iteration 665\nm 2.0000073699447984, b 2.9999733921608076, cost 1.288642198186094e-10, iteration 666\nm 2.0000072453572777, b 2.999973841961287, cost 1.2454419524876714e-10, iteration 667\nm 2.000007122875886, b 2.9999742841579753, cost 1.2036899452562e-10, iteration 668\nm 2.000007002465019, b 2.999974718879412, cost 1.1633376259671527e-10, iteration 669\nm 2.0000068840896743, b 2.999975146251965, cost 1.1243380717751598e-10, iteration 670\nm 2.000006767715443, b 2.999975566399866, cost 1.0866459327640935e-10, iteration 671\nm 2.0000066533084957, b 2.9999759794452467, cost 1.0502173792328122e-10, iteration 672\nm 2.0000065408355763, b 2.999976385508173, cost 1.0150100509467826e-10, iteration 673\nm 2.0000064302639906, b 2.999976784706683, cost 9.809830078389597e-11, iteration 674\nm 2.0000063215615964, b 2.9999771771568176, cost 9.480966821375748e-11, iteration 675\nm 2.0000062146967954, b 2.999977562972657, cost 9.163128327638914e-11, iteration 676\nm 2.0000061096385235, b 2.9999779422663524, cost 8.85594500254539e-11, iteration 677\nm 2.0000060063562417, b 2.99997831514816, cost 8.55905964502264e-11, iteration 678\nm 2.0000059048199277, b 2.9999786817264718, cost 8.272127026179532e-11, iteration 679\nm 2.000005805000066, b 2.9999790421078463, cost 7.994813494162406e-11, iteration 680\nm 2.00000570686764, b 2.999979396397042, cost 7.726796578267207e-11, iteration 681\nm 2.0000056103941235, b 2.999979744697046, cost 7.467764621326884e-11, iteration 682\nm 2.0000055155514738, b 2.9999800871091042, cost 7.217416412226742e-11, iteration 683\nm 2.000005422312121, b 2.999980423732752, cost 6.975460838028835e-11, iteration 684\nm 2.0000053306489627, b 2.9999807546658404, cost 6.741616546117413e-11, iteration 685\nm 2.000005240535352, b 2.9999810800045674, cost 6.515611615300902e-11, iteration 686\nm 2.0000051519450945, b 2.999981399843505, cost 6.297183239404791e-11, iteration 687\nm 2.0000050648524392, b 2.999981714275626, cost 6.086077423591382e-11, iteration 688\nm 2.000004979232068, b 2.9999820233923318, cost 5.882048686476721e-11, iteration 689\nm 2.0000048950590936, b 2.9999823272834782, cost 5.684859777527923e-11, iteration 690\nm 2.0000048123090473, b 2.9999826260374025, cost 5.494281400076067e-11, iteration 691\nm 2.0000047309578743, b 2.999982919740948, cost 5.310091943564308e-11, iteration 692\nm 2.0000046509819285, b 2.999983208479491, cost 5.132077226211889e-11, iteration 693\nm 2.00000457235796, b 2.999983492336963, cost 4.960030246637414e-11, iteration 694\nm 2.000004495063115, b 2.999983771395879, cost 4.7937509449327554e-11, iteration 695\nm 2.0000044190749247, b 2.9999840457373566, cost 4.633045964885095e-11, iteration 696\nm 2.0000043443713005, b 2.9999843154411434, cost 4.477728433935939e-11, iteration 697\nm 2.000004270930527, b 2.999984580585639, cost 4.3276177450206996e-11, iteration 698\nm 2.0000041987312556, b 2.999984841247917, cost 4.182539343647251e-11, iteration 699\nm 2.0000041277524994, b 2.9999850975037488, cost 4.042324528458939e-11, iteration 700\nm 2.0000040579736256, b 2.999985349427624, cost 3.9068102540890046e-11, iteration 701\nm 2.0000039893743504, b 2.9999855970927736, cost 3.7758389398623166e-11, iteration 702\nm 2.000003921934733, b 2.999985840571191, cost 3.6492582870728905e-11, iteration 703\nm 2.0000038556351694, b 2.999986079933652, cost 3.5269211055035074e-11, iteration 704\nm 2.0000037904563874, b 2.999986315249736, cost 3.4086851371645735e-11, iteration 705\nm 2.0000037263794406, b 2.999986546587846, cost 3.2944128930639126e-11, iteration 706\nm 2.0000036633857023, b 2.999986774015229, cost 3.183971494694807e-11, iteration 707\nm 2.000003601456861, b 2.9999869975979956, cost 3.07723251699722e-11, iteration 708\nm 2.0000035405749155, b 2.9999872174011375, cost 2.974071841272534e-11, iteration 709\nm 2.000003480722167, b 2.999987433488549, cost 2.874369507376329e-11, iteration 710\nm 2.0000034218812184, b 2.999987645923044, cost 2.7780095797015704e-11, iteration 711\nm 2.0000033640349653, b 2.999987854766374, cost 2.684880007748933e-11, iteration 712\nm 2.0000033071665912, b 2.999988060079247, cost 2.5948724970371792e-11, iteration 713\nm 2.000003251259567, b 2.999988261921345, cost 2.507882385038602e-11, iteration 714\nm 2.00000319629764, b 2.9999884603513407, cost 2.4238085164732487e-11, iteration 715\nm 2.0000031422648337, b 2.9999886554269146, cost 2.3425531268166648e-11, iteration 716\nm 2.000003089145442, b 2.999988847204773, cost 2.2640217311078108e-11, iteration 717\nm 2.000003036924024, b 2.999989035740663, cost 2.1881230095676326e-11, iteration 718\nm 2.0000029855853985, b 2.9999892210893897, cost 2.1147687054804785e-11, iteration 719\nm 2.0000029351146433, b 2.999989403304831, cost 2.0438735199707142e-11, iteration 720\nm 2.0000028854970866, b 2.999989582439955, cost 1.9753550137108215e-11, iteration 721\nm 2.0000028367183047, b 2.9999897585468336, cost 1.9091335119694867e-11, iteration 722\nm 2.000002788764119, b 2.9999899316766587, cost 1.8451320094444343e-11, iteration 723\nm 2.0000027416205906, b 2.999990101879757, cost 1.7832760836589962e-11, iteration 724\nm 2.0000026952740138, b 2.999990269205604, cost 1.7234938064485164e-11, iteration 725\nm 2.0000026497109173, b 2.9999904337028394, cost 1.665715661620695e-11, iteration 726\nm 2.0000026049180564, b 2.99999059541928, cost 1.6098744622883955e-11, iteration 727\nm 2.0000025608824106, b 2.999990754401935, cost 1.5559052748297336e-11, iteration 728\nm 2.000002517591178, b 2.999990910697018, cost 1.5037453423557878e-11, iteration 729\nm 2.000002475031777, b 2.9999910643499628, cost 1.4533340117440791e-11, iteration 730\nm 2.0000024331918334, b 2.9999912154054336, cost 1.4046126623555387e-11, iteration 731\nm 2.0000023920591867, b 2.9999913639073403, cost 1.3575246404275578e-11, iteration 732\nm 2.0000023516218794, b 2.9999915098988503, cost 1.3120151900518975e-11, iteration 733\nm 2.000002311868157, b 2.9999916534224016, cost 1.2680313917214425e-11, iteration 734\nm 2.000002272786464, b 2.9999917945197145, cost 1.2255220995842453e-11, iteration 735\nm 2.000002234365439, b 2.999991933231804, cost 1.1844378825633916e-11, iteration 736\nm 2.0000021965939148, b 2.9999920695989917, cost 1.144730966266491e-11, iteration 737\nm 2.000002159460911, b 2.999992203660918, cost 1.1063551790039631e-11, iteration 738\nm 2.0000021229556335, b 2.999992335456553, cost 1.069265895655465e-11, iteration 739\nm 2.000002087067471, b 2.999992465024208, cost 1.0334199880751169e-11, iteration 740\nm 2.0000020517859904, b 2.999992592401546, cost 9.98775772778755e-12, iteration 741\nm 2.0000020171009374, b 2.9999927176255943, cost 9.652929653631148e-12, iteration 742\nm 2.0000019830022278, b 2.9999928407327534, cost 9.329326301494666e-12, iteration 743\nm 2.0000019494799512, b 2.9999929617588097, cost 9.016571382771572e-12, iteration 744\nm 2.000001916524362, b 2.9999930807389434, cost 8.714301207897683e-12, iteration 745\nm 2.0000018841258806, b 2.9999931977077403, cost 8.422164296096917e-12, iteration 746\nm 2.0000018522750898, b 2.999993312699202, cost 8.139820937126918e-12, iteration 747\nm 2.0000018209627304, b 2.9999934257467546, cost 7.866942814839176e-12, iteration 748\nm 2.0000017901797005, b 2.99999353688326, cost 7.603212617312757e-12, iteration 749\nm 2.000001759917052, b 2.9999936461410237, cost 7.348323667581906e-12, iteration 750\nm 2.000001730165988, b 2.9999937535518058, cost 7.101979580900475e-12, iteration 751\nm 2.0000017009178594, b 2.9999938591468287, cost 6.863893893836879e-12, iteration 752\nm 2.0000016721641654, b 2.999993962956788, cost 6.633789755598632e-12, iteration 753\nm 2.000001643896547, b 2.9999940650118595, cost 6.4113995936900834e-12, iteration 754\nm 2.0000016161067875, b 2.9999941653417097, cost 6.196464804790306e-12, iteration 755\nm 2.0000015887868083, b 2.9999942639755024, cost 5.988735458150423e-12, iteration 756\nm 2.0000015619286686, b 2.9999943609419097, cost 5.787969998822859e-12, iteration 757\nm 2.00000153552456, b 2.999994456269118, cost 5.593934969619621e-12, iteration 758\nm 2.0000015095668084, b 2.999994549984838, cost 5.406404740308207e-12, iteration 759\nm 2.000001484047868, b 2.9999946421163117, cost 5.225161248104546e-12, iteration 760\nm 2.0000014589603197, b 2.99999473269032, cost 5.049993735868737e-12, iteration 761\nm 2.000001434296872, b 2.999994821733192, cost 4.880698511384026e-12, iteration 762\nm 2.000001410050355, b 2.9999949092708116, cost 4.717078715584633e-12, iteration 763\nm 2.0000013862137207, b 2.999994995328624, cost 4.558944082705912e-12, iteration 764\nm 2.000001362780041, b 2.9999950799316455, cost 4.406110730920679e-12, iteration 765\nm 2.0000013397425023, b 2.999995163104469, cost 4.258400943053837e-12, iteration 766\nm 2.000001317094409, b 2.9999952448712714, cost 4.115642955024992e-12, iteration 767\nm 2.0000012948291777, b 2.9999953252558216, cost 3.977670763065372e-12, iteration 768\nm 2.000001272940336, b 2.9999954042814863, cost 3.844323933460952e-12, iteration 769\nm 2.0000012514215206, b 2.999995481971237, cost 3.715447399487389e-12, iteration 770\nm 2.000001230266477, b 2.999995558347657, cost 3.5908913035833672e-12, iteration 771\nm 2.000001209469055, b 2.9999956334329485, cost 3.470510808255752e-12, iteration 772\nm 2.0000011890232097, b 2.999995707248937, cost 3.354165929665918e-12, iteration 773\nm 2.000001168922998, b 2.9999957798170804, cost 3.2417213798080293e-12, iteration 774\nm 2.000001149162576, b 2.9999958511584732, cost 3.133046404269629e-12, iteration 775\nm 2.0000011297362, b 2.999995921293853, cost 3.028014629540134e-12, iteration 776\nm 2.000001110638224, b 2.9999959902436077, cost 2.9265039270774435e-12, iteration 777\nm 2.0000010918630955, b 2.99999605802778, cost 2.8283962528031247e-12, iteration 778\nm 2.0000010734053566, b 2.9999961246660733, cost 2.7335775253709973e-12, iteration 779\nm 2.0000010552596423, b 2.999996190177859, cost 2.641937485659802e-12, iteration 780\nm 2.0000010374206783, b 2.9999962545821806, cost 2.5533695747275993e-12, iteration 781\nm 2.0000010198832783, b 2.999996317897759, cost 2.467770800422585e-12, iteration 782\nm 2.0000010026423443, b 2.9999963801429996, cost 2.385041626784717e-12, iteration 783\nm 2.000000985692866, b 2.9999964413359965, cost 2.3050858522492e-12, iteration 784\nm 2.0000009690299145, b 2.999996501494537, cost 2.2278105028723486e-12, iteration 785\nm 2.0000009526486475, b 2.999996560636109, cost 2.1531257228473724e-12, iteration 786\nm 2.0000009365443026, b 2.999996618777904, cost 2.080944661488985e-12, iteration 787\nm 2.0000009207121985, b 2.999996675936823, cost 2.011183387318859e-12, iteration 788\nm 2.000000905147733, b 2.999996732129481, cost 1.9437607800220985e-12, iteration 789\nm 2.000000889846383, b 2.999996787372213, cost 1.8785984391450676e-12, iteration 790\nm 2.000000874803698, b 2.9999968416810767, cost 1.8156205906873925e-12, iteration 791\nm 2.0000008600153074, b 2.9999968950718596, cost 1.7547540009769119e-12, iteration 792\nm 2.0000008454769116, b 2.9999969475600814, cost 1.6959278938663423e-12, iteration 793\nm 2.0000008311842845, b 2.9999969991609996, cost 1.639073864103742e-12, iteration 794\nm 2.0000008171332713, b 2.9999970498896142, cost 1.584125800162014e-12, iteration 795\nm 2.0000008033197885, b 2.9999970997606713, cost 1.531019807716224e-12, iteration 796\nm 2.0000007897398198, b 2.9999971487886676, cost 1.479694131441695e-12, iteration 797\nm 2.0000007763894176, b 2.999997196987855, cost 1.4300890898724016e-12, iteration 798\nm 2.0000007632647017, b 2.9999972443722442, cost 1.3821469991497743e-12, iteration 799\nm 2.0000007503618566, b 2.9999972909556094, cost 1.3358121129312504e-12, iteration 800\nm 2.0000007376771314, b 2.9999973367514916, cost 1.2910305501873252e-12, iteration 801\nm 2.0000007252068395, b 2.999997381773203, cost 1.2477502371883342e-12, iteration 802\nm 2.0000007129473554, b 2.999997426033831, cost 1.2059208478602702e-12, iteration 803\nm 2.0000007008951153, b 2.999997469546241, cost 1.165493739755016e-12, iteration 804\nm 2.000000689046616, b 2.999997512323082, cost 1.12642190497183e-12, iteration 805\nm 2.000000677398414, b 2.999997554376789, cost 1.0886599080593472e-12, iteration 806\nm 2.000000665947122, b 2.9999975957195857, cost 1.0521638386626042e-12, iteration 807\nm 2.000000654689412, b 2.9999976363634904, cost 1.016891257663486e-12, iteration 808\nm 2.0000006436220117, b 2.9999976763203176, cost 9.828011496074702e-13, iteration 809\nm 2.0000006327417035, b 2.9999977156016824, cost 9.49853872676285e-13, iteration 810\nm 2.000000622045325, b 2.9999977542190033, cost 9.180111156572275e-13, iteration 811\nm 2.0000006115297664, b 2.9999977921835055, cost 8.872358496389461e-13, iteration 812\nm 2.0000006011919718, b 2.999997829506225, cost 8.574922888732727e-13, iteration 813\nm 2.0000005910289356, b 2.999997866198011, cost 8.287458485114212e-13, iteration 814\nm 2.000000581037703, b 2.9999979022695293, cost 8.009630976958001e-13, iteration 815\nm 2.000000571215371, b 2.9999979377312656, cost 7.741117319137631e-13, iteration 816\nm 2.0000005615590832, b 2.9999979725935275, cost 7.481605285496285e-13, iteration 817\nm 2.0000005520660333, b 2.99999800686645, cost 7.230793083937528e-13, iteration 818\nm 2.0000005427334617, b 2.9999980405599946, cost 6.988389072279313e-13, iteration 819\nm 2.0000005335586555, b 2.999998073683957, cost 6.754111367634285e-13, iteration 820\nm 2.0000005245389474, b 2.9999981062479644, cost 6.527687564657364e-13, iteration 821\nm 2.000000515671716, b 2.9999981382614838, cost 6.308854357374561e-13, iteration 822\nm 2.0000005069543834, b 2.9999981697338205, cost 6.097357283025824e-13, iteration 823\nm 2.0000004983844155, b 2.9999982006741233, cost 5.892950407767063e-13, iteration 824\nm 2.000000489959321, b 2.999998231091386, cost 5.695396028770961e-13, iteration 825\nm 2.0000004816766523, b 2.999998260994451, cost 5.50446443698769e-13, iteration 826\nm 2.0000004735339996, b 2.99999829039201, cost 5.319933601503779e-13, iteration 827\nm 2.0000004655289967, b 2.9999983192926094, cost 5.141588952594075e-13, iteration 828\nm 2.0000004576593176, b 2.9999983477046492, cost 4.969223107200429e-13, iteration 829\nm 2.0000004499226733, b 2.999998375636389, cost 4.80263562408479e-13, iteration 830\nm 2.000000442316816, b 2.999998403095948, cost 4.641632800504082e-13, iteration 831\nm 2.000000434839534, b 2.9999984300913085, cost 4.48602740234265e-13, iteration 832\nm 2.000000427488654, b 2.9999984566303173, cost 4.335638500099177e-13, iteration 833\nm 2.0000004202620394, b 2.9999984827206894, cost 4.190291217469036e-13, iteration 834\nm 2.000000413157589, b 2.9999985083700085, cost 4.049816530329145e-13, iteration 835\nm 2.0000004061732386, b 2.999998533585731, cost 3.914051092413266e-13, iteration 836\nm 2.0000003993069573, b 2.9999985583751863, cost 3.7828370378161e-13, iteration 837\nm 2.0000003925567484, b 2.9999985827455804, cost 3.656021776813065e-13, iteration 838\nm 2.000000385920651, b 2.999998606703998, cost 3.53345785599507e-13, iteration 839\nm 2.000000379396736, b 2.999998630257403, cost 3.415002757809976e-13, iteration 840\nm 2.0000003729831053, b 2.999998653412642, cost 3.300518720681352e-13, iteration 841\nm 2.000000366677897, b 2.999998676176446, cost 3.1898726302058486e-13, iteration 842\nm 2.0000003604792767, b 2.9999986985554323, cost 3.0829358262574004e-13, iteration 843\nm 2.0000003543854428, b 2.999998720556106, cost 2.9795839519066697e-13, iteration 844\nm 2.000000348394624, b 2.9999987421848626, cost 2.8796968309699177e-13, iteration 845\nm 2.000000342505079, b 2.9999987634479894, cost 2.7831583068879833e-13, iteration 846\nm 2.0000003367150954, b 2.999998784351667, cost 2.6898561299552364e-13, iteration 847\nm 2.0000003310229904, b 2.9999988049019715, cost 2.5996818039649836e-13, iteration 848\nm 2.0000003254271093, b 2.999998825104877, cost 2.5125304669820297e-13, iteration 849\nm 2.0000003199258263, b 2.9999988449662567, cost 2.4283007745865203e-13, iteration 850\nm 2.0000003145175405, b 2.999998864491883, cost 2.346894789754408e-13, iteration 851\nm 2.000000309200681, b 2.9999988836874327, cost 2.2682178440692053e-13, iteration 852\nm 2.000000303973702, b 2.999998902558485, cost 2.1921784515801816e-13, iteration 853\nm 2.0000002988350842, b 2.999998921110526, cost 2.118688196449222e-13, iteration 854\nm 2.0000002937833337, b 2.999998939348948, cost 2.047661609660394e-13, iteration 855\nm 2.000000288816982, b 2.999998957279053, cost 1.9790161143633973e-13, iteration 856\nm 2.0000002839345856, b 2.9999989749060534, cost 1.9126718803865134e-13, iteration 857\nm 2.000000279134725, b 2.9999989922350725, cost 1.8485517532985508e-13, iteration 858\nm 2.000000274416006, b 2.9999990092711477, cost 1.7865811877800756e-13, iteration 859\nm 2.000000269777055, b 2.999999026019231, cost 1.7266881091168247e-13, iteration 860\nm 2.000000265216525, b 2.9999990424841916, cost 1.668802875136484e-13, iteration 861\nm 2.0000002607330902, b 2.9999990586708147, cost 1.612858183948942e-13, iteration 862\nm 2.0000002563254466, b 2.999999074583806, cost 1.5587889713893073e-13, iteration 863\nm 2.0000002519923132, b 2.9999990902277913, cost 1.5065323658331129e-13, iteration 864\nm 2.0000002477324315, b 2.9999991056073183, cost 1.4560276057617523e-13, iteration 865\nm 2.0000002435445614, b 2.999999120726857, cost 1.4072159561006822e-13, iteration 866\nm 2.0000002394274867, b 2.999999135590803, cost 1.3600406607216806e-13, iteration 867\nm 2.0000002353800106, b 2.9999991502034766, cost 1.3144468622286128e-13, iteration 868\nm 2.0000002314009557, b 2.9999991645691257, cost 1.2703815440934454e-13, iteration 869\nm 2.0000002274891666, b 2.9999991786919264, cost 1.2277934672988224e-13, iteration 870\nm 2.0000002236435055, b 2.999999192575984, cost 1.1866331041361673e-13, iteration 871\nm 2.000000219862854, b 2.999999206225334, cost 1.146852598956438e-13, iteration 872\nm 2.0000002161461143, b 2.9999992196439442, cost 1.1084056830964835e-13, iteration 873\nm 2.000000212492205, b 2.9999992328357155, cost 1.0712476589969412e-13, iteration 874\nm 2.000000208900065, b 2.9999992458044824, cost 1.035335318411679e-13, iteration 875\nm 2.0000002053686488, b 2.9999992585540145, cost 1.000626892397716e-13, iteration 876\nm 2.0000002018969307, b 2.9999992710880186, cost 9.670820273469178e-14, iteration 877\nm 2.0000001984839013, b 2.9999992834101374, cost 9.346617174250673e-14, iteration 878\nm 2.000000195128569, b 2.999999295523953, cost 9.033282588470707e-14, iteration 879\nm 2.0000001918299573, b 2.9999993074329874, cost 8.730452188680395e-14, iteration 880\nm 2.000000188587108, b 2.9999993191407013, cost 8.437773847769604e-14, iteration 881\nm 2.000000185399079, b 2.999999330650499, cost 8.15490718592595e-14, iteration 882\nm 2.0000001822649427, b 2.9999993419657254, cost 7.881523303898552e-14, iteration 883\nm 2.000000179183788, b 2.99999935308967, cost 7.617304314209119e-14, iteration 884\nm 2.00000017615472, b 2.9999993640255664, cost 7.361942966707699e-14, iteration 885\nm 2.000000173176858, b 2.9999993747765936, cost 7.115142305123352e-14, iteration 886\nm 2.000000170249336, b 2.999999385345877, cost 6.876615356274003e-14, iteration 887\nm 2.0000001673713035, b 2.9999993957364883, cost 6.64608474711055e-14, iteration 888\nm 2.000000164541923, b 2.9999994059514483, cost 6.423282421023085e-14, iteration 889\nm 2.000000161760373, b 2.9999994159937264, cost 6.20794927173344e-14, iteration 890\nm 2.0000001590258445, b 2.9999994258662417, cost 5.999834927665978e-14, iteration 891\nm 2.0000001563375434, b 2.999999435571864, cost 5.798697363479369e-14, iteration 892\nm 2.0000001536946863, b 2.999999445113415, cost 5.60430271370714e-14, iteration 893\nm 2.000000151096507, b 2.9999994544936675, cost 5.416424935069066e-14, iteration 894\nm 2.0000001485422487, b 2.9999994637153486, cost 5.234845502213933e-14, iteration 895\nm 2.0000001460311707, b 2.9999994727811394, cost 5.0593533356423743e-14, iteration 896\nm 2.000000143562541, b 2.999999481693674, cost 4.889744343015315e-14, iteration 897\nm 2.0000001411356436, b 2.9999994904555445, cost 4.7258212895503015e-14, iteration 898\nm 2.0000001387497726, b 2.999999499069297, cost 4.5673935814349034e-14, iteration 899\nm 2.0000001364042337, b 2.9999995075374355, cost 4.414276977286478e-14, iteration 900\nm 2.000000134098346, b 2.999999515862422, cost 4.2662934152756603e-14, iteration 901\nm 2.0000001318314387, b 2.9999995240466757, cost 4.1232708347313e-14, iteration 902\nm 2.0000001296028533, b 2.9999995320925765, cost 3.985042940526768e-14, iteration 903\nm 2.0000001274119414, b 2.999999540002463, cost 3.851448950374e-14, iteration 904\nm 2.000000125258067, b 2.9999995477786343, cost 3.722333575509332e-14, iteration 905\nm 2.000000123140603, b 2.9999995554233507, cost 3.597546621938791e-14, iteration 906\nm 2.0000001210589344, b 2.9999995629388345, cost 3.476943017405634e-14, iteration 907\nm 2.0000001190124563, b 2.9999995703272706, cost 3.360382509201315e-14, iteration 908\nm 2.0000001170005732, b 2.9999995775908066, cost 3.247729547478972e-14, iteration 909\nm 2.0000001150227007, b 2.999999584731554, cost 3.138853155316017e-14, iteration 910\nm 2.0000001130782636, b 2.9999995917515885, cost 3.033626725983189e-14, iteration 911\nm 2.000000111166697, b 2.9999995986529506, cost 2.931927887123225e-14, iteration 912\nm 2.0000001092874453, b 2.9999996054376465, cost 2.8336383608092867e-14, iteration 913\nm 2.000000107439962, b 2.9999996121076484, cost 2.7386439221387535e-14, iteration 914\nm 2.0000001056237093, b 2.999999618664895, cost 2.6468340212361254e-14, iteration 915\nm 2.0000001038381603, b 2.999999625111293, cost 2.5581019740181996e-14, iteration 916\nm 2.000000102082796, b 2.9999996314487154, cost 2.4723445333669195e-14, iteration 917\nm 2.0000001003571057, b 2.999999637679005, cost 2.38946203167441e-14, iteration 918\nm 2.0000000986605877, b 2.9999996438039727, cost 2.3093580759285032e-14, iteration 919\nm 2.0000000969927494, b 2.999999649825399, cost 2.2319395123905658e-14, iteration 920\nm 2.0000000953531054, b 2.9999996557450346, cost 2.157116292676566e-14, iteration 921\nm 2.0000000937411793, b 2.9999996615645994, cost 2.08480145988068e-14, iteration 922\nm 2.0000000921565024, b 2.9999996672857856, cost 2.0149109021022472e-14, iteration 923\nm 2.000000090598614, b 2.9999996729102563, cost 1.9473633349150317e-14, iteration 924\nm 2.0000000890670617, b 2.9999996784396465, cost 1.8820802154499093e-14, iteration 925\nm 2.0000000875614, b 2.9999996838755636, cost 1.8189856451786847e-14, iteration 926\nm 2.000000086081191, b 2.9999996892195875, cost 1.7580062301795868e-14, iteration 927\nm 2.000000084626005, b 2.9999996944732716, cost 1.6990711020980018e-14, iteration 928\nm 2.000000083195418, b 2.999999699638143, cost 1.6421117088770343e-14, iteration 929\nm 2.0000000817890156, b 2.999999704715703, cost 1.587061802231469e-14, iteration 930\nm 2.0000000804063878, b 2.9999997097074282, cost 1.533857373693351e-14, iteration 931\nm 2.000000079047133, b 2.999999714614769, cost 1.482436582493905e-14, iteration 932\nm 2.0000000777108564, b 2.999999719439152, cost 1.4327395982391942e-14, iteration 933\nm 2.0000000763971686, b 2.99999972418198, cost 1.3847086478450843e-14, iteration 934\nm 2.0000000751056892, b 2.9999997288446316, cost 1.3382878927636959e-14, iteration 935\nm 2.0000000738360417, b 2.999999733428462, cost 1.2934233314599523e-14, iteration 936\nm 2.0000000725878575, b 2.9999997379348033, cost 1.25006279763218e-14, iteration 937\nm 2.0000000713607733, b 2.999999742364966, cost 1.2081558883854148e-14, iteration 938\nm 2.000000070154433, b 2.999999746720237, cost 1.1676538512101057e-14, iteration 939\nm 2.0000000689684856, b 2.9999997510018837, cost 1.1285095991995437e-14, iteration 940\nm 2.000000067802586, b 2.9999997552111495, cost 1.0906776152488483e-14, iteration 941\nm 2.0000000666563964, b 2.9999997593492584, cost 1.054113900766254e-14, iteration 942\nm 2.000000065529583, b 2.9999997634174136, cost 1.0187759522072448e-14, iteration 943\nm 2.0000000644218177, b 2.999999767416797, cost 9.846226650405525e-15, iteration 944\nm 2.000000063332779, b 2.999999771348572, cost 9.51614327141886e-15, iteration 945\nm 2.0000000622621505, b 2.999999775213881, cost 9.197125557657086e-15, iteration 946\nm 2.0000000612096205, b 2.9999997790138475, cost 8.888802439960888e-15, iteration 947\nm 2.0000000601748837, b 2.9999997827495766, cost 8.59081564324814e-15, iteration 948\nm 2.000000059157639, b 2.999999786422154, cost 8.302818427973982e-15, iteration 949\nm 2.00000005815759, b 2.9999997900326467, cost 8.02447592498508e-15, iteration 950\nm 2.000000057174447, b 2.9999997935821052, cost 7.755464669841367e-15, iteration 951\nm 2.0000000562079236, b 2.9999997970715606, cost 7.495471615199351e-15, iteration 952\nm 2.0000000552577397, b 2.9999998005020276, cost 7.244194546597296e-15, iteration 953\nm 2.0000000543236176, b 2.999999803874503, cost 7.001341279521903e-15, iteration 954\nm 2.0000000534052877, b 2.999999807189967, cost 6.76662941558432e-15, iteration 955\nm 2.000000052502481, b 2.9999998104493844, cost 6.539785943448039e-15, iteration 956\nm 2.0000000516149363, b 2.9999998136537016, cost 6.320547139155734e-15, iteration 957\nm 2.000000050742396, b 2.9999998168038506, cost 6.1086580670245e-15, iteration 958\nm 2.0000000498846053, b 2.9999998199007467, cost 5.903872357156558e-15, iteration 959\nm 2.0000000490413155, b 2.9999998229452904, cost 5.705951768003548e-15, iteration 960\nm 2.0000000482122817, b 2.999999825938367, cost 5.5146664039926065e-15, iteration 961\nm 2.0000000473972617, b 2.9999998288808456, cost 5.329793532714747e-15, iteration 962\nm 2.0000000465960204, b 2.9999998317735828, cost 5.151118323105729e-15, iteration 963\nm 2.000000045808323, b 2.999999834617418, cost 4.9784329882561506e-15, iteration 964\nm 2.0000000450339424, b 2.9999998374131795, cost 4.811536784601224e-15, iteration 965\nm 2.0000000442726518, b 2.9999998401616788, cost 4.650235522341167e-15, iteration 966\nm 2.0000000435242313, b 2.9999998428637156, cost 4.494341762737905e-15, iteration 967\nm 2.000000042788462, b 2.9999998455200747, cost 4.343674118922934e-15, iteration 968\nm 2.0000000420651314, b 2.9999998481315284, cost 4.198057456493035e-15, iteration 969\nm 2.000000041354028, b 2.999999850698836, cost 4.057322425206821e-15, iteration 970\nm 2.000000040654946, b 2.999999853222744, cost 3.9213053459982765e-15, iteration 971\nm 2.0000000399676825, b 2.9999998557039858, cost 3.7898481100537784e-15, iteration 972\nm 2.0000000392920363, b 2.9999998581432825, cost 3.662797837453182e-15, iteration 973\nm 2.0000000386278116, b 2.9999998605413434, cost 3.5400067750076325e-15, iteration 974\nm 2.0000000379748157, b 2.9999998628988656, cost 3.4213321239081604e-15, iteration 975\nm 2.0000000373328586, b 2.9999998652165343, cost 3.3066358912535145e-15, iteration 976\nm 2.000000036701754, b 2.9999998674950232, cost 3.19578475120463e-15, iteration 977\nm 2.0000000360813175, b 2.9999998697349946, cost 3.0886497374873576e-15, iteration 978\nm 2.00000003547137, b 2.9999998719370997, cost 2.9851063174497633e-15, iteration 979\nm 2.000000034871733, b 2.9999998741019787, cost 2.8850340561132495e-15, iteration 980\nm 2.000000034282233, b 2.999999876230261, cost 2.7883165945746132e-15, iteration 981\nm 2.0000000337026984, b 2.999999878322565, cost 2.694841494003567e-15, iteration 982\nm 2.000000033132961, b 2.9999998803794994, cost 2.604500054368438e-15, iteration 983\nm 2.000000032572854, b 2.9999998824016614, cost 2.5171871837386626e-15, iteration 984\nm 2.000000032022216, b 2.999999884389639, cost 2.4328013292137906e-15, iteration 985\nm 2.0000000314808863, b 2.9999998863440105, cost 2.3512445003655312e-15, iteration 986\nm 2.000000030948708, b 2.9999998882653434, cost 2.2724217151034114e-15, iteration 987\nm 2.000000030425526, b 2.9999998901541964, cost 2.1962413822031206e-15, iteration 988\nm 2.0000000299111886, b 2.999999892011119, cost 2.122614926403684e-15, iteration 989\nm 2.000000029405545, b 2.9999998938366503, cost 2.051456696955813e-15, iteration 990\nm 2.0000000289084503, b 2.999999895631322, cost 1.9826840214252576e-15, iteration 991\nm 2.0000000284197585, b 2.9999998973956545, cost 1.9162168339910374e-15, iteration 992\nm 2.0000000279393277, b 2.9999998991301613, cost 1.8519778476029844e-15, iteration 993\nm 2.000000027467019, b 2.999999900835347, cost 1.7898924279063252e-15, iteration 994\nm 2.0000000270026943, b 2.9999999025117066, cost 1.729888367908757e-15, iteration 995\nm 2.0000000265462186, b 2.9999999041597274, cost 1.671895823340033e-15, iteration 996\nm 2.00000002609746, b 2.999999905779889, cost 1.6158474478226761e-15, iteration 997\nm 2.0000000256562873, b 2.999999907372662, cost 1.5616780553619768e-15, iteration 998\nm 2.0000000252225725, b 2.9999999089385097, cost 1.509324582997523e-15, iteration 999\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nplt.scatter(x, y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x19ec5009c60&gt;\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/","title":"Simple Logistic Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#simple-logistic-regression-binary-classification","title":"Simple Logistic Regression (Binary Classification)","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#description","title":"Description:","text":"<p>Logistic regression is a statistical technique used to analyze the relationship between a dependent variable and one or more independent variables. In binary classification problems, the dependent variable is dichotomous, meaning it has only two possible outcomes. For example, whether a customer will purchase a product or not, whether a person has a certain disease or not, or whether an email is spam or not.</p> <p>The goal of logistic regression is to find the best model that predicts the probability of an event occurring (in binary classification, the probability of the positive outcome). The logistic regression model uses a function called the sigmoid function, which maps any input value to a value between 0 and 1. This allows the model to output a probability score that can be interpreted as the likelihood of the positive outcome.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#02-reading-the-csv-file-with-pandas","title":"02. Reading the CSV File with Pandas","text":"<pre><code># Defining the path of the csv\ncsv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\HR_comma_sep.csv\"\n</code></pre> <pre><code># Reading the csv file with pandas library\ndf = pd.read_csv(csv_path)\ndf.head()\n</code></pre> satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years Department salary 0 0.38 0.53 2 157 3 0 1 0 sales low 1 0.80 0.86 5 262 6 0 1 0 sales medium 2 0.11 0.88 7 272 4 0 1 0 sales medium 3 0.72 0.87 5 223 5 0 1 0 sales low 4 0.37 0.52 2 159 3 0 1 0 sales low <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(14999, 10)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#03-data-cleaning","title":"03. Data Cleaning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#301-checking-the-dataframe-for-null-values","title":"3.01 Checking the DataFrame for Null Values","text":"<pre><code># Checking the dataframe if there is any null values\ndf.isnull().sum()\n</code></pre> <pre><code>satisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nDepartment               0\nsalary                   0\ndtype: int64\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#302-checking-the-datatypes-for-all-the-columns","title":"3.02 Checking the datatypes for all the Columns","text":"<pre><code>df.dtypes\n</code></pre> <pre><code>satisfaction_level       float64\nlast_evaluation          float64\nnumber_project             int64\naverage_montly_hours       int64\ntime_spend_company         int64\nWork_accident              int64\nleft                       int64\npromotion_last_5years      int64\nDepartment                object\nsalary                    object\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#04-data-exploration-and-visualization","title":"04. Data Exploration and Visualization","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#401-plotting-the-correlation-matrix-of-the-dataframe","title":"4.01 Plotting the Correlation Matrix of the DataFrame","text":"<p>Plotting the Correlation Matrix to check which variables have direct and clear impact on employee retention (i.e. whether they leave the company or continue to work) </p> <pre><code># Dropping the columns with qualitative data\ndata = df.loc[:, \"satisfaction_level\":\"promotion_last_5years\"]\n# Checking the shape of the data\ndata.shape\n</code></pre> <pre><code>(14999, 8)\n</code></pre> <pre><code>data.head()\n</code></pre> satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years 0 0.38 0.53 2 157 3 0 1 0 1 0.80 0.86 5 262 6 0 1 0 2 0.11 0.88 7 272 4 0 1 0 3 0.72 0.87 5 223 5 0 1 0 4 0.37 0.52 2 159 3 0 1 0 <pre><code># Plotting the correlation matrix\nsns.heatmap(data.corr(), annot=True)\nplt.title(\"Correlation Matrix\")\nplt.show()\n</code></pre> <p></p> <p>Interpretaion of Correlation Matrix:  A correlation matrix is a table that displays the pairwise correlations between all the variables in a dataset. Correlation coefficients range from -1 to 1, where a value of -1 indicates a perfect negative correlation, a value of 1 indicates a perfect positive correlation, and a value of 0 indicates no correlation.</p> <p>In the given dataset, the correlation matrix shows that the 'satisfaction_level' column has the maximum negative correlation coefficient of -0.39 with the \"left' column. This means that the satisfaction level of employees has a larger impact on their retention.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#402-plotting-bar-charts-between-employee-salaries-and-retention","title":"4.02 Plotting Bar Charts between Employee Salaries and Retention","text":"<p>The pandas crosstab() function is used to compute a cross-tabulation table of two or more factors. It is a useful tool in data analysis for examining the relationship between two or more variables, especially when one or both of the variables are categorical.</p> <p>Here, we are calculating the total number of employees in each salary bracket who stayed or left by summing across the rows using pandas crosstab() function.</p> <pre><code># Calculating the total number of employees who stayed or left in each salary bracket\nsalary_left_data = pd.crosstab(df.salary, df.left)\nsalary_left_data\n</code></pre> left 0 1 salary high 1155 82 low 5144 2172 medium 5129 1317 <pre><code># Renaming the column names\n# 0 = Retained\n# 1 = Left\ncolumnNames = {\n    0: \"Retained\",\n    1: \"Left\"\n}\nsalary_left_data.rename(columns=columnNames, inplace=True)\nsalary_left_data\n</code></pre> left Retained Left salary high 1155 82 low 5144 2172 medium 5129 1317 <pre><code># Plotting the bar chart\nsalary_left_data.plot(kind=\"bar\", color=[\"green\", \"red\"])\nplt.title(\"Bar Chart Showing Impact of Salaries on Retention\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Salary\")\n</code></pre> <pre><code>Text(0.5, 0, 'Salary')\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#403-plotting-bar-charts-between-departments-and-retention","title":"4.03 Plotting Bar Charts between Departments and Retention","text":"<p>Here, we are calculating the total number of employees in each department who stayed or left by summing across the rows using pandas crosstab() function.</p> <pre><code># Calculating the total number of employees who stayed or left in each department\ndept_left_data = pd.crosstab(df.Department, df.left)\ndept_left_data.head()\n</code></pre> left 0 1 Department IT 954 273 RandD 666 121 accounting 563 204 hr 524 215 management 539 91 <pre><code># Renaming the column names\n# 0 = Retained\n# 1 = Left\ndept_left_data.rename(columns=columnNames, inplace=True)\ndept_left_data.head()\n</code></pre> left Retained Left Department IT 954 273 RandD 666 121 accounting 563 204 hr 524 215 management 539 91 <pre><code># Plotting the bar chart\ndept_left_data.plot(kind=\"bar\", color=[\"green\", \"red\"])\nplt.title(\"Bar Chart Showing Correlaton between Departments and Employee Retention\")\nplt.xlabel(\"Department\")\nplt.ylabel(\"Count\")\nplt.show()\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#404-plotting-the-scatterplot-between-satisfaction_level-and-left","title":"4.04 Plotting the Scatterplot between 'satisfaction_level' and 'left'","text":"<pre><code># Extracting a small sample from the dataframe to represent the scatterplot\ntest_df = df.sample(50, random_state=75)\n</code></pre> <pre><code># Plotting the Scatterplot between 'satisfaction_level' and 'left'\nsns.scatterplot(x=test_df[\"satisfaction_level\"], y=test_df[\"left\"], color=\"red\")\nplt.title(\"Scatterplot between 'satisfaction_level' and 'left'\")\nplt.ylabel(\"Employee Retention (0=Retained, 1=Left)\")\nplt.xlabel(\"Satisfaction Level\")\nplt.grid()\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#05-dividing-the-data-into-training-and-testing-set","title":"05. Dividing the Data into Training and Testing Set","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#501-defifining-the-dependent-and-independent-variable","title":"5.01 Defifining the Dependent and Independent Variable","text":"<pre><code># Dependent Variable (y) = \"left\"\n# Independent Variable (x) = \"satisfaction_level\"\nx = df[[\"satisfaction_level\"]]\ny = df[[\"left\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#502-splitting-the-data-into-training-and-testing-set","title":"5.02 Splitting the Data into Training and Testing Set","text":"<pre><code># Importing the train_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Training data = 70% and Testing data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#06-instantiating-the-simple-logistic-regression-model","title":"06. Instantiating the Simple Logistic Regression Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#601-importing-logistic-regression-model-from-sklearn-library","title":"6.01 Importing Logistic Regression Model from sklearn Library","text":"<pre><code># Importing the Logistic Regression Model from sklearn library\nfrom sklearn.linear_model import LogisticRegression\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#602-generating-a-logistic-regression-object","title":"6.02 Generating a Logistic Regression Object","text":"<pre><code># Creating a linear regression object\nlog_reg = LogisticRegression()\n# Feeding the training data to the model\nlog_reg.fit(x_train, y_train)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#603-getting-the-coefficients-of-the-linear-regression-model","title":"6.03 Getting the Coefficients of the Linear Regression Model","text":"<pre><code># Getting the slope of the model\nlog_reg.coef_\n</code></pre> <pre><code>array([[-3.84563398]])\n</code></pre> <pre><code># Getting the y-intercept of the model\nlog_reg.intercept_\n</code></pre> <pre><code>array([0.97025215])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#07-validation-of-the-model","title":"07. Validation of the Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#701-validating-the-logistic-regression-model","title":"7.01 Validating the Logistic Regression Model","text":"<pre><code># Predicting the left status of the x_test (satisfaction_level) data\ny_predict = log_reg.predict(x_test)\ny_predict\n</code></pre> <pre><code>array([0, 0, 1, ..., 0, 0, 0], dtype=int64)\n</code></pre> <pre><code># Getting the accuracy score of the model\nlog_reg.score(x_test, y_test)\n</code></pre> <pre><code>0.7624444444444445\n</code></pre> <pre><code># Getting the prediction probability of the x_test data\nlog_reg.predict_proba(x_test)\n</code></pre> <pre><code>array([[0.9118948 , 0.0881052 ],\n       [0.62036288, 0.37963712],\n       [0.35762467, 0.64237533],\n       ...,\n       [0.61126493, 0.38873507],\n       [0.67300916, 0.32699084],\n       [0.93368009, 0.06631991]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Untitled/","title":"Untitled","text":"<pre><code>import geopandas as gpd\n</code></pre> <pre><code>\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/","title":"Simple Linear Regression Exercise","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#02-reading-the-csv-with-pandas","title":"02. Reading the CSV with Pandas","text":"<pre><code>df = pd.read_csv(\"GDP_per_capita_usa.csv\")\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 $23,315.08B $70,249 5.95% 1 2020 $21,060.47B $63,531 -2.77% 2 2019 $21,380.98B $65,120 2.29% 3 2018 $20,533.06B $62,823 2.95% 4 2017 $19,477.34B $59,908 2.24%"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#03-data-preprocessing","title":"03. Data Preprocessing","text":"<pre><code># Removing the dollar, comma and percentage symbol from the dataframe\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\"$\", \"\")\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\",\", \"\")\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\"B\", \"\")\ndf[\"Per Capita\"] = df[\"Per Capita\"].str.replace(\"$\", \"\")\ndf[\"Per Capita\"] = df[\"Per Capita\"].str.replace(\",\", \"\")\ndf[\"Growth\"] = df[\"Growth\"].str.replace(\"%\", \"\")\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 23315.08 70249 5.95 1 2020 21060.47 63531 -2.77 2 2019 21380.98 65120 2.29 3 2018 20533.06 62823 2.95 4 2017 19477.34 59908 2.24 <pre><code># Checking the datatypes of the columns\ndf.dtypes\n</code></pre> <pre><code>Year           int64\nGDP           object\nPer Capita    object\nGrowth        object\ndtype: object\n</code></pre> <pre><code># Using a dictionary to change the datatype of columns from object to int and float\ndt_convert = {\n    \"GDP\": float,\n    \"Per Capita\": float,\n}\n</code></pre> <pre><code># Changing the datatypes of the columns\ndf = df.astype(dt_convert)\ndf.dtypes\n</code></pre> <pre><code>Year            int64\nGDP           float64\nPer Capita    float64\nGrowth         object\ndtype: object\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 23315.08 70249.0 5.95 1 2020 21060.47 63531.0 -2.77 2 2019 21380.98 65120.0 2.29 3 2018 20533.06 62823.0 2.95 4 2017 19477.34 59908.0 2.24 <pre><code># Evaluating the general statistics of the dataframe\ndf.describe()\n</code></pre> Year GDP Per Capita count 62.000000 62.000000 62.000000 mean 1990.500000 7935.587903 27417.145161 std 18.041619 6738.805659 20172.070442 min 1960.000000 543.300000 3007.000000 25% 1975.250000 1732.027500 7998.750000 50% 1990.500000 6060.635000 24115.500000 75% 2005.750000 13621.492500 45757.250000 max 2021.000000 23315.080000 70249.000000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#04-create-a-scatterplot-between-year-and-gdp-per-capita","title":"04. Create a Scatterplot between Year and GDP Per Capita","text":"<pre><code># Selecting independent(x) and dependent variable(y)\n# independent variable = Year\n# dependent variable = GDP Per Capita (US $)\nx = df[[\"Year\"]].values\ny = df[[\"Per Capita\"]].values\n</code></pre> <pre><code>plt.plot(x, y)\nplt.title(\"USA GDP Per Capita (US$)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"GDP Per Capita (US$)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#05-creating-a-linear-regression-object","title":"05. Creating a Linear Regression Object","text":"<pre><code>lin_reg = linear_model.LinearRegression()\n</code></pre> <pre><code># Training the Linear Regression model\nlin_reg.fit(x, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#06-accessing-the-slope-and-intercept","title":"06. Accessing the Slope and Intercept","text":"<p>Linear Equation:  y = mx + c where,     y = independent variable     m = slope     x = dependent variable     c = intercept</p> <pre><code>slope = lin_reg.coef_\nprint(\"Slope(m):\", slope)\n</code></pre> <pre><code>Slope(m): [[1099.76930825]]\n</code></pre> <pre><code>intercept = lin_reg.intercept_\nprint(\"intercept(c):\", intercept)\n</code></pre> <pre><code>intercept(c): [-2161673.66291456]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#07-predicting-the-gdp-of-year-2030-2040-and-2050","title":"07. Predicting the GDP of Year 2030, 2040 and 2050","text":"<pre><code>lin_reg.predict([[2030], [2040], [2050]])\n</code></pre> <pre><code>array([[70858.03283725],\n       [81855.72591977],\n       [92853.41900229]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#assessing-the-accuracy-of-the-linear-regression-model","title":"Assessing the Accuracy of the Linear Regression Model","text":"<pre><code># Predicting the GDP of all the years in the dataframe\npredicted_gdp = lin_reg.predict(x)\n</code></pre> <pre><code># Plotting the Regression Line\nplt.plot(x, y)\nplt.plot(x, predicted_gdp, linestyle=\"--\", linewidth=1)\nplt.title(\"USA GDP Per Capita (US$)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"GDP Per Capita (US$)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/","title":"Decision Tree Classifier","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#decision-tree-classifier","title":"Decision Tree Classifier","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#setting-up-the-current-working-directory","title":"Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nprint(os.getcwd())\n</code></pre> <pre><code>D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\07_Decision_Tree\n</code></pre> <pre><code># Changing the location of the current working directory\nfile_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the working directory location\nprint(os.getcwd())\n</code></pre> <pre><code>D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#reading-the-dataset-using-pandas","title":"Reading the Dataset using Pandas","text":"<pre><code># Reading the DailyDelhiClimateTest.csv data\ndataset = \"DailyDelhiClimateTest.csv\"\ndata_path = file_path + \"\\\\\" + dataset\ndf = pd.read_csv(data_path)\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> date meantemp humidity wind_speed meanpressure 0 2017-01-01 15.913043 85.869565 2.743478 59.000000 1 2017-01-02 18.500000 77.222222 2.894444 1018.277778 2 2017-01-03 17.111111 81.888889 4.016667 1018.333333 3 2017-01-04 18.700000 70.050000 4.545000 1015.700000 4 2017-01-05 18.388889 74.944444 3.300000 1014.333333 <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(114, 5)\n</code></pre> <pre><code># Printing the column names\ndf.columns\n</code></pre> <pre><code>Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object')\n</code></pre> <pre><code># Checking the datatype of all the columns\ndf.dtypes\n</code></pre> <pre><code>date             object\nmeantemp        float64\nhumidity        float64\nwind_speed      float64\nmeanpressure    float64\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing","text":"<pre><code># Checking the dataframe whether there is any null value or not\ndf.isnull().sum()\n</code></pre> <pre><code>date            0\nmeantemp        0\nhumidity        0\nwind_speed      0\nmeanpressure    0\ndtype: int64\n</code></pre> <pre><code># Removing the data column from the dataset\ndf.drop([\"date\"], axis=1, inplace=True)\n</code></pre> <pre><code># Checking the shape of the data\ndf.shape\n</code></pre> <pre><code>(114, 4)\n</code></pre> <pre><code># Creating a blank array\narray = []\n\n# Generating a for loop to categorize the humidity into 3 levels\nfor i in df[\"humidity\"]:\n    if i &lt; 40:\n        array.append(1)\n    elif i &gt;= 40 and i &lt; 60:\n        array.append(2)\n    else:\n        array.append(3)\n</code></pre> <pre><code># Converting the array into numpy array\narray = np.array(array)\narray\n</code></pre> <pre><code>array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 1])\n</code></pre> <pre><code># Adding the humidity level data into the main dataframe\ndf[\"humidity_level\"] = array\n</code></pre> <pre><code># Checking the dataframe after adding 'humidity_level' column\ndf.head()\n</code></pre> meantemp humidity wind_speed meanpressure humidity_level 0 15.913043 85.869565 2.743478 59.000000 3 1 18.500000 77.222222 2.894444 1018.277778 3 2 17.111111 81.888889 4.016667 1018.333333 3 3 18.700000 70.050000 4.545000 1015.700000 3 4 18.388889 74.944444 3.300000 1014.333333 3"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#data-exploration-and-visualization","title":"Data Exploration and Visualization","text":"<pre><code># Visualizing the correlations between variables\nplt.figure(figsize=(8, 6))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")\nplt.title(\"Correlation between Variables\")\nplt.show()\n</code></pre> <pre><code># Visualizing the Scatterplot between mean temperature and humidity\nplt.figure(figsize=(8, 6))\nplt.scatter(x=df[\"meantemp\"], y=df[\"humidity\"], cmap=\"rainbow\", c=df[\"meantemp\"], edgecolor=\"gray\")\nplt.grid()\nplt.colorbar(orientation=\"horizontal\")\nplt.title(\"Scatterplot between Mean Temperature and Humidity\")\nplt.xlabel(\"Mean Temperature (\u00b0C)\")\nplt.xlim(10, 35)\nplt.ylabel(\"Humidity (g/m^3)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#preparing-the-training-and-testing-dataset","title":"Preparing the Training and Testing Dataset","text":"<pre><code># Selecting the independent and dependent variable\nx = df[[\"meantemp\", \"wind_speed\"]]\ny = df[[\"humidity_level\"]]\n</code></pre> <pre><code># Importing train_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Preparing training and testing dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=20)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#instantiating-the-decision-tree-classifier-model","title":"Instantiating the Decision Tree Classifier Model","text":"<pre><code># Importing the decision tree classifier model\nfrom sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Instantiating the model\ntreeModel = DecisionTreeClassifier()\n# Fitting the training data to the model\ntreeModel.fit(x_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier()</pre> <pre><code># Visualizing the tree split of the model\nfrom sklearn import tree\nplt.figure(figsize=(14, 10))\ntree.plot_tree(treeModel, filled=True, rounded=True)\nplt.show()\n</code></pre> <pre><code># Applying post prunning technique\ntreeModel2 = DecisionTreeClassifier(max_depth=2)\n# Fitting the training data to the model\ntreeModel2.fit(x_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier(max_depth=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(max_depth=2)</pre> <pre><code># Visualizing the tree split after post prunning\nplt.figure(figsize=(12, 6))\ntree.plot_tree(treeModel2, filled=True, rounded=True)\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#validation-of-the-model","title":"Validation of the Model","text":"<pre><code># Predict the test data using the decision tree model\ny_pred = treeModel2.predict(x_test)\n</code></pre> <pre><code># Import the model validation metrics from the sklearn library\nfrom sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>score = accuracy_score(y_test, y_pred)\nprint(\"Model Score: \", score)\n</code></pre> <pre><code>Model Score:  0.7142857142857143\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n           1       0.70      1.00      0.82         7\n           2       0.40      0.50      0.44         8\n           3       0.93      0.70      0.80        20\n\n    accuracy                           0.71        35\n   macro avg       0.68      0.73      0.69        35\nweighted avg       0.76      0.71      0.72        35\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/","title":"Predicting Life Expectancy Using Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#predicting-life-expectancy-using-machine-learning","title":"Predicting Life Expectancy using Machine Learning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#author-krishnagopal-halder","title":"Author: Krishnagopal Halder","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#project-description","title":"Project Description:","text":"<p>Numerous studies have been conducted in the past on factors that impact life expectancy, taking into account demographic variables, income composition, and mortality rates. However, these studies have neglected to consider the influence of immunization and the human development index. Additionally, some past research relied on a single-year dataset for all countries and utilized multiple linear regression. To address these issues, this study aims to develop a regression model based on mixed effects model and multiple linear regression, utilizing data spanning the period from 2000 to 2015 for all countries. The study will consider key immunization factors such as Hepatitis B, Polio, and Diphtheria, as well as other factors such as mortality, economics, and social factors, in order to gain a comprehensive understanding of the factors that contribute to life expectancy. By examining data from different countries, this study will help countries identify which factors are most strongly correlated with lower life expectancy values, and offer guidance for prioritizing areas for improvement to efficiently improve the life expectancy of their populations.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#01-importing-necessary-libraries","title":"01. Importing Necessary Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#02-importing-dataset","title":"02. Importing Dataset","text":"<p>Metadata Information: * Country: Country * Year: Year * Status: Developed or Developing status * Life expectancy: Life Expectancy in age * Adult Mortality: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population) * infant deaths: Number of Infant Deaths per 1000 population * Alcohol: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol) * percentage expenditure: Expenditure on health as a percentage of Gross Domestic Product per capita(%) * Hepatitis B: Hepatitis B (HepB) immunization coverage among 1-year-olds (%) * Measles: Measles - number of reported cases per 1000 population * BMI: Average Body Mass Index of entire population * under-five deaths: Number of under-five deaths per 1000 population * Polio: Polio (Pol3) immunization coverage among 1-year-olds (%) * Total expenditure: General government expenditure on health as a percentage of total government expenditure (%) * Diphtheria: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%) * HIV/AIDS: Deaths per 1 000 live births HIV/AIDS (0-4 years) * GDP: Gross Domestic Product per capita (in USD) * Population: Population of the country * thinness 1-19 years: Prevalence of thinness among children and adolescents for Age 10 to 19 (% ) * thinness 5-9 years: Prevalence of thinness among children for Age 5 to 9(%) * Income composition of resources: Human Development Index in terms of income composition of resources (index ranging from 0 to 1) * Schooling: Number of years of Schooling(years)</p> <pre><code># Reading the Life Expectancy Data.csv using Pandas\npath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\Life Expectancy Data.csv\"\ndf = pd.read_csv(path)\ndf.head()\n</code></pre> Country Year Status Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles ... Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 Afghanistan 2015 Developing 65.0 263.0 62 0.01 71.279624 65.0 1154 ... 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 Afghanistan 2014 Developing 59.9 271.0 64 0.01 73.523582 62.0 492 ... 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 Afghanistan 2013 Developing 59.9 268.0 66 0.01 73.219243 64.0 430 ... 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 Afghanistan 2012 Developing 59.5 272.0 69 0.01 78.184215 67.0 2787 ... 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 Afghanistan 2011 Developing 59.2 275.0 71 0.01 7.097109 68.0 3013 ... 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <p>5 rows \u00d7 22 columns</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#03-data-visualization","title":"03. Data Visualization","text":"<pre><code># Dropping the columns which contain qualitative values\ndf.drop([\"Country\", \"Status\"], axis=1, inplace=True)\ndf.head()\n</code></pre> Year Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles BMI under-five deaths Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 2015 65.0 263.0 62 0.01 71.279624 65.0 1154 19.1 83 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 2014 59.9 271.0 64 0.01 73.523582 62.0 492 18.6 86 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 2013 59.9 268.0 66 0.01 73.219243 64.0 430 18.1 89 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 2012 59.5 272.0 69 0.01 78.184215 67.0 2787 17.6 93 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 2011 59.2 275.0 71 0.01 7.097109 68.0 3013 17.2 97 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <pre><code># Plotting correlation matrix\nplt.figure(figsize=(14, 12))\nplt.title(\"Correlation Matrix\")\nsns.heatmap(df.corr(), annot=True)\n</code></pre> <pre><code>&lt;Axes: title={'center': 'Correlation Matrix'}&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#04-data-preprocessing","title":"04. Data Preprocessing","text":"<pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(2938, 20)\n</code></pre> <pre><code># Dropping the rows with Null values\ndf.dropna(inplace=True)\ndf.head()\n</code></pre> Year Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles BMI under-five deaths Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 2015 65.0 263.0 62 0.01 71.279624 65.0 1154 19.1 83 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 2014 59.9 271.0 64 0.01 73.523582 62.0 492 18.6 86 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 2013 59.9 268.0 66 0.01 73.219243 64.0 430 18.1 89 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 2012 59.5 272.0 69 0.01 78.184215 67.0 2787 17.6 93 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 2011 59.2 275.0 71 0.01 7.097109 68.0 3013 17.2 97 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <pre><code># Checking the shape of the data after dropping rows with null values\ndf.shape\n</code></pre> <pre><code>(1649, 20)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#05-preparing-data-for-training-and-testing","title":"05. Preparing Data for Training and Testing","text":"<pre><code># Selecting independent variables\nx = df.drop(\"Life expectancy \", axis=1).values\n# Selecting dependent variable\ny = df[\"Life expectancy \"].values\n</code></pre> <pre><code># Splitting the data into training and testing set\n# Training data = 70% and Testing data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#06-implementing-linear-regression-model","title":"06. Implementing Linear Regression Model","text":"<pre><code># Creating Linear Regression object\nlin_reg = linear_model.LinearRegression()\n# Training the model with training data\nlin_reg.fit(x_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#07-finding-the-multiple-linear-equation","title":"07. Finding the Multiple Linear Equation","text":"<p>In multiple linear regression, the equation takes the form of:</p> <p>y = b0 + b1x1 + b2x2 + ... + bnxn</p> <p>where y is the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the intercept, and b1, b2, ..., bn are the coefficients that represent the impact of each independent variable on the dependent variable.</p> <pre><code># Getting the intercept (b0)\nintercept = lin_reg.intercept_\nprint(\"Intercept:\", intercept.round(4))\n</code></pre> <pre><code>Intercept: 294.6671\n</code></pre> <pre><code># Getting the coefficients (b1, b2, ....., bn)\ncoefficients = lin_reg.coef_\n# Printing all the coefficients\nfor i in range(len(coefficients)):\n    print(f\"b{i+1} = {coefficients[i].round(4)}\", end=\", \")\n</code></pre> <pre><code>b1 = -0.1209, b2 = -0.0155, b3 = 0.0738, b4 = -0.1065, b5 = 0.0004, b6 = -0.0033, b7 = -0.0, b8 = 0.031, b9 = -0.0557, b10 = 0.0066, b11 = 0.0994, b12 = 0.0159, b13 = -0.4637, b14 = 0.0, b15 = -0.0, b16 = -0.092, b17 = 0.0386, b18 = 11.7172, b19 = 0.8605,\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#08-predicting-the-life-expectancyy-of-x_test-data","title":"08. Predicting the Life Expectancy(y) of x_test Data","text":"<pre><code># Predicting the y value based on x_test data\ny_predicted = lin_reg.predict(x_test)\n</code></pre> <pre><code># Plotting scatter diagram between y_test(actual) and y_predicted data\nsns.scatterplot(x=y_test, y=y_predicted)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_predicted\")\nplt.title(\"Scatter Plot between y_true(actual) and y_predicted\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#09-validation-of-the-model","title":"09. Validation of the Model","text":"<pre><code># Calculating the Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_predicted)\nprint(\"Mean Absolute Error(MAE) =\", mae.round(4))\n\n# Calculating Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_predicted)\nprint(\"Mean Squared Error(MSE) =\", mse.round(4))\n\n# Calculating Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error(RMSE) =\", rmse.round(4))\n</code></pre> <pre><code>Mean Absolute Error(MAE) = 2.7465\nMean Squared Error(MSE) = 12.9831\nRoot Mean Squared Error(RMSE) = 3.6032\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/","title":"Predicting Aqi Using Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/#reading-aqi-data-csv-file-using-pandas","title":"Reading AQI Data CSV File using Pandas","text":"<pre><code>csv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\AQI_data.csv\"\ndf = pd.read_csv(csv_path)\n</code></pre> <pre><code>df.head()\n</code></pre> City Date PM2.5 PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI AQI_Bucket 0 Ahmedabad 01-01-2015 NaN NaN 0.92 18.22 17.15 NaN 0.92 27.64 133.36 0.00 0.02 0.00 NaN NaN 1 Ahmedabad 02-01-2015 NaN NaN 0.97 15.69 16.46 NaN 0.97 24.55 34.06 3.68 5.50 3.77 NaN NaN 2 Ahmedabad 03-01-2015 NaN NaN 17.40 19.30 29.70 NaN 17.40 29.07 30.70 6.80 16.40 2.25 NaN NaN 3 Ahmedabad 04-01-2015 NaN NaN 1.70 18.48 17.97 NaN 1.70 18.59 36.08 4.43 10.14 1.00 NaN NaN 4 Ahmedabad 05-01-2015 NaN NaN 22.10 21.42 37.76 NaN 22.10 39.33 39.31 7.01 18.89 2.78 NaN NaN <pre><code>df.shape\n</code></pre> <pre><code>(29531, 16)\n</code></pre> <pre><code>df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 29531 entries, 0 to 29530\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   City        29531 non-null  object \n 1   Date        29531 non-null  object \n 2   PM2.5       24933 non-null  float64\n 3   PM10        18391 non-null  float64\n 4   NO          25949 non-null  float64\n 5   NO2         25946 non-null  float64\n 6   NOx         25346 non-null  float64\n 7   NH3         19203 non-null  float64\n 8   CO          27472 non-null  float64\n 9   SO2         25677 non-null  float64\n 10  O3          25509 non-null  float64\n 11  Benzene     23908 non-null  float64\n 12  Toluene     21490 non-null  float64\n 13  Xylene      11422 non-null  float64\n 14  AQI         24850 non-null  float64\n 15  AQI_Bucket  24850 non-null  object \ndtypes: float64(13), object(3)\nmemory usage: 3.6+ MB\n</code></pre> <pre><code>df.isnull().sum()\n</code></pre> <pre><code>City              0\nDate              0\nPM2.5          4598\nPM10          11140\nNO             3582\nNO2            3585\nNOx            4185\nNH3           10328\nCO             2059\nSO2            3854\nO3             4022\nBenzene        5623\nToluene        8041\nXylene        18109\nAQI            4681\nAQI_Bucket     4681\ndtype: int64\n</code></pre> <pre><code>df.describe()\n</code></pre> PM2.5 PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI count 24933.000000 18391.000000 25949.000000 25946.000000 25346.000000 19203.000000 27472.000000 25677.000000 25509.000000 23908.000000 21490.000000 11422.000000 24850.000000 mean 67.450578 118.127103 17.574730 28.560659 32.309123 23.483476 2.248598 14.531977 34.491430 3.280840 8.700972 3.070128 166.463581 std 64.661449 90.605110 22.785846 24.474746 31.646011 25.684275 6.962884 18.133775 21.694928 15.811136 19.969164 6.323247 140.696585 min 0.040000 0.010000 0.020000 0.010000 0.000000 0.010000 0.000000 0.010000 0.010000 0.000000 0.000000 0.000000 13.000000 25% 28.820000 56.255000 5.630000 11.750000 12.820000 8.580000 0.510000 5.670000 18.860000 0.120000 0.600000 0.140000 81.000000 50% 48.570000 95.680000 9.890000 21.690000 23.520000 15.850000 0.890000 9.160000 30.840000 1.070000 2.970000 0.980000 118.000000 75% 80.590000 149.745000 19.950000 37.620000 40.127500 30.020000 1.450000 15.220000 45.570000 3.080000 9.150000 3.350000 208.000000 max 949.990000 1000.000000 390.680000 362.210000 467.630000 352.890000 175.810000 193.860000 257.730000 455.030000 454.850000 170.370000 2049.000000 <pre><code>df.nunique()\n</code></pre> <pre><code>City             26\nDate           2009\nPM2.5         11716\nPM10          12571\nNO             5776\nNO2            7404\nNOx            8156\nNH3            5922\nCO             1779\nSO2            4761\nO3             7699\nBenzene        1873\nToluene        3608\nXylene         1561\nAQI             829\nAQI_Bucket        6\ndtype: int64\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>Index(['City', 'Date', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2',\n       'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n      dtype='object')\n</code></pre> <pre><code>sns.barplot(x=\"City\", y=\"AQI\", data=df)\n</code></pre> <pre><code>&lt;Axes: xlabel='City', ylabel='AQI'&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Practice/","title":"Practice","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>myList = [1, 2, 3, 4]\nmyList\n</code></pre> <pre><code>[1, 2, 3, 4]\n</code></pre> <pre><code>myList2 = [[1,2], [3,4]]\nmyList2\n</code></pre> <pre><code>[[1, 2], [3, 4]]\n</code></pre> <pre><code>myArr1 = np.array([1, 2, 3, 4])\nmyArr1\n</code></pre> <pre><code>array([1, 2, 3, 4])\n</code></pre> <pre><code>myArr1[3]\n</code></pre> <pre><code>4\n</code></pre> <pre><code>type(myArr1)\n</code></pre> <pre><code>numpy.ndarray\n</code></pre> <pre><code>myArr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"int64\")\nmyArr2\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]], dtype=int64)\n</code></pre> <pre><code>myArr2[2, 1]\n</code></pre> <pre><code>8\n</code></pre> <pre><code>myArr2.dtype\n</code></pre> <pre><code>dtype('int64')\n</code></pre> <pre><code>myArr2.nbytes\n</code></pre> <pre><code>72\n</code></pre> <pre><code>myArr2.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code>tup = ((1,2), (3, 4))\nmyArr3 = np.array(tup)\nmyArr3\n</code></pre> <pre><code>array([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>arr4 = np.ones((3, 3))\narr4\n</code></pre> <pre><code>array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n</code></pre> <pre><code>arr5 = np.zeros((3, 3))\narr5\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code>arr6 = np.arange(1, 101, 2)\narr6\n</code></pre> <pre><code>array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33,\n       35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67,\n       69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])\n</code></pre> <pre><code>arr7 = np.linspace(1, 10, 5)\narr7\n</code></pre> <pre><code>array([ 1.  ,  3.25,  5.5 ,  7.75, 10.  ])\n</code></pre> <pre><code>arr8 = np.empty((3, 3))\narr8\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code>identityMatrix = np.identity(6, dtype=\"int8\")\nidentityMatrix\n</code></pre> <pre><code>array([[1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1]], dtype=int8)\n</code></pre> <pre><code>identityMatrix.reshape((4, 9))\n</code></pre> <pre><code>array([[1, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 1]], dtype=int8)\n</code></pre> <pre><code>identityMatrix.ravel()\n</code></pre> <pre><code>array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], dtype=int8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled/","title":"Untitled","text":"<pre><code>import numpy as np\n</code></pre> <pre><code>myArr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nmyArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.sum(axis=0)\n</code></pre> <pre><code>array([12, 15, 18])\n</code></pre> <pre><code>myArr.max(axis=0)\n</code></pre> <pre><code>array([7, 8, 9])\n</code></pre> <pre><code>myArr.T\n</code></pre> <pre><code>array([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n</code></pre> <pre><code>myArr.flat\n</code></pre> <pre><code>&lt;numpy.flatiter at 0x1fda9706e40&gt;\n</code></pre> <pre><code>print(myArr)\n</code></pre> <pre><code>[[1 2 3]\n [4 5 6]\n [7 8 9]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled1/","title":"Untitled1","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\n</code></pre> <pre><code>df = pd.read_csv(path)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code>df.shape\n</code></pre> <pre><code>(1206, 9)\n</code></pre> <pre><code>df[\"Mag\"].max()\n</code></pre> <pre><code>9.1\n</code></pre> <pre><code>df[\"Location Name\"][df[\"Total Deaths\"] &gt; 50000]\n</code></pre> <pre><code>272         INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n320    PAKISTAN:  MUZAFFARABAD, URI, ANANTNAG, BARAMULA\n490                            CHINA:  SICHUAN PROVINCE\n607                              HAITI:  PORT-AU-PRINCE\nName: Location Name, dtype: object\n</code></pre> <pre><code>df[\"Location Name\"][df[\"Mag\"] &gt; 8.5]\n</code></pre> <pre><code>272    INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n294                      INDONESIA:  SUMATERA:  SW\n614          CHILE:  MAULE, CONCEPCION, TALCAHUANO\n674                                 JAPAN:  HONSHU\n736         INDONESIA:  N SUMATRA:  OFF WEST COAST\nName: Location Name, dtype: object\n</code></pre> <pre><code>df[\"Mag\"].mean()\n</code></pre> <pre><code>5.945054031587698\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>Index(['Year', 'Mo', 'Dy', 'Location Name', 'Latitude', 'Longitude',\n       'Focal Depth (km)', 'Mag', 'Total Deaths'],\n      dtype='object')\n</code></pre> <pre><code>df.fillna(0, inplace=True)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 0.0 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 0.0 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 0.0 <pre><code>new_df = df[[\"Location Name\", \"Mag\", \"Total Deaths\"]]\nnew_df\n</code></pre> Location Name Mag Total Deaths 0 INDIA-BANGLADESH BORDER:  MAHESHKHALI 4.6 0.0 1 CHINA:  LIAONING PROVINCE 5.1 0.0 2 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 5.9 7.0 3 IRAN:  BARDASKAN, KASHMAR 5.3 1.0 4 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI 4.5 0.0 ... ... ... ... 1201 PHILIPPINES:  MASBATE 6.6 2.0 1202 ALASKA 7.6 0.0 1203 GREECE:  SAMOS; TURKEY:  IZMIR 7.0 118.0 1204 CHILE:  OFF COAST CENTRAL 6.7 0.0 1205 BALKANS NW:  CROATIA:  PETRINJA 6.4 8.0 <p>1206 rows \u00d7 3 columns</p> <pre><code>new_df.describe()\n</code></pre> Mag Total Deaths count 1206.000000 1206.000000 mean 5.930265 681.509121 std 1.099304 11757.576367 min 0.000000 0.000000 25% 5.200000 0.000000 50% 5.900000 0.000000 75% 6.700000 2.000000 max 9.100000 316000.000000 <pre><code>plt.boxplot(new_df[\"Mag\"])\n</code></pre> <pre><code>{'whiskers': [&lt;matplotlib.lines.Line2D at 0x28fd619e1c0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x28fd619e460&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x28fd619e700&gt;,\n  &lt;matplotlib.lines.Line2D at 0x28fd619e9a0&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x28fd617eee0&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x28fd619ec40&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x28fd619eee0&gt;],\n 'means': []}\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled2/","title":"Untitled2","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\n</code></pre> <pre><code>df = pd.read_csv(path)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code>df[df[\"Mag\"] &gt; 8.5]\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 272 2004 12 26 INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST 3.316 95.854 30.0 9.1 227899.0 294 2005 3 28 INDONESIA:  SUMATERA:  SW 2.085 97.108 30.0 8.6 1313.0 614 2010 2 27 CHILE:  MAULE, CONCEPCION, TALCAHUANO -36.122 -72.898 23.0 8.8 558.0 674 2011 3 11 JAPAN:  HONSHU 38.297 142.372 30.0 9.1 18428.0 736 2012 4 11 INDONESIA:  N SUMATRA:  OFF WEST COAST 2.327 93.063 20.0 8.6 10.0 <pre><code>df.set_index(\"Year\", inplace=True)\n</code></pre> <pre><code>df.loc[2000]\n</code></pre> Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths Year 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN 2000 3 28 JAPAN:  VOLCANO ISLANDS 22.338 143.730 127.0 7.6 NaN 2000 4 5 GREECE:  CRETE 34.220 25.690 38.0 5.5 NaN 2000 5 4 INDONESIA:  SULAWESI:  LUWUK, BANGGAI, PELENG, -1.105 123.573 26.0 7.6 46.0 2000 5 7 TURKEY:  DOGANYOL, PUTURGE 38.164 38.777 5.0 4.1 NaN 2000 5 17 TAIWAN:  TAI-CHUNG COUNTY 24.223 121.058 10.0 5.4 3.0 2000 6 4 INDONESIA:  SUMATRA:  BENGKULU, ENGGANO -4.721 102.087 33.0 7.9 103.0 2000 6 6 TURKEY:  CERKES, CUBUK, ORTA 40.693 32.992 10.0 6.0 2.0 2000 6 7 CHINA:  YUNNAN PROVINCE:  LIUKU; MYANMAR 26.856 97.238 33.0 6.3 NaN 2000 6 7 INDONESIA:  SOUTHERN SUMATERA:  LAHAT -4.612 101.905 33.0 6.7 1.0 2000 6 10 TAIWAN:  NAN-TOU 23.843 121.225 33.0 6.4 2.0 2000 6 17 ICELAND:  VESTMANNAEYJAR, HELLA 63.966 -20.487 10.0 6.5 NaN 2000 6 18 AUSTRALIA:  S, COCOS ISLANDS -13.802 97.453 10.0 7.9 NaN 2000 6 21 ICELAND:  GRIMSNES, SELFOSS, EYRARBAKKI, STOKK... 63.980 -20.758 10.0 6.5 NaN 2000 7 1 JAPAN:  NEAR S COAST HONSHU:  KOZU-SHIMA 34.221 139.131 10.0 6.1 1.0 2000 7 6 NICARAGUA:  MASAYA 11.884 -85.988 33.0 5.4 7.0 2000 7 12 INDONESIA: JAWA:BANDUNG,CIBADAK,CIMANDIRI,KADU... -6.675 106.845 33.0 5.4 NaN 2000 7 15 JAPAN:  NEAR S COAST HONSHU:  NII-JIMA 34.319 139.260 10.0 6.1 NaN 2000 7 16 PHILIPPINES:  BASCO, MOUNT IRADA, BATAN ISLANDS 20.253 122.043 33.0 6.4 NaN 2000 7 30 JAPAN:  HONSHU:  S 33.901 139.376 10.0 6.5 NaN 2000 8 4 RUSSIA:  SAKHALIN ISLAND, UGLEGORSK, MAKAROV 48.786 142.246 10.0 6.8 NaN 2000 8 21 CHINA:  YUNNAN PROVINCE:  WUDING 25.826 102.194 33.0 4.2 1.0 2000 9 3 CALIFORNIA:  NAPA 38.379 -122.413 10.0 5.0 NaN 2000 10 2 TANZANIA:  NKANSI, RUKWA -7.977 30.709 34.0 6.5 NaN 2000 10 6 JAPAN:  HONSHU:  W:  OKAYAMA, TOTTORI 35.456 133.134 10.0 6.7 NaN 2000 10 30 AFGHANISTAN-TAJIKISTAN:  RAKHOR 37.542 69.582 33.0 5.1 NaN 2000 11 8 PANAMA-COLOMBIA:  JURADO 7.042 -77.829 17.0 6.5 NaN 2000 11 16 PAPUA NEW GUINEA:  NEW IRELAND, DUKE OF YORK -4.001 152.327 17.0 8.0 2.0 2000 11 16 PAPUA NEW GUINEA:  NEW IRELAND, NEW BRITAIN -5.233 153.102 30.0 7.8 NaN 2000 11 17 PAPUA NEW GUINEA:  NEW BRITAIN -5.496 151.781 33.0 7.8 NaN 2000 11 25 AZERBAIJAN:  BAKU 40.245 49.946 50.0 6.8 31.0 2000 12 6 TURKMENISTAN:  NEBITDAG-TURKMENBASHI 39.566 54.799 30.0 7.0 11.0 2000 12 15 TURKEY:  AFYON-BOLVADIN 38.457 31.351 10.0 6.0 6.0"},{"location":"data-science/Data-Wrangling-with-Xarray/","title":"Data Wrangling with Xarray: A Hands on Course for Multidimensional Data Analysis","text":"<p>This comprehensive and practical course empowers learners to efficiently manipulate and analyze multidimensional data using the powerful Xarray library in Python.</p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/","title":"Data Structures","text":""},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#xarrays-data-structures","title":"Xarray's Data Structures","text":""},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#introduction","title":"Introduction","text":"<p>N-dimensional arrays, also known as tensors, are integral to computational science and are used across various domains like physics, astronomy, geoscience, bioinformatics, engineering, finance, and deep learning. In Python, NumPy serves as the essential tool for handling these arrays. However, practical datasets go beyond simple numerical values; they often include labels that provide information about how the array values correspond to locations in space, time, and other dimensions.</p> <p>To illustrate, consider how we might organize a dataset for a weather forecast:</p> <p>Xarray distinguishes itself by not only keeping track of labels on arrays but using them to offer a robust and concise interface. For instance:</p> <ul> <li>Conduct operations across dimensions by name: <code>x.sum('time')</code>.</li> <li>Select values by label rather than integer location:     <code>x.loc['2014-01-01']</code> or <code>x.sel(time='2014-01-01')</code>.</li> <li>Mathematical operations (e.g., <code>x - y</code>) efficiently work across multiple dimensions (array broadcasting) based on dimension names, not shape.</li> <li>Utilize versatile split-apply-combine operations with groupby:     <code>x.groupby('time.dayofyear').mean()</code>.</li> <li>Achieve database-like alignment based on coordinate labels that adeptly handles missing values: <code>x, y = xr.align(x, y, join='outer')</code>.</li> <li>Retain arbitrary metadata using a Python dictionary: <code>x.attrs</code>.</li> </ul> <p>Xarray's N-dimensional data structures are well-suited for handling multi-dimensional scientific data. Its use of dimension names, rather than axis labels (e.g., <code>dim='time'</code> instead of <code>axis=0</code>), makes managing arrays more straightforward compared to raw NumPy ndarrays. With xarray, there's no need to keep track of the order of an array's dimensions or insert dummy dimensions of size 1 for alignment (e.g., using <code>np.newaxis</code>).</p> <p>The immediate benefit of using xarray is reduced code, and the long-term advantage is enhanced understanding when revisiting the code in the future.</p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#data-structures","title":"Data Structures","text":"<p>Xarray offers two primary data structures: the <code>DataArray</code> and <code>Dataset</code>. The <code>DataArray</code> class adds dimension names, coordinates, and attributes to multi-dimensional arrays, while the <code>Dataset</code> class combines multiple arrays.</p> <p>For practical examples, Xarray provides small real-world tutorial datasets on its GitHub repository here. We will utilize the xarray.tutorial.load_dataset function to download and open the <code>air_temperature</code> Dataset from the National Centers for Environmental Prediction by name.</p> <p>```python id=\"Vo1sfSUM9g6v\" import numpy as np import xarray as xr <pre><code>&lt;!-- #region id=\"yfeTsE9B3IfG\" --&gt;\n### **Dataset**\n`Dataset` objects function as container-like structures resembling dictionaries. They organize DataArrays, where each variable name is mapped to an associated DataArray within the dataset. This arrangement allows for a comprehensive and structured representation of multi-variable datasets.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 374} id=\"XubFsNNy9x4Q\" outputId=\"4da71b4a-2073-4a73-ed94-526927885f37\"\n# Reading built-in dataset with Xarray\nds = xr.tutorial.load_dataset(\"air_temperature\")\nds\n</code></pre></p> <p>We can access \"layers\" of the Dataset (individual DataArrays) with dictionary syntax.</p> <p>```python id=\"F9fhaQ7S_ADh\" ds[\"air\"] <pre><code>&lt;!-- #region id=\"_npCn9ZO_JjT\" --&gt;\nWe can save some typing by using the \"attribute\" or \"dot\" notation. This won't work for variable names that clash with built-in method names (for example, `mean`).\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 219} id=\"T5A_6o_mDeoT\" outputId=\"5c5f7f44-9a0c-4af0-9d72-7e6b42407f37\"\nds.air\n</code></pre></p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#understanding-string-representations","title":"Understanding String Representations","text":"<p>Xarray offers two types of representations: <code>\"html\"</code> (exclusive to notebooks) and <code>\"text\"</code>. You can specify your preference using the <code>display_style</code> option.</p> <p>Up to this point, our notebook has been set to automatically display the <code>\"html\"</code> representation (which we will stick with). The <code>\"html\"</code> representation is interactive, enabling you to collapse sections (using left arrows) and explore attributes and values for each entry (accessible through the right-hand sheet icon and data symbol).</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 374} id=\"2DKnYgOFGX0v\" outputId=\"a34a3edb-558f-4768-df8a-547da0e77ecc\" with xr.set_options(display_style=\"html\"):     display(ds) <pre><code>&lt;!-- #region id=\"uKUz1ynSHVL8\" --&gt;\nThe output includes:\n\n- A summary detailing all *dimensions* of the `Dataset` `(lat: 25, time: 2920, lon: 53)`. This information specifies that the first dimension, named `lat`, has a size of `25`, the second dimension, named `time`, has a size of `2920`, and the third dimension, named `lon`, has a size of `53`. Since we access dimensions by name, their order is not significant.\n- An unordered list presenting *coordinates* or dimensions with coordinates. Each item is listed on a separate line, providing the name, one or more dimensions in parentheses, the data type (dtype), and a preview of the values. Additionally, if a dimension coordinate is present, it is marked with a `*`.\n- An alphabetically sorted list of *dimensions without coordinates* (if any).\n- An unordered list detailing *attributes*, or metadata.\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"kP27qqBXHEDD\" --&gt;\n\ud83e\udd14 **Note:** The use of the `with` statement in Python is associated with context management. In this context, the `xr.set_options(display_style=\"html\")` is likely a context manager provided by the xarray library. When used within a `with` statement, it allows you to temporarily change a setting for a specific block of code, and once the block is exited, the original settings are automatically restored.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 260} id=\"DUGp_lfoICPI\" outputId=\"674d68a9-2f78-4dba-a091-322e40768889\"\nwith xr.set_options(display_style=\"text\"):\n    display(ds)\n</code></pre></p> <p>To understand each of the components better, we'll explore the \"air\" variable of our Dataset.</p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#dataarray","title":"DataArray","text":"<p>The <code>DataArray</code> class consists of an array (data) and its associated dimension names, labels, and attributes (metadata).</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 219} id=\"6b1wojjFI94P\" outputId=\"e74e4b4c-1945-4b8f-be22-231933e9f9d6\"</p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#selecting-air-variable-from-the-dataset","title":"Selecting 'air' variable from the dataset","text":"<p>da = ds[\"air\"] da <pre><code>&lt;!-- #region id=\"1bY-dYooV1zT\" --&gt;\n#### **Understanding String Representations**\nWe can use the same two representations (`\"html\"`, which is only available in\nnotebooks, and `\"text\"`) to display our `DataArray`.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 219} id=\"iJ7BHrS1V7cz\" outputId=\"41640828-354b-4124-aa58-61c7de573c0b\"\nwith xr.set_options(display_style=\"html\"):\n    display(da)\n</code></pre></p> <p>```python id=\"RA4F0D9HhkaG\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 1000} outputId=\"f92d89f2-e48d-42d4-cc84-245c785c3992\" with xr.set_options(display_style=\"text\"):     display(da) <pre><code>&lt;!-- #region id=\"CNsXMte6n_yx\" --&gt;\nWe can also access the data array directly:\n&lt;!-- #endregion --&gt;\n\n```python id=\"wZyz24vioBeS\"\nds.air.data # (or equivalently, `da.data`)\n</code></pre></p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#named-dimensions","title":"Named Dimensions","text":"<p><code>.dims</code> represent the named axes of your data, and they can either have associated values (dimension coordinates) or not (dimensions without coordinates). The names can take any form that is compatible with a Python <code>set</code> (i.e., calling <code>hash()</code> on it does not result in an error), but for practical use, they are typically strings.</p> <p>In this instance, there are two spatial dimensions, with shorthand names <code>lat</code> and <code>lon</code> representing <code>latitude</code> and <code>longitude</code>, respectively. Additionally, there is one temporal dimension, denoted as <code>time</code>.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"l-9bvHEFrJSG\" outputId=\"7ce9cd63-38d9-44f7-f228-e3814d21f36d\" ds.air.dims <pre><code>&lt;!-- #region id=\"EgXoySBhrPo4\" --&gt;\n#### Coordinates\n\n`.coords` serves as a straightforward [dict-like](https://docs.python.org/3/glossary.html#term-mapping) [data container](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#coordinates) that maps coordinate names to corresponding values. These values can take different forms:\n\n- Another `DataArray` object.\n- A tuple `(dims, data, attrs)`, where `attrs` is optional. This is akin to creating a new `DataArray` object with `DataArray(dims=dims, data=data, attrs=attrs)`.\n- A 1-dimensional `numpy` array or any convertible type (using [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)), such as a `list`. This array contains numbers, datetime objects, strings, etc., serving as labels for each point.\n\nIn the following example, we observe the actual timestamps and spatial positions associated with our air temperature data:\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"OxvJIcyBrrdC\" outputId=\"e194de16-0ff3-4d87-d021-99225986ae90\"\nds.air.coords\n</code></pre></p> <p>The distinction between dimension labels (dimension coordinates) and regular coordinates lies in the fact that, currently, indexing operations (<code>sel</code>, <code>reindex</code>, etc.) can only be applied to dimension coordinates. Additionally, while coordinates can have arbitrary dimensions, it is a requirement for dimension coordinates to be one-dimensional.</p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#attributes","title":"Attributes","text":"<p><code>.attrs</code> is a dictionary capable of holding diverse Python objects, including strings, lists, integers, dictionaries, etc., to store information about your data. The only constraint is that certain attributes might not be writable to specific file formats.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zEt5aY7Xseew\" outputId=\"fc5f4ea2-970e-4e64-b8e1-7c41dbcf44f3\" ds.air.attrs <pre><code>&lt;!-- #region id=\"vqGO7UgLvhY7\" --&gt;\n## **Bridging Pandas and Xarray**\nFrequently, the creation of `DataArray` and `Dataset` objects involves conversions from other libraries like [pandas](https://pandas.pydata.org/) or by reading data from storage formats such as [NetCDF](https://www.unidata.ucar.edu/software/netcdf/) or [zarr](https://zarr.readthedocs.io/en/stable/).\n\nTo facilitate conversion between `xarray` and `pandas`, you can utilize the [to_xarray](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_xarray.html) methods on Pandas objects or the [to_pandas](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.to_pandas.html) methods on `xarray` objects:\n&lt;!-- #endregion --&gt;\n\n```python id=\"x75fOW1pvtZM\"\nimport pandas as pd\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"F9t69wSZwaAP\" outputId=\"4745a914-0967-472e-b1c7-078b91df5bcf\" series = pd.Series(np.ones((10,)), index=list(\"abcdefghij\")) series <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 177} id=\"CP7tbyxlw5E0\" outputId=\"fd5078a1-30bc-440c-c5a0-384bad40ca2e\"\narr = series.to_xarray()\narr\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Bu5_DV6rxOcL\" outputId=\"cdaa816c-a8f2-45b3-c370-b2157293d6eb\" arr.to_pandas() <pre><code>&lt;!-- #region id=\"aBZ_90xpxUD0\" --&gt;\nWe can also control what `pandas` object is used by calling `to_series` /\n`to_dataframe`:\n\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"Bbketlp6x0WJ\" --&gt;\n**`to_series`**: This will always convert `DataArray` objects to\n`pandas.Series`, using a `MultiIndex` for higher dimensions\n\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"E4wNSCuuxWp1\" outputId=\"a35ddc17-cac0-4537-979f-80be6d334520\"\nds.air.to_series()\n</code></pre></p> <p><code>to_dataframe</code>: This will always convert <code>DataArray</code> or <code>Dataset</code> objects to a <code>pandas.DataFrame</code>. Note that <code>DataArray</code> objects have to be named for this.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 455} id=\"eiVzy9Q-xdCg\" outputId=\"eb493513-9ac5-4ef8-d500-f8640fb51156\" ds.air.to_dataframe() <pre><code>&lt;!-- #region id=\"3-uMQRk5ya6t\" --&gt;\nSince columns in a `DataFrame` need to have the same index, they are\nbroadcasted.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 455} id=\"tnH4ibc9xqjQ\" outputId=\"208d1572-e1cd-42fd-829f-250f98dc057d\"\nds.to_dataframe()\n</code></pre></p>"},{"location":"data-science/Data-Wrangling-with-Xarray/00_Fundamentals/00_Data_Structures/#to-pandas-and-back","title":"To Pandas and back","text":"<p><code>DataArray</code> and <code>Dataset</code> objects are commonly generated through the conversion of data from other libraries like pandas or by reading from various data storage formats such as NetCDF or zarr.</p> <p>To convert from / to <code>pandas</code>, we can use the <code>to_xarray</code> methods on pandas objects or the <code>to_pandas</code> methods on <code>xarray</code> objects:</p> <p>```python id=\"lNRIRHrQVl9F\" import pandas as pd <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"0yGLMXPJXCdH\" outputId=\"cd44c701-37cc-4154-8cb6-aaa8c4868209\"\nseries = pd.Series(np.ones((10,)), index=list(\"abcdefghij\"))\nseries\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 177} id=\"xxrSEdoiXXaR\" outputId=\"ad558a15-79d2-456b-e011-54058489e99c\" arr = series.to_xarray() arr <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"hhYiCkjVXQLn\" outputId=\"adc58d5c-5d62-417c-a11e-a4967619154d\"\narr.to_pandas()\n</code></pre></p> <p>We can also control what <code>pandas</code> object is used by calling <code>to_series</code> / <code>to_dataframe</code>:</p> <p><code>to_series</code>: This will always convert <code>DataArray</code> objects to <code>pandas.Series</code>, using a <code>MultiIndex</code> for higher dimensions</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"UAHtKZ8eYEAW\" outputId=\"b46cca99-9fdf-4348-b8dc-a97230e086a2\" ds.air.to_series() <pre><code>&lt;!-- #region id=\"KwVsMEqoYIrO\" --&gt;\n**`to_dataframe`**: This will always convert `DataArray` or `Dataset`\nobjects to a `pandas.DataFrame`. Note that `DataArray` objects have to be named\nfor this.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 455} id=\"hsqwOIUZYNi1\" outputId=\"69b4cf47-b592-470a-ed5a-87635bc6e6b9\"\nds.air.to_dataframe()\n</code></pre></p> <p>Since columns in a <code>DataFrame</code> need to have the same index, they are broadcasted.</p> <p><code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 455} id=\"J_hbaW0ZYj8g\" outputId=\"677b89fa-79b0-4d9c-c65a-85ec3d5d848e\" ds.to_dataframe()</code></p>"},{"location":"data-science/End-to-End-Machine-Learning/","title":"End to End Machine Learning","text":"<p>Welcome to the \"End to End Machine Learning\" repository! This curated collection offers an extensive array of original Jupyter Notebooks, meticulously crafted to delve into diverse facets of machine learning. Whether you're a novice eager to grasp the fundamentals or a seasoned practitioner in search of advanced techniques, this repository serves as your comprehensive guide to immersive, hands-on learning experiences. Explore and enhance your understanding of machine learning concepts, algorithms, and applications through interactive and engaging tutorials.</p>"},{"location":"data-science/End-to-End-Machine-Learning/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Gathering</li> <li>Exploratory Data Analysis</li> <li>Feature Engineering</li> <li>Machine Learning Algorithms</li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/#introduction","title":"Introduction","text":"<p>This repository is designed to be a comprehensive tutorial collection, guiding you through the intricacies of machine learning. Each notebook is crafted to explain concepts, provide practical examples, and encourage interactive learning. Whether you are interested in classical machine learning algorithms, deep learning architectures, or deployment strategies, you'll find a notebook tailored to your needs.</p>"},{"location":"data-science/End-to-End-Machine-Learning/#features","title":"Features","text":"<ul> <li>Diverse Topics: Covering a wide range of machine learning concepts, algorithms, and applications.</li> <li>Interactive Learning: Jupyter Notebooks provide an interactive environment for experimenting with code.</li> <li>Original Content: Authored with care to ensure clarity and originality in explanations and examples.</li> <li>Documentation: Clear explanations, step-by-step instructions, and visualizations to enhance understanding.</li> </ul>"},{"location":"data-science/End-to-End-Machine-Learning/#license","title":"License","text":"<p>This project is licensed under the MIT License. Share, learn, and enjoy the journey of machine learning! \ud83d\udcca\ud83d\ude80</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/","title":"Working With Csv Files","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#working-with-csv-files","title":"Working with CSV Files","text":""},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#open-a-local-csv-file","title":"Open a Local csv File","text":"<pre><code>csv_path = r\"D:\\Coding\\Datasets\\Iris.csv\"\ndf = pd.read_csv(csv_path)\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#open-a-csv-file-from-an-url","title":"Open a csv File from an URL","text":"<ol> <li> <p>requests Module: The requests module is used for making HTTP requests to interact with web resources. It simplifies the process of sending HTTP requests and handling the responses. You can use it to make GET, POST, PUT, DELETE, and other types of HTTP requests.</p> </li> <li> <p>io Module: The io module provides classes for working with streams and file-like objects. It's often used to handle input/output operations in a generic way. You can use it to work with in-memory streams, files, and other data sources.</p> </li> </ol> <pre><code>import requests\nfrom io import StringIO\n</code></pre> <pre><code>url = r\"https://media.githubusercontent.com/media/datablist/sample-csv-files/main/files/customers/customers-1000.csv\"\nreq = requests.get(url)\ndata = StringIO(req.text)\n</code></pre> <pre><code>pd.read_csv(data)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#sep-parameter","title":"'sep' Parameter","text":"<p>The sep parameter specifies the delimiter or separator used in the CSV file to distinguish between different fields or columns. By default, the comma (,) is used as the separator. However, in some cases, CSV files might use other delimiters such as tabs (\\t) or semicolons (;).</p> <pre><code># Reading Tab Separated File with pandas\ntsv_path = r\"D:\\Coding\\Datasets\\movie_titles_metadata.tsv\"\npd.read_csv(tsv_path, sep=\"\\t\")\n</code></pre> <pre><code># Giving the name of columns\ncolumn_names = [\"sl.no\", \"name\", \"release_year\", \"rating\", \"votes\", \"genres\"]\npd.read_csv(tsv_path, sep=\"\\t\", names=column_names)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#index_col-parameter","title":"'index_col' Parameter","text":"<p>The index_col parameter specifies which column(s) should be used as the DataFrame's index. The index is used to uniquely label rows in the DataFrame. By default, pandas assigns a numeric index starting from 0. Specifying index_col allows you to use one or more columns as the index.</p> <pre><code>pd.read_csv(csv_path, index_col=\"Id\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#header-parameter","title":"'header' Parameter","text":"<p>The header parameter specifies which row should be considered as the header (column names) when reading the CSV file. By default, the first row is treated as the header. If you set header=None, pandas will not use any row as the header, and columns will be labeled with numeric indices. You can also provide an integer row number to use as the header, or a list of integers to skip multiple initial rows.</p> <pre><code>test_csv_path = r\"D:\\Coding\\Datasets\\test.csv\"\npd.read_csv(test_csv_path)\n</code></pre> <pre><code># Changing the header row\npd.read_csv(test_csv_path, header=1)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#use_cols-parameter","title":"'use_cols' Parameter","text":"<p>The usecols parameter in the pandas read_csv() function is used to specify which columns from the CSV file should be read into the DataFrame. This parameter allows you to selectively read only a subset of columns, which can be useful when dealing with large datasets where not all columns are needed or when you want to focus on specific data.</p> <pre><code>train_csv_path = r\"D:\\Coding\\Datasets\\aug_train.csv\"\npd.read_csv(train_csv_path)\n</code></pre> <pre><code># Create a list of required columns\nrequired_columns = [\"enrollee_id\", \"gender\", \"education_level\"]\npd.read_csv(train_csv_path, usecols=required_columns)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#squeeze-parameters","title":"'squeeze' Parameters","text":"<p>The squeeze parameter is used when reading a CSV file that has only one column. It determines whether the resulting DataFrame should be \"squeezed\" into a Series if the CSV data consists of a single column. This can be particularly useful to simplify your data structure when you're dealing with single-column datasets.</p> <pre><code>pd.read_csv(train_csv_path, usecols=[\"gender\"], squeeze=True)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#skiprowsnrows-parameter","title":"'skiprows'/'nrows' Parameter","text":"<ol> <li> <p>skiprows Parameter: The skiprows parameter allows you to specify the number of rows at the beginning of the CSV file to skip while reading. You can pass an integer representing the number of rows to skip or a list of row indices (0-based) that should be skipped.</p> </li> <li> <p>nrows Parameter: The nrows parameter is used to limit the number of rows read from the CSV file. You can pass an integer representing the maximum number of rows to read.</p> </li> </ol> <pre><code>pd.read_csv(train_csv_path)\n</code></pre> <pre><code># Skipping the rows at index 1 and  3\npd.read_csv(train_csv_path, skiprows=[1, 3])\n</code></pre> <pre><code># Reading only the first 100 rows\npd.read_csv(train_csv_path, nrows=100)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#encodng-parameter","title":"'encodng' Parameter","text":"<p>The encoding parameter is used to specify the character encoding of the CSV file being read. Character encoding defines how characters are represented as bytes in a file. Different encodings are used for different languages and writing systems.</p> <p>When reading a CSV file, it's important to use the correct encoding to ensure that the data is interpreted correctly. If you encounter issues where characters are not displayed correctly or you see encoding-related errors, specifying the appropriate encoding can help resolve these problems.</p> <pre><code>zomato_csv_path = r\"D:\\Coding\\Datasets\\zomato.csv\"\n# pd.read_csv(zomato_csv_path) # Throws an error\npd.read_csv(zomato_csv_path, encoding=\"latin-1\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#skip-bad-lines","title":"Skip Bad Lines","text":"<p>The error_bad_lines parameter is used in the pandas read_csv() function to control how the function handles lines in a CSV file that have too many fields (columns) compared to the expected number of columns.</p> <pre><code>books_csv_path = r\"D:\\Coding\\Datasets\\BX-Books.csv\"\npd.read_csv(books_csv_path, encoding=\"latin-1\", error_bad_lines=False)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#dtype-parameter","title":"'dtype' Parameter","text":"<p>The dtype parameter allows you to explicitly specify the data types for columns when reading a CSV file into a DataFrame. This can be useful when you want to ensure that specific columns are interpreted with the correct data types.</p> <pre><code>pd.read_csv(train_csv_path)\n</code></pre> <pre><code>pd.read_csv(train_csv_path).info()\n</code></pre> <pre><code># Converting the datatype of the 'target' column into integer\npd.read_csv(train_csv_path, dtype={\"target\": \"int8\"}).info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#handling-dates","title":"Handling Dates","text":"<p>The parse_dates parameter allows you to specify columns that should be parsed as datetime objects when reading a CSV file. This can be especially useful when you have date or datetime information in your CSV file that you want to directly interpret as datetime objects in your DataFrame.</p> <pre><code>ipl_csv_path = r\"D:\\Coding\\Datasets\\IPL Matches 2008-2020.csv\"\npd.read_csv(ipl_csv_path)\n</code></pre> <pre><code>pd.read_csv(ipl_csv_path).info()\n</code></pre> <pre><code>pd.read_csv(ipl_csv_path, parse_dates=[\"date\"]).info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#converters-parameter","title":"'converters' Parameter","text":"<p>The converters parameter in the pandas read_csv() function is used to provide a dictionary of functions that allow you to customize the way specific columns are converted or transformed during the reading process. It's a powerful tool when you need more control over how the data is processed as it's being read from the CSV file.</p> <pre><code>pd.read_csv(ipl_csv_path).head()\n</code></pre> <pre><code># Creating a function to convert the name of the team into acronym\ndef renameTeam(name):\n    if name == \"Kolkata Knight Riders\":\n        return \"KKR\"\n    else: \n        return name\n</code></pre> <pre><code># Applying converters\npd.read_csv(ipl_csv_path, converters={\"team2\": renameTeam})\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#na_values-parameters","title":"'na_values' Parameters","text":"<p>The na_values parameter is used to specify a list of values that should be treated as missing or NaN (Not a Number) values when reading a CSV file into a DataFrame. This parameter allows you to define how specific values in your CSV file should be interpreted as missing data.</p> <pre><code>pd.read_csv(train_csv_path)\n</code></pre> <pre><code># Selecting 'Male' value as 'NaN' for example\npd.read_csv(train_csv_path, na_values=\"Male\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/00_Working_with_CSV_Files/#load-a-huge-dataset-in-chunks","title":"Load a Huge Dataset in Chunks","text":"<p>The chunksize parameter in the pandas read_csv() function is used to read a large CSV file in smaller, manageable chunks rather than reading the entire file into memory at once. This can be very useful when dealing with datasets that are too large to fit entirely in memory.</p> <pre><code>dfs = pd.read_csv(train_csv_path, chunksize=5000)\n</code></pre> <pre><code>for chunk in dfs:\n    print(chunk.shape)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/","title":"Working With Json And Sql","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#working-with-jsonsql","title":"Working with JSON/SQL","text":""},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#working-with-json","title":"Working with JSON","text":"<p>JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate. It is commonly used for transmitting data between a server and a web application, as well as for configuration files, APIs, and various other purposes.</p> <p>JSON is designed to be a language-independent data format, meaning it can be used with any programming language that has the capability to parse and generate JSON data. It is often used in web development for sending and receiving structured data, as it's more concise and easier to work with than XML.</p> <p>JSON data is represented as a collection of key-value pairs, where each key is a string and each value can be a string, number, boolean, object, array, or null. JSON objects are enclosed in curly braces {}, and each key-value pair is separated by a colon. JSON arrays are ordered lists of values and are enclosed in square brackets [].</p> <p>Example:</p> <pre><code>{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"isStudent\": false,\n  \"hobbies\": [\"reading\", \"swimming\", \"gardening\"]\n}\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#read-a-local-json-file","title":"Read a Local JSON File","text":"<pre><code>json_path = r\"D:\\Coding\\Datasets\\train.json\"\npd.read_json(json_path)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#read-a-json-file-from-url","title":"Read a JSON File from URL","text":"<pre><code>json_url = r\"https://raw.githubusercontent.com/LearnWebCode/json-example/master/animals-1.json\"\npd.read_json(json_url)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#working-with-sql","title":"Working with SQL","text":"<p>SQL stands for Structured Query Language. It's a domain-specific programming language used for managing and manipulating relational databases. SQL is used to create, modify, and query databases to store, retrieve, and manipulate data in a structured manner. It provides a standardized way to interact with databases, regardless of the specific database management system (DBMS) being used.</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/01_Working_with_JSON_and_SQL/#read-sql-data","title":"Read sql Data","text":"<p>mysql.connector is a Python library used to connect and interact with MySQL databases. It provides a Pythonic way to work with MySQL databases by allowing you to execute SQL queries, manage connections, and handle the results. This library is commonly used to integrate MySQL databases with Python applications.</p> <pre><code># !pip install mysql.connector\n</code></pre> <pre><code>import mysql.connector\n</code></pre> <pre><code>conn =mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"\", database=\"world\")\n</code></pre> <pre><code>pd.read_sql_query(\"SELECT * FROM city\", conn)\n</code></pre> <pre><code>pd.read_sql_query(\"SELECT * FROM city WHERE CountryCode LIKE 'IND'\", conn)\n</code></pre> <pre><code>pd.read_sql_query(\"SELECT * FROM country WHERE LifeExpectancy &gt; 60\", conn)\n</code></pre> <pre><code># Storing the dataframe in a variable\ndf = pd.read_sql_query(\"SELECT * FROM country WHERE LifeExpectancy &gt; 60\", conn)\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/","title":"Fetching Data From An Api","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#fetching-data-from-an-api","title":"Fetching Data from an API","text":"<p>Fetching data from APIs using Python involves utilizing the requests library to make HTTP requests to a remote server and retrieve data. APIs (Application Programming Interfaces) provide a standardized way for different software applications to communicate and exchange information. </p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"darkgrid\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#setup-the-api","title":"Setup the API","text":"<pre><code>url = \"https://air-quality.p.rapidapi.com/history/airquality\"\n\nquerystring = {\"lon\":\"87.0624\",\"lat\":\"23.1645\"}\n\nheaders = {\n    \"X-RapidAPI-Key\": \"YOUR_API_KEY\",\n    \"X-RapidAPI-Host\": \"air-quality.p.rapidapi.com\"\n}\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#send-the-request","title":"Send the Request","text":"<pre><code>response = requests.get(url, headers=headers, params=querystring)\n\nprint(response.json())\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#convert-json-into-dataframe","title":"Convert JSON into Dataframe","text":"<pre><code># Converting json into dataframe\ndf = pd.DataFrame(response.json()[\"data\"])\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>df.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#preparing-the-data","title":"Preparing the Data","text":"<pre><code># Extracting only required columns\nrequired_columns = [\"timestamp_local\", \"aqi\", \"no2\", \"so2\", \"o3\", \"co\", \"pm10\", \"pm25\"]\nweather_data = df[required_columns]\n</code></pre> <pre><code>weather_data.head()\n</code></pre> <pre><code>weather_data.info()\n</code></pre> <pre><code># Converting 'timestamp_local' column into datetime object\nweather_data[\"timestamp_local\"] = pd.to_datetime(weather_data[\"timestamp_local\"])\n</code></pre> <pre><code>weather_data.info()\n</code></pre> <pre><code># Sorting the data by timestamp\nweather_data.sort_values(by=\"timestamp_local\", inplace=True)\n</code></pre> <pre><code># Set the 'timestamp_local' as index\nweather_data.set_index(\"timestamp_local\", inplace=True)\nweather_data.head()\n</code></pre> <pre><code>weather_data.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#plot-the-data","title":"Plot the Data","text":"<pre><code># Plot a line graph\nplt.figure(figsize=(15, 5))\nsns.lineplot(data=weather_data, x=weather_data.index, y=weather_data.aqi,\n             marker=\"o\", label=\"AQI\")\nsns.lineplot(data=weather_data, x=weather_data.index, y=weather_data.no2, \n             marker=\"s\", label=\"NO2\")\nsns.lineplot(data=weather_data, x=weather_data.index, y=weather_data.so2, \n             marker=\"D\", label=\"SO2\")\nplt.xlabel(\"Time\", fontsize=14)\nplt.ylabel(\"Value\", fontsize=14)\nplt.title(\"Weather Data Time Series\", fontsize=16)\nplt.show()\n</code></pre> <pre><code># Plot a heatmap based on correlation of weather constituents\nsns.heatmap(weather_data.corr(), annot=True, cmap=\"PRGn\")\nplt.title(\"Correlation between different Weather Constituents\", weight=\"500\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/02_Fetching_Data_from_an_API/#save-the-dataframe-as-csv","title":"Save the Dataframe as CSV","text":"<pre><code>output_path = r\"D:\\Coding\\Datasets\\bankura_one_day_weather.csv\"\nweather_data.to_csv(output_path)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/","title":"Fetching Data Using Web Scrapping","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#fetching-data-using-web-scrapping","title":"Fetching Data using Web Scrapping","text":"<p>Web scraping is the process of automatically extracting information and data from websites. It involves writing code to retrieve specific pieces of content, such as text, images, tables, or other structured data, from web pages. Web scraping is commonly used to gather data for various purposes, such as research, analysis, data mining, automation, and more.</p> <p>Web scraping can range from simple tasks, like retrieving the title of an article from a news website, to more complex tasks, like extracting financial data from multiple web pages for analysis. However, it's important to note that while web scraping can be a powerful tool, it should be used ethically and responsibly. Some websites have terms of use that prohibit or restrict scraping, and improperly scraping data or overloading a website's server with requests can have legal and ethical implications.</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#fetch-webpage-from-the-server","title":"Fetch Webpage from the Server","text":"<pre><code>web_url = r\"https://www.ambitionbox.com/list-of-companies?page=2\"\nheaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36\"}\nwebpage = requests.get(web_url, headers=headers).text\n</code></pre> <pre><code># Creating an object of the webpage using BeautifulSoup Class\nsoup = BeautifulSoup(webpage, \"lxml\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#extract-companies-name-from-the-url","title":"Extract Companies Name from the URL","text":"<pre><code># Create an empty list of companies\ncompanies = []\nfor i in range(0, 20):\n    company = soup.find_all(\"h2\")[i].text.strip()\n    companies.append(company)\n</code></pre> <pre><code># Print all the companies name\ncompanies\n</code></pre> <pre><code># Print the length of companies\nlen(companies)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#extract-rating-of-all-companies","title":"Extract Rating of All Companies","text":"<pre><code># Create an empty list of ratings\nratings = []\nfor i in range(0, 20):\n    rating = soup.find_all(\"span\", class_=\"companyCardWrapper__companyRatingValue\")[i].text.strip()\n    ratings.append(rating)\n</code></pre> <pre><code># Print all the ratings\nratings\n</code></pre> <pre><code># Print the length of ratings\nlen(ratings)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#extract-content-of-all-companies","title":"Extract Content of All Companies","text":"<pre><code># Create a list of contents\ncontents = []\nfor i in range(0, 20):\n    content = soup.find_all(\"span\", class_=\"companyCardWrapper__interLinking\")[i].text.strip()\n    contents.append(content)\n</code></pre> <pre><code># Print all the content\nprint(contents)\n</code></pre> <pre><code># Print the length of contents\nlen(contents)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#write-a-function-to-automate-the-workflow","title":"Write a Function to Automate the Workflow","text":"<pre><code># Creating a function to fetch the data for each page\ndef fetchData(soup):\n\n    companies = soup.find_all(\"div\", class_=\"companyCardWrapper__companyDetails\")\n\n    # Extracting the data of the companies\n    companyNames = []\n    ratings = []\n    contents = []\n    industries = []\n    employees = []\n    ctypes = []\n    age = []\n    hq = []\n\n    for i in range(0, 20):\n        company = companies[i]\n\n        # Getting the name of the company\n        company_name = company.find(\"h2\", class_=\"companyCardWrapper__companyName\").text.strip()\n        companyNames.append(company_name)\n\n        # Extracting the ratings\n        rating = company.find(\"span\", class_=\"companyCardWrapper__companyRatingValue\").text.strip()\n        ratings.append(rating)\n\n        # Extracting the contents\n        content = company.find(\"span\", class_=\"companyCardWrapper__interLinking\").text.strip()\n        # Convert the content into a list\n        content = content.split(sep=\" | \")\n\n        # Extracting the industry\n        industry = content[0]\n        industries.append(industry)\n\n        # Extracting the employees\n        employee = \"\"\n        ctype = \"\"\n        old = \"\"\n        headquarter = \"\"\n\n        for j in content[1:]:\n            if \"Employees\" in j:\n                employee = j\n            elif \"old\" in j:\n                old = j\n            elif \"more\" in j:\n                headquarter = j.split(sep=\" \")[0:-2]\n                headquarter = \" \".join(headquarter)\n            else:\n                ctype = j\n\n        if len(employee) &gt; 0:\n            employees.append(employee[0:-10])\n        else:\n            employees.append(\"NaN\")\n\n        if len(old) &gt; 0:\n            age.append(old[0:-4])\n        else:\n            age.append(\"NaN\")\n\n        if len(headquarter) &gt; 0:\n            hq.append(headquarter)\n        else:\n            hq.append(\"NaN\")\n\n        if len(ctype) &gt; 0:\n            ctypes.append(ctype)\n        else:\n            ctypes.append(\"NaN\")\n\n    # Create a dataframe\n    columns = {\"name\": companyNames, \"rating\": ratings, \"industry\": industries,\n               \"employee\": employees, \"type\": ctypes, \"age\": age, \"headquarter\": hq}\n    df = pd.DataFrame(columns)\n\n    return df\n</code></pre> <pre><code># Checking the fetchData function\nfetchData(soup)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#fetch-the-data-for-all-the-webpages","title":"Fetch the Data for all the Webpages","text":"<pre><code># Store the headers in a variable\nheaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36\"}\n\n# Create a blank dataframe\nfinal_df = pd.DataFrame()\n\n# Storing data in the final dataframe for 500 pages\nfor i in range(1, 500):\n    web_url = \"https://www.ambitionbox.com/list-of-companies?page={}\".format(i)\n    webpage = requests.get(web_url, headers=headers).text\n\n    soup = BeautifulSoup(webpage, \"lxml\")\n\n    df = fetchData(soup)\n    final_df = pd.concat([final_df, df], ignore_index=True)\n</code></pre> <pre><code>final_df\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/03_Fetching_Data_using_Web_Scrapping/#export-the-final-dataframe-as-a-csv","title":"Export the Final Dataframe as a CSV","text":"<pre><code>output_path = r\"D:\\\\Coding\\\\Datasets\\\\\"\nfile_name = \"company_web_scrapping_data.csv\"\nfinal_df.to_csv(output_path+file_name)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/","title":"Understanding Your Data","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#understanding-your-data","title":"Understanding Your Data","text":"<p>Understanding the data is a critical step in the data science process. It involves gaining insights into the structure, content, quality, and characteristics of the data you're working with. Properly understanding the data sets the foundation for making informed decisions, building accurate models, and deriving meaningful insights.</p>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\Global YouTube Statistics.csv\", encoding=\"latin\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#how-big-is-the-data","title":"How Big is the Data?","text":"<pre><code>df.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#how-does-the-data-look-like","title":"How does the Data look like?","text":"<pre><code># Print first 5 rows of the dataframe\ndf.head()\n</code></pre> <pre><code># Randomly choose 5 rows and print it\ndf.sample(5)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#what-are-the-data-types-of-the-columns","title":"What are the Data Types of the Columns?","text":"<pre><code>df.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#are-there-any-missing-values","title":"Are there any Missing Values?","text":"<pre><code># Checking the number of missing values for each column\ndf.isnull().sum()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#how-does-the-data-look-mathematically","title":"How does the Data look Mathematically?","text":"<pre><code>df.describe()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#are-there-any-duplicate-rows","title":"Are there any Duplicate Rows?","text":"<pre><code>df.duplicated().sum()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/00_Data_Gathering/04_Understanding_Your_Data/#how-is-the-correlation-between-columns","title":"How is the Correlation between Columns?","text":"<pre><code># Extract the correlation between all the variables\ndf.corr()\n</code></pre> <pre><code># Extract the correlation between the 'subscribers' and other numerical columns\ndf.corr()[\"subscribers\"]\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/","title":"Eda Using Univariate Analysis","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#eda-using-univariate-analysis","title":"EDA Using Univariate Analysis","text":"<p>Exploratory Data Analysis (EDA) using univariate analysis focuses on examining individual variables in your dataset to gain insights into their distribution, characteristics, and potential outliers. Univariate analysis helps you understand the basic properties of each variable and identify patterns that can guide further analysis.</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#categorical-data","title":"Categorical Data","text":""},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#count-plot","title":"Count Plot","text":"<pre><code>df.shape\n</code></pre> <pre><code># Checking the number of people died and survived\nsns.countplot(x=df[\"Survived\"])\ndf[\"Survived\"].value_counts()\n</code></pre> <pre><code># Checking the number of people boared at each class\nsns.countplot(x=df[\"Pclass\"])\n</code></pre> <pre><code># Checking the number of male and female\nsns.countplot(x=df[\"Sex\"])\n</code></pre> <pre><code># Checking the travellers embarked location\nsns.countplot(x=df[\"Embarked\"])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#pie-chart","title":"Pie Chart","text":"<pre><code># Plot  the percent of people died and survived in a pie chart\ndf[\"Survived\"].value_counts().plot(kind=\"pie\", autopct=\"%.2f\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#numerical-data","title":"Numerical Data","text":""},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#histogram","title":"Histogram","text":"<pre><code>sns.histplot(x=df[\"Age\"], bins=40)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#dist-plotkde-plot","title":"Dist Plot/KDE Plot","text":"<pre><code>sns.kdeplot(data=df[\"Age\"])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#box-plot","title":"Box Plot","text":"<pre><code>sns.boxplot(x=df[\"Age\"])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/00_EDA_using_Univariate_Analysis/#descriptive-statistics","title":"Descriptive Statistics","text":"<pre><code># Checking the minimum 'Age'\ndf[\"Age\"].min()\n</code></pre> <pre><code># Checking the maximum 'Age'\ndf[\"Age\"].max()\n</code></pre> <pre><code># Checking the mean 'Age'\ndf[\"Age\"].mean()\n</code></pre> <pre><code># Checking the skewness of the 'Age' column\ndf[\"Age\"].skew()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/","title":"Eda Using Bivariate And Multivariate Analysis","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#eda-using-bivariate-and-multivariate-analysis","title":"EDA Using Bivariate and Multivariate Analysis","text":"<p>Exploratory Data Analysis (EDA) is a critical initial step in the data analysis process that involves summarizing, visualizing, and understanding the main characteristics and patterns present in a dataset. Bivariate and multivariate analysis are two important components of EDA that help you explore relationships and interactions between multiple variables in your dataset.</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#read-the-data","title":"Read the Data","text":"<pre><code># Reading the 'tips' data from seaborn\ntips = sns.load_dataset(\"tips\")\n</code></pre> <pre><code># Read the 'titanic' data\ntitanic = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\n</code></pre> <pre><code># Read the 'iris' dataset\niris = pd.read_csv(\"D:\\Coding\\Datasets\\Iris.csv\")\n</code></pre> <pre><code># Read the 'flight' data\nflights = pd.read_csv(r\"D:\\Coding\\Datasets\\flights.csv\")\n</code></pre> <pre><code>tips.head()\n</code></pre> <pre><code>tips.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#scatter-plot-numerical-numerical","title":"Scatter Plot (Numerical - Numerical)","text":"<pre><code># Plotting a scatterplot between 'total_bill' and 'tip' using univariate analysis\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\nplt.show()\n</code></pre> <pre><code># Plotting a scatterplot between 'total_bill' and 'tip' using multivariate analysis\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"sex\", style=\"smoker\", size=\"size\")\nplt.legend(bbox_to_anchor=(1, 1), loc=2)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#bar-plot-numerical-categorical","title":"Bar Plot (Numerical -Categorical)","text":"<pre><code>titanic.head()\n</code></pre> <pre><code># Plot the avergae age of a passsenger traveling in a particular class\nsns.barplot(data=titanic, x=\"Pclass\", y=\"Age\")\n</code></pre> <pre><code># Plot the avergae age of a passsenger (male and female) traveling in a particular class\nsns.barplot(data=titanic, x=\"Pclass\", y=\"Age\", hue=\"Sex\")\n</code></pre> <pre><code># Plot the avergae fare in a particular class\nsns.barplot(data=titanic, x=\"Pclass\", y=\"Fare\", hue=\"Sex\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#box-plot-numerical-categorical","title":"Box Plot (Numerical - Categorical)","text":"<pre><code># Plot a boxplot between 'Sex' and 'Age'\nsns.boxplot(data=titanic, x=\"Sex\", y=\"Age\")\n</code></pre> <pre><code># Plot a boxplot between 'Sex' and 'Age' also show 'Survived'\nsns.boxplot(data=titanic, x=\"Sex\", y=\"Age\", hue=\"Survived\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#dist-plot-numerical-categorical","title":"Dist Plot (Numerical - Categorical)","text":"<pre><code># Plot the relationship between 'Age' and 'Survived'\n# Plotting the Age of the people who could not survive\nsns.distplot(titanic[titanic[\"Survived\"]==0][\"Age\"], hist=False)\n# Plotting the Age of the people who could survive\nsns.distplot(titanic[titanic[\"Survived\"]==1][\"Age\"], hist=False)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#heat-map-categorical-categorical","title":"Heat Map (Categorical - Categorical)","text":"<pre><code>titanic.head()\n</code></pre> <pre><code># Get the number of people died and survived in each Pclass\npd.crosstab(index=titanic[\"Pclass\"], columns=titanic[\"Survived\"])\n</code></pre> <pre><code>sns.heatmap(pd.crosstab(index=titanic[\"Pclass\"], columns=titanic[\"Survived\"]), cmap=\"Reds\", \n            annot=True, fmt=\"\")\n</code></pre> <pre><code>titanic.groupby(\"Pclass\").mean()\n</code></pre> <pre><code># Percentage of people survived in each Pclass\n(titanic.groupby(\"Pclass\").mean()[\"Survived\"]*100).plot(kind=\"bar\")\nplt.ylabel(\"% of People Survived\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#cluster-map-categorical-categorical","title":"Cluster Map (Categorical - Categorical)","text":"<pre><code># Get the number of people died and survived for each SibSp\npd.crosstab(index=titanic[\"Parch\"], columns=titanic[\"Survived\"])\n</code></pre> <pre><code>sns.clustermap(pd.crosstab(index=titanic[\"Parch\"], columns=titanic[\"Survived\"]), cmap=\"Reds\", \n               annot=True, fmt=\"\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#pair-plot","title":"Pair Plot","text":"<pre><code>iris.head()\n</code></pre> <pre><code>sns.pairplot(iris.iloc[:, 1:], hue=\"Species\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/01_EDA_using_Bivariate_and_Multivariate_Analysis/#line-plot-numerical-numerical","title":"Line Plot (Numerical - Numerical)","text":"<pre><code>flights.head()\n</code></pre> <pre><code># Group the number of passengers and calculate total for each year\nflights_stat = flights.groupby(\"year\").sum().reset_index()\nflights_stat\n</code></pre> <pre><code># Create a simple line plot between years and passengers\nsns.lineplot(data=flights_stat, x=\"year\", y=\"passengers\", marker=\"o\")\n</code></pre> <pre><code># Create a heatmap \nflights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\")\n</code></pre> <pre><code>sns.heatmap(flights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\"),\n            cmap=\"Reds\")\n</code></pre> <pre><code>sns.clustermap(flights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\"),\n            cmap=\"Reds\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/","title":"Pandas Profiling","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/#pandas-profiling","title":"Pandas Profiling","text":"<p>Pandas Profiling is an open-source Python library that generates a comprehensive profile report of a Pandas DataFrame. It provides a quick and easy way to gain insights into the data contained within a DataFrame, which is particularly useful during the exploratory data analysis (EDA) phase of a data science project. The profile report generated by Pandas Profiling includes various statistics, visualizations, and summaries of the data, helping users to understand the data's distribution, missing values, unique values, correlations, and more.</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"E5j23VKLplVF\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/#installing-pandas-profilling","title":"Installing pandas-profilling","text":""},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/#pip-install-pandas-profiling","title":"!pip install pandas-profiling","text":"<pre><code>```python id=\"YFvCK1m9pX4y\"\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/01_Exploratory_Data_Analysis_%28EDA%29/02_Pandas_Profiling/#read-the-data","title":"Read the Data","text":"<p>```python id=\"fMrSA_OTp51P\" df = pd.read_csv(\"/content/titanic.csv\") <pre><code>&lt;!-- #region id=\"po5AZ30StmpC\" --&gt;\n## **Create a Profile Report Object**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 145, \"referenced_widgets\": [\"bff4dca73a7e402bb63995c034b2a68f\", \"eb1bc181f4f94491a41d1948cc4af7a0\", \"b89b2b32d4174f4c80197676f1dac12f\", \"c3d3404415e248fd8a258b109f73e5fc\", \"52390ba8b0cb43daa24640c9c0752ec4\", \"cee55f139b514d8c8729d1699758240f\", \"20c1bef411d146ce8cf1ab885e5699e8\", \"e33f7586011941da981099e15112d9e3\", \"50fb73bab4e14abfb9c1c87cb5e66205\", \"5e1b398cd7184d6ab8af5ec2e2512976\", \"8600c472cf094908b51b97a9c1b9b19b\", \"4f27c49286a94d588794b9be8d6e78e1\", \"b6e3ac7713084394b0bc508f31431ca6\", \"8492b634e3ff493787d951e17944ac2d\", \"c77f7b1bff914c08aed08fef5a9740ff\", \"200c5ae7cbfb437787a10d610e2dd304\", \"195b01a0a8974004b509d3dc026853cb\", \"c057035a1dd5453bb1f55cd21b2f2028\", \"dd3badd57e3848568a636258471390e8\", \"e4cbc78d28ef4d4caf66e65814d5c129\", \"46f35a267e2e48489e20c87c6fe72bd6\", \"2a83db37f46249df808a6900d99e84ec\", \"7b79ad0f9c854d15a5238dac61e772f3\", \"8ca151a182bc4f5381c17da10addad82\", \"31f07af0511c42148e9b2f762d9bad59\", \"671c96d19bb5415fa0c9a732b09399b3\", \"1564ef89187042d1a2839439a344fa0f\", \"748ac955194c49f2821954da6c804693\", \"2f4c7e5037fb4aae99aab85eb15ccc23\", \"2de0d04f743e44cbbc926a7a98e35d9d\", \"573918475bef48a8a649cbd4e5b036f2\", \"e0282c29b42242ac86c935cb35aa5e1f\", \"b4562e869681458eb841eed94d396c63\", \"2ed8160c4dd24162a31bb0d34c3d1e5f\", \"fd3dd3bedca94738b9de42355925a073\", \"c79761823d614fbdbed454aaa23451e7\", \"8795ab4912e9435c88e766d0926fa386\", \"6f40ba13368c476480acab462b19a1e2\", \"dcb7ec6a6fc74330bdf1e8cd6e60c455\", \"300ccbf6c80847439fc4c90769ba0879\", \"88b25ff483bf49e68a2a0d4208288892\", \"0819d79d15964c5ca0dd539cace0c9f7\", \"dd91fb01162b4b0d985c7dfb4f4dae7d\", \"a53f305d511e4dac90fbbe5f9fa60fc0\"]} id=\"p79Ba-MeqGyA\" outputId=\"6e741e53-8174-44bb-aec9-e177de6ba0f0\"\nprof = ProfileReport(df)\nprof.to_file(output_file=\"output.html\")\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/","title":"Standardization","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#standardization-feature-scaling","title":"Standardization - Feature Scaling","text":"<p>Standardization, also known as z-score normalization or standard scaling, is a technique used in data preprocessing to rescale the features of a dataset. The goal of standardization is to transform the data so that it has a mean of 0 and a standard deviation of 1. This process helps to make the features more comparable and can be particularly useful in machine learning algorithms that are sensitive to the scale of the input features, such as gradient descent-based optimization algorithms.</p> <p>1. Standard Deviation:  Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a set of data points. It provides a way to understand how spread out the values in a dataset are and how far individual data points are from the mean (average). In essence, it tells you whether the data points are clustered closely around the mean or scattered widely. <pre><code>\u03c3 (sigma) = \u221a[\u03a3(xi - \u03bc)\u00b2 / N]\n</code></pre></p> <p>2. Z-score: A Z-score, also known as a standard score, is a statistical measure that quantifies how far away a particular data point is from the mean (average) of a dataset when measured in terms of standard deviations. It's a way to standardize and normalize data, making it easier to compare values from different datasets or different parts of the same dataset. <pre><code>Z = (X - \u03bc) / \u03c3\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#when-to-use-standardization","title":"When to use Standardization?","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#example","title":"Example","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\Social_Network_Ads.csv\")\ndf.head()\n</code></pre> <pre><code>df.shape\n</code></pre> <pre><code># Checking the null values\ndf.isnull().sum()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"Purchased\", axis=1),\n                                                    df[\"Purchased\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#standard-scaler","title":"Standard Scaler","text":"<p>In scikit-learn, the StandardScaler is a preprocessing technique provided by the library for standardizing or scaling features in your dataset. It follows the standardization process I described earlier, where it scales the features to have a mean of 0 and a standard deviation of 1. This is done to ensure that all features have the same scale, making them more suitable for machine learning algorithms that are sensitive to feature scales.</p> <p>The 'fit' method is typically called on a machine learning model or a data preprocessing object to adapt it to the specific dataset you are working with. Its purpose is to learn from the data and update the internal state of the object.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the train set, it will learn the parameters\nscaler.fit(x_train)\n\n# Transform train and test sets\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n</code></pre> <pre><code># transform always returns a numpy array\n# Converting the scaled numpy arrays into pandas dataframes\nx_train_scaled = pd.DataFrame(data=x_train_scaled, columns=x_train.columns)\nx_test_scaled = pd.DataFrame(data=x_test_scaled, columns=x_test.columns)\n</code></pre> <pre><code>x_train_scaled.head()\n</code></pre> <pre><code># Describe the training data\nnp.round(x_train.describe(), 1)\n</code></pre> <pre><code># Describe the scaled training data\nnp.round(x_train_scaled.describe(), 1)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#effect-of-scaling","title":"Effect of Scaling","text":"<pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a scatter plot of the training data\nax1.scatter(x=x_train[\"Age\"], y=x_train[\"EstimatedSalary\"])\nax1.set_title(\"Before Standardization\")\nax1.set_xlabel(\"Age\")\nax1.set_ylabel(\"Estimated Salary\")\n\n# Creating a scatter plot of the scaled training data\nax2.scatter(x=x_train_scaled[\"Age\"], y=x_train_scaled[\"EstimatedSalary\"], color=\"red\")\nax2.set_title(\"After Standardization\")\nax2.set_xlabel(\"Standardized Age\")\nax2.set_ylabel(\"Standardized Estimated Salary\")\nplt.show()\n</code></pre> <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the training data\nax1.set_title(\"Before Standardization\")\nsns.kdeplot(data=x_train[\"Age\"], ax=ax1, label=\"Age\")\nsns.kdeplot(data=x_train[\"EstimatedSalary\"], ax=ax1, label=\"Estimated Salary\")\nax1.legend()\n\n# Creating a probabilty density plot of the scaled training data\nax2.set_title(\"After Standardization\")\nsns.kdeplot(data=x_train_scaled[\"Age\"], ax=ax2, label=\"Age\")\nsns.kdeplot(data=x_train_scaled[\"EstimatedSalary\"], ax=ax2, label=\"Estimated Salary\")\nax2.legend()\n\nax1.set_xlabel(\"\")\nax2.set_xlabel(\"\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#comparison-of-distribution","title":"Comparison of Distribution","text":"<pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the 'Age' column from the training data\nsns.kdeplot(x_train[\"Age\"], ax=ax1)\nax1.set_title(\"Age Distribution before Scaling\")\n\n# Creating a probabilty density plot of the 'Age' column from the scaled training data\nsns.kdeplot(x_train_scaled['Age'], ax=ax2)\nax2.set_title(\"Age Distribution after Scaling\")\n\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#why-scaling-is-important","title":"Why Scaling is Important?","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#comparison-on-logistic-regression-model","title":"Comparison on Logistic Regression Model","text":"<pre><code>from sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code># Creating two logistic regression model for the training data and scaled training data respectively\nlr = LogisticRegression()\nlr_scaled = LogisticRegression()\n</code></pre> <pre><code># Fitting the data to the models\nlr.fit(X=x_train, y=y_train)\nlr_scaled.fit(X=x_train_scaled, y=y_train)\n</code></pre> <pre><code># Predict the testing data\ny_pred = lr.predict(x_test)\ny_pred_scaled = lr_scaled.predict(x_test_scaled)\n</code></pre> <pre><code># Calculating the accuracy\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score on Actual Data:\", accuracy_score(y_test, y_pred).round(2))\nprint(\"Accuracy Score on Scaled Data:\", accuracy_score(y_test, y_pred_scaled).round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#comparison-on-decision-tree-model","title":"Comparison on Decision Tree Model","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Creating two decision tree model for the training data and scaled training data respectively\ndt = DecisionTreeClassifier()\ndt_scaled = DecisionTreeClassifier()\n</code></pre> <pre><code># Fitting the data to the models\ndt.fit(X=x_train, y=y_train)\ndt_scaled.fit(X=x_train_scaled, y=y_train)\n</code></pre> <pre><code># Predict the testing data\ny_pred = dt.predict(x_test)\ny_pred_scaled = dt_scaled.predict(x_test_scaled)\n</code></pre> <pre><code># Calculating the accuracy\nprint(\"Accuracy Score on Actual Data:\", accuracy_score(y_test, y_pred).round(2))\nprint(\"Accuracy Score on Scaled Data:\", accuracy_score(y_test, y_pred_scaled).round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#effect-of-outlier","title":"Effect of Outlier","text":"<p>Standardization, by itself, does not handle outliers in the data. In fact, standardization can sometimes exacerbate the impact of outliers, making them more prominent in the scaled data.</p> <pre><code># Adding some outliers to the data\noutliers = pd.DataFrame({\"Age\":[10, 90, 97], \"EstimatedSalary\":[1000, 250000, 350000], \"Purchased\": [0, 1, 1]})\noutliers\n</code></pre> <pre><code>new_df = pd.concat([df, outliers])\nnew_df\n</code></pre> <pre><code># Create a scatter plot\nplt.scatter(x=new_df[\"Age\"], y=new_df[\"EstimatedSalary\"])\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/00_Standardization/#applying-standardization","title":"Applying Standardization","text":"<pre><code># Splitting the data into training and testing\nx_train, x_test, y_train, y_test = train_test_split(new_df.drop([\"Purchased\"], axis=1), \n                                                    new_df[\"Purchased\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code># Creating a Standard Scaler object\nscaler = StandardScaler()\n\n# Fitting the data\nscaler.fit(x_train)\n\n# Applying Standardization on the training and testing data\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n</code></pre> <pre><code># Converting the scaled data into a pandas dataframe\nx_train_scaled = pd.DataFrame(data=x_train_scaled, columns=x_train.columns)\nx_test_scaled = pd.DataFrame(data=x_test_scaled, columns=x_test.columns)\nx_train_scaled.head()\n</code></pre> <pre><code># Plot the data\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Plotting the x_train data\nax1.scatter(x=x_train[\"Age\"], y=x_train[\"EstimatedSalary\"])\nax1.set_title(\"Before Standardization\")\nax1.set_xlabel(\"Age\")\nax1.set_ylabel(\"Estimated Salary\")\n\n# Plotting the scaled x_train data\nax2.scatter(x=x_train_scaled[\"Age\"], y=x_train_scaled[\"EstimatedSalary\"], color=\"red\")\nax2.set_title(\"After Standardization\")\nax2.set_xlabel(\"Standardized Age\")\nax2.set_ylabel(\"Standardized Estimated Salary\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/01_%20Normalization/","title":"Normalization","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/01_%20Normalization/#normalization-feature-scaling","title":"Normalization - Feature Scaling","text":"<p>Normalization, also known as Min-Max scaling, is a feature scaling technique used in data preprocessing to rescale numerical features to a specific range, typically between 0 and 1. The goal of normalization is to ensure that all feature values have similar scales, making them more suitable for machine learning algorithms that are sensitive to the magnitude of input features.</p>    ## **When to use Normalization?**      # **Example**   ## **Import Required Libraries**  <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre>  ## **Read the Data**  <pre><code># Read some specific column from the data\ncsv_path = r\"D:\\Coding\\Datasets\\wine_data.csv\"\ndf = pd.read_csv(csv_path, usecols=[\"Class\", \"Alcohol\", \"Malic acid\"])\ndf.head()\n</code></pre> <pre><code>df.shape\n</code></pre>  ## **Data Visualization**  <pre><code># Creating probabilty density plots of the data\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\nsns.kdeplot(data=df[\"Alcohol\"], ax=ax1, c=\"red\", label=\"Alcohol\")\nax1.set_title(\"Probability Distribution of Alcohol\")\nax1.legend()\n\nsns.kdeplot(data=df[\"Malic acid\"], ax=ax2, c=\"blue\", label=\"Malic acid\")\nax2.set_title(\"Probability Distribution of Malic acid\")\nax2.legend()\n\nplt.show()\n</code></pre> <pre><code># Creating a scatter plot of the data\nsns.scatterplot(x=df[\"Alcohol\"], y=df[\"Malic acid\"], hue=df[\"Class\"], palette=[\"red\", \"blue\", \"green\"])\n</code></pre>  ## **Train Test Split**  <pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"Class\", axis=1), \n                                                    df[\"Class\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>  ## **MinMax Scaler**  <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Creating a MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit the scaler to the train set, it will learn the parameters\nscaler.fit(x_train)\n\n# Transform train and test sets\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n</code></pre> <pre><code># transform always returns a numpy array\n# Converting the scaled numpy arrays into pandas dataframes\nx_train_scaled = pd.DataFrame(data=x_train_scaled, columns=[\"Alcohol\", \"Malic acid\"])\nx_test_scaled = pd.DataFrame(data=x_test_scaled, columns=[\"Alcohol\", \"Malic acid\"])\n</code></pre> <pre><code>x_train_scaled.head()\n</code></pre> <pre><code># Describe the training data\nnp.round(x_train.describe(), 2)\n</code></pre> <pre><code># Describe the scaled training data\nnp.round(x_train_scaled.describe(), 2)\n</code></pre>  ## **Effect of Scaling**  <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a scatter plot of the training data\nsns.scatterplot(data=x_train, x=\"Alcohol\", y=\"Malic acid\", ax=ax1, c=y_train)\nax1.set_title(\"Before Scaling\")\n\n# Creating a scatter plot of the scaled training data\nsns.scatterplot(data=x_train_scaled, x=\"Alcohol\", y=\"Malic acid\", ax=ax2, c=y_train)\nax2.set_title(\"After Scaling\")\n\nplt.show()\n</code></pre> <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probability density plot of the training data\nsns.kdeplot(data=x_train[\"Alcohol\"], ax=ax1, label=\"Alcohol\")\nsns.kdeplot(data=x_train[\"Malic acid\"], ax=ax1, label=\"Malic acid\")\nax1.legend()\nax1.set_xlabel(\"\")\nax1.set_title(\"Before Scaling\")\n\n# Creating a probability density plot of the scaled training data\nsns.kdeplot(data=x_train_scaled[\"Alcohol\"], ax=ax2, label=\"Alcohol\")\nsns.kdeplot(data=x_train_scaled[\"Malic acid\"], ax=ax2, label=\"Malic acid\")\nax2.legend()\nax2.set_xlabel(\"\")\nax2.set_title(\"After Scaling\")\n\nplt.show()\n</code></pre>  ## **Comparison of Distribution**  <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the 'Alcohol' column from the training data\nsns.kdeplot(x_train[\"Alcohol\"], ax=ax1)\nax1.set_title(\"Alcohol Distribution before Scaling\")\n\n# Creating a probabilty density plot of the 'Alcohol' column from the scaled training data\nsns.kdeplot(x_train_scaled['Alcohol'], ax=ax2)\nax2.set_title(\"Alcohol Distribution after Scaling\")\n\nplt.show()\n</code></pre> <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the 'Malic acid' column from the training data\nsns.kdeplot(x_train[\"Malic acid\"], ax=ax1)\nax1.set_title(\"Malic acid Distribution before Scaling\")\n\n# Creating a probabilty density plot of the 'Malic acid' column from the scaled training data\nsns.kdeplot(x_train_scaled['Malic acid'], ax=ax2)\nax2.set_title(\"Malic acid Distribution after Scaling\")\n\nplt.show()\n</code></pre>  ## **Difference between Standardization and Normalization** Standardization and normalization are two common techniques for feature scaling in data preprocessing, and they have different approaches and effects on the data:  **Goal:** * **Standardization:** The goal of standardization is to transform the data in such a way that it has a mean of 0 and a standard deviation of 1. It centers the data around zero and scales it by the standard deviation. * **Normalization:** The goal of normalization is to rescale the data to a specific range, typically between 0 and 1. It preserves the relative relationships between data points but scales them to fit within the chosen range."},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/","title":"Encoding Categorical Data","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#encoding-categorical-data","title":"Encoding Categorical Data","text":"<p>Encoding categorical data is an essential step in preparing data for machine learning models since most machine learning algorithms require numerical input data. Categorical data represents non-numeric data such as categories, labels, or classes.</p> <p>In Python, you can use various techniques to encode categorical data, and the choice of encoding method depends on the nature of your data and the machine learning algorithm you plan to use.</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\customer.csv\")\ndf.head()\n</code></pre> <pre><code>df.shape\n</code></pre> <pre><code># Extrcting the 'review', 'education' and 'purchased' colums from the dataframe\ndf = df.iloc[:, 2:]\n</code></pre> <pre><code>df.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df.drop(\"purchased\", axis=1),\n                                                    df[\"purchased\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#ordinal-encoding","title":"Ordinal Encoding","text":"<p>Ordinal encoding is a technique for encoding categorical data where the categories have a meaningful order or ranking. This method assigns a unique integer value to each category based on its order or priority. Ordinal encoding is appropriate when the categorical data represents ordered or ranked values, such as \"low,\" \"medium,\" and \"high\" or \"small,\" \"medium,\" \"large.\"</p> <pre><code>from sklearn.preprocessing import OrdinalEncoder\n</code></pre> <pre><code># Checking the unique values in each column\nprint(\"Unique values in each column:\")\nfor i in range(len(df.columns)):\n    print(f\"{df.columns[i]}: {df.iloc[:, i].unique()}\")\n</code></pre> <pre><code># Creating an object of ordinal encoder class\nordinal_encoder = OrdinalEncoder(categories=[[\"Poor\", \"Average\", \"Good\"], [\"School\", \"UG\", \"PG\"]],\n                                 dtype=np.int8)\n# Fit the training data\nordinal_encoder.fit(x_train)\n\n# Transform the training and testing data\nx_train_encoded = ordinal_encoder.transform(x_train)\nx_test_encoded = ordinal_encoder.transform(x_test)\n</code></pre> <pre><code>ordinal_encoder.categories_\n</code></pre> <pre><code># Converting the encoded array into pandas dataframe\nx_train_encoded = pd.DataFrame(x_train_encoded, columns=[\"review\", \"education\"])\nx_test_encoded = pd.DataFrame(x_test_encoded, columns=[\"review\", \"eucation\"])\n</code></pre> <pre><code># Print the non-encoded training data\nx_train.head()\n</code></pre> <pre><code># Print the encoded training data\nx_train_encoded.head()\n</code></pre> <pre><code># Print the encoded testing data\nx_test_encoded.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/02_Encoding_Categorical_Data/#label-encoding","title":"Label Encoding","text":"<p>Label encoding is a technique for encoding categorical data into numerical values, where each category is assigned a unique integer label. This encoding is suitable for categorical data where there is no inherent order or ranking among the categories.</p> <p>You can use the 'LabelEncoder' class from the sklearn.preprocessing module to perform label encoding. This encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y and not the input X.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n</code></pre> <pre><code># Creating an object of the label encoder class\nlabel_encoder = LabelEncoder()\n\n# Fit the training data\nlabel_encoder.fit(y_train)\n\n# Transform the training and testing data\ny_train_encoded = label_encoder.transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n</code></pre> <pre><code>label_encoder.classes_\n</code></pre> <pre><code># Print the y_train data\ny_train.head(10)\n</code></pre> <pre><code># Print the y_train_encoded data\ny_train_encoded\n</code></pre> <pre><code># Print the y_test_encoded data\ny_test_encoded\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/","title":"One Hot Encoding","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding","title":"One Hot Encoding","text":"<p>One-Hot Encoding is a popular technique used in machine learning and data preprocessing, especially when dealing with categorical data. It is used to represent categorical variables as binary vectors or matrices, where each category is mapped to a unique binary value. </p> <p>This transformation is necessary because many machine learning algorithms and models require numerical input, and categorical data in its raw form cannot be directly used in these algorithms.</p> <p></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\cars.csv\")\ndf.head()\n</code></pre> <pre><code>df.shape\n</code></pre> <pre><code># Check the number of unique brand names\ndf[\"brand\"].nunique()\n</code></pre> <pre><code># Count the values for each brand in 'brand' column\ndf[\"brand\"].value_counts()\n</code></pre> <pre><code># Count the values for each unique name in 'fuel' column\ndf[\"fuel\"].value_counts()\n</code></pre> <pre><code># Count the values for each unique name in 'owner' column\ndf[\"owner\"].value_counts()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding-with-pandas","title":"One Hot Encoding with Pandas","text":"<pre><code># Applying One Hot Encoding on 'fuel' and 'owner' columns\npd.get_dummies(data=df, columns=[\"fuel\", \"owner\"])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#k-1-one-hot-encoding-with-pandas","title":"K-1 One Hot Encoding with Pandas","text":"<p>When using the pd.get_dummies() function in Pandas, you can drop the first category (column) of each categorical variable to avoid multicollinearity, which can be useful in certain situations. This is done using the drop_first parameter. Setting drop_first=True will drop the first category from each categorical variable after one-hot encoding.</p> <pre><code># Applying One Hot Encoding on 'fuel' and 'owner' columns\n# Removing the first categorical variable to avoid multicolinearity\npd.get_dummies(data=df, columns=[\"fuel\", \"owner\"], drop_first=True)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding-using-sklearn","title":"One Hot Encoding using Sklearn","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Print the dataframe\ndf.head()\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df.drop(\"selling_price\", axis=1),\n                                                    df[\"selling_price\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, y_train.shape\n</code></pre> <pre><code>x_train.head()\n</code></pre> <pre><code>x_train.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#apply-ohe-on-fuel-and-owner-columns","title":"Apply OHE on 'fuel' and 'owner' Columns","text":"<pre><code>from sklearn.preprocessing import OneHotEncoder\n</code></pre> <pre><code># Creating an object of the One Hot Encode class\none_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False, dtype=np.int8)\n\n# Separating the 'fuel' and 'owner' columns from the x_train dataframe\n# Fit the separated training data\none_hot_encoder.fit(x_train[[\"fuel\", \"owner\"]])\n\n# Transform the separated training data\nx_train_encoded = one_hot_encoder.transform(x_train[[\"fuel\", \"owner\"]])\nx_train_encoded\n</code></pre> <pre><code>x_train_encoded.shape\n</code></pre> <pre><code># Merge the x_train_encoded columns with the 'brand' and 'km_driven' columns\nx_train_merged = np.hstack((x_train[[\"brand\", \"km_driven\"]], x_train_encoded))\nx_train_merged\n</code></pre> <pre><code>x_train_merged.shape\n</code></pre> <pre><code># Print the column names of the encoded x_train data\none_hot_encoder.get_feature_names_out()\n</code></pre> <pre><code># Define the column names in an array\ncolumn_names = np.concatenate((x_train.columns[0:2], one_hot_encoder.get_feature_names_out()), axis=0)\nprint(len(column_names))\ncolumn_names\n</code></pre> <pre><code># Convert the x_train_merged array into pandas dataframe\nx_train_encoded = pd.DataFrame(data=x_train_merged, columns=column_names)\nx_train_encoded\n</code></pre> <pre><code>x_train_encoded.shape\n</code></pre> <pre><code># Print the x_test data\nx_test.head()\n</code></pre> <pre><code># Encode x_test data\nx_test_encoded = one_hot_encoder.transform(x_test[[\"fuel\", \"owner\"]])\nx_test_encoded\n</code></pre> <pre><code># Merge the x_test_encoded columns with the 'brand' and 'km_driven' columns\nx_test_merged = np.hstack((x_test.iloc[:, 0:2], x_test_encoded))\nx_test_merged\n</code></pre> <pre><code># Convert the x_test_merged array into pandas dataframe\nx_test_encoded = pd.DataFrame(data=x_test_merged, columns=column_names)\nx_test_encoded\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/03_One_Hot_Encoding/#apply-ohe-on-brand-column-using-pandas","title":"Apply OHE on 'brand' Column using Pandas","text":"<pre><code># Count the values for each brand in 'brand' column\ncounts = df[\"brand\"].value_counts()\ncounts\n</code></pre> <pre><code># Check the total number of unique brands\ndf[\"brand\"].nunique()\n</code></pre> <pre><code># Define a threshold\nthreshold = 100\n</code></pre> <pre><code># Store the name of brands in a list where the value count is less than 100\nrepl = counts[counts &lt;= threshold].index\nrepl\n</code></pre> <pre><code># Replace the name of the brand with 'others'\nnew_df = df.replace(to_replace=repl, value=\"Others\")\nnew_df\n</code></pre> <pre><code>new_df[\"brand\"].value_counts()\n</code></pre> <pre><code># Apply OHE on 'brand' column of the new dataframe\npd.get_dummies(data=new_df[\"brand\"]).sample(20)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/","title":"Column Transformer","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#column-transformer","title":"Column Transformer","text":"<p>The ColumnTransformer is a feature in scikit-learn, a popular Python machine learning library, that allows you to apply different preprocessing steps to different subsets of the columns (features) in your dataset. It is particularly useful when you have a dataset with a mix of numerical and categorical features, and you want to apply different transformations to these feature types.</p> <p>Here's an overview of how the ColumnTransformer works:</p> <ol> <li> <p>Specify Transformers: First, you define a list of transformers, where each transformer specifies a particular preprocessing step to be applied to a subset of the columns. For example, you might have one transformer for numerical columns (e.g., scaling), another for categorical columns (e.g., one-hot encoding), and maybe even other transformers for specific subsets of columns.</p> </li> <li> <p>Specify Columns: For each transformer, you also specify which columns it should be applied to. This is done using the columns parameter, where you can specify either column indices or column names.</p> </li> <li> <p>Combine Transformers: You create a ColumnTransformer object and pass in the list of transformers. You can also specify what to do with the remaining columns that are not specified in any of the transformers, using the remainder parameter. Options include dropping them or passing them through without any transformation.</p> </li> <li> <p>Fit and Transform: You can then fit the ColumnTransformer on your dataset using the fit method, and subsequently transform your dataset using the transform method. The ColumnTransformer applies the specified transformations to the designated columns and returns a transformed dataset.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <pre><code>from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\covid_toy.csv\")\ndf\n</code></pre> <pre><code># Check the information of the columns\ndf.info()\n</code></pre> <pre><code># Check the number of null values in each column\ndf.isnull().sum()\n</code></pre> <pre><code># Check all the unique values of the categorical columns\nfor column in df.select_dtypes(include=\"object\").columns:\n    unique_values = df[column].unique()\n    print(f\"{column}: {unique_values}\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#preprocessing-without-column-transformer","title":"Preprocessing without Column Transformer","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"has_covid\", axis=1),\n                                                    df[\"has_covid\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train.head(10)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#fill-the-null-values-of-fever-column-using-simpleimputer","title":"Fill the Null Values of 'fever' Column using SimpleImputer","text":"<pre><code># Create a SimpleImputer object\nsimple_imputer = SimpleImputer()\n\n# Fit the 'fever' column of the training data\nsimple_imputer.fit(x_train[[\"fever\"]])\n\n# Transform the 'fever' column of the training and testing data\nx_train_fever = simple_imputer.transform(x_train[[\"fever\"]])\nx_test_fever = simple_imputer.transform(x_test[[\"fever\"]])\n</code></pre> <pre><code># Print the first ten values of the x_train_fever\nx_train_fever[:10]\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#apply-ordinal-encdoing-to-cough-column","title":"Apply Ordinal Encdoing to 'cough' Column","text":"<pre><code># Create an object of the OrdinalEncoder\nordinal_encoder = OrdinalEncoder(categories=[[\"Mild\", \"Strong\"]], dtype=int)\n\n# Fit the 'cough' column of the training data\nordinal_encoder.fit(x_train[[\"cough\"]])\n\n# Transform the 'cough' column of the training and testing data\nx_train_cough = ordinal_encoder.transform(x_train[[\"cough\"]])\nx_test_cough = ordinal_encoder.transform(x_test[[\"cough\"]])\n</code></pre> <pre><code># Print the first ten values of the x_train_cough\nx_train_cough[:10]\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#apply-one-hot-encdoing-to-gender-and-city-columns","title":"Apply One Hot Encdoing to 'gender' and 'city' Columns","text":"<pre><code># Create an object of the OneHotencoder\none_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False, dtype=int)\n\n# Fit the 'genedr' and 'city' columns of the training data\none_hot_encoder.fit(x_train[[\"gender\", \"city\"]])\n\n# Transform the 'genedr' and 'city' columns of the training and testing data\nx_train_gender_city = one_hot_encoder.transform(x_train[[\"gender\", \"city\"]])\nx_test_gender_city = one_hot_encoder.transform(x_test[[\"gender\", \"city\"]])\n</code></pre> <pre><code># Check the new column names after applying One Hot Encoding\none_hot_encoder.get_feature_names_out()\n</code></pre> <pre><code># Print the first ten values of the x_train_gender_city\nx_train_gender_city[:10]\n</code></pre> <pre><code>x_train_cough.shape\n</code></pre> <pre><code># Convert the 'age' column into numpy array \nx_train_age = np.array(x_train[\"age\"]).reshape((70, 1))\nx_test_age = np.array(x_test[\"age\"]).reshape((30, 1))\n</code></pre> <pre><code># Print the first ten values of the x_train_age\nx_train_age[:10]\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#concatenating-all-the-arrays-for-the-training-and-testing-data","title":"Concatenating all the Arrays for the Training and Testing Data","text":"<pre><code># Concatenating all the columns of the training data\nx_train_transformed = np.concatenate((x_train_age, x_train_fever, x_train_cough, x_train_gender_city), axis=1)\n\n# Concatenating all the columns of the training data\nx_test_transformed = np.concatenate((x_test_age, x_test_fever, x_test_cough, x_test_gender_city), axis=1)\n</code></pre> <pre><code># Defining the column names of the transformed dataframe\ncolumn_names = np.concatenate((np.array([\"age\", \"fever\", \"cough\"]), one_hot_encoder.get_feature_names_out()))\ncolumn_names\n</code></pre> <pre><code># Convert transformed data into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=column_names)\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=column_names)\n</code></pre> <pre><code># Print the transformed data\nx_train_transformed\n</code></pre> <pre><code># Print the information of the transformed training data\nx_train_transformed.info()\n</code></pre> <pre><code>x_test_transformed.head(10)\n</code></pre> <pre><code># Print the information of the transformed testing data\nx_test_transformed.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/04_Column_Transformer/#preprocessing-with-column-transformer","title":"Preprocessing with Column Transformer","text":"<pre><code>from sklearn.compose import ColumnTransformer\n</code></pre> <pre><code># Create an object of the ColumnTransformer\ntransformer = ColumnTransformer(transformers=[\n    (\"tranformer_1\", SimpleImputer(), [\"fever\"]),\n    (\"transformer_2\", OrdinalEncoder(categories=[[\"Mild\", \"Strong\"]]), [\"cough\"]),\n    (\"transformer_3\", OneHotEncoder(drop=\"first\", sparse_output=False), [\"gender\", \"city\"])\n], remainder=\"passthrough\")\n</code></pre> <pre><code># Fit and transform the training data\nx_train_transformed = transformer.fit_transform(x_train)\nx_train_transformed.shape\n</code></pre> <pre><code># Transform the testing data\nx_test_transformed = transformer.transform(x_test)\nx_test_transformed.shape\n</code></pre> <pre><code># Checking the new column names of the transformed data\ntransformer.get_feature_names_out()\n</code></pre> <pre><code># Convert the transformed array into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=transformer.get_feature_names_out())\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=transformer.get_feature_names_out())\n</code></pre> <pre><code>x_train_transformed\n</code></pre> <pre><code>x_test_transformed.head(10)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/","title":"Ml Without Pipeline 1","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#machine-learning-without-pipeline-1","title":"Machine Learning without Pipeline - 1","text":"<p>\"Machine learning without pipelines\" refers to the practice of implementing machine learning tasks and workflows without utilizing the structured concept of pipelines. In machine learning, pipelines are a systematic and organized approach for data preprocessing, feature engineering, model selection, training, and evaluation. However, there are situations where you may choose not to use pipelines, especially in simple or small-scale machine learning projects.</p> <p></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\ndf\n</code></pre> <pre><code># Dropping the unnecessary columns\ndf.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], inplace=True)\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"Survived\", axis=1),\n                                                    df[\"Survived\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train.head()\n</code></pre> <pre><code>y_train.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code># Check the information of the columns\ndf.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#apply-simpleimputer-on-age-and-embarked-columns","title":"Apply SimpleImputer on 'Age' and 'Embarked' Columns","text":"<pre><code>from sklearn.impute import SimpleImputer\n</code></pre> <pre><code># Create an object of the SimpleImputer class\nsimple_imputer_age = SimpleImputer()\nsimple_imputer_embarked = SimpleImputer(strategy=\"most_frequent\")\n\n# Fit the training data\nsimple_imputer_age.fit(x_train[[\"Age\"]])\nsimple_imputer_embarked.fit(x_train[[\"Embarked\"]])\n\n# Transform the 'Age' and 'Embarked' columns of the training data\nx_train_age = simple_imputer_age.transform(x_train[[\"Age\"]])\nx_train_embarked = simple_imputer_embarked.transform(x_train[[\"Embarked\"]])\n\n# Transform the 'Age' and 'Embarked' columns of the testing data\nx_test_age = simple_imputer_age.transform(x_test[[\"Age\"]])\nx_test_embarked = simple_imputer_embarked.transform(x_test[[\"Embarked\"]])\n</code></pre> <pre><code># Print the first 5 values of x_train_age\nx_train_age[:5]\n</code></pre> <pre><code># Print the first 5 values of x_train_embarked\nx_train_embarked[:5]\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#apply-onehot-encoder-on-sex-and-embarked-columns","title":"Apply OneHot Encoder on 'Sex' and 'Embarked' Columns","text":"<pre><code>from sklearn.preprocessing import OneHotEncoder\n</code></pre> <pre><code># Create an object of the OneHotEncoder class\none_hot_encoder_sex = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\none_hot_encoder_embarked = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n\n# Fit the training data\none_hot_encoder_sex.fit(x_train[[\"Sex\"]])\none_hot_encoder_embarked.fit(x_train_embarked)\n\n# Transform the 'Sex' and 'Embarked' columns of the training data\nx_train_sex = one_hot_encoder_sex.transform(x_train[[\"Sex\"]])\nx_train_embarked = one_hot_encoder_embarked.transform(x_train_embarked)\n\n# Transform the 'Sex' and 'Embarked' columns of the testing data\nx_test_sex = one_hot_encoder_sex.transform(x_test[[\"Sex\"]])\nx_test_embarked = one_hot_encoder_embarked.transform(x_test_embarked)\n</code></pre> <pre><code># Print the first 5 values of x_train_sex\nx_train_sex[:5]\n</code></pre> <pre><code># Print the first 5 values of x_train_embarked\nx_train_embarked[:5]\n</code></pre> <pre><code># Drop the 'Age', 'Sex' and 'Embarked' column from the training data\nx_train_remaining = x_train.drop(columns=[\"Age\", \"Sex\", \"Embarked\"])\nx_train_remaining.head()\n</code></pre> <pre><code># Drop the 'Age', 'Sex' and 'Embarked' column from the testing data\nx_test_remaining = x_test.drop(columns=[\"Age\", \"Sex\", \"Embarked\"])\nx_test_remaining.head()\n</code></pre> <pre><code># Merge the processed columns with the reamaining dataframe\nx_train_transformed = np.concatenate((x_train_remaining, x_train_age, x_train_sex, x_train_embarked), axis=1)\nx_test_transformed = np.concatenate((x_test_remaining, x_test_age, x_test_sex, x_test_embarked), axis=1)\n</code></pre> <pre><code># Print the x_train_transformed data\nx_train_transformed\n</code></pre> <pre><code>one_hot_encoder_embarked.get_feature_names_out()\n</code></pre> <pre><code># Assemble the column names of the transformed data\nx_transformed_columns = np.array(x_train_remaining.columns)\nx_transformed_columns = np.concatenate((x_transformed_columns, \n                                        simple_imputer_age.get_feature_names_out(),\n                                        one_hot_encoder_sex.get_feature_names_out(),\n                                        one_hot_encoder_embarked.get_feature_names_out()))\n</code></pre> <pre><code>x_transformed_columns\n</code></pre> <pre><code># Convert the tranformed arrays into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=x_transformed_columns)\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=x_transformed_columns)\n</code></pre> <pre><code>x_train_transformed.head()\n</code></pre> <pre><code>x_test_transformed.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#build-a-decisiontree-classifier","title":"Build a DecisionTree Classifier","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Instantiate a DecisionTreeClassifier object\ndt_classifier = DecisionTreeClassifier(random_state=0)\n\n# Fit the training data\ndt_classifier.fit(x_train_transformed, y_train)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the x_test_transformed data\ny_pred = dt_classifier.predict(x_test_transformed)\ny_pred\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score\n</code></pre> <pre><code># Print the overall accuracy of the decision tree model\naccuracy_score(y_test, y_pred)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/05_ML_without_Pipeline_1/#export-the-model","title":"Export the Model","text":"<pre><code>import pickle\n</code></pre> <pre><code># Exporting the one_hot_encoder_sex\npickle.dump(one_hot_encoder_sex, file=open(\"D:\\Coding\\Models\\ohe_sex.pkl\", \"wb\"))\n\n# Exporting the one_hot_encoder_embarked\npickle.dump(one_hot_encoder_embarked, file=open(\"D:\\Coding\\Models\\ohe_embarked.pkl\", \"wb\"))\n\n# Exporting the decision tree classifier\npickle.dump(dt_classifier, file=open(\"D:\\Coding\\Models\\decision_tree_model.pkl\", \"wb\"))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/","title":"Ml Without Pipeline 2","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#machine-learning-without-pipeline-2","title":"Machine Learning without Pipeline - 2","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pickle\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#load-the-model-and-encoders","title":"Load the Model and Encoders","text":"<pre><code># Load the model and the encoders\nohe_sex = pickle.load(open(\"D:\\Coding\\Models\\ohe_sex.pkl\", \"rb\"))\nohe_embarked = pickle.load(open(\"D:\\Coding\\Models\\ohe_embarked.pkl\", \"rb\"))\nclf = pickle.load(open(\"D:\\Coding\\Models\\decision_tree_model.pkl\", \"rb\"))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#take-user-input","title":"Take User Input","text":"<pre><code># Assume user input\n# Pclass / Sex / Age / SibSp / Parch / Fare / Embarked\ntest_input = np.array([2, \"male\", 22.0, 1, 0, 25.0, \"S\"], dtype=object).reshape(1, 7)\n</code></pre> <pre><code>test_input\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#transform-the-input-using-encoders","title":"Transform the Input using Encoders","text":"<pre><code> # Encode the 'Sex'\ntest_input_sex = ohe_sex.transform(test_input[:, 1].reshape(1, 1))\ntest_input_sex\n</code></pre> <pre><code># Encode the 'Embarked'\ntest_input_embarked = ohe_embarked.transform(test_input[:, -1].reshape(1, 1))\ntest_input_embarked\n</code></pre> <pre><code># Extract the 'Age'\ntest_input_age = test_input[:, 2].reshape(1, 1)\ntest_input_age\n</code></pre> <pre><code># Extract the remaining columns from test_input data\ntest_input_rem = test_input[:, [0, 3, 4, 5]]\ntest_input_rem\n</code></pre> <pre><code># Merge the encoded data with the test input data\ntest_input_merged = np.concatenate((test_input_rem, test_input_age, test_input_sex, test_input_embarked), axis=1)\ntest_input_merged\n</code></pre> <pre><code>test_input_merged.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/06_ML_without_Pipeline_2/#predict-the-input-data-with-the-model","title":"Predict the Input Data with the Model","text":"<pre><code>predict = clf.predict(test_input_merged)\npredict\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/","title":"Ml With Pipeline 1","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#machine-learning-with-pipeline-1","title":"Machine Learning with Pipeline - 1","text":"<p>Machine Learning with a pipeline is a common practice in the field of data science and machine learning. A pipeline is a series of data processing components (transformers and an estimator) that are chained together to streamline the workflow in machine learning tasks. It helps in organizing and automating the various steps involved in building and evaluating machine learning models.</p> <p></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\ndf\n</code></pre> <pre><code># Dropping the unecessary columns\ndf.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], inplace=True)\ndf.head()\n</code></pre> <pre><code># Check to column informations\ndf.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"Survived\", axis=1),\n                                                    df[\"Survived\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#preprocess-the-data-using-column-transformer","title":"Preprocess the Data using Column Transformer","text":"<pre><code>from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.compose import ColumnTransformer\n</code></pre> <pre><code># Create an imputation transformer for 'Age' and 'Embarked' columns\ntransformer_1 = ColumnTransformer([\n    (\"impute_age\", SimpleImputer(), [2]),\n    (\"impute_embarked\", SimpleImputer(strategy=\"most_frequent\"), [6])\n], remainder=\"passthrough\")\n</code></pre> <pre><code># Create an One Hot Encoding tranformer for 'Sex' and 'Embarked' columns\ntransformer_2 = ColumnTransformer([\n    (\"ohe_sex_embarked\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), [1, 6])\n], remainder=\"passthrough\")\n</code></pre> <pre><code># Create a transformer for scale the values\ntransformer_3 = ColumnTransformer([\n    (\"scale\", MinMaxScaler(), slice(0, 10))\n])\n</code></pre> <pre><code># Create a transformer to select best 8 features\ntransformer_4 = SelectKBest(score_func=chi2, k=8)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#train-a-decision-tree-model","title":"Train a Decision Tree Model","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Train a decision tree classifier\ndt_classifier = DecisionTreeClassifier(random_state=0)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#create-pipeline","title":"Create Pipeline","text":"<pre><code>from sklearn.pipeline import Pipeline\n</code></pre> <pre><code>pipe = Pipeline([\n    (\"transformer_1\", transformer_1),\n    (\"transformer_2\", transformer_2),\n    (\"transformer_3\", transformer_3),\n    (\"transformer_4\", transformer_4),\n    (\"transformer_5\", dt_classifier)\n])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#pipeline-vs-make_pipeline","title":"Pipeline vs make_pipeline","text":"<pre><code>from sklearn.pipeline import make_pipeline\n</code></pre> <pre><code># Alternate Syntax\npipe = make_pipeline(transformer_1, transformer_2, transformer_3, transformer_4, dt_classifier)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#train-the-model-using-pipeline","title":"Train the Model using Pipeline","text":"<pre><code># Train the model\npipe.fit(x_train, y_train)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#explore-the-pipeline","title":"Explore the Pipeline","text":"<pre><code># Print the steps in the pipeline\npipe.named_steps\n</code></pre> <pre><code># Check the mean value of the SimpleImputer object for 'age' column\npipe.named_steps[\"columntransformer-1\"].transformers_[0][1].statistics_\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = pipe.predict(x_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score\n</code></pre> <pre><code># Print the overall accuracy of the model\naccuracy_score(y_test, y_pred)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#cross-validation-using-pipeline","title":"Cross Validation using Pipeline","text":"<pre><code>from sklearn.model_selection import cross_val_score\n</code></pre> <pre><code># Cross validation using cross_val_score\ncross_val_score(pipe, x_train, y_train, cv=5, scoring=\"accuracy\").mean()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#gridsearch-using-pipeline","title":"GridSearch using Pipeline","text":"<pre><code># Define the parameters for GridSearch\nparams = {\n    \"decisiontreeclassifier__max_depth\":[1, 2, 3, 4, 5, None]\n}\n</code></pre> <pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre> <pre><code># Create an object of the GridSearchCV Class\ngrid = GridSearchCV(estimator=pipe, param_grid=params, cv=5, scoring=\"accuracy\")\n\n# Fit the training data\ngrid.fit(x_train, y_train)\n</code></pre> <pre><code># Print the best parameters for the model\ngrid.best_params_\n</code></pre> <pre><code># Print the overall accuracy\ngrid.best_score_\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/07_ML_with_Pipeline_1/#export-the-pipeline","title":"Export the Pipeline","text":"<pre><code>import pickle\n</code></pre> <pre><code>pickle.dump(pipe, file=open(\"D:\\Coding\\Models\\pipe.pkl\", \"wb\"))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/","title":"Ml With Pipeline 2","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/#machine-learning-with-pipeline-2","title":"Machine Learning with Pipeline - 2","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import pickle\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/#load-the-model","title":"Load the Model","text":"<pre><code>pipe = pickle.load(open(\"D:\\Coding\\Models\\pipe.pkl\", \"rb\"))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/#take-user-input","title":"Take User Input","text":"<pre><code># Assume user input\ntest_input = np.array([2, \"male\", 22.0, 1, 0, 25.0, \"S\"], dtype=\"object\").reshape(1, 7)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/08_ML_with_Pipeline_2/#predict-the-input-data-with-the-model","title":"Predict the Input Data with the Model","text":"<pre><code>pipe.predict(test_input)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/","title":"Function Transformer","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#function-transformer","title":"Function Transformer","text":"<p>FunctionTransformer is a class in scikit-learn that allows you to apply a custom transformation function to your data as part of a scikit-learn pipeline. It is particularly useful when you have a transformation that is not available as a built-in preprocessing step in scikit-learn, and you want to incorporate it into your machine learning workflow.</p> <p>Here are some key points about FunctionTransformer:</p> <ul> <li> <p>Custom Transformation: FunctionTransformer is used to apply a custom-defined function to transform your data. This function can be any valid Python function that takes an input array-like or pandas DataFrame and returns a transformed version of the data.</p> </li> <li> <p>Seamless Integration: You can use FunctionTransformer seamlessly within a scikit-learn pipeline, which allows you to create a sequence of data processing and modeling steps. This is helpful for ensuring that your custom transformation is applied consistently to both training and testing data.</p> </li> </ul>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\ndf.head()\n</code></pre> <pre><code># Selct only 'Age', 'Fare' and 'Survived' columns\ndf = df[[\"Survived\", \"Age\", 'Fare']]\ndf.head()\n</code></pre> <pre><code># Check the column information of the dataframe\ndf.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code># Fill the null values of the 'Age' column\ndf[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\ndf.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(\"Survived\", axis=1),\n                                                    df['Survived'],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#data-visualization","title":"Data Visualization","text":"<pre><code># Plot the distribution of the 'Age' column\nplt.figure(figsize=(12, 4))\n\nplt.subplot(121)\nsns.distplot(x_train[\"Age\"])\nplt.title(\"Age PDF\")\n\nplt.subplot(122)\nstats.probplot(x_train[\"Age\"], dist=\"norm\", plot=plt)\nplt.title(\"Age QQ Plot\")\n\nplt.show()\n</code></pre> <pre><code># Plot the distribution of the 'Fare' column\nplt.figure(figsize=(12, 4))\n\nplt.subplot(121)\nsns.distplot(x_train[\"Fare\"])\nplt.title(\"Fare PDF\")\n\nplt.subplot(122)\nstats.probplot(x_train[\"Fare\"], dist=\"norm\", plot=plt)\nplt.title(\"Fare QQ Plot\")\n\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#train-a-classifier","title":"Train a Classifier","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Create object of two different models\nlr_clf = LogisticRegression()\ndt_clf = DecisionTreeClassifier()\n</code></pre> <pre><code># Fit the training data to Logistic Regression model\nlr_clf.fit(x_train, y_train)\n</code></pre> <pre><code># Fit the training data to Decision Tree model\ndt_clf.fit(x_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred_lr = lr_clf.predict(x_test)\ny_pred_dt = dt_clf.predict(x_test)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#assess-the-accuracy","title":"Assess the Accuracy","text":"<pre><code>from sklearn.metrics import accuracy_score\n</code></pre> <pre><code>print(\"Accuarcy Score of LR Model:\", accuracy_score(y_test, y_pred_lr))\nprint(\"Accuracy Score of DT Model:\", accuracy_score(y_test, y_pred_dt))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#apply-transformation","title":"Apply Transformation","text":"<p>Log Transformation: Log transformation is a common data preprocessing technique used in various fields such as statistics, data analysis, and machine learning. It involves taking the logarithm of a dataset, typically the natural logarithm (base e) or the base 10 logarithm, to reduce the variation between data points and make the data more suitable for certain analyses or modeling techniques. Log transformation is particularly useful when dealing with data that exhibits exponential or multiplicative growth.</p> <pre><code>from sklearn.preprocessing import FunctionTransformer\n</code></pre> <pre><code># Apply log transformation on 'Fare' column because the PDF is positively/right skewed\n\n# Create an object of FunctionTransformer\nlog_tranformer = FunctionTransformer(func=np.log1p)\n\n# Fit and transform the 'Fare' column of the training data\nx_train_fare = log_tranformer.fit_transform(x_train[[\"Fare\"]])\n\n# Transform the test data\nx_test_fare = log_tranformer.transform(x_test[[\"Fare\"]])\n</code></pre> <pre><code># Concatenate the columns\nx_train_transformed = np.concatenate((x_train[[\"Age\"]], x_train_fare), axis=1)\nx_test_transformed = np.concatenate((x_test[[\"Age\"]], x_test_fare), axis=1)\n</code></pre> <pre><code># Convert the transformed array into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=x_train.columns)\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=x_train.columns)\n</code></pre> <pre><code>x_train_transformed.head()\n</code></pre> <pre><code># Plot the distribution of 'Fare' column after applying log transformation\nplt.figure(figsize=(12, 4))\n\nplt.subplot(121)\nsns.distplot(x_train_transformed[\"Fare\"])\nplt.title(\"Fare PDF after Log Transformation\")\n\nplt.subplot(122)\nstats.probplot(x_train_transformed[\"Fare\"], dist=\"norm\", plot=plt)\nplt.title(\"Fare QQ Plot after Log Transformation\")\n\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#train-a-classifier_1","title":"Train a Classifier","text":"<pre><code># Train the models with log transformed data\nlr_clf2 = LogisticRegression()\ndt_clf2 = DecisionTreeClassifier()\n\n# Fit the transformed training data\nlr_clf2.fit(x_train_transformed, y_train)\ndt_clf2.fit(x_train_transformed, y_train)\n\n# Predict the transformed train data\ny_pred_lr = lr_clf2.predict(x_test_transformed)\ny_pred_dt = dt_clf2.predict(x_test_transformed)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#assess-the-accuracy_1","title":"Assess the Accuracy","text":"<pre><code># Assess the Accuracy\nprint(\"Accuracy Score of LR Model after log transformation:\", accuracy_score(y_test, y_pred_lr))\nprint(\"Accuracy Score of DT Model after log transformation:\", accuracy_score(y_test, y_pred_dt))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#apply-cross-validation","title":"Apply Cross Validation","text":"<pre><code>from sklearn.model_selection import cross_val_score\n</code></pre> <pre><code># Apply log transformation to 'Age' and 'Fare' columns\nx_transformed_2 = log_tranformer.fit_transform(df.drop(\"Survived\", axis=1))\n</code></pre> <pre><code>lr_clf3 = LogisticRegression()\ndt_clf3 = DecisionTreeClassifier()\n\nprint(\"Accuracy Score of LR after Cross Validation:\")\nprint(np.mean(cross_val_score(lr_clf3, x_transformed_2, df[\"Survived\"], scoring=\"accuracy\", cv=10)))\n\nprint(\"Accuracy Score of DT after Cross Validation:\")   \nprint(np.mean(cross_val_score(dt_clf3, x_transformed_2, df[\"Survived\"], scoring=\"accuracy\", cv=10)))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#create-a-function-to-apply-different-transformation","title":"Create a Function to Apply Different Transformation","text":"<pre><code>from sklearn.compose import ColumnTransformer\n</code></pre> <pre><code>def apply_transformation(transformer):\n    # Define the training data\n    x = df.drop(\"Survived\", axis=1)\n    y = df[\"Survived\"]\n\n    # Create a function transformer\n    transformer = FunctionTransformer(func=transformer)\n\n    # Fit the data to the transformer\n    x_transformed_fare = transformer.fit_transform(x[[\"Fare\"]])\n\n    # Concatenate the columns\n    x_transformed = np.concatenate((x[[\"Age\"]], x_transformed_fare), axis=1)\n    x_transformed = pd.DataFrame(x_transformed, columns=x.columns)\n\n    # Instantiate a logistic regression model\n    lr = LogisticRegression()\n\n    # Print the accuracy of the model after cross validation\n    print(\"Accuracy Score:\")\n    print(np.mean(cross_val_score(lr, x_transformed, y, scoring=\"accuracy\", cv=10)))\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(121)\n    stats.probplot(x[\"Fare\"], dist=\"norm\", plot=plt)\n    plt.title(\"Fare QQ Plot Before Transform\")\n\n    plt.subplot(122)\n    stats.probplot(x_transformed[\"Fare\"], dist=\"norm\", plot=plt)\n    plt.title(\"Fare QQ Plot Before Transform\")\n\n    plt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#apply-square-transform-to-fare-column","title":"Apply Square Transform to 'Fare' Column","text":"<pre><code>apply_transformation(lambda x:x**2)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#apply-reciprocal-transform-to-fare-column","title":"Apply Reciprocal Transform to 'Fare' Column","text":"<pre><code>apply_transformation(lambda x: 1/(x+0.0000001))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/09_Function_Transformer/#apply-square-root-transform-to-fare-column","title":"Apply Square Root Transform to 'Fare' Column","text":"<pre><code>apply_transformation(lambda x: x**0.5)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/","title":"Power Transformer","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#power-transformer","title":"Power Transformer","text":"<p>Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\concrete.csv\")\ndf\n</code></pre> <pre><code># Check the null values\ndf.isnull().sum()\n</code></pre> <pre><code># Describe the data\ndf.describe()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop([\"strength\"], axis=1),\n                                                    df[\"strength\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#train-a-linear-regression-model","title":"Train a Linear Regression Model","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a LinearRegression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(x_train, y_train)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data with the LR model\ny_pred = lr.predict(x_test)\n</code></pre> <pre><code># Calculate the R2 score\nfrom sklearn.metrics import r2_score\n\nprint(\"R2 Score of Linear Regression Model:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Calculate accuracy after cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a new Linear Regression model\nlr = LinearRegression()\n\n# Print the cross validation score\nprint(\"R2 Score of Linear Regression model after Cross Validation:\", \n      np.mean(cross_val_score(estimator=lr, X=x_train, y=y_train, cv=10, scoring=\"r2\")))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#data-visualization","title":"Data Visualization","text":"<pre><code># Plot the histplot of each and every column without any transformation\n\nfor column in x_train.columns:\n    plt.figure(figsize=(14, 4))\n\n    plt.subplot(121)\n    sns.histplot(x_train[column], kde=True, bins=30)\n    plt.title(column.title() + \" Histplot\")\n\n    plt.subplot(122)\n    stats.probplot(x_train[column], dist=\"norm\", plot=plt)\n    plt.title(column.title() +\" QQ plot\")\n\n    plt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#apply-box-cox-transformation","title":"Apply Box-Cox Transformation","text":"<p>The Box-Cox transformation is a mathematical technique used in statistics to stabilize variance and make data approximately follow a normal distribution. It is particularly useful when you are dealing with data that violates the assumptions of normality and constant variance, which are common assumptions in many statistical methods, including linear regression.</p> <p>Formula:</p> <p>Here: - y is the original data. - y^(lambda) is the transformed data. - lambda is the transformation parameter. It can be any real number, but in practice, it is often chosen to maximize the normality of the transformed data. This parameter determines the type of transformation applied to the data.</p> <p>Significance:</p> <ol> <li> <p>Stabilizes Variance: When lambda is not equal to 1, the transformation can be used to stabilize the variance of the data. If the data has varying variances at different levels, the Box-Cox transformation can help make the variances more constant.</p> </li> <li> <p>Normalizes Distribution: When lambda is chosen appropriately, the transformed data tends to follow a normal distribution more closely. This can be beneficial when using statistical techniques that assume normally distributed data.</p> </li> <li> <p>Handles Non-Negative Data: The Box-Cox transformation is typically applied to non-negative data, as it involves taking logarithms and raising to powers. If your data contains negative values, you may need to add a constant to make it non-negative before applying the transformation.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#transform-the-data-using-box-cox-transformer","title":"Transform the Data using Box-Cox Transformer","text":"<pre><code>from sklearn.preprocessing import PowerTransformer\n</code></pre> <pre><code># Create an object of the PowerTransformer class\nbox_cox_transformer = PowerTransformer(method=\"box-cox\", standardize=True)\n\n# Fit and Tranform the training data\n# Add a small value because Box-Cox transformation can only be applied to strictly positive data\nx_train_transformed = box_cox_transformer.fit_transform(x_train + 0.00000001)\n\n# Transform the testing data\nx_test_transformed = box_cox_transformer.transform(x_test + 0.00000001)\n</code></pre> <pre><code># Convert the transformed array into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=x_train.columns)\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=x_test.columns)\n</code></pre> <pre><code>x_train_transformed.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#fetch-the-value-of-for-each-column","title":"Fetch the value of \u03bb for Each Column","text":"<pre><code># Convert the \u03bb values into pandas series\nlambda_values_box_cox = pd.Series(data=box_cox_transformer.lambdas_, index=x_train.columns)\nlambda_values_box_cox\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#apply-linear-regression-on-box-cox-transformed-data","title":"Apply Linear Regression on Box-Cox Transformed Data","text":"<pre><code># Instantiate a Linear Regression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(x_train_transformed, y_train)\n\n# Prdict the test data\ny_pred = lr.predict(x_test_transformed)\n\n# Calculate R2 Score\nprint(\"R2 Score of Linear Regression Model after Box-Cox Transformation:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Check accuracy after cross validation\nprint(\"R2 Score of Linear Regression Model after Box-Cox Transformation and Cross Validation:\",\n      np.mean(cross_val_score(lr, x_train_transformed, y_train, scoring=\"r2\", cv=10)))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#display-the-box-cox-transformed-data","title":"Display the Box-Cox Transformed Data","text":"<pre><code>for column in x_train_transformed.columns:\n    plt.figure(figsize=(14, 4))\n\n    plt.subplot(121)\n    sns.histplot(x_train[column], kde=True, bins=30)\n    plt.title(column.title() + \" Histplot\")\n\n    plt.subplot(122)\n    sns.histplot(x_train_transformed[column], kde=True, bins=30)\n    plt.title(column.title() + \" Histplot after Transformation\")\n\n    plt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#apply-yeo-johnson-transformer","title":"Apply Yeo-Johnson Transformer","text":"<p>The Yeo-Johnson transformation is a data transformation technique used in statistics and data analysis. It is primarily used for stabilizing variance and making data more closely follow a normal distribution. This transformation is an extension of the more commonly known Box-Cox transformation and was introduced by Yeo and Johnson in 2000 to address some of its limitations.</p> <p>The Yeo-Johnson transformation works by applying a mathematical formula to each data point in a given dataset. Unlike the Box-Cox transformation, which is defined only for positive values, the Yeo-Johnson transformation can be applied to both positive and negative values and even zero values.</p> <p>Formula:</p> <p>Here: - y is the original data point. - y^(lambda) is the transformed value. - lambda is a parameter that determines the transformation. It can take any real value, including zero.</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#transform-the-data-using-yeo-johnson-transformer","title":"Transform the Data using Yeo-Johnson Transformer","text":"<pre><code># Create an object of the PowerTransformer class\nyeo_johnson_transformer = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n\n# Fit and Tranform the training data\nx_train_transformed = yeo_johnson_transformer.fit_transform(x_train)\n\n# Transform the testing data\nx_test_transformed = yeo_johnson_transformer.transform(x_test)\n</code></pre> <pre><code># Convert the transformed array into pandas dataframe\nx_train_transformed = pd.DataFrame(x_train_transformed, columns=x_train.columns)\nx_test_transformed = pd.DataFrame(x_test_transformed, columns=x_test.columns)\n</code></pre> <pre><code>x_train_transformed.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#fetch-the-value-of-for-each-column_1","title":"Fetch the value of \u03bb for Each Column","text":"<pre><code>lambda_values_yeo_johnson = pd.Series(data=yeo_johnson_transformer.lambdas_, index=x_train.columns)\nlambda_values_yeo_johnson\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#apply-linear-regression-on-box-cox-transformed-data_1","title":"Apply Linear Regression on Box-Cox Transformed Data","text":"<pre><code># Instantiate a Linear Regression object\nlr = LinearRegression()\n\n# Fit the yeo-johnson transformed training data\nlr.fit(x_train_transformed, y_train)\n\n# Predict the test data\ny_pred = lr.predict(x_test_transformed)\n\n# Calculate the R2 Score\nprint(\"R2 Score of Linear Regression Model after Yeo-Johnson Transformation:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Check accuracy after cross validation\nprint(\"R2 Score of Linear Regression Model after Yeo-Johnson Transformation and Cross Validation:\",\n       np.mean(cross_val_score(lr, x_train_transformed, y_train, scoring=\"r2\", cv=10)))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#display-the-yeo-johnson-transformed-data","title":"Display the Yeo-Johnson Transformed Data","text":"<pre><code>for column in x_train_transformed.columns:\n    plt.figure(figsize=(14, 4))\n\n    plt.subplot(121)\n    sns.histplot(x_train[column], kde=True, bins=30)\n    plt.title(column.title() + \" Histplot\")\n\n    plt.subplot(122)\n    sns.histplot(x_train_transformed[column], kde=True, bins=30)\n    plt.title(column.title() + \" Histplot after Transformation\")\n\n    plt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/10_Power_Transformer/#compare-the-values-between-box-cox-and-yeo-johnson-transformation","title":"Compare the \u03bb Values between Box-Cox and Yeo-Johnson Transformation","text":"<pre><code># Convert the lambda values into a dataframe\nlambda_values = pd.concat((lambda_values_box_cox, lambda_values_yeo_johnson), axis=1)\nlambda_values.columns = [\"Box_Cox_Lambdas\", \"Yeo_Johnson_Lambdas\"]\n</code></pre> <pre><code>lambda_values\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/","title":"Discretization","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#discretization","title":"Discretization","text":"<p>Discretization refers to the process of transforming continuous numerical features (variables) into discrete bins or categories. This can be useful in various machine learning tasks, such as decision tree-based algorithms or Naive Bayes, where discrete features are more suitable. Scikit-learn provides a class called KBinsDiscretizer for this purpose.</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(r\"D:\\Coding\\Datasets\\titanic.csv\")\ndf.head()\n</code></pre> <pre><code># Select only necessary columns\ndf = df[[\"Age\", \"Fare\", \"Survived\"]]\ndf.head()\n</code></pre> <pre><code># Check the information of all the columns\ndf.info()\n</code></pre> <pre><code># Drop the null rows\ndf.dropna(inplace=True)\ndf.info()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df.drop(\"Survived\", axis=1),\n                                                    df[\"Survived\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#train-a-classifier","title":"Train a Classifier","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Instantiate a DecisionTreeClassifier object\ndt_clf = DecisionTreeClassifier(random_state=0)\n\n# Fit the training data\ndt_clf.fit(x_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = dt_clf.predict(x_test)\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#check-the-accuracy","title":"Check the Accuracy","text":"<pre><code>from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n</code></pre> <pre><code>print(\"Accuracy of Decision Tree Model:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>print(\"Accuracy of Decision Tree Model after Cross Validation:\",\n      np.mean(cross_val_score(dt_clf, x_train, y_train, cv=10, scoring=\"accuracy\")))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#apply-discretization","title":"Apply Discretization","text":"<p>In discretization, the \"strategy\" refers to the method or approach used to determine the bin edges or thresholds when converting continuous numerical features into discrete bins or categories. Scikit-learn's KBinsDiscretizer provides several strategies to choose from:</p> <ul> <li> <p>'uniform': In this strategy, the bins are uniformly spaced across the range of the input data. It divides the range into equal-width intervals. This strategy is simple and can work well when the data distribution is approximately uniform.</p> </li> <li> <p>'quantile': This strategy divides the data into bins such that each bin contains approximately the same number of data points. It is useful when you want to ensure that each bin has a roughly equal number of samples, even if the data distribution is skewed.</p> </li> <li> <p>'kmeans': In the 'kmeans' strategy, the bin edges are determined using the k-means clustering algorithm. The number of bins is specified by the n_clusters parameter. This strategy can work well when the data has complex distribution patterns and you want to capture those patterns in the discretization.</p> </li> </ul> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import KBinsDiscretizer\n</code></pre> <pre><code># Create a discretization object for \"Age\" column\nkbin_age = KBinsDiscretizer(n_bins=10, strategy=\"quantile\", encode=\"ordinal\")\n\n# Create a discretization object for \"Fare\" column\nkbin_fare = KBinsDiscretizer(n_bins=10, strategy=\"quantile\", encode=\"ordinal\")\n</code></pre> <pre><code># Create a ColumnTransformer object for transforming the columns\ntransformer = ColumnTransformer(transformers=[(\"trf1\", kbin_age, [\"Age\"]),\n                                             (\"trf2\", kbin_fare, [\"Fare\"])],\n                                remainder=\"passthrough\")\n</code></pre> <pre><code># Fit and transform the training data\nx_train_transformed = transformer.fit_transform(x_train)\n\n# Transform the testing data\nx_test_transformed = transformer.transform(x_test)\n</code></pre> <pre><code># Check the name of the transformers\ntransformer.named_transformers_\n</code></pre> <pre><code># Check the first transformer\ntransformer.named_transformers_[\"trf1\"]\n</code></pre> <pre><code># Check the number of bins for the first transformer\ntransformer.named_transformers_[\"trf1\"].n_bins_\n</code></pre> <pre><code># Check the discretization intervals for both the transformers\ntransformer.named_transformers_[\"trf1\"].bin_edges_\n</code></pre> <pre><code>transformer.named_transformers_[\"trf2\"].bin_edges_\n</code></pre> <pre><code># Convert the transformed array into pandas dataframe\nx_train_transformed_df = pd.DataFrame(x_train_transformed, columns=[\"Age\", \"Fare\"])\nx_test_transformed_df = pd.DataFrame(x_test_transformed, columns=[\"Age\", \"Fare\"])\n</code></pre> <pre><code>x_train_transformed_df.head()\n</code></pre> <pre><code># Create a dataframe to compare the transformed values\noutput = pd.DataFrame({\n    \"age\": x_train[\"Age\"],\n    \"age_trf\": x_train_transformed[:, 0],\n    \"fare\": x_train[\"Fare\"],\n    \"fare_trf\": x_train_transformed[:, 1]\n})\n</code></pre> <pre><code>output[\"age_labels\"] = pd.cut(x=x_train[\"Age\"], \n                              bins=transformer.named_transformers_[\"trf1\"].bin_edges_[0].tolist())\noutput[\"fare_labels\"] = pd.cut(x=x_train[\"Fare\"],\n                               bins=transformer.named_transformers_[\"trf2\"].bin_edges_[0].tolist())\noutput\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#train-a-classifier-with-discretized-data","title":"Train a Classifier with Discretized Data","text":"<pre><code>dt_clf = DecisionTreeClassifier(random_state=0)\n\n# Fit the transformed training data\ndt_clf.fit(x_train_transformed_df, y_train)\n</code></pre> <pre><code># Predict the transformed testing data\ny_pred = dt_clf.predict(x_test_transformed_df)\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#check-the-accuracy_1","title":"Check the Accuracy","text":"<pre><code>print(\"Accuracy of Decision Tree Model:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code># Check the accuracy after cross validation\nprint(\"Accuracy of Decision Tree Model after Cross Validation:\",\n      np.mean(cross_val_score(dt_clf, x_train_transformed_df, y_train, cv=10, scoring=\"accuracy\")))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/11_Discretization/#create-a-function-to-plot-the-discretization","title":"Create a Function to Plot the Discretization","text":"<pre><code>def discretize(bins, strategy):\n    kbin_age = KBinsDiscretizer(n_bins=bins, strategy=strategy, encode=\"ordinal\")\n    kbin_fare = KBinsDiscretizer(n_bins=bins, strategy=strategy, encode=\"ordinal\")\n\n    transformer = ColumnTransformer(transformers=[(\"trf1\", kbin_age, [0]),\n                                                  (\"trf2\", kbin_fare, [1])],\n                                    remainder=\"passthrough\")\n\n    x_transformed = transformer.fit_transform(x_train)\n\n    plt.figure(figsize=(14, 4))\n\n    plt.subplot(121)\n    sns.histplot(x_train[\"Age\"], bins=bins)\n    plt.title(f\"Age Before Discretization ({strategy})\")\n\n    plt.subplot(122)\n    sns.histplot(x_train_transformed[:, 0])\n    plt.title(f\"Age After Discretization ({strategy})\")\n\n    plt.show()\n\n    plt.figure(figsize=(14, 4))\n\n    plt.subplot(121)\n    sns.histplot(x_train[\"Fare\"], bins=bins)\n    plt.title(f\"Fare Before Discretization ({strategy})\")\n\n    plt.subplot(122)\n    sns.histplot(x_train_transformed[:, 1])\n    plt.title(f\"Fare After Discretization ({strategy})\")\n    plt.show()\n</code></pre> <pre><code># Display the tranformation after applying 'quantile' strategy\ndiscretize(10, \"quantile\")\n</code></pre> <pre><code># Display the tranformation after applying 'uniform' strategy\ndiscretize(10, \"uniform\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/","title":"Pca Step By Step","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#principal-component-analysis-pca-step-by-step","title":"Principal Component Analysis (PCA): Step by Step","text":"<p>PCA, or Principal Component Analysis, is a dimensionality reduction technique commonly used in machine learning and statistics. Its primary purpose is to transform high-dimensional data into a lower-dimensional form while retaining as much of the original data's variance as possible. This is achieved by identifying the principal components, which are linear combinations of the original features that capture the most significant sources of variation in the data.</p> <p>Here's a brief overview of the PCA process:</p> <ol> <li> <p>Mean Centering: The mean of each feature is subtracted from the data to center it around the origin.</p> </li> <li> <p>Covariance Matrix Calculation: The covariance matrix of the mean-centered data is computed. This matrix describes the relationships between different features and their variances.</p> </li> <li> <p>Eigendecomposition: The next step involves finding the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of variance in those directions.</p> </li> <li> <p>Selection of Principal Components: The eigenvectors are ranked by their corresponding eigenvalues, and the top k eigenvectors (where k is the desired lower dimensionality) are selected to form a new matrix, known as the transformation matrix.</p> </li> <li> <p>Projection: The original data is then projected onto this lower-dimensional subspace using the transformation matrix, resulting in a new set of features called principal components.</p> </li> </ol> <p>PCA is widely used for various purposes, including data compression, noise reduction, visualization, and as a preprocessing step in machine learning tasks to enhance model performance by reducing the dimensionality of the input data.</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"AZ48CytAykB6\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification import plotly.express as px import plotly.graph_objects as go from sklearn.preprocessing import StandardScaler</p> <p>import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"ix_eNw7Vy9tj\" --&gt;\n## **Make the Data for Classification**\n&lt;!-- #endregion --&gt;\n\n```python id=\"_jzvf4DYzISk\"\n# Create a data with 3 features\nX, y = make_classification(\n    n_samples=200,  # Total number of samples\n    n_features=3,   # Number of features\n    n_informative=2,  # Number of informative features\n    n_redundant=0,    # Number of redundant features\n    n_classes=2,      # Number of classes (binary classification)\n    random_state=42   # Random seed for reproducibility\n)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"vjWyOItHz786\" outputId=\"43899479-edc0-4c13-fd81-38a860b01ed7\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#convert-the-data-array-into-pandas-dataframe","title":"Convert the data array into pandas dataframe","text":"<p>df = pd.DataFrame({\"feature1\": X[:, 0], \"feature2\": X[:, 1], \"feature3\": X[:, 2], \"target\": y}) print(df.shape) df.head() <pre><code>&lt;!-- #region id=\"zm3nzIja0eoe\" --&gt;\n## **Plot the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 617} id=\"ZfXURmB40nBu\" outputId=\"c0a92bac-515f-4d95-8892-a23b1b249fdc\"\n# Plot a 3D scatter plot\nfig = px.scatter_3d(df, x=df[\"feature1\"], y=df[\"feature2\"], z=df[\"feature3\"],\n                    color=df[\"target\"].astype(\"str\"), width=600, height=600)\nfig.update_traces(marker=dict(size=4, line=dict(width=2, color=\"DarkSlateGrey\")),\n                  selector=dict(mode=\"markers\"))\nfig.show()\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#step-1-mean-centering","title":"Step-1: Mean Centering","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"Aq-R4ul32TCh\" outputId=\"e087f49f-4193-4415-98a4-f4c340e2ce8e\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#instantiate-a-standardscaler-object","title":"Instantiate a StandardScaler object","text":"<p>scaler = StandardScaler()</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#fit-and-transform-input-features","title":"Fit and transform input features","text":"<p>X_scaled = scaler.fit_transform(df.iloc[:, :-1])</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#convert-the-scaled-array-into-pandas-dataframe","title":"Convert the scaled array into pandas dataframe","text":"<p>X_scaled = pd.DataFrame(X_scaled, columns=df.columns[:-1]) X_scaled.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"K8nnzj7421K9\" outputId=\"1b8ac7b8-dd27-4763-e448-a29381d69529\"\nX_scaled.describe().round(2)\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#step-2-covariance-matrix-calculation","title":"Step-2: Covariance Matrix Calculation","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 161} id=\"lymj5-u93aGi\" outputId=\"fba1d81a-d637-42d8-eb33-652e717bf6d2\" covariance_matrix = X_scaled.cov() print(\"Covariance Matrix:\") covariance_matrix <pre><code>&lt;!-- #region id=\"UgD6ffHM347m\" --&gt;\n## **Step-3: Eigendecomposition**\n&lt;!-- #endregion --&gt;\n\n```python id=\"lgJhXRSH4A3e\"\n# Calculate the Eigen Vectors and Eigen Values of the Variance-Covariance matrix\neigen_values, eigen_vectors = np.linalg.eig(np.array(covariance_matrix))\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"fqPANKey4q8j\" outputId=\"65fbd9f6-215d-4d4c-eafe-a852a561ec4c\" eigen_values <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"WyuQDa354sIR\" outputId=\"93ac37b9-a579-4c88-8ac4-2529670b33ac\"\neigen_vectors\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#step-4-selection-of-principal-components","title":"Step-4: Selection of Principal Components","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 617} id=\"2JffaK1TK0_F\" outputId=\"598728d9-b687-4c29-eaad-4f1675255e14\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#plot-the-eigen-vectors-in-3d-space","title":"Plot the eigen vectors in 3D space","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#create-a-3d-scatter-plot","title":"Create a 3D scatter plot","text":"<p>i = 1</p> <p>fig = go.Figure()</p> <p>for eig_vector in eigen_vectors:     eig_vector = np.hstack((np.array([0, 0, 0]), eig_vector))     fig.add_trace(go.Scatter3d(         x=[eig_vector[0], eig_vector[3]],         y=[eig_vector[1], eig_vector[4]],         z=[eig_vector[2], eig_vector[5]],         mode='lines+markers',         marker=dict(size=6),         line=dict(width=4),         name='Eigen Vector ' + str(i)     ))</p> <pre><code>i+=1\n</code></pre> <p>fig.add_trace(go.Scatter3d(     x=X_scaled.iloc[:, 0],     y=X_scaled.iloc[:, 1],     z=X_scaled.iloc[:, 2],     mode='markers',     marker=dict(size=3, color=df[\"target\"], colorscale='RdYlGn',                 opacity=0.4, line=dict(width=1, color=\"black\")),     name='Scatter Points' ))</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#set-axis-labels","title":"Set axis labels","text":"<p>fig.update_layout(scene=dict(xaxis_title='feature1', yaxis_title='feature2', zaxis_title='feature3'), width=800, height=600)</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#show-the-interactive-plot","title":"Show the interactive plot","text":"<p>fig.show() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"5Jv2sj5QXKCl\" outputId=\"037c1a6a-2e9e-4bbd-841c-d9df9cfa7689\"\n# Get the sorted indices that would sort eigen values in descending order\nsorted_indices = np.argsort(eigen_values)[::-1]\nsorted_indices\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9QXBZ4l_ZXLU\" outputId=\"02f717a3-7d69-4e57-9fb8-db5ba1b21ece\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/20_PCA_Step_by_Step/#select-top-two-principal-components","title":"Select top two principal components","text":"<p>pc = eigen_vectors[sorted_indices[:2]] pc <pre><code>&lt;!-- #region id=\"P33puAJ_Zsxu\" --&gt;\n## **Step-5: Projection**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"ldYrzZKvZ6ZZ\" outputId=\"60f26837-d996-4a5a-fcf7-34c96add27f4\"\n# Project the scaled data onto principal components\ntransformed_df = np.dot(X_scaled, pc.T)\nnew_df = pd.DataFrame(transformed_df, columns=[\"PC1\", \"PC2\"])\n\n# Add the 'target' column\nnew_df[\"target\"] = df[\"target\"].values\nnew_df.head()\n</code></pre></p> <p><code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472} id=\"e97Bbzu3a4Hf\" outputId=\"b68421c4-9a6e-49ca-d76e-c2cd695eaf24\" sns.scatterplot(x=new_df[\"PC1\"], y=new_df[\"PC2\"], c=new_df[\"target\"], cmap=\"RdYlGn\") plt.title(\"Scatterplot between PC1 and PC2\") plt.show()</code></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/","title":"Pca Implementation On Mnist Data","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#pca-implementation-on-mnist-data","title":"PCA Implementation on MNIST Data","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"u79cl7QgLyUC\" outputId=\"36518ce7-ff09-44a0-e052-6576b00d4388\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"f0mGFygaL9iL\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#read-the-data-from-kaggle","title":"Read the Data from Kaggle","text":"<p>```python id=\"vIfnrPBxMR01\" %mkdir ~/.kaggle %cp kaggle.json ~/.kaggle/ <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"jonjhLq-NWiE\" outputId=\"44913ebd-550a-46c5-cfa0-22eb259cd142\"\n!kaggle datasets download -d animatronbot/mnist-digit-recognizer\n</code></pre></p> <p>```python id=\"CcGck5o8Nosf\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#extract-the-data-from-zipfile","title":"Extract the data from Zipfile","text":"<p>import zipfile zipref = zipfile.ZipFile(\"/content/mnist-digit-recognizer.zip\") zipref.extractall(\"/content\") zipref.close() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 273} id=\"MKrZPx0cUnJ9\" outputId=\"6d30b3a3-b16b-427d-f1c7-35888b24c88c\"\n# Read the data\ndf = pd.read_csv(\"/content/train.csv\")\nprint(df.shape)\ndf.head()\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#plot-the-data","title":"Plot the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"j6989QeVWcrR\" outputId=\"f938abf9-321d-46c3-cfaa-09cfefd529b4\" plt.imshow(df.iloc[3][1:].values.reshape(28, 28)) plt.show() <pre><code>&lt;!-- #region id=\"nDu2dvG9WyU0\" --&gt;\n## **Train Test Split**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"CjD7cQYGW1tl\" outputId=\"82b94fa3-1166-4307-c52b-5cf3bb51ff41\"\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:],\n                                                    df.iloc[:, 0],\n                                                    test_size=0.3,\n                                                    random_state=42)\nX_train.shape, X_test.shape\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#apply-k-nearest-neighbour-classifier","title":"Apply K-Nearest Neighbour Classifier","text":"<p>```python id=\"aQ50bUY0sAfl\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#apply-standardization","title":"Apply Standardization","text":"<p>scaler = StandardScaler()</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#fit-and-transform-the-training-and-testing-data","title":"Fit and transform the training and testing data","text":"<p>X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"7nWu7PIkcE4I\" outputId=\"1c17c325-cf2b-46a5-b323-84e8d891cfa8\"\n# Instantiate a 'KNeighborsClassifier' object\nknn = KNeighborsClassifier()\n\n# Fit the training data\nknn.fit(X_train_scaled, y_train)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"yAFn0_KxccCC\" outputId=\"a53eff32-7f51-4ff3-e35a-02669fc35f6c\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#predict-the-test-data","title":"Predict the test data","text":"<p>start = time.time() y_pred = knn.predict(X_test_scaled) print(\"Time taken:\", time.time() - start, \"Sec\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wPex2_BndFEW\" outputId=\"5edf9756-4d5c-473d-d431-acde22e8aeea\"\n# Check the accuracy score\naccuracy_score(y_test, y_pred).round(2)\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#apply-pca-before-classification","title":"Apply PCA before Classification","text":"<p>```python id=\"pJbjoSnhdZe-\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#instantiate-a-pca-object","title":"Instantiate a PCA object","text":"<p>pca = PCA(n_components=200)</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#fit-and-transform-the-training-data","title":"Fit and transform the training data","text":"<p>X_train_transformed = pca.fit_transform(X_train_scaled) X_test_transformed = pca.transform(X_test_scaled) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"YYHc0uYCeQpt\" outputId=\"9eb3738e-9cb3-48b5-c91b-b16f1c836af3\"\nX_train_transformed.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"n-NUwYQPe7NA\" outputId=\"b3115f2b-62ac-448d-ad0c-0181b0f11b64\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#instantiate-a-kneighborsclassifier-object","title":"Instantiate a 'KNeighborsClassifier' object","text":"<p>knn = KNeighborsClassifier()</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#fit-the-transformed-training-data","title":"Fit the transformed training data","text":"<p>knn.fit(X_train_transformed, y_train) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"s1o5JOMVfUmY\" outputId=\"dc5c70ed-c975-4eaa-9f21-9bbec4c875fb\"\n# Predict the transformed test data\nstart = time.time()\ny_pred = knn.predict(X_test_transformed)\nprint(\"Time taken:\", time.time() - start, \"Sec\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ryIPVyaxfcXk\" outputId=\"fb350843-16e6-47df-b153-87ba87b4e1c5\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#check-the-accuracy-score","title":"Check the accuracy score","text":"<p>accuracy_score(y_test, y_pred).round(2) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"L7WlaKLmudj8\" outputId=\"85b6c621-8362-4235-ecc1-55cb3df4d703\"\nX_train_scaled.shape\n</code></pre></p> <p>```python id=\"Yf16L6gngMrS\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#store-the-accuracy-based-on-number-of-principal-components-in-a-dictionary","title":"# Store the accuracy based on number of principal components in a dictionary","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#this-process-might-take-time","title":"# This process might take time","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#accuracy_dict","title":"accuracy_dict = {}","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#for-i-in-range1-785","title":"for i in range(1, 785):","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#printno-of-pc-i","title":"print(\"No. of PC:\", i)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#pca-pcan_componentsi","title":"pca = PCA(n_components=i)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#x_train_transformed-pcafit_transformx_train_scaled","title":"X_train_transformed = pca.fit_transform(X_train_scaled)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#x_test_transformed-pcatransformx_test_scaled","title":"X_test_transformed = pca.transform(X_test_scaled)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#knn-kneighborsclassifier","title":"knn = KNeighborsClassifier()","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#knnfitx_train_transformed-y_train","title":"knn.fit(X_train_transformed, y_train)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#y_pred-knnpredictx_test_transformed","title":"y_pred = knn.predict(X_test_transformed)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#accuracy-accuracy_scorey_test-y_pred","title":"accuracy = accuracy_score(y_test, y_pred)","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#accuracy_dicti-accuracy","title":"accuracy_dict[i] = accuracy","text":"<pre><code>```python id=\"BJXXotZ3vUsc\"\n# sns.lineplot(pd.Series(accuracy_dict), marker=\"o\")\n# plt.grid()\n# plt.xlabel(\"No. of Principal Components\")\n# plt.ylabel(\"Accuracy\")\n# plt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#data-visualization","title":"Data Visualization","text":""},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#2d-visualization","title":"2D Visualization","text":"<p>```python id=\"Xkc4iqSWwGUa\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#transforming-the-data-into-a-2d-coordinate-system","title":"Transforming the data into a 2D coordinate system","text":"<p>pca = PCA(n_components=2)</p> <p>X_train_transformed = pca.fit_transform(X_train_scaled) X_test_transformed = pca.transform(X_test_scaled) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ZD0G93aWwhbb\" outputId=\"0061e8d5-2123-4072-cf20-e3beeb1acc1a\"\nX_train_transformed\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 542} id=\"Ec0HJx89wzP2\" outputId=\"8ad3b194-069e-4d06-b208-24dca3610afd\" fig = px.scatter(x=X_train_transformed[:, 0],                  y=X_train_transformed[:, 1],                  color=y_train.astype(\"str\"),                  color_discrete_sequence=px.colors.qualitative.G10)</p> <p>fig.update_traces(marker=dict(size=8, line=dict(width=0.5, color='black')))</p> <p>fig.show() <pre><code>&lt;!-- #region id=\"CsjYIm3X0-VB\" --&gt;\n### **3D Visualization**\n&lt;!-- #endregion --&gt;\n\n```python id=\"uYhsAJfT1B7B\"\n# Transforming the data into a 2D coordinate system\npca = PCA(n_components=3)\n\nX_train_transformed = pca.fit_transform(X_train_scaled)\nX_test_transformed = pca.transform(X_test_scaled)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"LuZ014XX1cgN\" outputId=\"78e3eda6-6276-4b56-fcc3-c759027172d0\" X_train_transformed <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 617} id=\"pplih8Tr1m9d\" outputId=\"463d2a21-ac26-4077-b4bd-ff29fb24aecf\"\nfig = px.scatter_3d(x=X_train_transformed[:, 0],\n                    y=X_train_transformed[:, 1],\n                    z=X_train_transformed[:, 2],\n                    color=y_train.astype(\"str\"),\n                    color_discrete_sequence=px.colors.qualitative.G10)\n\nfig.update_traces(marker=dict(size=4, line=dict(width=0.5, color=\"black\")))\nfig.update_layout(width=800, height=600)\n\nfig.show()\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#check-eigen-values-and-eigen-vectors","title":"Check Eigen Values and Eigen Vectors","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"REhyxBFXMHei\" outputId=\"a4b4de74-a00a-446d-c15f-8cc2e83f0d35\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#print-the-eigen-values","title":"Print the Eigen Values","text":"<p>print(\"Eigen Values:\") pca.explained_variance_ <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wgpNYO96Mib9\" outputId=\"b532ff57-f4f2-4c26-9d0d-59fc529bdb27\"\n# Print the Eigen Vectors\nprint(\"Eigen Vectors:\")\nprint(pca.components_.shape)\npca.components_\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#find-optimum-number-of-principal-components","title":"Find Optimum Number of Principal Components","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"pkMbbj_WNT7T\" outputId=\"9ed376c6-5194-4366-b4ca-8b5b5b374ffb\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/02_Feature_Engineering/21_PCA_Implementation_on_MNIST_Data/#check-the-percent-of-variance-explained-by-first-3-principal-components","title":"Check the percent of variance explained by first 3 principal components","text":"<p>pca.explained_variance_ratio_ <pre><code>```python id=\"E1cnUczDODIa\"\n# Apply PCA\npca = PCA(n_components=None)\n\nX_train_transformed = pca.fit_transform(X_train_scaled)\nX_test_transformed = pca.transform(X_test_scaled)\n</code></pre></p> <p><code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} id=\"ewPUzJonOOIb\" outputId=\"44d7fa9e-a0c3-4647-d2b8-4145a9857047\" sns.lineplot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel(\"No. of Principal Components\") plt.ylabel(\"Cumulative Variance\") plt.grid() plt.show()</code></p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/","title":"Simple Linear Regression","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#simple-linear-regression","title":"Simple Linear Regression","text":"<p>Simple linear regression is a statistical method used to model the relationship between two variables, typically denoted as X and Y. It is a straightforward approach to understanding how changes in one variable (X) are associated with changes in another variable (Y). The goal of simple linear regression is to find a linear equation that best represents this relationship.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\Placement_SLR.csv\")\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#plot-the-data","title":"Plot the Data","text":"<pre><code>sns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df[\"cgpa\"],\n                                                    df[\"package\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#train-a-simple-linear-regression-model","title":"Train a Simple Linear Regression Model","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a LinearRegression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(x_train.values.reshape(140, 1), y_train)\n</code></pre> <pre><code># Predict the Test data\ny_pred = lr.predict(x_test.values.reshape(60, 1))\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#plot-the-best-fit-line","title":"Plot the Best Fit Line","text":"<pre><code>sns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nsns.lineplot(x=x_train, y=lr.predict(x_train.values.reshape(140, 1)), c=\"red\",\n             label=\"Regression Line\")\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#fetch-the-slope-and-y-intercept-value","title":"Fetch the Slope and Y-intercept Value","text":"<pre><code># Extract the slope value\nm = lr.coef_[0]\n# Extract the y-intercept value\nc = lr.intercept_\n\nprint(\"Slope (m):\", m)\nprint(\"Y-intercept (c):\", c)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/00_Simple_Linear_Regression/#check-the-rmse","title":"Check the RMSE","text":"<pre><code>from sklearn.metrics import mean_squared_error\n</code></pre> <pre><code># Calculate the Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"Mean Squared Error (MSE):\", mse.round(2))\nprint(\"Root Mean Squared Error (RMSE):\", rmse.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/","title":"Mathematical Formulation Of Slr","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#mathematical-formulation-of-simple-linear-regression","title":"Mathematical Formulation of Simple Linear Regression","text":"<p>In simple linear regression, the slope (m) and the intercept (b) of the linear equation can be calculated using the following mathematical formulas:</p> <pre><code>class CustomLR:\n    # Constructor\n    def __init__(self):\n        self.m = None # Slope\n        self.b = None # Y-intercept\n\n    def fit(self, x_train, y_train):\n\n        # Algorithm to find slope (m)\n        numerator = 0\n        denominator = 0\n        for i in range(x_train.shape[0]):\n            numerator += ((x_train[i] - x_train.mean()) * (y_train[i] - y_train.mean()))\n            denominator += (x_train[i] - x_train.mean()) ** 2\n\n        self.m = numerator/denominator\n\n        # Algorithm to find y-intercept (b)\n        self.b = y_train.mean() - ((numerator/denominator) * x_train.mean())\n\n    def predict(self, x_test):\n        x_pred = (x_test * self.m) + self.b\n        return np.array(x_pred)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\Placement_SLR.csv\")\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#plot-the-data","title":"Plot the Data","text":"<pre><code>sns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df[\"cgpa\"],\n                                                    df[\"package\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#apply-linear-regression-with-custom-model","title":"Apply Linear Regression with Custom Model","text":"<pre><code># Instantiate a Linear Regression Object\nlr = CustomLR()\n\n# Fit the training data\nlr.fit(x_train.values, y_train.values)\n</code></pre> <pre><code># Print the slope and y-intercept value of the LR Model\nprint(\"Slope (m):\", lr.m)\nprint(\"Y-intercept (b):\", lr.b)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(x_test)\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/01_Mathematical_Formulation_of_SLR/#plot-the-best-fit-line","title":"Plot the Best Fit Line","text":"<pre><code>sns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nsns.lineplot(x=x_train, y=lr.predict(x_train), c=\"red\",\n             label=\"Regression Line\")\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/","title":"Regression Metrics","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Regression metrics are used to evaluate the performance of predictive models that aim to estimate a continuous target variable. These metrics help assess how well the model's predictions align with the actual values, allowing you to understand the accuracy, precision, and goodness of fit of your regression model. Here are some commonly used regression metrics:</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#read-the-data","title":"Read the Data","text":"<pre><code>df = pd.read_csv(\"D:\\Coding\\Datasets\\Placement_SLR.csv\")\ndf.head()\n</code></pre> <pre><code># Check for the null values\ndf.isnull().sum()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(df.drop(\"package\", axis=1),\n                                                    df[\"package\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape , x_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#plot-the-data","title":"Plot the Data","text":"<pre><code>sns.scatterplot(x=x_train[\"cgpa\"], y=y_train)\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#train-a-simple-linear-regression-model","title":"Train a Simple Linear Regression Model","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate an object of the LinearRegression class\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(x_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(x_test)\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#plot-the-regression-line","title":"Plot the Regression Line","text":"<pre><code>sns.scatterplot(x=x_train[\"cgpa\"], y=y_train)\nsns.lineplot(x=x_test[\"cgpa\"], y=lr.predict(x_test), c=\"red\", label=\"Regression Line\")\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#check-the-accuracy-using-regression-metrics","title":"Check the Accuracy using Regression Metrics","text":"<pre><code>from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE):","text":"<p>MAE is the average of the absolute differences between the predicted and actual values. It measures the average magnitude of errors and is easy to understand.</p> <p>Advantages: * Easy to interpret as it represents the average absolute error. * Resistant to outliers, as it does not square errors.</p> <p>Disadvantages: * Does not penalize larger errors more heavily. * May not work well if the error distribution is not symmetric.</p> <p>Formula:</p> <pre><code>mae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE):","text":"<p>MSE measures the average of the squared differences between the predicted and actual values. It gives more weight to larger errors and penalizes them.</p> <p>Advantages: * Provides a measure of how well the model performs while penalizing larger errors. * Mathematically convenient and commonly used in optimization algorithms.</p> <p>Disadvantages: * The squared nature of the metric makes it sensitive to outliers. * The units of MSE are not the same as the target variable, making it less interpretable.</p> <p>Formula:</p> <pre><code>mse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error (MSE):\", mse.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE):","text":"<p>RMSE is the square root of the MSE. It provides a more interpretable metric in the same units as the target variable.</p> <p>Advantages: * Shares the same unit as the target variable, which makes it more interpretable than MSE. * Balances the sensitivity to outliers found in MSE.</p> <p>Disadvantages: * Like MSE, it can still be sensitive to outliers. * Not as intuitive as MAE.</p> <p>Formula:</p> <pre><code>rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error (RMSE):\", rmse.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#r-squared-r2","title":"R-squared (R\u00b2):","text":"<p>R-squared measures the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, with higher values indicating a better fit.</p> <p>Advantages: * Provides a measure of goodness of fit, indicating how well the model explains the variance in the data. * Values range from 0 to 1, where higher values suggest a better fit.</p> <p>Disadvantages: * It may increase when adding more predictors, even if they are irrelevant (overfitting). * R-squared alone doesn't reveal the direction or magnitude of individual errors.</p> <p>Formula:</p> <pre><code>r2 = r2_score(y_test, y_pred)\nprint(\"R2 Score:\", r2.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/02_Regression_Metrics/#adjusted-r-squared","title":"Adjusted R-squared:","text":"<p>Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. It penalizes the addition of irrelevant predictors.</p> <p>Advantages: * Adjusts R-squared to account for the number of predictors, helping to mitigate overfitting. * Offers a more realistic assessment of model fit in multiple regression.</p> <p>Disadvantages: * It can still be influenced by outliers and unrepresentative samples.</p> <p>Formula:</p> <pre><code>n = len(x_test) # Total Sample Size\np = len(x_test.columns) # Number of independent variable\n</code></pre> <pre><code>adusted_r2 = 1 - ((1 - r2)*(n - 1) / (n - p - 1))\nprint(\"Adjusted R2 Score:\", adusted_r2.round(2))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/","title":"Multiple Linear Regression","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<p>Multiple linear regression is a statistical method used in predictive modeling and data analysis. It extends simple linear regression, which involves modeling the relationship between a dependent variable (also known as the response variable) and a single independent variable (predictor), to cases where there are multiple independent variables. In multiple linear regression, you have more than one predictor variable.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#create-a-data-for-regression","title":"Create a Data for Regression","text":"<pre><code>from sklearn.datasets import make_regression\n</code></pre> <pre><code># Create the regression data\n# X = independent featues\n# y = dependent feature\nX, y = make_regression(n_samples=100, \n                       n_features=2, \n                       n_informative=2,\n                       n_targets=1,\n                       noise=50)\n</code></pre> <pre><code># Create a dataframe\ndata_dict = {\"feature1\": X[:, 0], \"feature2\": X[:, 1], \"target\":y}\ndf = pd.DataFrame(data=data_dict)\nprint(df.shape)\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#plot-the-data","title":"Plot the Data","text":"<pre><code># Plot a 3-dimensional scatter plot\nfig = px.scatter_3d(data_frame=df, x=\"feature1\", y=\"feature2\", \n                    z=\"target\", width=600, height=600)\nfig.update_traces(marker={'size': 4})\nfig.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(df.drop(\"target\", axis=1),\n                                                    df[\"target\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#train-a-linear-regression-model","title":"Train a Linear Regression Model","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a linear Regression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients\nprint(\"Coefficients:\", lr.coef_)\n</code></pre> <pre><code># Print the intercept value\nprint(\"Intercept:\", lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\ny_pred\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code>from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n</code></pre> <pre><code>print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\nprint(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/03_Multiple_Linear_Regression/#plot-the-regression-plane","title":"Plot the Regression Plane","text":"<pre><code># Check the minimum value of the data\ndf.min()\n</code></pre> <pre><code># Check the maximum value of the data\ndf.max()\n</code></pre> <pre><code># Make a mesh grid\nx = np.linspace(start=-3, stop=3, num=10)\ny = np.linspace(start=-3, stop=3, num=10)\nxGrid, yGrid = np.meshgrid(y, x)\n</code></pre> <pre><code># Combine x and y cor=ordinates grid\nfinal = np.vstack((xGrid.ravel().reshape(1, 100), yGrid.ravel().reshape(1, 100))).T\n\n# Predict the z value\nfinal_z = lr.predict(final).reshape(10, 10)\nz = final_z\n</code></pre> <pre><code>fig = px.scatter_3d(data_frame=df, x=\"feature1\", y=\"feature2\", \n                    z=\"target\", width=600, height=600)\nfig.add_trace(go.Surface(x=x, y=y, z=z))\nfig.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/","title":"Mathematical Formulation Of Mlr","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#mathematical-formulation-of-multiple-linear-regression","title":"Mathematical Formulation of Multiple Linear Regression","text":"<p>In multiple linear regression, the slope (m) and the intercept (b) of the linear equation can be calculated using the following mathematical formula:</p> <p>Read this Blog: https://towardsdatascience.com/building-linear-regression-least-squares-with-linear-algebra-2adf071dd5dd</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#read-the-data","title":"Read the Data","text":"<pre><code># Read the diabetes data from scikit learn\nfrom sklearn.datasets import load_diabetes\n</code></pre> <pre><code>X, y = load_diabetes(return_X_y=True)\n</code></pre> <pre><code>X.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#train-a-linear-regression-model","title":"Train a Linear Regression Model","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a LinearRegression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients\nlr.coef_\n</code></pre> <pre><code># Print the intercept\nlr.intercept_\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code>from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code>print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\nprint(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/00_Linear%20Regression/04_Mathematical_Formulation_of_MLR/#build-a-custom-linear-regression-model","title":"Build a Custom Linear Regression Model","text":"<pre><code>class CustomLR:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X_train, y_train):\n        X = np.insert(X_train, 0, 1, axis=1)\n        y = y_train\n        betas = np.linalg.inv(np.dot(X.T, X)).dot(X.T).dot(y)\n        self.intercept_ = betas[0]\n        self.coef_ = betas[1:]\n\n    def predict(self, X_test):\n        y_pred = X_test.dot(self.coef_) + self.intercept_\n        return y_pred\n</code></pre> <pre><code># Instantiate a CustomLR object\nlr = CustomLR()\n\n# Fit the training data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients\nlr.coef_\n</code></pre> <pre><code># Print the intercept\nlr.intercept_\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code># Check the accuarcy\nprint(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\nprint(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/","title":"Gradient Descent Step By Step 1","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#gradient-descent-step-by-step","title":"Gradient Descent: Step by Step","text":"<p>Gradient descent is an optimization algorithm commonly used in machine learning and deep learning for minimizing a cost function or loss function. Its primary goal is to find the optimal parameters or weights for a model that minimize the error between predicted and actual values. It is widely used in training machine learning models, particularly for tasks like linear regression and neural network training.</p> <p>Here's a basic explanation of how gradient descent works:</p> <ol> <li> <p>Initialization: Gradient descent starts by initializing the model's parameters with some initial values. These parameters can be weights in a neural network, coefficients in a linear regression model, or any other adjustable values.</p> </li> <li> <p>Calculate the Gradient: The gradient of the cost function with respect to the model parameters is computed. The gradient is a vector that points in the direction of the steepest increase in the cost function. It indicates how the cost function would change if the parameters were adjusted.</p> </li> <li> <p>Update the Parameters: The parameters are updated by moving in the opposite direction of the gradient. This means subtracting a fraction of the gradient from the current parameter values. The fraction by which the gradient is scaled is called the learning rate and is a hyperparameter that needs to be set beforehand.</p> </li> </ol> <p>New Parameter Value = Current Parameter Value - (Learning Rate * Gradient)</p> <ol> <li>Repeat: Steps 2 and 3 are repeated iteratively until a stopping condition is met. Common stopping conditions include a maximum number of iterations or a minimum threshold for the change in the cost function.</li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#make-a-data","title":"Make a Data","text":"<pre><code>from sklearn.datasets import make_regression\n</code></pre> <pre><code># Make a data for regression\nX, y = make_regression(n_samples=4, \n                       n_features=1, \n                       n_informative=1,\n                       n_targets=1,\n                       noise=80,\n                       random_state=0)\n</code></pre> <pre><code># Plot the data\nsns.scatterplot(x=X.flatten(), y=y)\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#apply-linear-regression-with-ordinary-least-squares-ols","title":"Apply Linear Regression with Ordinary Least Squares (OLS)","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate an object of the LinearRegression class\nlin_reg = LinearRegression()\n\n# Fit the data\nlin_reg.fit(X, y)\n</code></pre> <pre><code># Print the coefficent value\nprint(\"Coefficient (m):\", lin_reg.coef_)\n</code></pre> <pre><code># Print the intercept value\nprint(\"Intercept (b):\", lin_reg.intercept_)\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y)\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X), c=\"#b10026\", label=\"Regression Line\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#apply-linear-regression-with-gradient-descent","title":"Apply Linear Regression with Gradient Descent","text":"<pre><code># Let's apply Gradient Descent assuming slope is constant m = -82.6424188\n# And let's assume the starting value for intercept b = 0\nm = -82.6424188\nb = 0\ny_pred = ((X * m) + b).reshape(4)\ny_pred\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y)\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X), c=\"#b10026\", label=\"OLS\")\nsns.lineplot(x=X.flatten(), y=y_pred, c=\"#ffeda0\", label=\"b = 0\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#iteration-1","title":"Iteration 1","text":"<pre><code># Calculate Loss Slope (dL/dB)\nloss_slope = -2 * np.sum(y - m*X.ravel() - b)\nprint(\"Loss Slope\", loss_slope)\n</code></pre> <pre><code># Let's take a learning rate = 0.1\nlr = 0.1\n\n# Calculate Step Size\nstep_size = lr*loss_slope\nprint(\"Step Size:\", step_size)\n</code></pre> <pre><code># Calculate the new intercept value\nb = b - step_size\nprint(\"Intercept (b)\", b)\n</code></pre> <pre><code># Predict the values\ny_pred1 = ((X * m) + b).reshape(4)\ny_pred1\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y)\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X), c=\"#b10026\", label=\"OLS\")\nsns.lineplot(x=X.flatten(), y=y_pred, c=\"#ffeda0\", label=\"b = 0\")\nsns.lineplot(x=X.flatten(), y=y_pred1, c=\"#fed976\", label=\"b = 164.80\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#iteration-2","title":"Iteration 2","text":"<pre><code># Calculate Loss Slope (dL/dB)\nloss_slope = -2 * np.sum(y - m*X.ravel() - b)\nprint(\"Loss Slope:\", loss_slope)\n\nstep_size = lr * loss_slope\nprint(\"Step Size:\", step_size)\n\n# Calculate the new intercept value\nb = b - step_size\nprint(\"Intercept (b):\", b)\n</code></pre> <pre><code># Predict the values\ny_pred2 = ((X * m) + b).ravel()\ny_pred2\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y)\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X), c=\"#b10026\", label=\"OLS\")\nsns.lineplot(x=X.flatten(), y=y_pred, c=\"#ffeda0\", label=\"b = 0\")\nsns.lineplot(x=X.flatten(), y=y_pred1, c=\"#fed976\", label=\"b = 164.80\")\nsns.lineplot(x=X.flatten(), y=y_pred2, c=\"#fed976\", label=\"b = 197.76\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#iteration-3","title":"Iteration 3","text":"<pre><code># Calculate Loss Slope (dL/dB)\nloss_slope = -2 * np.sum(y - m*X.ravel() - b)\nprint(\"Loss Slope:\", loss_slope)\n\nstep_size = lr * loss_slope\nprint(\"Step Size:\", step_size)\n\n# Calculate the new intercept value\nb = b - step_size\nprint(\"Intercept (b):\", b)\n</code></pre> <pre><code># Predict the values\ny_pred3 = ((X * m) + b).ravel()\ny_pred3\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y)\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X), c=\"#b10026\", label=\"OLS\")\nsns.lineplot(x=X.flatten(), y=y_pred, c=\"#ffeda0\", label=\"b = 0\")\nsns.lineplot(x=X.flatten(), y=y_pred1, c=\"#fed976\", label=\"b = 164.80\")\nsns.lineplot(x=X.flatten(), y=y_pred2, c=\"#fed976\", label=\"b = 197.76\")\nsns.lineplot(x=X.flatten(), y=y_pred3, c=\"#feb24c\", label=\"b = 204.35\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/00_Gradient_Descent_Step_by_Step_1/#apply-gradient-descent-using-loop","title":"Apply Gradient Descent using Loop","text":"<pre><code># Define the initial values\nm = -82.6424188 # assume to be constant\nb = 0\nlr = 0.1\n\nepochs = 10\n\nplt.figure(figsize=(8, 6))\ncolor_palette = sns.color_palette(palette=\"Reds\", n_colors=epochs)\n\nfor i in range(epochs):\n    loss_slope = -2 * np.sum(y - m*X.ravel() - b)\n    step_size = lr * loss_slope\n    b = b - step_size\n\n    y_pred = (X * m) + b\n\n    plt.plot(X.ravel(), y_pred.ravel(), label=f\"b={b.round(4)}\", c=color_palette[i])\n    plt.legend()\n\nplt.scatter(X.ravel(), y)\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/","title":"Gradient Descent Step By Step 2","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#gradient-descent-step-by-step-2","title":"Gradient Descent: Step by Step 2","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#make-a-data","title":"Make a Data","text":"<pre><code>from sklearn.datasets import make_regression\n</code></pre> <pre><code># Make a data for regression\nX, y = make_regression(n_samples=100,\n                       n_features=1,\n                       n_informative=1,\n                       n_targets=1,\n                       noise=20,\n                       random_state=0)\n</code></pre> <pre><code># Plot the data\nsns.scatterplot(x=X.flatten(), y=y)\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#apply-linear-regression-with-ordinary-least-squares-ols","title":"Apply Linear Regression with Ordinary Least Squares (OLS)","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate an object of the LinearRegression class\nlin_reg = LinearRegression()\n\n# Fit the data\nlin_reg.fit(X, y)\n</code></pre> <pre><code># Print the coefficient value\nprint(\"Coefficient (m):\", lin_reg.coef_)\n</code></pre> <pre><code># Print the intercept value\nprint(\"Intercept (b):\", lin_reg.intercept_)\n</code></pre> <pre><code># Calculate the accuracy on the test data\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>y_pred = lin_reg.predict(X_test)\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X_train.flatten(), y=y_train)\nsns.lineplot(x=X_train.flatten(), y=lin_reg.predict(X_train), c=\"#b10026\", label=\"Regression Line\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/01_Gradient_Descent_Step_by_Step_2/#apply-linear-regression-with-gradient-descent","title":"Apply Linear Regression with Gradient Descent","text":"<pre><code># Create a class to apply gradient descent\nclass GDRegressor:\n    def __init__(self, learning_rate, epochs):\n        self.m = 100\n        self.b = -120\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n\n    def fit(self, X, y):\n        # Calculating the slope(m) and intercept(b) using GD\n        for i in range(self.epochs):\n            loss_slope_b = -2 * np.sum(y - self.m*X.ravel() - self.b)\n            loss_slope_m = -2 * np.sum((y - self.m*X.ravel() - self.b)*X.ravel())\n\n            self.b = self.b - (self.learning_rate * loss_slope_b)\n            self.m = self.m - (self.learning_rate * loss_slope_m)\n\n        print(\"Coefficient (m):\", self.m)\n        print(\"Intercept (b):\", self.b)\n\n    def predict(self, X):\n        return self.m * X + self.b\n</code></pre> <pre><code># Instantiate a GDRegressor object\ngd = GDRegressor(learning_rate=0.001, epochs=50)\n\n# Fit the data\ngd.fit(X_train, y_train)\n</code></pre> <pre><code># Check the accuracy in the test dataset\ny_pred = gd.predict(X_test)\nprint(\"R2 Score:\", r2_score(y_test, gd.predict(X_test)))\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X_train.flatten(), y=y_train)\nsns.lineplot(x=X_train.flatten(), \n             y=gd.predict(X_train).flatten(), \n             c=\"#b10026\", label=\"Regression Line\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/02_Batch_Gradient_Descent/","title":"Batch Gradient Descent","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/02_Batch_Gradient_Descent/#batch-gradient-descent","title":"Batch Gradient Descent","text":"<p>Batch Gradient Descent is a popular optimization algorithm used in machine learning and deep learning for training models, particularly for supervised learning tasks like linear regression and neural network training. It's a type of gradient descent algorithm that updates the model's parameters based on the average gradient of the loss function with respect to the entire training dataset.</p>    Here's how Batch Gradient Descent works:  1. **Initialization**: Initialize the model parameters randomly or with some predefined values.  2. **Batch Selection**: Divide the training dataset into smaller subsets called batches. Each batch contains a fixed number of training examples. The batch size is a hyperparameter that you can choose, and it determines how many examples are used in each parameter update.  3. **Compute Gradient**: For each batch, compute the gradient of the loss function with respect to the model parameters. This gradient represents the direction and magnitude of the steepest ascent of the loss function for that batch.  4. **Update Parameters**: Update the model parameters by moving in the opposite direction of the gradient. The update rule typically follows this formula:     <pre><code>\u03b8_new = \u03b8_old - learning_rate * (\u2207(Loss) / \u2207(\u03b8))\n</code></pre>     where:    - \u03b8_new is the updated parameter values.    - \u03b8_old is the current parameter values.    - learning_rate is a hyperparameter that controls the step size or learning rate of the optimization.    - \u2207(Loss) represents the gradient of the loss function.    - \u2207(\u03b8) represents the gradient with respect to the model parameters.  5. **Repeat**: Repeat steps 3 and 4 for all the batches in the training dataset. This constitutes one epoch.  6. **Convergence Check**: Monitor the convergence of the algorithm by checking if the loss function has sufficiently decreased or other convergence criteria are met. If not, repeat steps 3 to 5 for more epochs.  7. **Termination**: Stop the training process when a stopping criterion is met. This could be a maximum number of epochs, a certain level of loss convergence, or other criteria.  Batch Gradient Descent has some advantages, such as stable and consistent updates, but it can be computationally expensive, especially when working with large datasets. It also requires the entire dataset to fit in memory, which may not be feasible for very large datasets. In such cases, variants of gradient descent, like Stochastic Gradient Descent (SGD) and Mini-Batch Gradient Descent, are often used to overcome these limitations.   ## **Import Required Libraries**  <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>  ## **Load a Data**  <pre><code>from sklearn.datasets import load_diabetes\n</code></pre> <pre><code># Read the Diabetes data\nX, y = load_diabetes(return_X_y=True)\n</code></pre> <pre><code>X.shape, y.shape\n</code></pre>  ## **Train Test Split**  <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>  ## **Apply Linear Regression with Ordinary Least Squares (OLS)**  <pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a linear regression object\nlr = LinearRegression()\n\n# Fit the data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", lr.coef_, \"\\n\")\nprint(\"Intercept:\", lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>  ## **Apply Multiple Linear Regression with Batch Gradient Descent**  <pre><code># Create a class to apply gradient descent\nclass GDRegressor:\n    # Constructor\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X_train, y_train):\n        self.intercept_ = 0\n        self.coef_ = np.ones(X_train.shape[1])\n\n        for i in range(self.epochs):\n            # Update all the coefficients and intercept value\n            y_hat = np.dot(X_train, self.coef_) + self.intercept_\n            intercept_der = -2 * np.mean(y_train - y_hat)\n            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n\n            coef_der = -2 * np.dot((y_train - y_hat), X_train) / X_train.shape[0]\n            self.coef_ = self.coef_ - (self.lr * coef_der)\n\n    def predict(self, X_test):\n        return np.dot(X_test, self.coef_) + self.intercept_ \n</code></pre> <pre><code># Instantiate a GDRegressor object\ngdr = GDRegressor(lr=0.3, epochs=1000)\n\n# Fit the data\ngdr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", gdr.coef_, \"\\n\")\nprint(\"Intercept:\", gdr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = gdr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nfrom sklearn.metrics import r2_score\n\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/03_Stochastic_Gradient_Descent/","title":"Stochastic Gradient Descent","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/03_Stochastic_Gradient_Descent/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>Stochastic Gradient Descent (SGD) with a single-row update, also known as online SGD, is a variant of the traditional SGD algorithm where instead of using mini-batches of data for each parameter update, you update the model's parameters one data point (or row) at a time. In other words, after processing each individual data point, the model's parameters are updated based on the gradient computed for that single data point. This approach is sometimes referred to as \"online learning.\"</p>    Here's how SGD with a single-row update works:  1. **Initialization**: Start with an initial set of model parameters.  2. **Data Shuffling**: The training dataset is often shuffled to ensure that the order of data points does not bias the training process.  3. **Iterative Updates**: For each training iteration (or epoch), the algorithm processes one data point from the training set. The model's parameters are updated based on the gradient of the loss function with respect to that single data point.  4. **Gradient Computation**: The gradient of the loss function with respect to the model parameters is computed by backpropagating errors through the network (for neural networks) or using analytical derivatives (for simpler models). The gradient represents how the loss changes with small perturbations in the model parameters for that specific data point.  5. **Parameter Update**: The model parameters are adjusted in the opposite direction of the gradient, just like in traditional SGD. The learning rate controls the step size during each update.  6. **Repeat**: Steps 3-5 are repeated for the entire training dataset or until convergence criteria are met.  Online SGD can have some advantages and disadvantages:  **Advantages:**  1. **Efficiency**: Online SGD can be very efficient, as it processes one data point at a time, making it suitable for streaming data or scenarios with limited memory.  2. **Quick Convergence**: Online SGD can converge quickly, especially when the data is abundant and diverse.  3. **Adaptability**: It can adapt to changing data distributions and non-stationary data, making it useful in online learning and real-time applications.  **Disadvantages:**  1. **High Variability**: Since updates are based on individual data points, the parameter updates can be highly variable and noisy, which may result in a less stable convergence.  2. **Slower Convergence**: Online SGD can converge slower than traditional SGD with mini-batches due to the high variance in parameter updates.  3. **Difficulty in Hyperparameter Tuning**: Choosing an appropriate learning rate and other hyperparameters can be more challenging because of the high variance in updates.  Online SGD is typically used in situations where computational resources or memory are limited, or when the data distribution is constantly changing. It's commonly employed in online learning scenarios, such as recommendation systems, where new data arrives continuously and must be processed as it comes in. However, it may require careful tuning and monitoring to achieve optimal convergence and performance.   ## **Import Required Libraries**  <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>  ## **Load a Data**  <pre><code>from sklearn.datasets import load_diabetes\n</code></pre> <pre><code># Read the Diabetes data\nX, y = load_diabetes(return_X_y=True)\n</code></pre> <pre><code>X.shape\n</code></pre>  ## **Train Test Split**  <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>  ## **Apply Linear Regression with Ordinary Least Squares (OLS)**  <pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Store the initial time in a variable\nstart = time.time()\n\n# Instantiate a linear regression object\nlr = LinearRegression()\n\n# Fit the data\nlr.fit(X_train, y_train)\n\n# Print the actual time taken to fit the data\nprint(\"The Time taken is:\", time.time() - start)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", lr.coef_, \"\\n\")\nprint(\"Intercept:\", lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>  ## **Apply Multiple Linear Regression with Stochastic Gradient Descent**  <pre><code># Create a class to apply gradient descent\nclass SGDRegressor:\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X_train, y_train):\n        self.intercept_ = 0\n        self.coef_ = np.ones(X_train.shape[1])\n\n        for i in range(self.epochs):\n            for j in range(X_train.shape[0]):\n                idx = np.random.randint(0, X_train.shape[0])\n\n                # Predict the y_hat\n                y_hat = np.dot(X_train[idx], self.coef_) + self.intercept_\n\n                # Update the intercept using a single row\n                intercept_der = -2 * (y_train[idx] - y_hat)\n                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n\n                # Update the coefficients using a single row\n                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n                self.coef_ = self.coef_ - (self.lr * coef_der)\n\n    def predict(self, X):\n        return np.dot(X, self.coef_) + self.intercept_\n</code></pre> <pre><code># Store the initial time in a variable\nstart = time.time()\n\n# Instantiate a SGDRegressor object\nsgdr = SGDRegressor(lr=0.03, epochs=100)\n\n# Fit the data\nsgdr.fit(X_train, y_train)\n\n# Print the actual time taken to fit the data\nprint(\"The Time taken is:\", time.time() - start)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", sgdr.coef_, \"\\n\")\nprint(\"Intercept:\", sgdr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = sgdr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>  ## **Stochastic Gradient Descent with Scikit-Learn**  <pre><code>from sklearn.linear_model import SGDRegressor\n</code></pre> <pre><code># Instantiate a SGDRegressor object\nreg = SGDRegressor(loss='squared_error', learning_rate='constant', eta0=0.01)\n\n# Fit the data\nreg.fit(X_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = reg.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/04_Mini_Batch_Gradient_Descent/","title":"Mini Batch Gradient Descent","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/01_Gradient_Descent/04_Mini_Batch_Gradient_Descent/#mini-batch-gradient-descent","title":"Mini-Batch Gradient Descent","text":"<p>Mini-Batch Gradient Descent is a compromise between Stochastic Gradient Descent (SGD) and Batch Gradient Descent. In Mini-Batch Gradient Descent, the dataset is divided into small batches, and the model parameters are updated based on the average gradient of the loss function computed over each batch. This approach combines some of the advantages of both SGD and Batch Gradient Descent.</p>    Here's how Mini-Batch Gradient Descent works:  1. **Initialization**: Start with an initial set of model parameters.  2. **Data Batching**: Divide the training dataset into small batches. The size of these batches is a hyperparameter known as the batch size.  3. **Data Shuffling**: Optionally, shuffle the batches to introduce some randomness and ensure that the order of batches doesn't bias the training.  4. **Iterative Updates**: For each training iteration (or epoch), process one mini-batch at a time. The model's parameters are updated based on the average gradient of the loss function computed over the data points in the mini-batch.  5. **Gradient Computation**: Compute the gradient of the loss function with respect to the model parameters by backpropagating errors through the network (for neural networks) or using analytical derivatives (for simpler models).  6. **Parameter Update**: Adjust the model parameters in the opposite direction of the average gradient. The learning rate controls the size of the step during each update.  7. **Repeat**: Steps 4-6 are repeated for each mini-batch in the dataset until convergence criteria are met.  Mini-Batch Gradient Descent has several advantages:  - **Efficiency**: It takes advantage of vectorized operations, making it more computationally efficient than pure Stochastic Gradient Descent, especially when implemented on hardware that is optimized for matrix operations (e.g., GPUs).  - **Regularization**: The mini-batch updates introduce a level of noise that can act as a form of regularization, potentially helping to prevent overfitting.  - **Parallelization**: Mini-Batch Gradient Descent allows for parallelization, as multiple mini-batches can be processed simultaneously.  - **Balanced Approach**: It strikes a balance between the high variance of SGD (processing one data point at a time) and the high computational requirements of Batch Gradient Descent (processing the entire dataset at once).  The choice of the batch size is a crucial hyperparameter. A small batch size introduces more noise into the parameter updates but can provide faster convergence, while a larger batch size may result in more stable updates but slower convergence and potentially increased memory requirements.  In practice, Mini-Batch Gradient Descent is widely used in training deep learning models due to its efficiency and balanced characteristics. The choice of batch size depends on factors such as the dataset size, available computational resources, and the model architecture.   ## **Import Required Libraries**  <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>  ## **Load a Data**  <pre><code>from sklearn.datasets import load_diabetes\n</code></pre> <pre><code># Read the Diabetes data\nX, y = load_diabetes(return_X_y=True)\n</code></pre> <pre><code>X.shape\n</code></pre>  ## **Train Test Split**  <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>  ## **Apply Linear Regression with Ordinary Least Squares (OLS)**  <pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a 'LinearRegression' object\nlr = LinearRegression()\n\n# Fit the data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", lr.coef_, \"\\n\")\nprint(\"Intercept:\", lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>  ## **Apply Multiple Linear Regression with Mini-Batch Gradient Descent**  <pre><code>import random\n</code></pre> <pre><code># Create a class to apply gradient descent\nclass MBGDRegressor:\n    def __init__(self, batch_size, lr=0.01, epochs=100):\n        self.batch_size = batch_size\n        self.lr = lr\n        self.epochs = epochs\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X_train, y_train):\n        # Initialize the coefficients\n        self.intercept_ = 0\n        self.coef_ = np.ones(X_train.shape[1])\n\n        for i in range(self.epochs):\n\n            for j in range(int(X_train.shape[0]/self.batch_size)):\n                # Generate a list with random numbers \n                idx = random.sample(range(X_train.shape[0]), self.batch_size)\n\n                y_hat = np.dot(X_train[idx], self.coef_) + self.intercept_\n\n                # Update all the coefficients and intercept value\n                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n\n                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n                self.coef_ = self.coef_ - (self.lr * coef_der)\n\n    def predict(self, X):\n        return np.dot(X, self.coef_) + self.intercept_\n</code></pre> <pre><code># Instantiate a GDRegressor object\nmbgdr = MBGDRegressor(batch_size=int(X_train.shape[0]/10), lr=0.01, epochs=50)\n\n# Fit the data\nmbgdr.fit(X_train, y_train)\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", mbgdr.coef_, \"\\n\")\nprint(\"Intercept:\", mbgdr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = mbgdr.predict(X_test)\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>  ## **Mini-Batch Gradient Descent with Scikit-Learn**  <pre><code>from sklearn.linear_model import SGDRegressor\n</code></pre> <pre><code># Instantiate a SGDRegressor object\nreg = SGDRegressor(learning_rate=\"constant\", eta0=0.15)\n</code></pre> <pre><code># Define the batch size and epochs\nbatch_size = 35\nepochs = 100\n\nfor i in range(epochs):\n    idx = random.sample(range(X_train.shape[0]), batch_size)\n    reg.partial_fit(X_train[idx], y_train[idx])\n</code></pre> <pre><code># Print the coefficients and intercept\nprint(\"Coefficients:\\n\", reg.coef_, \"\\n\")\nprint(\"Intercept:\", reg.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = reg.predict(X_test)\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/","title":"Polynomial Regression","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/#make-a-data-for-polynomial-regression","title":"Make a Data for Polynomial Regression","text":"<pre><code># Generate X value\nX = 6 * np.random.rand(200, 1) - 3\n\n# y = 0.8X^2 + 0.9x + 2\n# Generate the y values\ny = 0.8 * X**2 + 0.9 * X + 2 + np.random.randn(200, 1)\n</code></pre> <pre><code># Plot the data\nsns.scatterplot(x=X.flatten(), y=y.flatten())\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/#apply-linear-regression","title":"Apply Linear Regression","text":"<pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Instantiate a LinearRegressin object\nlr = LinearRegression()\n\n# Fit the data\nlr.fit(X_train, y_train)\n</code></pre> <pre><code># Print intercept and coefficient values\nprint(\"Coefficients:\", lr.coef_)\nprint(\"intercept:\", lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = lr.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>print(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y.flatten())\nsns.lineplot(x=X_test.flatten(), y=lr.predict(X_test).flatten(), c=\"red\", label=\"Regression Line\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/02_Polynomial_Regression/00_Polynomial_Regression/#apply-polynomial-linear-regression","title":"Apply Polynomial Linear Regression","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\n</code></pre> <pre><code># Extract polynomial features\n# Degree = 2\n# Include bias parameter\npoly = PolynomialFeatures(degree=2, include_bias=True)\n\nX_train_transformed = poly.fit_transform(X_train)\nX_test_transformed = poly.transform(X_test)\n</code></pre> <pre><code># Check the first 5 rows of transformed x_train array\nprint(\"Polynomial Features: X^0, X^1, X^2\")\nX_train_transformed[:5, :]\n</code></pre> <pre><code># Instantiate a LinearRegression object for Polynomial Regression\npoly_lr = LinearRegression()\n\n# Fit the data\npoly_lr.fit(X_train_transformed, y_train)\n</code></pre> <pre><code># Print intercept and coefficient values\nprint(\"Coefficients:\", poly_lr.coef_)\nprint(\"intercept:\", poly_lr.intercept_)\n</code></pre> <pre><code># Predict the test data\ny_pred = poly_lr.predict(X_test_transformed)\n</code></pre> <pre><code># Calculate the 'R2 Score'\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre> <pre><code># Plot the regression line\nsns.scatterplot(x=X.flatten(), y=y.flatten())\nsns.lineplot(x=X_test.flatten(), \n             y=poly_lr.predict(X_test_transformed).flatten(), \n             c=\"red\", label=\"Regression Line\")\nplt.show()\n</code></pre> <pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n</code></pre> <pre><code># Write a function to see the effect of degree in polynomial regression\ndef polynomial_regression(degree):\n    X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n    X_new_poly = poly.transform(X_new)\n\n    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n    std_scaler = StandardScaler()\n    lin_reg = LinearRegression()\n    polynomial_regression = Pipeline([\n        (\"poly_features\", polybig_features),\n        (\"std_scaler\", std_scaler),\n        (\"lin_reg\", lin_reg)\n    ])\n    polynomial_regression.fit(X, y)\n    y_newbig = polynomial_regression.predict(X_new)\n    plt.plot(X_new, y_newbig, \"r\", label=\"Degree \"+str(degree), linewidth=2)\n\n    plt.plot(X_train, y_train, \"b.\", linewidth=3)\n    plt.plot(X_test, y_test, \"g.\", linewidth=3)\n    plt.legend(loc=\"best\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.axis([-3, 3, 0, 10])\n    plt.show()\n</code></pre> <pre><code>polynomial_regression(2)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/","title":"Perceptron Trick","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#perceptron-trick","title":"Perceptron Trick","text":"<p>A perceptron is one of the simplest and fundamental building blocks in deep learning and artificial neural networks. It was developed by Frank Rosenblatt in the late 1950s and is a type of artificial neuron or node that can be used for binary classification tasks. While perceptrons are limited in their capabilities compared to more complex neural network architectures, they serve as a foundational concept for understanding how neural networks work.</p> <p>Here's an introduction to perceptrons in deep learning:</p> <ol> <li>Basic Structure: A perceptron takes multiple binary inputs (0 or 1) and produces a single binary output (0 or 1). Each input is associated with a weight, and there is also an additional parameter called the bias. Mathematically, the output of a perceptron is calculated as the weighted sum of inputs plus the bias, followed by applying a step function (often the Heaviside step function or a similar activation function) to the sum.</li> </ol> \\[y = \\text{Activation Function}\\left(\\sum_{i=1}^{n} \\text{weight}_i \\cdot \\text{input}_i + \\text{bias}\\right)\\] <ol> <li> <p>Weights and Bias: The weights in a perceptron represent the strength of the connection between the inputs and the output. A larger weight means that the corresponding input has a stronger influence on the output. The bias acts as an offset, allowing the perceptron to produce different outputs even when all inputs are zero.</p> </li> <li> <p>Activation Function: The activation function determines whether the perceptron should fire (output 1) or not (output 0) based on the weighted sum of inputs plus the bias. The choice of activation function is crucial, as it introduces non-linearity into the model. Common activation functions include the step function, sigmoid, ReLU (Rectified Linear Unit), and others.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"2a6b1568-306f-4105-8ba0-c3a014073875\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression</p> <p>import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"5c0e0687-e8b2-48b9-a003-d6f6a400b99a\" --&gt;\n## **Make a Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"83591f0d-f6e7-4591-a4b6-abde048366c5\" outputId=\"88d2482d-91d0-4664-faf7-bfc4a779c1f1\"\n# Make a sample classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=1, n_redundant=0, n_classes=2,\n                           n_clusters_per_class=1, random_state=0, hypercube=False, class_sep=1.5)\n\n# Plot the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nplt.show()\n</code></pre></p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#build-the-perceptron-algorithm","title":"Build the Perceptron Algorithm","text":"<p>```python id=\"878142c1-a751-44a6-835d-bfa927ad07e7\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#write-a-function-to-build-the-algorithm-of-a-step-function","title":"Write a function to build the algorithm of a step function","text":"<p>def step(z):     \"\"\"     This function returns 0 if value is less than or equals to 0 and returns 1     if value is greater than 0.     \"\"\"     return 0 if z &lt;= 0 else 1 <pre><code>```python id=\"24c0abae-e61a-4353-a915-9e27009ec698\"\n# Write a function to build the algorithm of a perceptron\ndef perceptron(X, y, epochs):\n\n    # Add an extra column for intercept term\n    X = np.insert(X, 0, 1, axis=1)\n\n    # Initialize the weights\n    weights = np.ones(X.shape[1])\n\n    # Initialize a learning rate\n    lr = 0.01\n\n    for i in range(epochs):\n        # Select a random index\n        n = np.random.randint(len(X))\n        # Calculate the y-predicted\n        y_hat = step(np.dot(X[n], weights))\n        # Update the weights\n        weights = weights + lr * (y[n]-y_hat) * X[n]\n\n    return weights[0], weights[1:]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"561f2bbf-515d-4632-bbd3-a04bcd37f15c\" outputId=\"24f78a20-79ea-4202-c3db-a1004f7ed3b3\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#calculate-the-intercept-and-coefficients","title":"Calculate the intercept and coefficients","text":"<p>intercept_, coef_ = perceptron(X, y, epochs=1000)</p> <p>print(\"Intercept(w0):\", intercept_) print(\"Coefficients(w1, w2):\", coef_) <pre><code>&lt;!-- #region id=\"a47deb8f-0f5a-4302-a19f-2c8f2e1f653a\" --&gt;\nGeneral Equation of a Line is:&lt;br&gt;\n$$ \\ Ax + By + C = 0 \\ $$\n\nWe an also write it like:&lt;br&gt;\n$$ \\ y = mx + c \\ $$ where $ \\ m\\ $ is the slope and $ \\ c\\ $ is the y-intercept.&lt;br&gt;\nor,\n$$ \\ y = -\\frac{A}{B}x - \\frac{C}{B} \\ $$\nwhere $ -\\frac{A}{B} \\ $ is the slope and $ -\\frac{C}{B} \\ $ is the intercept.&lt;br&gt;\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"85be8ff3-ddc9-4a31-92e8-066528790a9e\" outputId=\"dfe73a20-2e6a-4a4b-bc10-d1df790d9ff2\"\nm = -(coef_[0]/coef_[1])\nc = -(intercept_/coef_[1])\nprint(\"Slope(m):\", m)\nprint(\"Y-Intercept(c):\", c)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"c593620d-012c-4895-88d7-1553df7357ed\" outputId=\"c297ffaf-2bd3-49dd-de43-e54060e07d1e\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#plot-the-decision-boundary","title":"Plot the decision boundary","text":"<p>X_line = np.linspace(-1, 1, 50) y_line = X_line * m + c</p> <p>sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y) sns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Decision Boundary\") plt.ylim((-2.5, 2.5)) plt.show() <pre><code>&lt;!-- #region id=\"f3ab6cbb-d3c1-4c99-9b22-aff819d6020a\" --&gt;\n## **Apply the Logistic Regression**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"d6fa6dfd-ea12-436e-81e0-24febdb464f2\" outputId=\"2faee3ff-6d15-4b3d-d277-eed660b92a42\"\n# Instantiate a Logistic Regression model\nlr = LogisticRegression()\n\n# Fit the data\nlr.fit(X, y)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"6c468496-495b-4ae7-b515-ef0b97d59f55\" outputId=\"6c9403cb-dc80-448b-c960-d604446dd808\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#print-intercept-and-coefficients","title":"Print intercept and coefficients","text":"<p>print(\"Intercept of LR Model (w0):\", lr.intercept_) print(\"Coefficients of LR Model (w1, w2):\", lr.coef_) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"a654fc03-16e6-4535-9c37-6dc404d481e0\" outputId=\"cca29d24-60cc-4889-d2b4-9cb06a126a5f\"\n# Calculate the slope(m) and y-intercept(c) of the LR model\nm_lr = -(lr.coef_[0][0] / lr.coef_[0][1])\nc_lr = -(lr.intercept_[0] / lr.coef_[0][1])\n\nprint(\"Slope(m) of LR:\", m_lr)\nprint(\"Y-Intercept(c) of LR:\", c_lr)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"5988bebf-ea8f-4efc-8645-2f035c846570\" outputId=\"a8fc18e8-ec31-4659-9cca-c5e26af130e6\"</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/00_Perceptron_Trick/#plot-the-decision-boundary_1","title":"Plot the decision boundary","text":"<p>X_line = np.linspace(-1, 1, 50) y_line_lr = X_line * m_lr + c_lr</p> <p>sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y) sns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Perceptron\") sns.lineplot(x=X_line, y=y_line_lr, c=\"green\", label=\"Logistic Regression\") plt.ylim((-2.5, 2.5)) plt.show() ```</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/","title":"Sigmoid Function","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#sigmoid-function","title":"Sigmoid Function","text":"<p>The sigmoid function, also known as the logistic function, is a crucial component of logistic regression. Logistic regression is a statistical method used for binary classification problems, where the goal is to predict the probability of an instance belonging to a particular class.</p> <p>The sigmoid function is defined as:</p> <p>$\\sigma(z) = \\frac{1}{1 + e^{-z}} $</p> <p>Here, \\(\\sigma(z)\\) represents the sigmoid function, and \\(z\\) is the linear combination of input features and their corresponding weights in logistic regression:</p> <p>\\(z = w_0x_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n\\)</p> <p>In this equation:</p> <ul> <li>\\(\\sigma(z)\\) outputs values between 0 and 1.</li> <li>\\(e\\) is the base of the natural logarithm (approximately 2.71828).</li> <li>\\(w_0, w_1, \\ldots, w_n\\) are the weights associated with the features \\(x_0, x_1, \\ldots, x_n\\).</li> <li>\\(x_0\\) is typically set to 1, corresponding to the bias term.</li> </ul> <p>The sigmoid function has a characteristic S-shaped curve, which maps any real-valued number to the range of 0 and 1. This property is essential in logistic regression because it allows us to interpret the output of the sigmoid function as a probability.</p> <p>Interpreting \\(\\sigma(z)\\) as the probability that a given instance belongs to the positive class, the logistic regression model predicts:</p> <ul> <li>If \\(\\sigma(z) \\geq 0.5\\), the instance is classified as the positive class.</li> <li>If \\(\\sigma(z) &lt; 0.5\\), the instance is classified as the negative class.</li> </ul> <p>The logistic regression model is trained to find the optimal values for the weights \\(w_0, w_1, \\ldots, w_n\\) by minimizing a cost function, such as the cross-entropy loss, which measures the difference between the predicted probabilities and the actual class labels.</p> <p>In summary, the sigmoid function in logistic regression provides a smooth and differentiable way to model the probability of a binary outcome, making it a fundamental component of the logistic regression algorithm.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#make-a-data","title":"Make a Data","text":"<pre><code># Make a sample classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=1, n_redundant=0,\n                           n_classes=2, n_clusters_per_class=1, random_state=42)\n\n# Plot the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#build-the-perceptron-algorithm","title":"Build the Perceptron Algorithm","text":"<pre><code># Write a function to build the algorithm of a step function\ndef step(z):\n    return 0 if z &lt;= 0 else 1\n</code></pre> <pre><code># Write a function to build the algorithm of a perceptron\ndef perceptron(X, y, epochs):\n    # Add an extra column for intercept term\n    X = np.insert(X, 0, 1, axis=1)\n    # Initialize the weights\n    weights = np.ones(X.shape[1])\n    # Initialize a learning rate\n    lr = 0.01\n\n    for i in range(epochs):\n        # Select a random index\n        n = np.random.randint(X.shape[0])\n        # Calculate the y-predicted\n        y_hat = step(np.dot(weights, X[n]))\n        # Update the weights\n        weights = weights + lr * (y[n] - y_hat) * X[n]\n\n    return weights[0], weights[1:]\n</code></pre> <pre><code># Calculate the intercept and coefficients\nintercept_, coef_ = perceptron(X, y, 1000)\n\nprint(\"Bias(w0):\", intercept_)\nprint(\"Weights(w1, w2):\", coef_)\n</code></pre> <p>Calculate the slope(m) and y-intercept(c) <code>AX + BY + C = 0</code> <code>y = mX + c</code> <code>m = -(A/B)</code> &amp; <code>c = -(C/B)</code></p> <pre><code>m = -(coef_[0] / coef_[1])\nc = -(intercept_ / coef_[1])\n\nprint(\"Slope(m):\", m)\nprint(\"y-intercept(c):\", c)\n</code></pre> <pre><code># Plot the decision boundary\nX_line = np.linspace(-2.5, 2.5, 50)\ny_line = X_line * m + c\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nsns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Decision Boundary\")\nplt.ylim((-2.5, 2.5))\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#apply-the-logistic-regression","title":"Apply the Logistic Regression","text":"<pre><code># Instantiate a Logistic Regression class\nlr = LogisticRegression()\n\n# Fit the data\nlr.fit(X, y)\n</code></pre> <pre><code># Print intercept and coefficients\nprint(\"Bias of LR Model (w0):\", lr.intercept_)\nprint(\"Weights of LR Model (w1, w2):\", lr.coef_)\n</code></pre> <pre><code># Calculate the slope(m) and y-intercept(c) of the LR model\nm_lr = -(lr.coef_[0][0] / lr.coef_[0][1])\nc_lr = -(lr.intercept_[0] / lr.coef_[0][1])\n\nprint(\"Slope(m) of LR:\", m_lr)\nprint(\"Y-Intercept(c) of LR:\", c_lr)\n</code></pre> <pre><code># Plot the decision boundary\ny_line_lr = X_line * m_lr + c_lr\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nsns.lineplot(x=X_line, y=y_line_lr, c=\"red\", label=\"Decision Boundary\")\nplt.ylim((-2.5, 2.5))\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#build-the-perceptron-algorithm-with-sigmoid-function","title":"Build the Perceptron Algorithm with Sigmoid Function","text":"<pre><code># Write a function to build the algorithm of a sigmoid function\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n</code></pre> <pre><code># Write a function to build the algorithm of a perceptron\ndef perceptron(X, y, epochs):\n    # Add an extra column for intercept term\n    X = np.insert(X, 0, 1, axis=1)\n    # Initialize the weights\n    weights = np.ones(X.shape[1])\n    # Initialize a learning rate\n    lr = 0.01\n\n    for i in range(epochs):\n        # Select a random index\n        n = np.random.randint(X.shape[0])\n        # Calculate the y-predicted\n        y_hat = sigmoid(np.dot(weights, X[n]))\n        # Update the weights\n        weights = weights + lr * (y[n] - y_hat) * X[n]\n\n    return weights[0], weights[1:]\n</code></pre> <pre><code># Calculate the intercept and coefficients\nintercept_, coef_ = perceptron(X, y, 1000)\n\nprint(\"Bias(w0) with sigmoid function:\", intercept_)\nprint(\"Weights(w1, w2) with sigmoid function:\", coef_)\n</code></pre> <pre><code>m_sg = -(coef_[0] / coef_[1])\nc_sg = -(intercept_ / coef_[1])\n\nprint(\"Slope(m) with sigmoid function:\", m_sg)\nprint(\"y-intercept(c) with sigmoid function::\", c_sg)\n</code></pre> <pre><code># Plot the decision boundary\ny_line_sg = X_line * m_sg + c_sg\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nsns.lineplot(x=X_line, y=y_line_sg, c=\"red\", label=\"Decision Boundary\")\nplt.ylim((-2.5, 2.5))\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/03_Logistic_Regression/01_Sigmoid_Function/#plot-all-the-decision-boundaries","title":"Plot all the Decision Boundaries","text":"<pre><code>sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\n\nsns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Step Func\")\nsns.lineplot(x=X_line, y=y_line_lr, c=\"green\", label=\"Logistic Reg\")\nsns.lineplot(x=X_line, y=y_line_sg, c=\"blue\", label=\"Sigmoid Func\")\nplt.xlim([-2, 3])\nplt.ylim([-2.5, 3])\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/","title":"Support Vector Machine","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#support-vector-machine","title":"Support Vector Machine","text":"<p>Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification, regression, and outlier detection tasks. Introduced by Vladimir Vapnik and his colleagues in the 1990s, SVM has gained immense popularity due to its effectiveness in handling high-dimensional data and its ability to generalize well even in cases of limited training samples.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#how-svm-works","title":"How SVM Works:","text":"<ol> <li>Basic Concept:    At its core, SVM aims to find the hyperplane that best separates different classes in the feature space. This hyperplane is chosen in such a way that it maximizes the margin, which is the distance between the hyperplane and the nearest data point (called a support vector) from each class.</li> </ol> <ol> <li>Kernel Trick:    SVM can efficiently handle nonlinear decision boundaries using what's known as the kernel trick. Instead of explicitly mapping data points into higher-dimensional spaces, SVM computes the inner products between the data points in the feature space, often avoiding the need to compute the transformation explicitly. Popular kernels include linear, polynomial, radial basis function (RBF), and sigmoid.</li> </ol> <ol> <li> <p>Optimization Objective:    SVM aims to minimize the classification error while maximizing the margin. This is formulated as an optimization problem, typically solved using techniques like convex optimization. The decision function is derived from the support vectors, which are the data points closest to the hyperplane.</p> </li> <li> <p>Margin and Regularization:    SVM allows for a trade-off between maximizing the margin and minimizing the classification error. This trade-off is controlled by a regularization parameter (C), which balances the importance of maximizing the margin against allowing some misclassifications.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#advantages-of-svm","title":"Advantages of SVM:","text":"<ol> <li> <p>Effective in High-Dimensional Spaces:    SVM performs well even in cases where the number of dimensions exceeds the number of samples, making it suitable for high-dimensional data.</p> </li> <li> <p>Robust to Overfitting:    SVM's regularization parameter helps prevent overfitting by controlling the complexity of the model.</p> </li> <li> <p>Effective with Nonlinear Data:    The kernel trick allows SVM to handle nonlinear decision boundaries effectively.</p> </li> <li> <p>Works Well with Small/Medium-Sized Datasets:    SVM generalizes well even with limited training samples, making it suitable for datasets with a small to medium number of samples.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nsns.set_style(\"darkgrid\")\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#svm-with-hard-margin","title":"SVM with Hard Margin","text":"<p>Support Vector Machine (SVM) with Hard Margin is a variant of SVM where the algorithm aims to find the hyperplane that completely separates the classes in the feature space, without allowing any misclassification. This means that the decision boundary must have a margin such that no data points fall within it.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#characteristics-of-svm-with-hard-margin","title":"Characteristics of SVM with Hard Margin:","text":"<ol> <li> <p>No Misclassification: In SVM with Hard Margin, the objective is to find a hyperplane that separates the classes such that each data point is correctly classified. There is no allowance for misclassification in this setting.</p> </li> <li> <p>Large Margin: The hyperplane chosen in SVM with Hard Margin is the one that maximizes the margin between the classes. This margin is the distance between the hyperplane and the nearest data points from each class, also known as the support vectors.</p> </li> <li> <p>Linearly Separable Data: SVM with Hard Margin works well when the data is linearly separable, meaning that there exists a hyperplane that can perfectly separate the classes without any misclassification.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#mathematical-formulation","title":"Mathematical Formulation:","text":"<p>The optimization problem for SVM with Hard Margin can be formulated as follows:</p> <p>Minimize: $ \\frac{1}{2} ||\\mathbf{w}||^2 $</p> <p>Subject to: $ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 $</p> <p>Here, $ \\mathbf{w} $ represents the weight vector perpendicular to the hyperplane, $ b $ is the bias term, and $ (\\mathbf{x}_i, y_i) $ are the training samples with feature vectors $ \\mathbf{x}_i $ and corresponding class labels $ y_i $. The constraint ensures that all data points are correctly classified with a margin of at least 1.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#advantages-and-limitations","title":"Advantages and Limitations:","text":"<p>Advantages: - SVM with Hard Margin guarantees a unique solution when the data is linearly separable. - It can result in a more interpretable model since it relies only on the support vectors.</p> <p>Limitations: - It assumes that the data is perfectly separable, which might not always be the case in real-world scenarios. - It can be sensitive to outliers since it aims to achieve a strict separation between classes.</p> <p>In practice, SVM with Hard Margin is not commonly used due to its sensitivity to outliers and the requirement of perfectly separable data. Instead, Soft Margin SVM, which allows for some misclassification, is often preferred as it provides more flexibility and robustness to the model.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#make-a-perfectly-linearly-separable-dataset","title":"Make a Perfectly Linearly Separable Dataset","text":"<pre><code># Generate the data\nX, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n\n# Plor the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\"black\", linewidth=0.5);\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#model-training-and-validation","title":"Model Training and Validation","text":"<pre><code># Instantiate a Support Vector Classifier object\nsvc = SVC(kernel=\"linear\", C=1)\n\n# Fit the training data\nsvc.fit(X, y)\n</code></pre> <pre><code># Plot the decision boundaries\ndef plot_svc_decision_function(svc):\n    '''Plot the decision function for a 2D SVC'''\n\n    # Get the separating hyperplane coefficients\n    w = svc.coef_[0]\n    b = svc.intercept_[0]\n\n    # Plot the data points\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\"black\", linewidth=0.5)\n\n    # Plot the decision boundary, positive and negative margins\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    Z = svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n                linestyles=['--', '-', '--'])\n\n    # Plot the support vectors\n    plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=100,\n                linewidth=1, facecolors='none', edgecolors='red')\n\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Decision boundary and margin of SVC')\n    plt.show()\n\nplot_svc_decision_function(svc)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#the-importance-of-support-vectors","title":"The Importance of Support Vectors","text":"<p>Support vectors are crucial elements in the context of Support Vector Machines (SVM) and have significant importance in defining the decision boundary and maximizing the margin. Here's why support vectors are important:</p> <ol> <li> <p>Definition of Decision Boundary:    Support vectors are the data points that lie closest to the decision boundary (hyperplane). They directly influence the position and orientation of the decision boundary because they are the ones that determine the margin.</p> </li> <li> <p>Maximization of Margin:    The margin in SVM is defined as the distance between the decision boundary and the closest data point from either class. Support vectors are the ones that lie on the margin boundary. By maximizing the margin, SVM finds the hyperplane that best separates the classes, and support vectors play a pivotal role in this process.</p> </li> <li> <p>Robustness to Outliers:    Support vectors are typically the data points that are most difficult to classify or lie on the margin boundary. Since SVM aims to maximize the margin, it is less affected by outliers that are not support vectors. This property makes SVM robust to noise and outliers in the data.</p> </li> <li> <p>Dimensionality Reduction:    In many cases, the number of support vectors is significantly smaller than the total number of training samples. This property leads to dimensionality reduction in the SVM model, making it computationally efficient, especially in high-dimensional spaces.</p> </li> <li> <p>Interpretability:    Since support vectors define the decision boundary, they provide insights into the structure and characteristics of the data. Analyzing support vectors can help in understanding which data points are critical for classification and how the decision boundary is determined.</p> </li> <li> <p>Generalization Performance:    SVM focuses on learning from the most informative data points, which are the support vectors. By emphasizing these critical examples, SVM tends to generalize well to unseen data, leading to better performance on test datasets.</p> </li> </ol> <pre><code># Train two SVC with the datasets containing different number of sample points\ndef plot_svm(N=[100, 200]):\n\n    fig, axes = plt.subplots(ncols=len(N), nrows=1, figsize=(len(N)*5, 5))\n    axes = axes.flatten()\n\n    for i in np.arange(len(N)):\n        n = N[i] # Number of samples\n\n        # Generate data\n        X, y = make_blobs(n_samples=n, centers=2, random_state=0, cluster_std=0.60)\n\n        # Train a SVC\n        svc = SVC(kernel=\"linear\", C=10)\n        svc.fit(X, y)\n\n        # Get the separating hyperplane coefficients\n        w = svc.coef_[0]\n        b = svc.intercept_[0]\n\n        # Plot the data points\n        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\"black\", linewidth=0.5, ax=axes[i])\n\n        # Plot the decision boundary, positive and negative margins\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                             np.linspace(y_min, y_max, 100))\n        Z = svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n\n        axes[i].contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n                    linestyles=['--', '-', '--'])\n\n        # Plot the support vectors\n        axes[i].scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=100,\n                    linewidth=1, facecolors='none', edgecolors='red')\n\n        axes[i].set_xlabel('Feature 1')\n        axes[i].set_ylabel('Feature 2')\n        axes[i].set_title(f'Decision boundary and margin of SVC\\n(N={n}) (C=10)')\n\n    plt.tight_layout(w_pad=2)\n\nplot_svm()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#svm-with-soft-margin","title":"SVM with Soft Margin","text":"<p>Support Vector Machine (SVM) with soft margin is an extension of the SVM algorithm that allows for some degree of misclassification in the training data. Unlike SVM with hard margin, which requires all data points to be correctly classified and strictly enforces a maximum-margin decision boundary, SVM with soft margin introduces a regularization parameter (usually denoted as $ C) $ to control the trade-off between maximizing the margin and tolerating misclassifications.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#key-characteristics","title":"Key Characteristics:","text":"<ol> <li> <p>Tolerance for Misclassification:    SVM with soft margin allows for some data points to fall within the margin or even on the wrong side of the decision boundary. This flexibility is particularly useful in scenarios where the data is not perfectly separable or contains outliers.</p> </li> <li> <p>Regularization Parameter $ ( C ) $:    The regularization parameter $ ( C ) $ controls the penalty imposed on misclassified data points. A smaller value of $ C $ results in a wider margin and more tolerance for misclassification, while a larger value of $ C $ imposes a stricter penalty for misclassification, potentially leading to a narrower margin.</p> </li> <li> <p>Optimization Objective:    The goal of SVM with soft margin is to find the decision boundary that separates the classes with the maximum margin while minimizing the sum of the margin violations and the regularization term. This is achieved through convex optimization techniques.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#advantages","title":"Advantages:","text":"<ul> <li>Robustness: SVM with soft margin is more robust to noise and outliers compared to SVM with hard margin, as it allows for some degree of misclassification.</li> <li>Flexibility: It provides a more flexible approach to classification, allowing the algorithm to handle non-linearly separable data or cases where a strict separation is not feasible.</li> </ul>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/00_Support_Vector_Machine/#applications","title":"Applications:","text":"<ul> <li>Real-world Data: SVM with soft margin is commonly used in scenarios where the data is noisy or contains outliers, such as image classification, text categorization, and bioinformatics.</li> <li>Imbalanced Datasets: It is also useful for dealing with imbalanced datasets, where one class may have significantly fewer samples than the other.</li> </ul> <p>In summary, SVM with soft margin provides a balance between maximizing the margin and tolerating misclassifications, making it a versatile and effective algorithm for various classification tasks, especially in situations where strict separation is not possible or practical.</p> <pre><code># Train two SVC with different C value\ndef plot_svm(C=[1, 10]):\n\n    fig, axes = plt.subplots(ncols=len(C), nrows=1, figsize=(len(C)*5, 5))\n    axes = axes.flatten()\n\n    for i in np.arange(len(C)):\n        c = C[i] # Value of C\n\n        # Generate data\n        X, y = make_blobs(n_samples=300, centers=2, random_state=0, cluster_std=0.60)\n\n        # Train a SVC\n        svc = SVC(kernel=\"linear\", C=c)\n        svc.fit(X, y)\n\n        # Get the separating hyperplane coefficients\n        w = svc.coef_[0]\n        b = svc.intercept_[0]\n\n        # Plot the data points\n        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\"black\", linewidth=0.5, ax=axes[i])\n\n        # Plot the decision boundary, positive and negative margins\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                             np.linspace(y_min, y_max, 100))\n        Z = svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n\n        axes[i].contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n                        linestyles=['--', '-', '--'])\n\n        # Plot the support vectors\n        axes[i].scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=100,\n                        linewidth=1, facecolors='none', edgecolors='red')\n\n        axes[i].set_xlabel('Feature 1')\n        axes[i].set_ylabel('Feature 2')\n        axes[i].set_title(f'Decision boundary and margin of SVC\\n(N=200) (C={c})')\n\n    plt.tight_layout(w_pad=2)\n\nplot_svm(C=[0.1, 1, 10])\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/","title":"Kernels In Svm","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#kernels-in-svm","title":"Kernels in SVM","text":"<p>In Support Vector Machines (SVM), kernels play a crucial role in extending the algorithm's capability to handle non-linearly separable data. Kernels enable SVM to implicitly map the input data into a higher-dimensional feature space where a linear decision boundary can be found. This process is known as the \"kernel trick,\" and it allows SVM to effectively handle complex relationships between features without explicitly computing the transformation.</p> <p>Here are some common types of kernels used in SVM:</p> <ol> <li> <p>Linear Kernel:    The linear kernel is the simplest kernel, and it computes the dot product between the feature vectors in the original input space. It is suitable for linearly separable data or cases where a linear decision boundary is appropriate.</p> </li> <li> <p>Polynomial Kernel:    The polynomial kernel computes the similarity between two vectors as the polynomial of their dot product. It introduces additional parameters such as the degree of the polynomial and a coefficient. It is useful for capturing non-linear relationships between features.</p> </li> <li> <p>Radial Basis Function (RBF) Kernel:    The RBF kernel, also known as the Gaussian kernel, computes the similarity between two vectors based on the radial distance between them. It is defined by a single parameter called the gamma (\u03b3) parameter. The RBF kernel is highly flexible and can capture complex non-linear decision boundaries.</p> </li> <li> <p>Sigmoid Kernel:    The sigmoid kernel computes the similarity between two vectors using a sigmoid function. It introduces additional parameters such as the slope and the intercept. The sigmoid kernel can be useful for modeling non-linear relationships, but it is less commonly used compared to linear and RBF kernels.</p> </li> <li> <p>Custom Kernels:    In addition to the standard kernels mentioned above, SVM also allows for the use of custom kernels. Custom kernels can be defined based on domain knowledge or specific problem characteristics, allowing for more flexibility in modeling complex relationships in the data.</p> </li> </ol> <p>The choice of kernel in SVM depends on the characteristics of the data and the problem at hand. Experimentation and cross-validation are often used to determine the most appropriate kernel for a given task. Additionally, tuning kernel parameters, such as the degree of the polynomial or the gamma parameter in the RBF kernel, can significantly impact the performance of the SVM model.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport plotly.express as px\n\nsns.set_style(\"darkgrid\")\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#make-a-circular-dataset","title":"Make a Circular Dataset","text":"<pre><code># Generate a cicular dataset\nX, y = make_circles(n_samples=200, noise=0.1, factor=0.2, random_state=0)\n\n# Plot the Data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\"black\", linewidth=0.5);\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#train-test-split","title":"Train Test Split","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#application-of-svc-with-different-kernels","title":"Application of SVC with Different Kernels","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#svc-with-linear-kernel","title":"SVC with Linear Kernel","text":"<pre><code># Instantiate a SVC classifier object with 'linear' kernel\nsvc = SVC(kernel=\"linear\", C=1.0)\n\n# Fit the training data\nsvc.fit(X_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = svc.predict(X_test)\n\n# Check the accuracy\nprint(\"Accuracy of SVC with Linear Kernel:\", accuracy_score(y_test, y_pred).round(4))\n</code></pre> <pre><code># Write a function to plot the decision boundary\ndef plot_decision_boundary(X, y, kernel=\"linear\", C=1.0, degree=1):\n    '''Plot the decision boundary for a 2D SVC'''\n\n    # Train the SVC model\n    svc = SVC(kernel=kernel, C=C, degree=degree)\n\n    # Fit the training data\n    svc.fit(X, y)\n\n    # Create a mesh grid to plot the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n\n    # Predict the labels for all points in the mesh grid\n    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary and the data points\n    plt.contourf(xx, yy, Z, cmap=\"RdYlGn\", alpha=0.5)\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette=[\"red\", \"green\"], edgecolors='k', linewidth=0.5)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(f\"Decision boundary of SVC with '{kernel}' kernel\")\n    plt.show()\n</code></pre> <pre><code>plot_decision_boundary(X, y, kernel=\"linear\", C=1.0)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#svc-with-rbf-kernel","title":"SVC with RBF Kernel","text":"<p>The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is a popular choice in Support Vector Machine (SVM) algorithms, particularly for handling non-linearly separable data. It measures the similarity between two data points in a high-dimensional space and is defined by a single parameter called the gamma (\u03b3) parameter.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#mathematical-formulation","title":"Mathematical Formulation:","text":"<p>The RBF kernel $ K(\\mathbf{x}_i, \\mathbf{x}_j) $ between two feature vectors $ \\mathbf{x}_i $ and $ \\mathbf{x}_j $ is computed as:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2\\right) \\] <p>Here: - $ ||\\mathbf{x}_i - \\mathbf{x}_j||^2 $ represents the squared Euclidean distance between the two feature vectors. - $ \\gamma $ (gamma) is a hyperparameter that controls the influence of each training example on the decision boundary. It determines the \"spread\" of the kernel function.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#characteristics","title":"Characteristics:","text":"<ol> <li> <p>Flexibility: The RBF kernel is highly flexible and can capture complex non-linear decision boundaries in the data.</p> </li> <li> <p>Implicit Mapping: It implicitly maps the input data into a higher-dimensional feature space, where a linear decision boundary can be found, even if the original data is not linearly separable.</p> </li> <li> <p>Smoothness: The RBF kernel produces smooth decision boundaries, which can be advantageous in many real-world classification tasks.</p> </li> <li> <p>Hyperparameter Sensitivity: The performance of the RBF kernel is sensitive to the choice of the gamma parameter. Higher values of gamma lead to more complex decision boundaries and may result in overfitting, while lower values of gamma lead to smoother decision boundaries and may result in underfitting.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#applications","title":"Applications:","text":"<ul> <li> <p>Non-linear Classification: The RBF kernel is commonly used in SVM for tasks involving non-linearly separable data, such as image classification, text classification, and bioinformatics.</p> </li> <li> <p>Clustering: The RBF kernel can also be used in clustering algorithms, such as the Gaussian Radial Basis Function (RBF) kernel in K-means clustering.</p> </li> </ul> <p>In summary, the RBF kernel is a powerful tool in SVM algorithms for handling non-linear relationships in the data and is widely used in various machine learning applications due to its flexibility and effectiveness. However, careful tuning of the gamma parameter is essential to achieve optimal performance and avoid overfitting or underfitting.</p> <pre><code># Write a function to apply RBF on the data\ndef applyRBF(X, y, gamma=1, X_center=0):\n    r = np.exp(-gamma*((X - X_center) ** 2)).sum(axis=1)\n\n    fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=r, color=y.astype(\"str\"))\n    fig.update_traces(marker=dict(size=5, line=dict(width=0.5, color=\"black\")))\n    fig.update_layout(width=800, height=600)\n    fig.show()\n\napplyRBF(X, y, gamma=1, X_center=0)\n</code></pre> <pre><code># Instantiate a SVC classifier object with 'rbf' kernel\nsvc = SVC(kernel=\"rbf\", C=1.0)\n\n# Fit the training data\nsvc.fit(X_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = svc.predict(X_test)\n\n# Check the accuracy\nprint(\"Accuracy of SVC with RBF Kernel:\", accuracy_score(y_test, y_pred).round(4))\n</code></pre> <pre><code>plot_decision_boundary(X, y, kernel=\"rbf\", C=1.0)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#svc-with-polynomial-kernel","title":"SVC with Polynomial Kernel","text":"<p>The Polynomial Kernel is a kernel function commonly used in Support Vector Machines (SVM) for handling non-linear relationships in the data. It maps the input data into a higher-dimensional space using polynomial functions, allowing SVM to find non-linear decision boundaries. The Polynomial Kernel is defined by a degree parameter, which determines the degree of the polynomial used for the mapping.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#mathematical-formulation_1","title":"Mathematical Formulation:","text":"<p>The Polynomial Kernel $ K(\\mathbf{x}_i, \\mathbf{x}_j) $ between two feature vectors $ \\mathbf{x}_i $ and $ \\mathbf{x}_j $ is computed as:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d \\] <p>Here: - $ \\mathbf{x}_i^T \\mathbf{x}_j $ represents the dot product between the two feature vectors. - $ d $ is the degree of the polynomial, which determines the complexity of the decision boundary. - $ c $ is an optional parameter known as the bias or offset, which can be used to control the influence of lower-degree polynomial terms.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#characteristics_1","title":"Characteristics:","text":"<ol> <li> <p>Non-Linearity: The Polynomial Kernel allows SVM to capture non-linear relationships between features by mapping them into a higher-dimensional space.</p> </li> <li> <p>Flexibility: The degree parameter allows for varying levels of complexity in the decision boundary. Higher degree polynomials can capture more complex relationships but may also lead to overfitting.</p> </li> <li> <p>Computationally Efficient: While the Polynomial Kernel increases the dimensionality of the feature space, it is computationally more efficient compared to other non-linear kernels like the Gaussian Radial Basis Function (RBF) kernel.</p> </li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/04_Support_Vector_Machine/01_Kernels_in_SVM/#applications_1","title":"Applications:","text":"<ul> <li> <p>Non-linear Classification: The Polynomial Kernel is commonly used in SVM for tasks involving non-linearly separable data, such as image classification, text classification, and pattern recognition.</p> </li> <li> <p>Feature Engineering: The Polynomial Kernel can be used as a form of feature engineering to transform the input features into a higher-dimensional space, where linear separation may be easier to achieve.</p> </li> </ul> <p>In summary, the Polynomial Kernel is a powerful tool in SVM algorithms for handling non-linear relationships in the data. By adjusting the degree parameter, practitioners can control the complexity of the decision boundary and tailor the SVM model to the specific characteristics of the data.</p> <pre><code># Instantiate a SVC classifier object with 'poly' kernel\nsvc = SVC(kernel=\"poly\", C=1.0, degree=2)\n\n# Fit the training data\nsvc.fit(X_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = svc.predict(X_test)\n\n# Check the accuracy\nprint(\"Accuracy of SVC with Polynomial Kernel:\", accuracy_score(y_test, y_pred).round(4))\n</code></pre> <pre><code>plot_decision_boundary(X, y, kernel=\"poly\", C=1.0, degree=2)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/","title":"Overfitting And Underfitting In Decision Trees","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#overfitting-and-underfitting-in-decision-trees","title":"Overfitting and Underfitting in Decision Trees","text":"<p>Decision trees are a type of supervised learning algorithm used in classification and regression tasks. Overfitting and underfitting are common issues encountered when building decision tree models. Let's discuss each:</p> <p>Overfitting:</p> <p>Definition: Overfitting occurs when the decision tree model captures noise and patterns that are not present in the true relationship between the features and the target variable. The model becomes too complex and fits the training data too closely. As a result, the model's performance degrades when applied to new, unseen data.</p> <p>Causes: - Deep Trees: Decision trees with too many levels or nodes tend to overfit the training data. As the tree gets deeper, it captures more details of the training data, including noise, which may not be present in the test data. - Small Minimum Samples per Leaf: A small minimum number of samples required to be at a leaf node can also cause overfitting. This condition allows the tree to make decisions based on very few data points, leading to a high variance in the model. - No Pruning: Decision trees, by default, are grown until all leaves are pure (i.e., they only contain data points from a single class). Without pruning, the decision tree will continue to grow and potentially overfit the data.</p> <p>Signs of Overfitting: - High accuracy on training data but low accuracy on testing data. - Decision boundaries that are too complex and irregular. - The model performs well on the training set but poorly on unseen data.</p> <p>Solutions to Overfitting: - Pruning: Prune the decision tree to remove unnecessary nodes and limit its depth. - Increase Minimum Samples per Leaf: Increase the minimum number of samples required to be at a leaf node. - Limit Tree Depth: Limit the maximum depth of the tree. - Cross-Validation: Use cross-validation techniques such as k-fold cross-validation to tune hyperparameters and evaluate model performance.</p> <p>Underfitting:</p> <p>Definition: Underfitting occurs when the decision tree model is too simple to capture the underlying structure of the data. The model fails to learn the relationships between the features and the target variable, resulting in poor performance on both the training and testing data.</p> <p>Causes: - Shallow Trees: Decision trees with too few levels or nodes may not fully capture the underlying structure of the data. The model is too simplistic and does not learn enough from the training data. - Large Minimum Samples per Leaf: A large minimum number of samples required to be at a leaf node can also cause underfitting. This condition may lead to a lack of flexibility in the model. - High Bias: The model is biased and makes oversimplified assumptions about the data.</p> <p>Signs of Underfitting: - Low accuracy on both training and testing data. - Decision boundaries that are too simplistic and fail to capture the underlying patterns in the data. - The model performs poorly on both the training set and unseen data.</p> <p>Solutions to Underfitting: - Increase Complexity: Increase the maximum depth of the tree or decrease the minimum number of samples required to be at a leaf node. - Feature Engineering: Introduce additional features or transformations of existing features to help the model capture the underlying patterns. - Change Model: Switch to a more complex model that can better capture the underlying structure of the data, such as a Random Forest or Gradient Boosting model.</p> <p>Conclusion:</p> <p>Balancing between overfitting and underfitting is crucial when building decision tree models. Regularization techniques such as pruning, limiting the tree depth, and adjusting hyperparameters help to strike the right balance between the model's complexity and its ability to generalize to new, unseen data. Regular monitoring of model performance using techniques like cross-validation is essential to ensure the decision tree model's effectiveness in real-world scenarios.</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code># %pip install mlxtend\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import accuracy_score\nfrom ipywidgets import interact, IntSlider, Dropdown, Layout\n\nplt.rcParams[\"font.family\"] = \"Times New Roman\"\nplt.rcParams[\"font.size\"] = 12\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#generate-a-data-for-classification","title":"Generate a Data for Classification","text":"<pre><code>X, y = make_blobs(n_samples=400, n_features=2, centers=2, cluster_std=4, random_state=42)\n\n# Plot the data\nplt.figure()\nplt.grid(c=\"lightgrey\")\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=45)\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\");\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#train-test-split","title":"Train Test Split","text":"<pre><code># Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#build-the-algorithm-to-plot-the-decision-boundary","title":"Build the Algorithm to Plot the Decision Boundary","text":"<pre><code># Define function to visualize decision boundary\ndef plot_decision_boundary(criterion='gini', splitter='best', max_depth=None, \n                           min_samples_split=2, min_samples_leaf=1):\n\n    # Train the Decision Tree Classifier\n    clf = DecisionTreeClassifier(criterion=criterion,\n                                 splitter=splitter,\n                                 max_depth=max_depth,\n                                 min_samples_split=min_samples_split,\n                                 min_samples_leaf=min_samples_leaf,\n                                 random_state=42)\n\n    clf.fit(X_train, y_train)\n\n    # Evaluate the accuracy on training and testing data\n    y_pred_train = clf.predict(X_train)\n    training_accuracy = accuracy_score(y_train, y_pred_train)\n\n    y_pred_test = clf.predict(X_test)\n    testing_accuracy = accuracy_score(y_test, y_pred_test)\n\n    # Plot the decision boundary\n    fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n    plot_decision_regions(X, y, clf=clf, ax=ax)\n    ax.set_xlabel(\"Feature 1\")\n    ax.set_ylabel(\"Feature 2\")\n    ax.set_title(f\"\"\"Decision Region of the Classifier\n    criterion={criterion}, max_depth={max_depth}, splitter={splitter}, min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}\n    Training Accuracy={training_accuracy.round(2)}, Validating Accuracy={testing_accuracy.round(2)}\"\"\")\n    plt.show();\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/00_Overfitting_and_Underfitting_in_Decision_Trees/#create-an-interactive-widget","title":"Create an Interactive Widget","text":"<pre><code>style = {'description_width': 'initial'}\ninteract(plot_decision_boundary, \n         max_depth=IntSlider(min=1, max=10, step=1, value=2, style=style, layout=Layout(width='30%')),\n         min_samples_split=IntSlider(min=2, max=10, step=1, value=2, style=style, layout=Layout(width='30%')),\n         min_samples_leaf=IntSlider(min=1, max=10, step=1, value=1, style=style, layout=Layout(width='30%')),\n         criterion=Dropdown(options=['gini', 'entropy'], value='gini'),\n         splitter=Dropdown(options=['best', 'random'], value='best'));\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/","title":"Regression Tree","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#regression-trees","title":"Regression Trees","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#load-the-data","title":"Load the Data","text":"<pre><code># Read the California housing data\ndf = pd.read_csv(\"D:\\Coding\\Datasets\\housing.csv\")\nprint(df.shape)\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code># Apply one hot encoding on 'ocean_proximity' column\ndf = pd.get_dummies(df, columns=[\"ocean_proximity\"], drop_first=True)\ndf.head()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(df.drop(\"median_house_value\", axis=1),\n                                                    df[\"median_house_value\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#train-a-decision-tree-regression-model","title":"Train a Decision Tree Regression Model","text":"<pre><code>from sklearn.tree import DecisionTreeRegressor\n</code></pre> <pre><code># Instantiate a decision tree regressor object\ndtr = DecisionTreeRegressor()\n\n# Fit the training data\ndtr.fit(X_train, y_train)\n</code></pre> <pre><code># Predict the test data\ny_pred = dtr.predict(X_test)\n</code></pre> <pre><code># Check the cross validation score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>print(\"Cross Validation R2 Score:\", \n      np.mean(cross_val_score(dtr, X_train, y_train, scoring=\"r2\", cv=10)))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre> <pre><code># Instantiate another decision tree regressor object\ndtr2 = DecisionTreeRegressor()\n</code></pre> <pre><code># Create the parameters grid\ndtr2_param_grid = {\n    \"max_depth\": [2, 4, 8, 10, None],\n    \"criterion\": [\"absolute_error\", \"squared_error\"],\n    \"min_samples_split\": [4, 8, 10, 12, None],\n    \"min_samples_leaf\": [1, 2, 4, 6, None],\n    \"max_features\": [0.25, 0.5, 0.75, 1.0, None]\n}\n</code></pre> <pre><code># Apply the GridSearchCV to find the best hyperparameters\ndtr2_grid = GridSearchCV(estimator=dtr2,\n                         param_grid=dtr2_param_grid,\n                         scoring=\"r2\",\n                         n_jobs=-1,\n                         cv=5,\n                         verbose=2)\n\n# Fit the training data\ndtr2_grid.fit(X_train, y_train)\n</code></pre> <pre><code># Print the best parameters\ndtr2_grid.best_params_\n</code></pre> <pre><code># Print the best score\ndtr2_grid.best_score_\n</code></pre> <pre><code># Train the model with best parameters\ndtr2 = dtr2_grid.best_estimator_\ndtr2\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = dtr2.predict(X_test)\n</code></pre> <pre><code># Calculate the R2 Score\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/05_Decision_Tree/01_Regression_Tree/#feature-importance","title":"Feature Importance","text":"<pre><code># Extract the feature importance in a dataframe\nfeature_importance = pd.Series(dtr2.feature_importances_, index=X_train.columns)\\\n                       .sort_values(ascending=False)\nfeature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(4, 5))\nsns.barplot(x=feature_importance, y=feature_importance.index)\nplt.title(\"Feature Importance\")\nplt.show()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/","title":"Adaboost Step By Step","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#adaboost-step-by-step","title":"AdaBoost - Step by Step","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#create-a-dataframe","title":"Create a DataFrame","text":"<pre><code># Create a custom dataframe\ndata = {\n    \"x1\": [1, 2, 3, 4, 5, 6, 6, 7, 9, 9],\n    \"x2\": [5, 3, 6, 8, 1, 9, 5, 8, 9, 2],\n    \"label\": [1, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n}\ndf = pd.DataFrame(data)\ndf\n</code></pre> <pre><code># Plot the data\nsns.scatterplot(x=df[\"x1\"], y=df[\"x2\"], hue=df[\"label\"]);\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-1-initialize-weights","title":"Step-1: Initialize Weights","text":"<p>Start with your training dataset. Each example in the dataset is initially given the same weight. This means that every example is equally important at the beginning.</p> <pre><code># Assign weights to each rows\n# Initial weights = 1/n\ndf[\"weights\"] = 1 / df.shape[0]\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-2-train-the-first-decision-stump","title":"Step-2: Train the First Decision Stump","text":"<pre><code>X = df.iloc[:, :2].values\ny = df.iloc[:, 2].values\n</code></pre> <pre><code># Train a decision stump / weak classifier\ndt1 = DecisionTreeClassifier(max_depth=1)\ndt1.fit(X, y)\n</code></pre> <pre><code># Plot the tree\nplot_tree(dt1);\n</code></pre> <pre><code># Plot the decision region\nplot_decision_regions(X, y, clf=dt1, legend=2);\n</code></pre> <pre><code># Calculate prediction of the first model\ndf[\"y_pred\"] = dt1.predict(X)\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-3-calculate-model-weight","title":"Step-3: Calculate Model Weight","text":"<pre><code># Write a function to calculate the model weight (alpha)\ndef calculate_model_weights(data):\n    df = data\n    df[\"weights\"] = 1 / df.shape[0]\n\n    error = 0\n\n    for index, row in df.iterrows():\n        if row[\"label\"] != row[\"y_pred\"]:\n            error += row[\"weights\"]\n\n    alpha = 0.5 * np.log((1 - error) / (error + 0.000000001))\n\n    return alpha\n</code></pre> <pre><code># Calculate weight of the first model\nalpha1 = calculate_model_weights(df)\nalpha1\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-4-update-the-row-weights","title":"Step-4: Update the Row Weights","text":"<pre><code># Write a function to update the weights of each row\ndef update_row_weights(row, alpha):\n    if row[\"label\"] == row[\"y_pred\"]:\n        return row[\"weights\"] * np.exp(-alpha)\n\n    else:\n        return row[\"weights\"] * np.exp(alpha)\n</code></pre> <pre><code># Update the weights\ndf[\"updated_weights\"] = df.apply(lambda row: update_row_weights(row, alpha1), axis=1)\ndf\n</code></pre> <pre><code># Normalized the weights\ndf[\"normalized_weights\"] = df[\"updated_weights\"] / df[\"updated_weights\"].sum()\ndf\n</code></pre> <pre><code>df[\"normalized_weights\"].sum()\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-5-initialize-ranges","title":"Step-5: Initialize Ranges","text":"<pre><code>def initialize_ranges(data):\n    range_data = pd.DataFrame(data)\n    range_data[\"cumsum_upper\"] = np.cumsum(range_data[\"normalized_weights\"])\n    range_data[\"cumsum_lower\"] = (\n        range_data[\"cumsum_upper\"] - range_data[\"normalized_weights\"]\n    )\n    range_data = range_data[\n        [\n            \"x1\",\n            \"x2\",\n            \"label\",\n            \"weights\",\n            \"y_pred\",\n            \"updated_weights\",\n            \"normalized_weights\",\n            \"cumsum_lower\",\n            \"cumsum_upper\",\n        ]\n    ]\n    return range_data\n</code></pre> <pre><code>df = initialize_ranges(df)\ndf\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-6-create-a-new-dataset-for-upsampling","title":"Step-6: Create a New Dataset for Upsampling","text":"<pre><code># Write a function to create a new dataset based on ranges\ndef create_new_dataset(data):\n    indices = []\n\n    for i in range(data.shape[0]):\n        a = np.random.random()\n        for index, row in data.iterrows():\n            if row[\"cumsum_lower\"] &lt; a &lt; row[\"cumsum_upper\"]:\n                indices.append(index)\n\n    print(indices)\n    new_data = data.iloc[indices, [0, 1, 2, 3]]\n\n    return new_data\n</code></pre> <pre><code>df2 = create_new_dataset(df)\ndf2\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#steps-7-iterate-the-process-for-all-the-decision-stumps","title":"Steps-7: Iterate the Process for All the Decision Stumps","text":""},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#second-decision-stumps","title":"Second Decision Stumps","text":"<pre><code>X = df2.iloc[:, :2].values\ny = df2.iloc[:, 2].values\n</code></pre> <pre><code># Initialize the second decision stump\ndt2 = DecisionTreeClassifier(max_depth=1)\ndt2.fit(X, y)\n</code></pre> <pre><code># Plot the tree\nplot_tree(dt2);\n</code></pre> <pre><code># Plot the decision region\nplot_decision_regions(X, y, clf=dt2, legend=2);\n</code></pre> <pre><code># Calculate prediction of the second model\ndf2[\"y_pred\"] = dt2.predict(X)\ndf2\n</code></pre> <pre><code># Calculate weight(alpha) for the second model\nalpha2 = calculate_model_weights(df2)\nalpha2\n</code></pre> <pre><code># Update the row weights\ndf2[\"updated_weights\"] = df2.apply(lambda row: update_row_weights(row, alpha2), axis=1)\ndf2\n</code></pre> <pre><code># Normalized the weights\ndf2[\"normalized_weights\"] = df2[\"updated_weights\"] / df2[\"updated_weights\"].sum()\ndf2\n</code></pre> <pre><code># Initialize ranges\ndf2 = initialize_ranges(df2)\ndf2\n</code></pre> <pre><code># Create a new dataframe\ndf3 = create_new_dataset(df2)\ndf3\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#third-decision-stumps","title":"Third Decision Stumps","text":"<pre><code>X = df3.iloc[:, :2].values\ny = df3.iloc[:, 2].values\n</code></pre> <pre><code># Initialize the third decision stump\ndt3 = DecisionTreeClassifier(max_depth=1)\ndt3.fit(X, y)\n</code></pre> <pre><code># Plot the tree\nplot_tree(dt3);\n</code></pre> <pre><code># Plot the decision region\nplot_decision_regions(X, y, clf=dt3, legend=2);\n</code></pre> <pre><code># Calculate prediction of the third model\ndf3[\"y_pred\"] = dt3.predict(X)\ndf3\n</code></pre> <pre><code># Calculate weight(alpha) for the third model\nalpha3 = calculate_model_weights(df3)\nalpha3\n</code></pre> <pre><code>print(\"alpha1:\", alpha1)\nprint(\"alpha2:\", alpha2)\nprint(\"alpha3:\", alpha3)\n</code></pre>"},{"location":"data-science/End-to-End-Machine-Learning/03_Machine_Learning_Algorithms/AdaBoost/00_AdaBoost_Step_by_Step/#step-8-calculate-the-output","title":"Step-8: Calculate the Output","text":"<pre><code># Select one query points\nquery1 = df.iloc[0, :2].values.reshape(1, 2)\nquery1\n</code></pre> <pre><code># Predict the query with three decision stumps\npred1 = (\n    (alpha1 * dt1.predict(query1)[0])\n    + (alpha2 * dt2.predict(query1)[0])\n    + (alpha3 * dt3.predict(query1))[0]\n)\npred1 = np.sign(pred1)\npred1\n</code></pre> <pre><code># Select another query points\nquery2 = df.iloc[8, :2].values.reshape(1, 2)\nquery2\n</code></pre> <pre><code># Predict the query with three decision stumps\npred2 = (\n    (alpha1 * dt1.predict(query2)[0])\n    + (alpha2 * dt2.predict(query2)[0])\n    + (alpha3 * dt3.predict(query2))[0]\n)\npred2 = np.sign(pred2)\npred2\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/","title":"End to End Deep Learning","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/","title":"Activation Functions In Deep Learning","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: geenext     language: python     name: python3</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#activation-functions-in-deep-learning","title":"Activation Functions in Deep Learning","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#what-are-activation-functions","title":"What are Activation Functions?","text":"<p>In artificial neural networks, each neuron forms a weighted sum of its inputs and passes the resulting scalar value through a function referred to as an activation function or transfer function. If a neuron has n inputs \\(x_1, x_2, ..., x_n\\) then the output of a neuron is: $\\(a = g(w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b)\\)$ The function \\(g\\) is referred to as the activation function. If the function \\(g\\) is taken as the linear function \\(g(z) = z\\) then the neuron performs linear regression or classification. In general \\(g\\) is taken to be a nonlinear function to do nonlinear regression and solve classification problems that are not linearly separable. When \\(g\\) is taken to be a sigmoidal or 's' shaped function varying from 0 to 1 or -1 to 1, the output value of the neuron can be interpreted as a YES/NO answer or binary decision. However saturating activation function can cause the vanishing gradient problem in deep networks. Replacing saturating sigmoidal activation functions with activation functions like ReLU that have larger derivative values allowed deeper networks to be trained for the first time. Non-monotonic and oscillating activation functions that significantly outperform ReLU have since been found. In particular oscillating activation functions improve gradient flow, speedup training and allow single neurons to learn the XOR function like certain human cerebral neurons.</p> <p></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#why-activation-functions-are-needed","title":"Why Activation Functions are needed?","text":"<p>Activation functions are essential in neural networks for the following reasons:</p> <ol> <li> <p>Introduce Non-Linearity: They allow networks to learn complex, non-linear relationships in data, which linear models can't capture.</p> </li> <li> <p>Enable Deep Learning: They help in learning hierarchical feature representations, allowing deeper layers to capture more abstract patterns.</p> </li> <li> <p>Universal Approximation: With activation functions, networks can approximate any continuous function, as per the Universal Approximation Theorem.</p> </li> <li> <p>Gradient-Based Optimization: They provide the necessary gradients for training via backpropagation, making learning possible and efficient.</p> </li> <li> <p>Biological Inspiration: They mimic the non-linear activation of biological neurons, adding biological plausibility to artificial models.</p> </li> <li> <p>Output Characteristics: Different activation functions offer various output ranges, suited for specific tasks like classification.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#code-example","title":"Code Example","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom mlxtend.plotting import plot_decision_regions\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams[\"font.serif\"] = \"Times New Roman\"\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#make-a-dataset","title":"Make a Dataset","text":"<pre><code># Generate a classification dataset\nX, y = make_classification(\n    n_samples=200, \n    n_features=2, \n    n_informative=2, \n    n_redundant=0,\n    n_clusters_per_class=1,\n    class_sep=1,\n    random_state=32)\n\n# Plot the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y);\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#neural-network-with-linear-activation-in-the-hidden-layers","title":"Neural Network with Linear Activation in the Hidden Layers","text":"<pre><code># Build a neural network with linear activation used in the hidden layers\nmodel = Sequential()\n\nmodel.add(Dense(128, activation=\"linear\", input_dim=2))\nmodel.add(Dense(128, activation=\"linear\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(X, y, epochs=500, validation_split=0.2, verbose=1)\n</code></pre> <pre><code># Plot the decision region\nplot_decision_regions(X, y, clf=model, legend=2);\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#neural-network-with-non-linear-activation-in-the-hidden-layers","title":"Neural Network with Non-Linear Activation in the Hidden Layers","text":"<pre><code># Build a neural network with non-linear activation used in the hidden layers\nmodel = Sequential()\n\nmodel.add(Dense(128, activation=\"relu\", input_dim=2))\nmodel.add(Dense(128, activation=\"relu\", input_dim=2))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(X, y, epochs=500, validation_split=0.2, verbose=1)\n</code></pre> <pre><code># Plot the decision region\nplot_decision_regions(X, y, clf=model, legend=2);\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#conclusion","title":"Conclusion","text":"<p>The first model with linear activations can only separate the data with a straight line, limiting its ability to handle complex patterns. The second model with ReLU activations in the hidden layers can create a more flexible and complex decision boundary, leading to better performance on non-linear data distributions.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#characteristics-of-an-ideal-activation-function","title":"Characteristics of an Ideal Activation Function","text":"<p>An ideal activation function should have the following characteristics:</p> <ol> <li>Non-Linear: Non-linearity allows the network to model complex patterns and relationships in the data, enabling the network to learn and approximate any function, not just linear relationships.</li> <li>Differentiable: Differentiability ensures that the activation function can be used in gradient-based optimization methods, such as backpropagation.</li> <li>Computationally Inexpensive: The activation function should be computationally efficient to evaluate, especially for large-scale neural networks, to ensure that training and inference are feasible and fast.</li> <li>Zero-Centered: A zero-centered activation function outputs values that are centered around zero, helping to ensure that the gradients are balanced and preventing one side of the weights from dominating.</li> <li>Non-Saturating: Non-saturating activation functions do not suffer from the vanishing gradient problem, where gradients become very small, leading to slow learning or no learning at all. For example, functions like ReLU do not saturate in the positive region, allowing gradients to remain significant, thus helping deep networks to continue learning effectively as the number of layers increases.</li> <li>Smooth: Smoothness ensures that the activation function doesn\u2019t introduce any abrupt changes in the output, which can help in more stable learning.</li> <li>Monotonic: A monotonic function ensures that the gradient doesn't change signs, which can simplify the optimization process.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#different-types-of-activation-functions","title":"Different Types of Activation Functions","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#sigmoid-activation-function","title":"Sigmoid Activation Function","text":"<p>The sigmoid activation function is a popular activation function used in neural networks, especially in binary classification tasks. It is defined by the following formula: $\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)$ where \\(x\\) is the input to the neuron.</p> <p></p> <p>Characteristics of the Sigmoid Activation Function:</p> <ol> <li> <p>Output Range: The output of the sigmoid function ranges between 0 and 1. It is useful for models that need to predict probabilities or binary outcomes.</p> </li> <li> <p>Non-Linearity: The sigmoid function is non-linear, allowing the network to capture complex patterns.</p> </li> <li> <p>Smooth and Differentiable: The sigmoid function is smooth and differentiable, making it suitable for gradient-based optimization methods like backpropagation.</p> </li> <li> <p>Saturating: The sigmoid function can saturate, meaning that for very large or very small input values, the output approaches 1 or 0, respectively. This can lead to very small gradients (vanishing gradients), slowing down learning in deep networks.</p> </li> <li> <p>Not Zero-Centered: The output is always positive, which can lead to inefficiencies during training since the gradients will also always be positive or always negative.</p> </li> <li> <p>Applications: Commonly used in the output layer of binary classification models. Also used in the hidden layers of older neural network models, though ReLU and its variants are more commonly used now due to better performance in deeper networks.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#tanh-activation-function","title":"Tanh Activation Function","text":"<p>The tanh (hyperbolic tangent) activation function is another widely used activation function in neural networks. It is defined by the following formula: $\\(\\text{tanh}(x) = \\frac{(e^x - e^{-x})}{e^x + e^{-x}}\\)$ where \\(x\\) is the input to the neuron.</p> <p></p> <p>Characteristics of the Tanh Activation Function:</p> <ol> <li> <p>Output Range: The output of the tanh function ranges between \\(-1\\) and \\(1\\). It provides a symmetric output centered around zero, which is an advantage over the sigmoid function.</p> </li> <li> <p>Non-Linearity: Like the sigmoid function, tanh is non-linear, allowing the network to model complex relationships.</p> </li> <li> <p>Smooth and Differentiable: The tanh function is smooth and differentiable, making it suitable for gradient-based optimization methods like backpropagation.</p> </li> <li> <p>Zero-Centered: Unlike the sigmoid function, tanh is zero-centered, meaning that the output is symmetrically distributed around zero. This helps in balancing the gradients and making learning more efficient.</p> </li> <li> <p>Saturating: Tanh can also saturate, meaning for very large or very small input values, the output approaches 1 or -1. This can lead to the vanishing gradient problem, similar to the sigmoid function.</p> </li> <li> <p>Applications: Often used in hidden layers of neural networks. It was more commonly used before ReLU and its variants became popular due to better performance in deep networks.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#relu-activation-function","title":"ReLU Activation Function","text":"<p>The ReLU (Rectified Linear Unit) activation function is one of the most widely used activation functions in modern neural networks, especially in deep learning. It is defined by the following formula: $\\(\\text{ReLU}(x) = \\text{max}(0, x)\\)$ where $ x $ is the input to the neuron.</p> <p></p> <p>Characteristics of the ReLU Activation Function:</p> <ol> <li> <p>Output Range: The output of the ReLU function ranges from \\(0\\) to \\(\\infty\\). The function outputs zero for any negative input and the input itself for any positive input.</p> </li> <li> <p>Non-Linearity: ReLU is non-linear, allowing the network to model complex patterns and interactions between features.</p> </li> <li> <p>Computationally Efficient: ReLU is simple to compute since it only involves a comparison operation, making it computationally efficient.</p> </li> <li> <p>Non-Saturating: Unlike sigmoid and tanh, ReLU does not saturate for positive inputs, which helps avoid the vanishing gradient problem. This allows for faster learning in deep networks.</p> </li> <li> <p>Not Zero-Centered: The output of ReLU is not zero-centered since it only outputs non-negative values. This can sometimes cause inefficiencies in learning, but these are usually outweighed by the benefits.</p> </li> <li> <p>Sparse Activation: ReLU leads to sparse activation, meaning that in a large network, many neurons will output zero for a given input, which can help with computational efficiency and reducing overfitting.</p> </li> <li> <p>Potential Issue (Dying ReLU): A potential drawback of ReLU is the \"dying ReLU\" problem, where neurons can get stuck outputting zero and stop learning. This happens if the weights are updated in such a way that they always produce a negative input for certain neurons.</p> </li> <li> <p>Variants: Variants like Leaky ReLU, Parametric ReLU (PReLU), and ELU have been introduced to address the dying ReLU problem by allowing a small, non-zero gradient for negative inputs.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#variants-of-relu-activation","title":"Variants of ReLU Activation","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#the-dying-relu-problem","title":"The Dying ReLU Problem","text":"<p>The Dying ReLU problem occurs in neural networks when neurons become inactive and only output zero for any input, effectively \"dying\" and ceasing to contribute to the learning process. This problem arises due to the nature of the ReLU activation function, especially during the training process when weights are updated via backpropagation.</p> <p>During training, if a large number of neurons in a layer consistently receive negative inputs, these neurons will output zero and their gradients will also be zero. Consequently, these neurons do not contribute to the model's learning since their weights do not get updated (because the gradient of ReLU for $ x \\leq 0 $ is zero). If too many neurons \"die\" this way, it can severely degrade the model's performance.</p> <p>Mathematical Formulation:</p> <ul> <li> <p>Let \\(a_i\\) be the activation of the \\((i)\\)-th neuron after applying ReLU:     $\\(a_i = \\text{ReLU}(z_i) = \\max(0, z_i)\\)$     where \\(z_i = w_i^Tx + b\\), with \\(w_i\\) being the weight vector, \\(x\\) the input, and \\(b_i\\) the bias.</p> </li> <li> <p>The gradient of the loss function \\(L\\) with respect to input \\(z_i\\) of ReLU is:     $$     \\frac{\\partial L}{\\partial z_i} = \\begin{cases}     \\frac{\\partial L}{\\partial a_i} &amp; \\text{if } z_i &gt; 0 \\     0 &amp; \\text{if } z_i \\leq 0     \\end{cases}     $$</p> </li> <li> <p>If $ z_i \\leq 0 $, the gradient becomes zero, leading to no weight update:   $$   \\Delta w_i = -\\eta \\frac{\\partial L}{\\partial w_i} = 0   $$   where $ \\eta $ is the learning rate.</p> </li> </ul>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#1-leaky-relu","title":"1. Leaky ReLU:","text":"<p>Leaky ReLU is a variant of the Rectified Linear Unit (ReLU) activation function designed to address the Dying ReLU problem. It is defined by the following formula: $$     \\text{Leaky ReLU}(x) = \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\     0.01 x &amp; \\text{if } x \\leq 0     \\end{cases} $$ where \\(0.01\\) is a small positive constant.</p> <p></p> <p></p> <p>Characteristics: 1. Non-Linearity: Like ReLU, Leaky ReLU is non-linear, allowing the network to capture complex patterns.</p> <ol> <li> <p>Non-Zero Gradient for Negative Inputs: Unlike standard ReLU, which outputs zero for negative inputs, Leaky ReLU assigns a small, non-zero slope for negative inputs. This ensures that the neuron continues to learn, as the gradient is non-zero for negative inputs.</p> </li> <li> <p>Output Range: The output of Leaky ReLU can range from \\(-\\infty\\) to $ \\infty $, depending on the input.</p> </li> <li> <p>Avoiding the Dying ReLU Problem: By allowing a small negative slope for negative inputs, Leaky ReLU prevents neurons from getting stuck and \"dying\" (outputting zero for all inputs).</p> </li> <li> <p>Differentiable: The Leaky ReLU function is differentiable, making it compatible with gradient-based optimization methods like backpropagation.</p> </li> <li> <p>Zero-Centered: Leaky ReLU is not zero-centered, as it outputs positive values for positive inputs and negative values (though small) for negative inputs. However, it still performs well in practice.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#2-parametric-relu-prelu","title":"2. Parametric ReLU (PReLU):","text":"<p>Parametric ReLU (PReLU) is a variant of the ReLU (Rectified Linear Unit) activation function designed to improve upon the standard ReLU and its variant, Leaky ReLU, by allowing the slope of the function for negative inputs to be learned during training. This provides more flexibility and adaptability to the network. It is defined by the following formula:</p> <p>$$     \\text{PReLU}(x) = \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\     \\alpha_i x &amp; \\text{if } x \\leq 0     \\end{cases} $$ where $ \\alpha_i $ is a learnable parameter for each neuron, rather than a fixed constant like in Leaky ReLU.</p> <p></p> <p>Characteristics: 1. Learnable Slope: Unlike Leaky ReLU, where the slope for negative inputs \\(( \\alpha )\\) is fixed, PReLU allows this slope to be a parameter that is learned from the data during the training process. This means each neuron can have its own slope $ \\alpha_i $ for negative inputs, allowing the network to adapt more effectively to the specific data.</p> <ol> <li> <p>Non-Linearity: Like ReLU and Leaky ReLU, PReLU is non-linear, which enables the network to model complex relationships in the data.</p> </li> <li> <p>Output Range: The output of PReLU can range from \\(-\\infty\\) to $ \\infty $, depending on the input.</p> </li> <li> <p>Avoids the Dying ReLU Problem: Similar to Leaky ReLU, PReLU mitigates the Dying ReLU problem by ensuring that neurons have a non-zero gradient even for negative inputs. However, it improves upon Leaky ReLU by allowing the network to learn the best slope for negative values during training.</p> </li> <li> <p>Differentiable: PReLU is differentiable, making it compatible with gradient-based optimization methods like backpropagation.</p> </li> <li> <p>Zero-Centered: Like ReLU and Leaky ReLU, PReLU is not zero-centered, meaning it can output positive or negative values depending on the input.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#3-exponential-linear-unit-elu","title":"3. Exponential Linear Unit (ELU):","text":"<p>The Exponential Linear Unit (ELU) is an activation function designed to address some of the limitations of the ReLU (Rectified Linear Unit) and its variants. ELU helps to improve the learning dynamics of deep neural networks by introducing a smoother transition from negative to positive values, which can lead to faster learning and better performance. It is defined by the following formula: $$ \\text{ELU}(x) = \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\     \\alpha (e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases} $$ where $ \\alpha $ is a hyperparameter that controls the value to which ELU saturates for negative inputs. Typically, $ \\alpha $ is set to 1.</p> <p> </p> <p>Characteristics:</p> <ol> <li> <p>Non-Linearity: ELU is a non-linear activation function, which enables the network to model complex patterns in the data.</p> </li> <li> <p>Smoother Transition: Unlike ReLU, which has a sharp change at $ x = 0 $, ELU provides a smooth and continuous transition between the negative and positive parts of the function. This smoothness can improve the learning dynamics and make optimization easier.</p> </li> <li> <p>Negative Values: ELU allows negative values for inputs less than zero. This negative output helps to push the mean activation closer to zero, which can make the learning process more efficient by reducing bias shifts.</p> </li> <li> <p>Avoids the Dying ReLU Problem: ELU helps mitigate the Dying ReLU problem by allowing negative values. Even for negative inputs, ELU outputs a non-zero value (except for large negative inputs where the output saturates to $-\\alpha$, which helps in maintaining active gradients and continued learning.</p> </li> <li> <p>Differentiable: The ELU function is differentiable, which is essential for backpropagation during training.</p> </li> <li> <p>Output Range: The output of ELU ranges from \\(-\\alpha\\) to $ \\infty $. For positive inputs, the output is unbounded, similar to ReLU, while for negative inputs, the output saturates at \\(-\\alpha\\).</p> </li> <li> <p>Parameter $ \\alpha $: The parameter $ \\alpha $ controls the saturation point for negative inputs. It is usually set to 1, but it can be tuned depending on the specific application.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/activation_functions_in_deep_learning/#4-scaled-exponential-linear-unit-selu","title":"4. Scaled Exponential Linear Unit (SELU):","text":"<p>The Scaled Exponential Linear Unit (SELU) is an activation function designed to improve the learning dynamics of deep neural networks. It is a self-normalizing activation function, meaning it helps to maintain a mean of zero and unit variance throughout the network, which can significantly enhance training stability and performance. It is defined by the following formula:</p> <p>$$     \\text{SELU}(x) = \\lambda \\begin{cases}     x &amp; \\text{if } x &gt; 0 \\     \\alpha (e^x - 1) &amp; \\text{if } x \\leq 0     \\end{cases}  $$</p> <p>where $ \\alpha $ and $ \\lambda $ are predefined constants:    - Typically, $ \\alpha \\approx 1.6733 $    - Typically, $ \\lambda \\approx 1.0507 $</p> <p></p> <p>Characteristics: 1. Non-Linearity: SELU is a non-linear activation function, which enables the network to capture complex patterns in the data.</p> <ol> <li> <p>Self-Normalizing: The primary advantage of SELU is its self-normalizing property. It helps to maintain zero mean and unit variance throughout the layers of the network, which stabilizes the training process and allows for faster convergence.</p> </li> <li> <p>Negative Values: Similar to ELU, SELU outputs negative values for negative inputs. This negative saturation helps to maintain zero-centered outputs, reducing bias shifts.</p> </li> <li> <p>Avoids the Dying ReLU Problem: By allowing negative values and ensuring a non-zero gradient for all inputs, SELU helps to avoid the Dying ReLU problem, where neurons stop learning due to zero gradients.</p> </li> <li> <p>Differentiable: The SELU function is differentiable, making it compatible with gradient-based optimization methods such as backpropagation.</p> </li> <li> <p>Output Range: The output of SELU ranges from $ \\lambda \\alpha (e^x - 1) $ for negative inputs to $ \\lambda x $ for positive inputs.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/","title":"Introduction To Perceptron","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#introduction-to-perceptron","title":"Introduction to Perceptron","text":"<p>A perceptron is one of the simplest and fundamental building blocks in deep learning and artificial neural networks. It was developed by Frank Rosenblatt in the late 1950s and is a type of artificial neuron or node that can be used for binary classification tasks. While perceptrons are limited in their capabilities compared to more complex neural network architectures, they serve as a foundational concept for understanding how neural networks work.</p> <p>Here's an introduction to perceptrons in deep learning:</p> <ol> <li>Basic Structure: A perceptron takes multiple binary inputs (0 or 1) and produces a single binary output (0 or 1). Each input is associated with a weight, and there is also an additional parameter called the bias. Mathematically, the output of a perceptron is calculated as the weighted sum of inputs plus the bias, followed by applying a step function (often the Heaviside step function or a similar activation function) to the sum.</li> </ol> \\[y = \\text{Activation Function}\\left(\\sum_{i=1}^{n} \\text{weight}_i \\cdot \\text{input}_i + \\text{bias}\\right)\\] <ol> <li> <p>Weights and Bias: The weights in a perceptron represent the strength of the connection between the inputs and the output. A larger weight means that the corresponding input has a stronger influence on the output. The bias acts as an offset, allowing the perceptron to produce different outputs even when all inputs are zero.</p> </li> <li> <p>Activation Function: The activation function determines whether the perceptron should fire (output 1) or not (output 0) based on the weighted sum of inputs plus the bias. The choice of activation function is crucial, as it introduces non-linearity into the model. Common activation functions include the step function, sigmoid, ReLU (Rectified Linear Unit), and others.</p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1brKm_Yxvcgs\" outputId=\"04f02813-7c06-469f-c7e7-74acff863910\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"75becf96-2a01-4b66-95f5-12a40768048c\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import accuracy_score\nfrom mlxtend.plotting import plot_decision_regions\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#read-the-data","title":"Read the Data","text":"<p>```python id=\"c77b28f6-6c15-4d65-9a7b-7874dcebd69a\" outputId=\"c8394dff-0fb9-45c6-be3e-0d025c021186\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Placement.csv\") df <pre><code>```python id=\"0bc3120c-d519-474e-abab-320b9860ab35\" outputId=\"43c2e10a-ac1b-48cc-c0f1-d7231863b0b0\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check if there is any null values\ndf.info()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#train-test-split","title":"Train Test Split","text":"<p>```python id=\"073928d4-26cb-445a-80e1-ceedb0e9fc0f\" outputId=\"ab3a038e-fd8c-4707-9a17-9db0136bced8\" colab={\"base_uri\": \"https://localhost:8080/\"} X_train, X_test, y_train, y_test = train_test_split(df.drop(\"placed\", axis=1),                                                     df[\"placed\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>&lt;!-- #region id=\"e6171f13-7408-4145-9a90-f86401950ed3\" --&gt;\n## **Data Visualization**\n&lt;!-- #endregion --&gt;\n\n```python id=\"89303e37-1bda-472f-8be1-31b8fb87cec0\" outputId=\"88022cf8-ca64-458f-fe23-6ad504a172ab\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472}\n# Display a scatterplot between CGPA and IQ\nsns.scatterplot(data=df, x=\"cgpa\", y=\"resume_score\", hue=df[\"placed\"])\nplt.title(\"Scatterplot between CGPA and IQ\")\nplt.show()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#train-a-perceptron-for-classification","title":"Train a Perceptron for Classification","text":"<p>```python id=\"6237277f-3e1d-40c4-9d46-287b17772ee0\" outputId=\"60a8ac3a-4d7a-4a6e-a9b9-cd1cd426aa9b\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#create-an-object-of-the-perceptron-class","title":"Create an object of the Perceptron class","text":"<p>perceptron = Perceptron(random_state=0)</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#fit-the-training-data","title":"Fit the training data","text":"<p>perceptron.fit(X_train, y_train) <pre><code>```python id=\"663ca708-746b-4b2a-98e9-01df30942769\" outputId=\"31da0612-85ad-46bf-93bf-73689f2ebe69\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check all the coefficients (weights)\nperceptron.coef_\n</code></pre></p> <p>```python id=\"920f95ff-4f1f-4f93-a301-496fd0cfe91c\" outputId=\"846d0cdb-c07e-40ef-d1a9-03bdc7965a6e\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#check-the-intercept-bias","title":"Check the intercept (bias)","text":"<p>perceptron.intercept_ <pre><code>```python id=\"d05fce88-b34d-45d5-b9ff-3a588dc0ad9f\" outputId=\"ede65587-e648-49ca-dd1e-a4c3f56d7562\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Predict the test data\ny_pred = perceptron.predict(X_test)\ny_pred\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/00_Introduction_to_Perceptron/#accuracy-assessment","title":"Accuracy Assessment","text":"<p>```python id=\"c9905b33-13fa-4e01-a386-68780db67622\" outputId=\"180b85d1-b5c3-4da7-a7e7-2777bc4d6aa0\" colab={\"base_uri\": \"https://localhost:8080/\"} print(\"Accuracy of Perceptron Model:\", accuracy_score(y_test, y_pred).round(2)) <pre><code>&lt;!-- #region id=\"a07e65f4-7d3a-440d-8335-1c3a0bef6caf\" --&gt;\n## **Display the Decision Boundary**\n&lt;!-- #endregion --&gt;\n\n```python id=\"f14f6a9d-5fe1-4765-b354-1f71352ab7f0\" outputId=\"38d46188-c5be-4ad4-9388-c8583c2c74cf\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449}\nplot_decision_regions(X_train.values, y_train.values, clf=perceptron, legend=2)\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/","title":"Perceptron Tricks","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#perceptron-trick-how-to-train-a-perceptron","title":"Perceptron Trick: How to Train a Perceptron","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#import-rquired-libraries","title":"Import Rquired Libraries","text":"<p>```python id=\"i9u_ADko9370\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#pip-install-ipympl","title":"%pip install ipympl","text":"<pre><code>```python id=\"efeb428f-2098-4830-a24e-eca2062a9f2f\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#create-a-dataset-for-classification","title":"Create a Dataset for Classification","text":"<p>```python id=\"d1948dd7-0088-4d85-a296-8773c602a4dc\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#create-a-classification-dataset-with-two-features","title":"Create a classification dataset with two features","text":"<p>X, y = make_classification(n_samples=100,                            n_features=2,                            n_informative=1,                            n_redundant=0,                            n_classes=2,                            n_clusters_per_class=1,                            random_state=0,                            hypercube=False,                            class_sep=1) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"6cd476fd-8c8a-42ee-badf-98e1065080e7\" outputId=\"b7d28603-6c4b-465b-ca71-87489ed77fd9\"\n# Plot the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=50)\nplt.title(\"Classification Dataset\")\nplt.show()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"388bcccc-8b34-4453-92c3-149f48e617fe\" outputId=\"b24ab15c-eac0-424c-c619-44e75de4460f\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#print-the-first-5-rows-of-x","title":"Print the first 5 rows of x","text":"<p>X[:5, :] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"4dec12e1-4160-4376-a8df-662eaf135a55\" outputId=\"ad335202-facd-4c82-f30a-61f579bae7f0\"\n# Print the first 5 items of y\ny[:5]\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#create-a-function-to-implement-perceptron-algorithm","title":"Create a Function to Implement Perceptron Algorithm","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#formula-to-update-the-weights-in-a-peceptron-model","title":"Formula to Update the Weights in a Peceptron Model:","text":"<p>```python id=\"bca56c1a-55fc-4302-8334-b909179db1b9\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#define-a-step-function","title":"Define a step function","text":"<p>def step(z):     \"\"\"     This function returns 0 if value is less than or equals to 0 and returns 1     if value is greater than 0.     \"\"\"     return 0 if z &lt;= 0 else 1 <pre><code>```python id=\"9e2eba2a-b135-4399-b807-978f7143186c\"\ndef perceptron(X, y, epoch):\n    # Add an extra column with the X to represent bias\n    X = np.insert(arr=X, obj=0, values=1, axis=1)\n\n    # Define a variable for weights\n    weights = np.ones(X.shape[1])\n\n    # Define the learning rate\n    lr = 0.1\n\n    for i in range(epoch):\n        j = np.random.randint(0, 100)\n\n        # Select a random row from x\n        X_random = X[j]\n        y_random = y[j]\n\n        # Predict the y for x_random\n        y_hat = step(np.dot(X_random, weights))\n\n        # Update the weights\n        weights = weights + lr*(y_random - y_hat)*X_random\n\n    return weights\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#apply-the-perceptron-model-on-the-classification-dataset","title":"Apply the Perceptron Model on the Classification Dataset","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"a625909b-4d49-45a4-994a-b5ddf5f3e2e2\" outputId=\"23062495-9db9-412e-a1ac-43d1a24db997\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#apply-the-perceptron-model-on-the-dataset-and-extract-the-weights","title":"Apply the Perceptron model on the dataset and extract the weights","text":"<p>weights = perceptron(X, y, epoch=1000) weights <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"5619a711-80bb-42d1-839b-c012787e8224\" outputId=\"c1ebb1fc-dbca-4ec0-eac6-5733cb8be297\"\n# Extract the intercept and coefficients value\nintercept_ = weights[0]\ncoeff_ = weights[1:]\nprint(\"Intercept:\", intercept_)\nprint(\"Coefficients:\", coeff_)\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#derive-the-separation-line-from-the-coefficients-and-intercept-value","title":"Derive the Separation Line from the Coefficients and Intercept Value","text":"<p>General Equation of a Line is: $$ \\ Ax + By + C = 0 \\ $$</p> <p>We an also write it like: $$ \\ y = mx + c \\ $$ where $ \\ m\\ $ is the slope and $ \\ c\\ $ is the y-intercept. or, $$ \\ y = -\\frac{A}{B}x - \\frac{C}{B} \\ $$ where $ -\\frac{A}{B} \\ $ is the slope and $ -\\frac{C}{B} \\ $ is the intercept.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"36f49036-4977-477f-af6e-31ab7662090c\" outputId=\"099f0b85-9990-450c-d641-abfe519201e5\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#define-the-slope-and-intercept","title":"Define the slope and intercept","text":"<p>m = -(coeff_[0]/coeff_[1]) print(\"Slope (m):\", m) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"25ba112c-0e7c-4e80-9f37-79793d275606\" outputId=\"c6f4d5d4-2261-47fd-8c3b-4b4df80ef8ef\"\nc = -(intercept_/coeff_[1])\nprint(\"Y-intercept (c):\", c)\n</code></pre></p> <p>```python id=\"3eb1eecc-759c-44ec-9a84-58e08ba6761d\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#plot-the-line-on-the-scatter-plot","title":"Plot the line on the scatter plot","text":"<p>X_input = np.linspace(start=-1, stop=1, num=100) y_input = (m * X_input) + c <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"db9dc667-c4ac-42fe-95a7-72ef86e49b78\" outputId=\"42fd545b-6adf-4cb5-bcc4-cfdb8e1e457c\"\nax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=50)\nsns.lineplot(x=X_input, y=y_input, ax=ax, c=\"red\")\nplt.xlim((-1.1, 0.85))\nplt.ylim((-3, 3))\nplt.title(\"Perceptron Model\")\nplt.show()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#visualize-the-change-in-model-with-an-animation","title":"Visualize the Change in Model with an Animation","text":"<p>```python id=\"4f089f41-b6cd-46cd-b31f-b42387da089b\" def perceptron(X, y):     m = []     c = []</p> <pre><code>X = np.insert(X, 0, 1, axis=1)\nweights = np.ones(X.shape[1])\nlr = 0.1\n\nfor i in range(1000):\n    j = np.random.randint(0, 100)\n    y_hat = step(np.dot(X[j], weights))\n    weights = weights + lr*(y[j] - y_hat)*X[j]\n\n    m.append(-(weights[1]/weights[2]))\n    c.append(-(weights[0]/weights[2]))\n\nreturn m, c\n</code></pre> <p><code></code>python id=\"42cf285a-470a-450b-b821-028dee7f8edc\" m, c = perceptron(X, y) ```</p> <p>```python id=\"01786bc5-a299-42cd-87b1-9d7b0e942ab3\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/01_Perceptron_Tricks/#plot-the-animation","title":"Plot the animation","text":"<p>%matplotlib ipympl from matplotlib.animation import FuncAnimation import matplotlib.animation as animation <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 589, \"referenced_widgets\": [\"bfe3dd1a89b84fc29091f047cb952d66\", \"41604381b9c24837b2f5cb980a526a63\", \"4d937df3e58e44daaa0281a24f1b14cc\", \"a38174bfcac74ab1b80ea47331ee0fd5\"]} id=\"a823da1b-c3a2-4ccf-a4ca-2ebc7be98d61\" outputId=\"0740536a-bd18-4f32-ef00-24016ca69f4f\"\nfig, ax = plt.subplots(figsize=(9, 5))\n\nx_i = np.arange(-1, 1, 0.1)\ny_i = x_i*m[0] + c[0]\nax.scatter(X[:, 0], X[:, 1], s=50)\nline, = ax.plot(x_i, x_i*m[0]+c[0], \"r-\", linewidth=2)\nplt.ylim(-3, 3)\ndef update(i):\n    label = f\"epoch {i}\"\n    line.set_ydata(x_i*m[i] + c[i])\n    ax.set_xlabel(label)\n\nanim = FuncAnimation(fig, update, repeat=True, frames=1000, interval=1000000)\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/02_Problem_with_Perceptron/","title":"Problem With Perceptron","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/02_Problem_with_Perceptron/#problem-with-perceptron","title":"Problem with Perceptron","text":"<p>The perceptron is a basic type of artificial neural network used in machine learning and deep learning. It is a simple model that can be used for binary classification tasks.</p> <p>Limitations:</p> <ul> <li> <p>Limited to Linear Relationships: Perceptrons can only model linear relationships between input features and the output. They cannot capture non-linear patterns in the data.</p> </li> <li> <p>Inability to Solve Non-Linear Problems: If your data is not linearly separable, a perceptron will not be able to learn and make accurate predictions.</p> </li> <li> <p>Lack of Complexity: Perceptrons cannot represent complex functions or understand intricate data patterns, making them unsuitable for tasks that require higher-level abstractions.</p> </li> <li> <p>Poor for Image and Text Data: In tasks involving images, text, or other high-dimensional data, perceptrons may struggle because these data types often contain non-linear structures.</p> </li> <li> <p>Limited Feature Interaction: Perceptrons treat features independently and do not capture interactions between them, which is a key limitation for many real-world problems.</p> </li> </ul>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/02_Problem_with_Perceptron/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"c4c8a31b-8ab0-4c38-93f2-2d4b66e7cf6c\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import Perceptron</p> <p>import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"b3694b0a-ded1-4b86-b4fe-536f4bb12893\" --&gt;\n## **Make the DataFrames**\n&lt;!-- #endregion --&gt;\n\n```python id=\"775254dc-9605-4607-9eb1-2a467f5f11ab\"\nor_dict = {\"input1\": [1, 1, 0, 0],\n           \"input2\": [1, 0, 1, 0],\n           \"output\": [1, 1, 1, 0]}\n\nand_dict = {\"input1\": [1, 1, 0, 0],\n            \"input2\": [1, 0, 1, 0],\n            \"output\": [1, 0, 0, 0]}\n\nxor_dict = {\"input1\": [1, 1, 0, 0],\n            \"input2\": [1, 0, 1, 0],\n            \"output\": [0, 1, 1, 0]}\n</code></pre></p> <p>```python id=\"58dc2de3-2f78-4e28-aac7-d1431890ffc4\" or_data = pd.DataFrame(data=or_dict) and_data = pd.DataFrame(data=and_dict) xor_data = pd.DataFrame(data=xor_dict) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 192} id=\"d6be8f16-c2ee-4fe3-89e0-fa93b66a8998\" outputId=\"851f19ed-b7f9-4474-a2bc-d3ce3db3967e\"\nprint(\"OR DataFrame:\")\nor_data\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 192} id=\"ba3cb10b-8b36-403b-ab2c-00778c221864\" outputId=\"6143e924-bf69-4757-e93e-d3f38a3956a9\" print(\"AND DataFrame:\") and_data <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 192} id=\"d55b8176-bccc-4047-8af6-86e7e0350410\" outputId=\"ee0fb78a-24c8-434a-f8a2-bfeebb823843\"\nprint(\"XOR DataFrame:\")\nxor_data\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/02_Problem_with_Perceptron/#plot-the-dataframes","title":"Plot the DataFrames","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 410} id=\"fe818bbe-60ba-4eca-a121-0ab9ba1b7223\" outputId=\"77706cf4-81cd-4467-e624-bc94fbd4f55e\" fig, axes = plt.subplots(ncols=3, figsize=(14, 4)) axes = axes.flatten()</p> <p>dataframes = [or_data, and_data, xor_data] plot_titles = [\"OR\", \"AND\", \"XOR\"]</p> <p>for i in range(len(dataframes)):     data = dataframes[i]     sns.scatterplot(x=data[\"input1\"], y=data[\"input2\"], hue=data[\"output\"], ax=axes[i])     axes[i].set_title(plot_titles[i]) <pre><code>&lt;!-- #region id=\"f9c6f4a9-f899-4801-9105-f33b49b5b990\" --&gt;\n## **Train Perceptron Models**\n&lt;!-- #endregion --&gt;\n\n```python id=\"7e557e98-e4bf-4c68-81c0-2e9c80cc9b9a\"\n# Instantiate 3 different classifiers for 3 datasets\nor_clf = Perceptron()\nand_clf = Perceptron()\nxor_clf = Perceptron()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"26357aa8-0000-4e60-8d85-c1a87e1e2e72\" outputId=\"650aed00-e73b-4daa-e089-507c81fb4f23\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/02_Problem_with_Perceptron/#fit-the-data","title":"Fit the data","text":"<p>or_clf.fit(or_data.iloc[:, :2], or_data.iloc[:, -1]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"aab5f37d-6a3d-4bd9-8f3b-cc56cab47eda\" outputId=\"3732077f-f67f-48e2-948a-4c385f04c3f6\"\nand_clf.fit(and_data.iloc[:, :2], and_data.iloc[:, -1])\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"62343b05-9076-483b-93d4-c7bd6712611f\" outputId=\"a670c5c0-32ba-4513-81a0-738184b3c774\" xor_clf.fit(xor_data.iloc[:, :2], xor_data.iloc[:, -1]) <pre><code>&lt;!-- #region id=\"0dd6734f-3634-4b83-8bad-d12c00f1bf1d\" --&gt;\n## **Check the Coefficients and Intercept**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"44d13821-a1a6-4318-8c1c-47ba4e8a2fbc\" outputId=\"1e14ade2-8054-4941-a3e4-8c95f00445fb\"\nprint(\"Coeffients (OR Classifier):\", or_clf.coef_)\nprint(\"Coeffients (AND Classifier):\", and_clf.coef_)\nprint(\"Coeffients (XOR Classifier):\", xor_clf.coef_)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ab33ef7f-09ed-4328-a088-9141defe972a\" outputId=\"216c8c10-3224-4012-a82a-211c8b6ce97c\" print(\"Intercept (OR Classifier):\", or_clf.intercept_) print(\"Intercept (AND Classifier):\", and_clf.intercept_) print(\"Intercept (XOR Classifier):\", xor_clf.intercept_) <pre><code>&lt;!-- #region id=\"fb0df213-9f9f-46e1-b7c5-6c425b7a045e\" --&gt;\n## **Plot the Decision Boundaries of the Models**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 410} id=\"14833442-82e3-416e-a8e7-cd18a1cf4dd3\" outputId=\"47144ad6-fd70-47aa-bdae-849475e4915e\"\nclassifiers = [or_clf, and_clf, xor_clf]\n\nfig, axes = plt.subplots(ncols=3, figsize=(14, 4))\naxes = axes.flatten()\n\nfor index, clf in enumerate(classifiers):\n    data = dataframes[index]\n    slope = clf.coef_.flatten()[0] / clf.coef_.flatten()[1]\n    intercept = clf.intercept_[0] / clf.coef_.flatten()[1]\n\n    X = np.linspace(0, 1, 5)\n    y = -X * slope - intercept\n\n    # sns.lineplot(x, y, ax=axes[index])\n    axes[index].plot(X, y, color=\"red\")\n    sns.scatterplot(x=data[\"input1\"], y=data[\"input2\"], hue=data[\"output\"], ax=axes[index])\n    axes[index].set_title(f\"Decision Boundary of {plot_titles[index]} Classifier\")\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/","title":"Customer Churn Prediction Using Ann","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#customer-churn-prediction-using-ann","title":"Customer Churn Prediction using ANN","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ruF49494xq2S\" outputId=\"eab9d0d9-235b-43c1-e35d-e9f450571ee6\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"XzuuFrixxW5r\" outputId=\"15604840-fda2-4955-d828-ed332106c330\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"whitegrid\")\nprint(tf.__version__)\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#read-the-data-from-kaggle","title":"Read the Data from Kaggle","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Ts3ieKnByn1L\" outputId=\"8fa4b258-9925-4cf0-95e0-fe2e5009dcae\" ! mkdir ~/.kaggle ! cp kaggle.json ~/.kaggle/ <pre><code>```python id=\"3NorY02KztVq\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"8b55be0c-70d4-46e0-c025-ce508b6a32e9\"\n# Download the data from kaggle\n!kaggle datasets download -d rjmanoj/credit-card-customer-churn-prediction\n</code></pre></p> <p>```python id=\"E6hhwo7jz7Pt\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#extract-the-data-from-zipfile","title":"Extract the data from Zipfile","text":"<p>import zipfile zipref = zipfile.ZipFile(\"/content/credit-card-customer-churn-prediction.zip\") zipref.extractall(\"/content\") zipref.close() <pre><code>```python id=\"7ptGHXcWxW5r\" outputId=\"de765b7b-9ad4-4bff-9fce-1c531670ebc2\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 243}\n# Read the data in a pandas dataframe\ndf = pd.read_csv(\"/content/Churn_Modelling.csv\")\nprint(df.shape)\ndf.head()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#data-preprocessing","title":"Data Preprocessing","text":"<p>```python id=\"_Vwg3rcaxW5s\" outputId=\"1e7e7286-5410-4a97-e776-de99271be724\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#check-for-the-missing-values","title":"Check for the missing values","text":"<p>df.isnull().sum() <pre><code>```python id=\"j60XxGiOxW5s\" outputId=\"929e4996-d471-4732-da41-c0f5bb699e45\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check the information of all the columns\ndf.info()\n</code></pre></p> <p>```python id=\"VFGwdg8fxW5s\" outputId=\"dab55928-a14f-42ca-bc71-f14825fe8012\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#drop-the-irrelevant-columns-eg-columns-with-object-datatype","title":"Drop the irrelevant columns (e.g., columns with 'object' datatype)","text":"<p>df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True) df.head() <pre><code>```python id=\"i2ZAFvytxW5s\" outputId=\"ff04f31e-266d-4d01-9f73-7b3ee1713261\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check for the duplicated rows\ndf.duplicated().sum()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#exploratory-data-analysis","title":"Exploratory Data Analysis","text":"<p>```python id=\"RzOpgSrpxW5s\" outputId=\"491125cb-783a-46cd-8fff-a6aba3a99de4\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#check-the-number-of-people-who-exited","title":"Check the number of people who Exited","text":"<p>df[\"Exited\"].value_counts() <pre><code>```python id=\"HPpLJfJgxW5s\" outputId=\"ad2431aa-f2d1-4ec8-8152-8620dc366e9a\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449}\n# Plot the categorical variables\nsns.countplot(x=df[\"Gender\"], hue=df[\"Gender\"])\nplt.show()\n</code></pre></p> <p>```python id=\"ysWpi3rwxW5t\" outputId=\"84503550-7d76-41ce-c2a2-11aaeac7d00e\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} sns.countplot(x=df[\"Geography\"], hue=df[\"Geography\"]) plt.show() <pre><code>&lt;!-- #region id=\"rqsgzJGC17OM\" --&gt;\n## **One Hot Encoding**\n&lt;!-- #endregion --&gt;\n\n```python id=\"Ys9fqlNNxW5t\" outputId=\"655fb330-63d0-422a-8467-ba60da6ab08a\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 226}\n# Apply One Hot Encoding on Categorical columns\ndf = pd.get_dummies(df, columns=[\"Geography\", \"Gender\"], drop_first=True, dtype=int)\ndf.head()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#train-test-split","title":"Train Test Split","text":"<p>```python id=\"LYB5JC_2xW5t\" outputId=\"5afcd739-c7cb-4d82-c8eb-6cd822ba56e9\" colab={\"base_uri\": \"https://localhost:8080/\"} X_train, X_test, y_train, y_test = train_test_split(df.drop(\"Exited\", axis=1),                                                     df[\"Exited\"],                                                     test_size=0.2,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>&lt;!-- #region id=\"uzCZhZX5xW5t\" --&gt;\n## **Feature Scaling**\n&lt;!-- #endregion --&gt;\n\n```python id=\"1u_5il5mxW5t\"\n# Create an object of the StandardScaler class\nscaler = StandardScaler()\n\n# Fit the training data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre></p> <p>```python id=\"Av0iZ_57xW5u\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#convert-the-scaled-array-into-a-pandas-dataframe","title":"Convert the scaled array into a pandas dataframe","text":"<p>X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns) X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 243} id=\"UrwtceQH2sev\" outputId=\"ccc2a84a-2f2b-4bc3-a4c2-402abab80b30\"\nprint(X_train_scaled.shape)\nX_train_scaled.head()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#build-an-artifical-neural-network-architecture","title":"Build an Artifical Neural Network Architecture","text":"<p>```python id=\"mTO4h2mZxW5u\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#instantiate-a-sequential-model","title":"Instantiate a Sequential model","text":"<p>model = Sequential()</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#add-two-dense-layer-with-11-nodes-into-the-sequential-model","title":"Add two Dense layer with 11 nodes into the Sequential Model","text":"<p>model.add(Dense(11, activation=\"relu\", input_dim=11)) model.add(Dense(11, activation=\"relu\"))</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#add-another-layer-for-the-output-with-a-single-node","title":"Add another layer for the output with a single node","text":"<p>model.add(Dense(1, activation=\"sigmoid\")) <pre><code>```python id=\"d0hUj9J7xW5u\" outputId=\"eed36020-c1a3-4f4c-f28b-90bdf73c7079\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Print the model summary\nmodel.summary()\n</code></pre></p> <p>```python id=\"7n6AtBN6xW5u\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#compile-the-model","title":"Compile the model","text":"<p>model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"]) <pre><code>```python id=\"LqTqTr_kxW5u\" outputId=\"7ced0e38-f51a-4d2c-e992-72a1fb6ba8f1\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Fit the training data\nhistory = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2)\n</code></pre></p> <p>```python id=\"_ZtkE8hExW5u\" outputId=\"025e504b-aaac-4101-8f71-9532deb7e8a4\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#check-all-132-weights-of-the-first-layer","title":"Check all 132 weights of the first layer","text":"<p>model.layers[0].get_weights()[0] <pre><code>```python id=\"SjeOCBu4xW5u\" outputId=\"e19616ee-bed9-419e-c806-0dfa76064690\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check all the weights of the second layer\nmodel.layers[1].get_weights()\n</code></pre></p> <p>```python id=\"Ept_j2dixW5v\" outputId=\"ff692cad-f91f-43b4-d304-14d0b5ff2a7c\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#check-all-the-weights-of-the-last-layer","title":"Check all the weights of the last layer","text":"<p>model.layers[2].get_weights() <pre><code>&lt;!-- #region id=\"oV_7VfvVxW5v\" --&gt;\n## **Predict the Test Data**\n&lt;!-- #endregion --&gt;\n\n```python id=\"nMBTwbDrxW5v\" outputId=\"ced56437-0974-486b-9a79-639f2d722262\" colab={\"base_uri\": \"https://localhost:8080/\"}\ny_pred = model.predict(X_test_scaled)\ny_pred\n</code></pre></p> <p>```python id=\"-a9bkGsOxW5v\" outputId=\"6457bec6-dfaa-4f1b-c1fe-abbe03de05d5\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#convert-the-predicted-probability-into-binary-classes","title":"Convert the predicted probability into binary classes","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#assume-a-threshold-value","title":"Assume a threshold value","text":"<p>threshold = 0.5</p> <p>y_pred = np.where(y_pred &gt; threshold, 1, 0) y_pred <pre><code>&lt;!-- #region id=\"FGinDmm0xW5v\" --&gt;\n## **Accuracy Assessment**\n&lt;!-- #endregion --&gt;\n\n```python id=\"4kZaBhNxxW5v\" outputId=\"1a66f230-3c80-4604-d0c7-86909af852b5\" colab={\"base_uri\": \"https://localhost:8080/\"}\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#plot-the-training-and-validation-loss","title":"Plot the Training and Validation Loss","text":"<p>```python id=\"s7Q3eTs-xW51\" outputId=\"fee43287-dad7-423a-e6d5-6ef903b2a510\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/03_Customer_Churn_Prediction_using_ANN/#print-the-key-names-of-the-history-dictionary","title":"Print the key names of the history dictionary","text":"<p>history.history.keys() <pre><code>```python id=\"LkC2pPwaxW52\" outputId=\"e6e392e0-5211-4b86-b7ba-6135f2fa19d2\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449}\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n</code></pre></p> <p><code>python id=\"paCJuouixW52\" outputId=\"897031e1-92e2-490b-9431-c43bc580bccf\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\") plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy\") plt.legend() plt.show()</code></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/","title":"Handwritten Digit Classification","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#handwritten-digit-classification","title":"Handwritten Digit Classification","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"FuYtaLqeMgwr\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200358984, \"user_tz\": -330, \"elapsed\": 414, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from tensorflow import keras from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, Flatten from sklearn.metrics import accuracy_score import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"JnZJsvtAPfPU\" --&gt;\n## **Load MNIST Handwritten Digit Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"iAItE_k_PkWc\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200359995, \"user_tz\": -330, \"elapsed\": 559, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"3873f7bf-a5c6-4148-b8ce-69e43bbdf0c9\"\n# Store the MNIST data\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"nFf8MU9RQEUQ\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200359995, \"user_tz\": -330, \"elapsed\": 6, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"c37a82d5-dce5-4a5d-b84f-50009cd6936f\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#plot-a-digit","title":"Plot a digit","text":"<p>plt.imshow(X_train[1, :, :]) plt.show() <pre><code>&lt;!-- #region id=\"lVl0O7P1RnlC\" --&gt;\n## **Scale the Pixel Values**\n&lt;!-- #endregion --&gt;\n\n```python id=\"bbl55WOTRthr\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200359996, \"user_tz\": -330, \"elapsed\": 6, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}}\n# Normalize the values\nX_train = X_train / 255\nX_test = X_test / 255\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#build-an-ann-model","title":"Build an ANN Model","text":"<p>```python id=\"O607FUmBSm7j\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200359996, \"user_tz\": -330, \"elapsed\": 6, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}}</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#initialize-a-model","title":"Initialize a model","text":"<p>model = Sequential()</p> <p>model.add(Flatten(input_shape=(28, 28)))    # Input Layer model.add(Dense(128, activation=\"relu\"))    # First Hidden Layer model.add(Dense(64, activation=\"relu\"))     # Second Hidden Layer model.add(Dense(32, activation=\"relu\"))     # Third Hidden Layer model.add(Dense(10, activation=\"softmax\"))  # Output Layer <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"JMOOpujdTgX9\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200359996, \"user_tz\": -330, \"elapsed\": 6, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"8d5719a2-c878-44ee-8ec5-71774972da06\"\n# Summarize the model\nmodel.summary()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"w1TkQ7BZUUoX\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200557398, \"user_tz\": -330, \"elapsed\": 143583, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"ab621775-458e-4398-faf6-07d522d7cffe\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#compile-the-model","title":"Compile the model","text":"<p>model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#fit-the-training-data","title":"Fit the training data","text":"<p>history = model.fit(X_train, y_train, epochs=25, validation_split=0.2) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"12aPbcZGVHV6\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200559908, \"user_tz\": -330, \"elapsed\": 1119, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"77cddb43-bc9a-4d28-f342-c00198c4c691\"\n# Predict the test data\ny_prob = model.predict(X_test)\ny_pred = np.argmax(y_prob, axis=1)\ny_pred\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#accuracy-score","title":"Accuracy Score","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DH_1tKxYV4eJ\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200559909, \"user_tz\": -330, \"elapsed\": 9, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"61de407b-e4e0-41c0-d11f-cfc22c284d90\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#calculate-the-accuracy-of-model","title":"Calculate the accuracy of model","text":"<p>accuracy = accuracy_score(y_test, y_pred) accuracy <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 489} id=\"MHgKYnbsaEY9\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200559909, \"user_tz\": -330, \"elapsed\": 8, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"2d0df916-676e-4c45-d357-4b3e496ca85b\"\n# Plot the 'Loss' with respect to 'Epochs'\nsns.lineplot(history.history[\"loss\"])\nplt.title(\"Loss vs Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"mICitVA_aRvP\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200560895, \"user_tz\": -330, \"elapsed\": 992, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"804f0b21-68b7-4132-eaa2-f9f8d3e47ded\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#plot-the-training-loss-with-respect-to-validation-loss","title":"Plot the training loss with respect to validation loss","text":"<p>sns.lineplot(history.history[\"loss\"], label=\"Training Loss\") sns.lineplot(history.history[\"val_loss\"], label=\"Validation Loss\") plt.title(\"Training vs Validation Loss\") plt.show() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"upknp43pcvrm\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200560895, \"user_tz\": -330, \"elapsed\": 4, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"1937772b-cb5b-46c3-85e7-7c205423b5dc\"\n# Plot the training loss with respect to validation loss\nsns.lineplot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nsns.lineplot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.title(\"Training vs Validation Accuracy\")\nplt.show()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#model-prediction","title":"Model Prediction","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 465} id=\"TzRXFeA6dtFX\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701200970829, \"user_tz\": -330, \"elapsed\": 4, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"127e44d9-039f-45d1-db0d-9239995b3c89\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/04_Handwritten_Digit_Classification/#plot-a-random-image-from-the-test-data","title":"Plot a random image from the test data","text":"<p>plt.imshow(X_test[100]) plt.show() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Lelxw5lkd0nI\" executionInfo={\"status\": \"ok\", \"timestamp\": 1701201042127, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"0bbd32c6-0963-44ce-a109-5bd54adc84db\"\n# Predict the image with the model\nnp.argmax(model.predict(X_test[100].reshape(1, 28, 28)), axis=1)\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/","title":"Graduate Admission Prediction Using Ann","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#graduate-admission-prediction-using-ann","title":"Graduate Admission Prediction using ANN","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"PI_1_6_3EemW\" outputId=\"5edec3a2-47c5-4801-f735-887a7e21c4a4\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"9v9DoBH0ECBE\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.metrics import r2_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#read-the-data-from-kaggle","title":"Read the Data from Kaggle","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DSsQvk57EZoF\" outputId=\"4ef531a4-f86a-477f-bca3-b3533f13950b\" ! mkdir ~/.kaggle ! cp kaggle.json ~/.kaggle/ <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"fLQUdsG1EpBg\" outputId=\"e0011507-ca95-46f4-c383-eb9ed7d3571a\"\n# Download the data from kaggle\n!kaggle datasets download -d mohansacharya/graduate-admissions\n</code></pre></p> <p>```python id=\"nWHgjEhxEvVP\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#extract-the-zipfile","title":"Extract the Zipfile","text":"<p>import zipfile zipref = zipfile.ZipFile(\"/content/graduate-admissions.zip\") zipref.extractall(\"/content\") zipref.close() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"pmiO1gM8FI0j\" outputId=\"af8112f0-0067-4a7e-9804-2444e8f6636d\"\n# Read the data in a pandas dataframe\ndf = pd.read_csv(\"/content/Admission_Predict_Ver1.1.csv\")\nprint(df.shape)\ndf.head()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#data-preprocessing","title":"Data Preprocessing","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"nYPq31F8FbZG\" outputId=\"23c5a23b-adff-41a7-c72b-c67738d6451c\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#check-the-information-of-the-columns","title":"Check the information of the columns","text":"<p>df.info() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"60FBZB6IFhwP\" outputId=\"06acc1fa-e249-429b-dc98-bea6f6a03245\"\n# Check for duplicated rows\ndf.duplicated().sum()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"4EYrPBiaFsPW\" outputId=\"5e88ae9d-42ce-473c-e58a-b34b8d748352\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#drop-the-serial-no-column","title":"Drop the 'Serial No.' column","text":"<p>df.drop(\"Serial No.\", axis=1, inplace=True) df.head() <pre><code>&lt;!-- #region id=\"6OZPUVUWGEjp\" --&gt;\n## **Train Test Split**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2UO7LK1UGM3D\" outputId=\"bfc2d699-5125-4a46-ad5f-d4319366e5b8\"\nX_train, X_test, y_train, y_test = train_test_split(df.drop(\"Chance of Admit \", axis=1),\n                                                    df[\"Chance of Admit \"],\n                                                    test_size=0.2,\n                                                    random_state=0)\n\nX_train.shape, X_test.shape\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#feature-scaling","title":"Feature Scaling","text":"<p>```python id=\"_MGTn1nsG4N5\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#apply-minmaxscaler-to-all-the-columns","title":"Apply MinMaxScaler to all the columns","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#instantiate-a-minmaxscaler-object","title":"Instantiate a MinMaxScaler object","text":"<p>minmax_scaler = MinMaxScaler()</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#fit-the-training-data","title":"Fit the training data","text":"<p>minmax_scaler.fit(X_train)</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#transform-the-training-and-testing-data","title":"Transform the training and testing data","text":"<p>X_train_scaled = minmax_scaler.transform(X_train) X_test_scaled = minmax_scaler.transform(X_test) <pre><code>```python id=\"psHZUAOpHcA1\"\n# Convert the scaled arrays into pandas dataframe\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"mFopptKGHtyn\" outputId=\"55c1a69f-a11f-48e5-a1d5-221373aabb74\" X_train_scaled.head() <pre><code>&lt;!-- #region id=\"CQPZK8-pHyD2\" --&gt;\n## **Build an Artifical Neural Network Architecture**\n&lt;!-- #endregion --&gt;\n\n```python id=\"MRRZXN0AIJSB\"\n# Instantiate a Sequential model\nmodel = Sequential()\n\n# Add a Dense layer with 7 nodes into the Sequential Model\nmodel.add(Dense(7, activation=\"relu\", input_dim=7))\nmodel.add(Dense(7, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"linear\"))\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"eWnjFEPOIdF8\" outputId=\"12e07678-c4d7-41d0-cce9-c77083613ec3\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#print-the-model-summary","title":"Print the model summary","text":"<p>model.summary() <pre><code>```python id=\"zFVUbNS_ImD0\"\n# Compile the model\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IkOfSSmCI3cf\" outputId=\"8a333ac0-e242-4dab-9b4b-fa5a683083e6\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#fit-the-training-data_1","title":"Fit the training data","text":"<p>history = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2) <pre><code>&lt;!-- #region id=\"5ChbjIdaJG_v\" --&gt;\n## **Accuracy Assessment**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"G96p2s68JTnq\" outputId=\"da9cf1aa-e56b-4db5-f672-16a9e4f42d2f\"\n# Predict the test data\ny_pred = model.predict(X_test_scaled)\n# y_pred\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IcaO3dnEJZy6\" outputId=\"68acc7e4-4347-4434-fc20-515f223dabbb\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/05_Graduate_Admission_Prediction_using_ANN/#check-the-accurcay-of-the-model","title":"Check the accurcay of the model","text":"<p>print(\"R2 Score:\", r2_score(y_test, y_pred)) <pre><code>&lt;!-- #region id=\"gBRwOiIfKQLg\" --&gt;\n## **Plot the Training and Validation Loss**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Z3BC5pcmKjqT\" outputId=\"50029304-2941-4c6f-b386-e06c2f120ade\"\n# Print the key names of the history dictionary\nhistory.history.keys()\n</code></pre></p> <p><code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} id=\"tQR2-R2CKdYK\" outputId=\"79e22e0c-7037-4557-ba10-717e0a36847e\" plt.plot(history.history[\"loss\"], label=\"Training Loss\") plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() plt.show()</code></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/","title":"Backpropagation On Regression Data","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#backpropagation-on-regression-data","title":"Backpropagation on Regression Data","text":"<p>Backpropagation, short for \"backward propagation of errors,\" is a supervised learning algorithm used to train artificial neural networks, which are a fundamental component of deep learning. The purpose of backpropagation is to minimize the error between the predicted output of the neural network and the actual target output by adjusting the network's weights and biases.</p> <p>Here's a step-by-step explanation of how backpropagation works:</p> <ol> <li>Forward Pass:</li> <li>The input data is passed through the neural network to generate a predicted output.</li> <li> <p>Each neuron in the network performs a weighted sum of its inputs, applies an activation function to the result, and passes the output to the next layer.</p> </li> <li> <p>Calculate Error:</p> </li> <li> <p>The predicted output is compared to the actual target output to calculate the error. Common loss functions, such as mean squared error or cross-entropy, are used for this purpose.</p> </li> <li> <p>Backward Pass (Backpropagation):</p> </li> <li>The error is then propagated backward through the network to update the weights and biases.</li> <li>The partial derivative of the error with respect to each weight and bias is computed using the chain rule of calculus. This indicates how much the error would increase or decrease with a small change in each weight and bias.</li> <li> <p>The weights and biases are adjusted in the opposite direction of the gradient to minimize the error.</p> </li> <li> <p>Update Weights and Biases:</p> </li> <li>The learning rate is applied to control the size of the weight and bias updates.</li> <li> <p>The weights and biases are updated based on the calculated gradients, nudging them towards values that reduce the error.</p> </li> <li> <p>Iterative Process:</p> </li> <li>Steps 1-4 are repeated iteratively for multiple epochs until the neural network converges to a set of weights and biases that minimize the error on the training data.</li> </ol> <p>The backpropagation algorithm allows neural networks to learn from data by adjusting their parameters to improve performance. It's a crucial component of training deep learning models and has been instrumental in the success of various applications, including image and speech recognition, natural language processing, and many others.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"0f697383-a9ed-453d-aa2c-42fb9ca34af8\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"782015cc-269d-4715-b762-9d218870538d\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Dense print(tf.version)</p> <p>import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"13508119-6857-4a91-8dca-4f90ec6d136d\" --&gt;\n## **Create a DataFrame**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 175} id=\"2ed4b0c6-ce7b-4043-bf62-9b1660c85f5e\" outputId=\"3a404d18-dd43-4825-e0d9-062c63283eb4\"\ndf = pd.DataFrame([[8, 8, 4], [7, 9, 5], [6, 10, 6], [5, 12, 17]], columns=[\"cgpa\", \"profile_score\", \"lpa\"])\ndf\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#build-the-algorithm","title":"Build the Algorithm","text":"<p>```python id=\"2db41aab-5201-4eae-ab84-fb8e279b6eed\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#create-a-function-to-initialize-the-weights-and-biases","title":"Create a function to initialize the weights and biases","text":"<p>def initialize_parameters(layer_dims):</p> <pre><code>np.random.seed(3)\nparameters = {}\nL = len(layer_dims)\n\nfor l in range(1, L):\n\n    parameters[\"W\" + str(l)] = np.ones((layer_dims[l-1], layer_dims[l])) * 0.1\n    parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n\nreturn parameters\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7QtwfZySVC1P\" outputId=\"b7a6d307-e740-4ca7-e3c4-24877cd866c7\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#check-the-function","title":"Check the function","text":"<p>initialize_parameters([2, 2, 1]) ```</p> <p>```python id=\"U6Y3QX4xVISo\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#write-a-function-to-linearly-forward-the-data","title":"Write a function to linearly forward the data","text":"<p>def linear_forward(A_prev, W, b):</p> <pre><code>Z = np.dot(W.T, A_prev) + b\n\nreturn Z\n</code></pre> <p><code></code>python id=\"-6s9lhPmW9Yk\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#write-a-function-for-forward-propagation","title":"Write a function for Forward Propagation","text":"<p>def L_layer_forward(X, parameters):      A = X     L = len(parameters) // 2 # Number of layers in the neural network      for l in range(1, L+1):         A_prev = A         Wl = parameters[\"W\" + str(l)]         bl = parameters[\"b\" + str(l)]         # print(\"A\"+str(l-1)+\": \", A_prev)         # print(\"W\"+str(l)+\": \", Wl)         # print(\"b\"+str(l)+\": \", bl)         # print(\"__\"20)          A = linear_forward(A_prev, Wl, bl)         # print(\"A\" + str(l)+\": \", A)         # print(\"\"20)      return A, A_prev ```</p> <p>```python id=\"G44qyZseaTkN\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#extract-the-x-and-y-value-of-a-particular-student","title":"Extract the X and y value of a particular student","text":"<p>X = df[[\"cgpa\", \"profile_score\"]].values[0].reshape(2, 1) # Shape(no of features, no. of training example) y = df[[\"lpa\"]].values[0][0]</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#initialize-the-parameters","title":"Initialize the parameters","text":"<p>parameters = initialize_parameters([2, 2, 1])</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#apply-forward-propagation-on-that-particular-student","title":"Apply forward propagation on that particular student","text":"<p>y_hat, A1 = L_layer_forward(X, parameters) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"kLOHTObMj7Ks\" outputId=\"b8d8161a-c2d6-49a0-bf2a-c1a8ec5ed746\"\ny_hat = y_hat[0][0]\nprint(y_hat)\nprint(A1)\nprint(\"Loss:\", (y-y_hat)**2)\n</code></pre></p> <p>```python id=\"vm6j9tA7dSMT\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#write-a-function-to-update-the-parameters","title":"Write a function to update the parameters","text":"<p>def update_parameters(parameters, y, y_hat, A1, X, lr=0.001):     parameters[\"W2\"][0][0] = parameters[\"W2\"][0][0] + (lr * 2 * (y - y_hat) * A1[0][0])     parameters[\"W2\"][1][0] = parameters[\"W2\"][1][0] + (lr * 2 * (y - y_hat) * A1[1][0])     parameters[\"b2\"][0][0] = parameters[\"b2\"][0][0] + (lr * 2 * (y - y_hat))</p> <pre><code>parameters[\"W1\"][0][0] = parameters[\"W1\"][0][0] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][0][0] * X[0][0])\nparameters[\"W1\"][0][1] = parameters[\"W1\"][0][1] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][0][0] * X[1][0])\nparameters[\"b1\"][0][0] = parameters[\"b1\"][0][0] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][0][0])\n\nparameters[\"W1\"][1][0] = parameters[\"W1\"][1][0] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][1][0] * X[0][0])\nparameters[\"W1\"][1][1] = parameters[\"W1\"][1][1] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][1][0] * X[1][0])\nparameters[\"b1\"][1][0] = parameters[\"b1\"][1][0] + (lr * 2 * (y - y_hat) * parameters[\"W2\"][1][0])\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zlYntRM7jSEo\" outputId=\"3d281964-11fc-48b6-a77f-9623e026cf4c\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#apply-the-function-to-update-the-parameters-value","title":"Apply the function to update the parameters value","text":"<p>update_parameters(parameters, y, y_hat, A1, X, lr=0.001) parameters ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"JXzmD9qUkUU3\" outputId=\"49f11c6e-f813-4d26-f1b2-dc3012c22394\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#epoch-implementation","title":"Epoch implementation","text":"<p>parameters = initialize_parameters([2, 2, 1]) epochs = 5</p> <p>for i in range(epochs):</p> <pre><code>Loss = []\n\nfor j in range(df.shape[0]):\n    X = df[[\"cgpa\", \"profile_score\"]].values[j].reshape(2, 1)\n    y = df[[\"lpa\"]].values[j][0]\n\n    # Parameter initialization\n    y_hat, A1 = L_layer_forward(X, parameters)\n    y_hat = y_hat[0][0]\n\n    update_parameters(parameters, y, y_hat, A1, X)\n\n    Loss.append((y - y_hat)**2)\nprint(f\"Epoch - {i+1}, Loss - {np.array(Loss).mean()}\")\n</code></pre> <p>parameters <pre><code>&lt;!-- #region id=\"Akg-mdDzK1Wf\" --&gt;\n## **Backpropagation in Keras**\n&lt;!-- #endregion --&gt;\n\n```python id=\"SOfw887DK84-\"\n# Build the same model architecture using Keras\nmodel = Sequential()\n\nmodel.add(Dense(2, activation=\"linear\", input_dim=2))\nmodel.add(Dense(1, activation=\"linear\"))\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GIr5rBg3LnFN\" outputId=\"c56b8697-80cc-446b-f398-0ea10fcdfd7f\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#print-the-summary-of-the-model","title":"Print the summary of the model","text":"<p>model.summary() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"TKr48G7YLkpW\" outputId=\"199cca8a-75fb-4f83-eecd-c5d07d869d45\"\n# Cehck the value of random weights initialized by Keras\nmodel.get_weights()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IGG6ARBhL8We\" outputId=\"47f1e72f-3024-4ca4-97e9-4c290be2c2cc\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#set-the-initial-weights-to-01-and-biases-to-0","title":"Set the initial weights to 0.1 and biases to 0","text":"<p>new_weights = [np.array([[0.1 , 0.1 ],                          [0.1 , 0.1]], dtype=np.float32),                np.array([0., 0.], dtype=np.float32),                np.array([[ 0.1],                          [0.1 ]], dtype=np.float32),                np.array([0.], dtype=np.float32)]</p> <p>model.set_weights(new_weights) model.get_weights() <pre><code>```python id=\"YwWVmyerNThv\"\n# Define the optimizer and compile the model\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wpFUnFk1R_rF\" outputId=\"ee7a70d0-4bc7-4a50-d981-41f8be74ba82\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/01_ANN/06_Backpropagation_on_Regression_Data/#fit-the-training-data","title":"Fit the training data","text":"<p>history = model.fit(df.iloc[:, 0:-1].values, df[\"lpa\"].values, epochs=1000, verbose=1, batch_size=1) ```</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/","title":"Introduction To Cnn","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#introduction-to-convolutional-neural-network-cnn","title":"Introduction to Convolutional Neural Network (CNN)","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#what-is-cnn","title":"What is CNN?","text":"<p>Convolutional neural network, also known as convnet, or CNNs, are a special kind of neural network for processing data that has a known grid-like topology like time series data(1D) or images(2D).</p> <p>The key feature of CNNs is the use of convolutional layers, which apply convolution operations to input data. Convolution involves sliding a small filter (also called a kernel) over the input data, performing element-wise multiplication and summing the results to produce a feature map. CNNs also typically include pooling layers to downsample the spatial dimensions of the data, reducing computational complexity and making the network more robust to variations in input. Additionally, fully connected layers are often used at the end of the network to make final predictions based on the extracted features.</p> <p>CNNs have been highly successful in various computer vision tasks, and their architecture is designed to mimic the visual processing that occurs in the human brain. They have played a crucial role in advancing the field of deep learning and have been widely used in applications like image and video recognition, medical image analysis, and more.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#history-of-cnn","title":"History of CNN","text":"<p>Convolutional Neural Networks (CNNs) have evolved over several decades, with key contributions from researchers in the field of computer vision and artificial intelligence. The development of CNNs can be traced back to the 1960s, but they gained significant prominence in the 2010s. Here's a brief overview of their evolution:</p> <ol> <li> <p>Hubel and Wiesel's Discoveries (1962): In the early 1960s, David Hubel and Torsten Wiesel conducted groundbreaking experiments on the visual cortex of cats. They discovered that neurons in the visual cortex have receptive fields that respond to specific features in the visual stimulus, such as edges and corners. This laid the foundation for understanding hierarchical visual processing. </p> </li> <li> <p>Neocognitron (1980s): Kunihiko Fukushima proposed the Neocognitron, an artificial neural network architecture inspired by the findings of Hubel and Wiesel. Neocognitron was designed for handwritten character recognition and featured alternating layers of convolutional and subsampling (pooling) layers. It introduced the concept of local receptive fields and shared weights, which are fundamental to modern CNNs.</p> </li> <li> <p>LeNet-5 (1998): Yann LeCun, along with collaborators, developed LeNet-5, a convolutional neural network for handwritten digit recognition. It consisted of convolutional layers, pooling layers, and fully connected layers. LeNet-5 demonstrated the effectiveness of CNNs in image classification tasks and laid the groundwork for future developments.</p> </li> <li> <p>AlexNet (2012): AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, marked a significant breakthrough in the use of CNNs. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, showcasing the power of deep learning for image classification. AlexNet's success spurred increased interest and research in deep neural networks.</p> </li> <li> <p>VGGNet (2014): The Visual Geometry Group (VGG) introduced VGGNet, which demonstrated that increasing the depth of CNNs could improve performance. VGGNet had a simple and uniform architecture, with small convolutional filters stacked on top of each other. This idea of deeper networks contributed to subsequent developments.</p> </li> <li> <p>GoogLeNet (Inception) (2014): Google's Inception model, also known as GoogLeNet, introduced the concept of inception modules, which allowed the network to capture features at multiple scales. This architecture aimed to address the trade-off between depth and computational efficiency.</p> </li> <li> <p>ResNet (2015): Microsoft Research introduced Residual Networks (ResNet), which employed residual learning to handle the challenges of training very deep networks. The use of residual connections allowed the network to skip certain layers during training, facilitating the training of extremely deep models.</p> </li> </ol> <p>These milestones and innovations have collectively shaped the development of CNNs, making them a cornerstone in computer vision and deep learning research. Today, CNNs are widely used in various applications, ranging from image recognition and object detection to medical image analysis and autonomous vehicles.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#why-not-use-ann","title":"Why not use ANN?","text":"<p>Artificial Neural Networks (ANNs), including traditional feedforward neural networks, are a powerful class of models used in various machine learning tasks. However, there are specific reasons why Convolutional Neural Networks (CNNs) are preferred over traditional ANNs for certain types of tasks, particularly in the field of computer vision. Here are some key reasons:</p> <ol> <li>High Computational Cost:</li> <li>Traditional ANNs: Fully connected layers in traditional artificial neural networks (ANNs) have a large number of parameters, leading to a high computational cost. As the network scales, the number of parameters grows significantly, making training and inference computationally intensive.</li> <li> <p>CNNs: Convolutional Neural Networks (CNNs) reduce the computational cost by using shared weights and local connectivity in convolutional layers. The use of pooling layers also helps downsample the data, decreasing the overall computational load.</p> </li> <li> <p>Overfitting:</p> </li> <li>Traditional ANNs: ANNs, especially with a large number of parameters, are prone to overfitting. Overfitting occurs when the model learns the training data too well, including noise and outliers, and fails to generalize to unseen data.</li> <li> <p>CNNs: CNNs are designed to mitigate overfitting through techniques like parameter sharing, which reduces the number of parameters, and the use of dropout and regularization. Additionally, pooling layers contribute to a form of implicit regularization by focusing on the most salient features.</p> </li> <li> <p>Loss of Spatial Arrangement of Pixels:</p> </li> <li>Traditional ANNs: When using traditional ANNs for tasks like image processing, the spatial arrangement of pixels is often lost. Flattening the input image into a vector discards the spatial relationships between pixels, making it challenging to capture local patterns.</li> <li>CNNs: CNNs excel in preserving spatial information through convolutional layers that capture local patterns and hierarchies. This is essential for tasks like image recognition, where the spatial arrangement of features is crucial for understanding the content of an image.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#cnn-intuition","title":"CNN Intuition","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/00_Introduction_to_CNN/#applications-of-cnn","title":"Applications of CNN","text":"<p>Convolutional Neural Networks (CNNs) have found widespread applications across various domains due to their ability to effectively handle grid-like data, particularly in image processing. Here is a brief overview of some key applications of CNNs:</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/01_CNN_Vs_Visual_Cortex/","title":"Cnn Vs Visual Cortex","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/01_CNN_Vs_Visual_Cortex/#cnn-vs-visual-cortex","title":"CNN Vs Visual Cortex","text":"<p>Convolutional Neural Networks (CNNs) draw inspiration from the human visual system, particularly the visual cortex. While CNNs attempt to mimic certain aspects of visual processing in the brain, it's important to note that they are simplified models and do not replicate the full complexity of the biological visual system.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/01_CNN_Vs_Visual_Cortex/#human-visual-cortex","title":"Human Visual Cortex","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/01_CNN_Vs_Visual_Cortex/#hubel-and-wiesel-cat-experiment","title":"Hubel and Wiesel Cat Experiment","text":"<p>David Hubel and Torsten Wiesel conducted groundbreaking experiments in the early 1960s that significantly advanced our understanding of the visual system. Their work, which focused on the cat's visual cortex, provided key insights into how neurons respond to visual stimuli and laid the foundation for our understanding of feature detection and processing in the brain.</p> <p></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/","title":"Convolution Operation","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#convolution-operation","title":"Convolution Operation","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#introduction","title":"Introduction","text":"<p>Convolutional Neural Networks (CNNs) consist of multiple layers, each designed to perform specific operations to extract hierarchical features from input data, especially images. The key layers in a CNN include:</p> <ol> <li>Input Layer:</li> <li>Represents the raw data, often an image with pixel values.</li> <li> <p>The size of the input layer corresponds to the dimensions of the input data, such as the height, width, and number of color channels.</p> </li> <li> <p>Convolutional Layer:</p> </li> <li>Applies convolutional operations to the input data using filters or kernels.</li> <li>Each filter captures specific features or patterns within local receptive fields.</li> <li> <p>Activations from this layer form feature maps that represent learned features.</p> </li> <li> <p>Pooling (Subsampling) Layer:</p> </li> <li>Performs downsampling to reduce spatial dimensions and computational complexity.</li> <li> <p>Common pooling operations include max pooling or average pooling, which retain the most salient features within local regions.</p> </li> <li> <p>Fully Connected (Dense) Layer:</p> </li> <li>Neurons in this layer are connected to all neurons from the previous layer.</li> <li>Transforms high-level features into predictions or class scores.</li> <li> <p>Commonly used in the final layers of the network.</p> </li> <li> <p>Output Layer:</p> </li> <li>Produces the final output of the network.</li> <li>The number of neurons in this layer corresponds to the number of classes in a classification task.</li> <li>The activation function is chosen based on the nature of the task (e.g., softmax for classification).</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#basics-of-image","title":"Basics of Image","text":"<p>In CNNs, images are fundamental inputs that undergo processing through various layers to extract hierarchical features. The nature of the image, whether grayscale or RGB (Red, Green, Blue), affects the input dimensions and the way the network processes the information.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#grayscale-images","title":"Grayscale Images:","text":"<ol> <li>Representation:</li> <li>Grayscale images are represented using a single channel, where each pixel has a single intensity value (typically ranging from 0 to 255).</li> <li> <p>In CNNs, a grayscale image is usually treated as a 2D matrix, where each element represents the intensity of a pixel.</p> </li> <li> <p>Input Dimensions:</p> </li> <li>For a grayscale image of size H x W, the input tensor to the CNN would have dimensions (H, W, 1).</li> <li>The third dimension (1) signifies the single channel.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#rgb-images","title":"RGB Images:","text":"<ol> <li>Representation:</li> <li>RGB images are represented using three channels (Red, Green, Blue), where each channel represents the intensity of a specific color.</li> <li> <p>Each pixel has three intensity values, forming a 3D matrix.</p> </li> <li> <p>Input Dimensions:</p> </li> <li>For an RGB image of size H x W, the input tensor to the CNN would have dimensions (H, W, 3).</li> <li>The three channels correspond to Red, Green, and Blue.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#edge-detection-convolutional-operation","title":"Edge Detection (Convolutional Operation)","text":"<p>Edge detection is a fundamental operation in image processing and computer vision that aims to identify boundaries within an image. One common approach to edge detection involves using convolutional filters, such as the Sobel filters, to highlight changes in intensity that correspond to edges. The Sobel filters are particularly effective for detecting edges in both the vertical and horizontal directions.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#example-of-a-vertical-edge-detection","title":"Example of a Vertical Edge Detection","text":"<p>Demo: Click on this link.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#calculation-of-feature-map-size","title":"Calculation of Feature Map Size","text":"<p>If the stride is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after a convolution operation simplifies to:</p> \\[ \\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 \\] <p>Here, the terms are:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> </ul> <p>Example: $\\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 $</p> <p>Suppose you have a grayscale image with an input size of 28x28 and a filter size of 3x3:</p> <p>$\\text{Output Size} = 28 - 3 + 1 = 26 $</p> <p>So, in this case, the feature map size after the convolution operation would be 26x26. If you are not using any stride $\\text{Stride} = 1 $, this simplified formula is applicable.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/02_Convolution_Operation/#working-with-rgb-images","title":"Working with RGB Images","text":"<p>Convolution Operation on RGB Images:</p> <p>Applying Multiple Filters on same RGB Image: </p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/","title":"Padding And Strides In Cnn","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#padding-and-strides-in-cnn","title":"Padding and Strides in CNN","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#what-is-padding","title":"What is Padding?","text":"<p>Padding is a technique used to preserve spatial information during the convolutional and pooling operations. It involves adding extra pixels (usually with a value of zero) around the borders of an input feature map or image.</p> <p>The main purposes of padding are:</p> <ol> <li>Preserving Spatial Information:</li> <li>Without padding, the spatial dimensions of the feature map decrease with each convolutional layer, potentially leading to a significant reduction in information at the edges.</li> <li> <p>Padding helps maintain the spatial size, ensuring that information near the borders is given proper consideration.</p> </li> <li> <p>Mitigating the Loss of Information:</p> </li> <li>In the absence of padding, the pixels at the edges of the feature map are involved in fewer convolution operations, leading to a loss of information.</li> <li> <p>Padding ensures that each pixel in the input has the opportunity to be the center of the receptive field for convolutional filters.</p> </li> <li> <p>Handling Stride and Filter Size:</p> </li> <li>Padding becomes especially useful when using larger filter sizes or strides greater than 1. Without padding, the spatial size reduction becomes more pronounced.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#types-of-padding-in-keras","title":"Types of Padding in Keras","text":"<p>In Keras, a popular deep learning library, you can specify different types of padding for convolutional layers. The main types of padding available in Keras are:</p> <ol> <li>Valid Padding (No Padding):</li> <li>This is the default setting in Keras.</li> <li>No padding is added to the input feature map.</li> <li>The convolution operation is applied only to the valid part of the input.</li> </ol> <ol> <li>Same Padding (Zero Padding):</li> <li>Padding is added to the input feature map to ensure that the spatial dimensions of the output feature map remain the same as the input.</li> <li>The padding is distributed evenly on all sides.</li> <li>Useful for preserving spatial information and handling larger filter sizes.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#calculation-of-feature-map-size","title":"Calculation of Feature Map Size","text":"<p>If the stride \\((\\text{Stride})\\) is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after padding simplifies further. For \"same\" padding, the formula becomes:</p> \\[\\text{Output Size} = {{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size} + 1}}\\] <p>Here, the terms are the same as in the previous formula:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation with padding and stride set to 1.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> <li>\\(\\text{Padding}\\): The number of zero-padding pixels added to the input on each side.</li> </ul>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#implementation-of-padding-in-keras","title":"Implementation of Padding in Keras","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"FNfPCR7sZzlY\" outputId=\"7f434a71-6d4e-4769-9e59-0c783c61adb9\" import tensorflow from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, Dense, Flatten from keras.datasets import mnist print(tensorflow.version) <pre><code>&lt;!-- #region id=\"kiB8qW3zbJ4A\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python id=\"oiGjkWo3bOrI\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#build-the-model-architecture-with-valid-padding","title":"Build the Model Architecture with <code>valid</code> Padding","text":"<p>```python id=\"W5XaAUCmbvw8\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#build-the-model-architecture-with-valid-padding-in-the-convolution-layers","title":"Build the model architecture with 'valid' padding in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"H9B2_Kr_h4Ls\" outputId=\"b023cf54-25a4-4ab0-8357-72ecf777166b\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#build-the-model-architecture-with-samezero-padding","title":"Build the Model Architecture with <code>same/zero</code> Padding","text":"<p>```python id=\"NhUEadZajEXr\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#build-the-model-architecture-with-zero-padding-in-the-convolution-layers","title":"Build the model architecture with 'zero' padding in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GR1cxM2GkKaL\" outputId=\"d22b576a-47eb-433f-8881-4890795b9cee\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#what-is-strides","title":"What is Strides?","text":"<p>In the context of CNNs, \"strides\" refer to the step size or the number of pixels the convolutional filter (kernel) moves at each step during the convolution operation. The stride parameter determines the distance between consecutive applications of the filter to the input, influencing the spatial dimensions of the output feature map. <code>Strided convolution</code> involves using a stride value greater than 1, meaning that the convolutional filter moves more than one pixel at a time while scanning the input.</p> <p>Key points about strides:</p> <ol> <li>Stride Value:</li> <li>Strides are usually set as positive integers.</li> <li>Common values are 1, indicating that the filter moves one pixel at a time, and 2, indicating that the filter moves two pixels at a time.</li> <li> <p>Larger stride values result in a more aggressive reduction of the spatial dimensions.</p> </li> <li> <p>Effect on Output Size:</p> </li> <li>Increasing the stride reduces the spatial dimensions of the output feature map.</li> <li> <p>Smaller strides lead to larger feature maps but may increase computational complexity.</p> </li> <li> <p>Strides and Subsampling:</p> </li> <li>Strides can be used to achieve subsampling or down-sampling by skipping pixels during the convolution.</li> <li>Subsampling can be beneficial for reducing the computational load and focusing on important features.</li> </ol> <p>Example of a Convolution Operation when the Stride is set to 2: </p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#calculation-of-feature-map-size_1","title":"Calculation of Feature Map Size","text":"<p>The effect of strides on the output size can be described by the following formula:</p> \\[\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{{\\text{Stride}}} + 1\\] <p>Here are the terms in the formula:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> <li>\\(\\text{Padding}\\): The number of zero-padding pixels added to the input on each side.</li> <li>\\(\\text{Stride}\\): The step size or the number of pixels the filter moves at each step during convolution.</li> </ul>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#why-strides-are-required","title":"Why Strides are required?","text":"<p>Strides in convolutional neural networks (CNNs) are required for several reasons:</p> <ol> <li>Downsampling and Efficiency:</li> <li>Strides enable downsampling of the input, reducing the spatial dimensions of the feature maps.</li> <li> <p>Downsampling is crucial for efficiency, reducing computational complexity and memory requirements.</p> </li> <li> <p>Feature Extraction:</p> </li> <li>Larger strides skip pixels during convolution, allowing the network to focus on more significant features and patterns.</li> <li> <p>This can be beneficial for capturing high-level features and reducing the spatial size of the feature maps.</p> </li> <li> <p>Control Over Model Complexity:</p> </li> <li>Strides provide a way to control the complexity of the model by influencing the spatial dimensions of the feature maps.</li> <li>They allow practitioners to balance between capturing fine-grained details and computational efficiency.</li> </ol> <p>In summary, strides are essential for controlling the trade-off between computational efficiency and feature representation in CNNs. They allow practitioners to tailor the network architecture to the specific requirements of the task at hand, ensuring effective feature extraction and model efficiency.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#implementation-of-strides-in-keras","title":"Implementation of Strides in Keras","text":"<p>```python id=\"-S7iEUyupiYd\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/03_Padding_and_Strides_in_CNN/#build-the-model-architecture-with-a-2-2-strides-in-the-convolution-layers","title":"Build the model architecture with a (2, 2) strides in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DIsVx4g5qWvD\" outputId=\"58a35826-4714-432a-9b20-d3c0dc219be0\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/","title":"Pooling Layer In Cnn","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#pooling-layer-in-cnn","title":"Pooling Layer in CNN","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#the-problem-with-convolution","title":"The Problem with Convolution","text":"<p>The convolution operation, while highly effective in capturing spatial hierarchies and features in data, comes with certain challenges. Two notable challenges associated with convolution are memory issues and translation variance.</p> <p>The convolution operation, while highly effective in capturing spatial hierarchies and features in data, comes with certain challenges. Two notable challenges associated with convolution are memory issues and translation variance.</p> <ol> <li>Memory Issues:</li> <li> <p>High Computational Cost:      Convolutional operations, especially with large filter sizes and deep networks, can be computationally expensive. The number of parameters and computations increases, leading to higher memory and processing requirements.</p> <p></p> </li> <li> <p>Translation Variance:</p> </li> <li>Positional Sensitivity:      Traditional convolution is sensitive to the absolute position of features in the input. This means that the network may not recognize an object if it appears in a different position within the input.</li> <li> <p>Lack of Invariance:      Convolutional networks may lack translation invariance, meaning that slight shifts or translations in the input can significantly affect the network's ability to recognize patterns.</p> <p></p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#what-is-pooling","title":"What is Pooling?","text":"<p>Pooling, in CNNs, is a down-sampling operation that reduces the spatial dimensions of the input feature maps while retaining essential information. Pooling is typically applied after convolutional layers to progressively reduce the spatial size, decrease the computational load, and capture the most important features.</p> <p>Two common types of pooling operations are used in CNNs:</p> <ol> <li>Max Pooling:</li> <li> <p>Max pooling involves selecting the maximum value from a group of neighboring pixels in the input feature map.</p> </li> <li> <p>Average Pooling:</p> </li> <li>Average pooling calculates the average value of a group of neighboring pixels in the input feature map.</li> </ol> <p>Key points about pooling:</p> <ul> <li> <p>Spatial Reduction:   Pooling reduces the spatial dimensions of the feature map, effectively downsampling the information.</p> </li> <li> <p>Translation Invariance:   Pooling contributes to translation invariance by selecting the most relevant features and reducing sensitivity to small shifts or translations in the input. </p> </li> </ul> <p>Demo: Click on this link.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#pooling-on-volumes","title":"Pooling on Volumes","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#implementation-of-pooling-in-keras","title":"Implementation of Pooling in Keras","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"N6P9czPHclyl\" outputId=\"b2afcc81-8b44-40ce-e85b-eb8ce3ae3e6b\" import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from keras.datasets import mnist print(tf.version) <pre><code>&lt;!-- #region id=\"yZBFyQQAdB_P\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"yNSrQsoNdGeZ\" outputId=\"64309f2a-c4b1-4a1a-aa29-67656b189757\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#build-the-model-architecture-with-maxpooling-layer","title":"Build the Model Architecture with <code>MaxPooling</code> Layer","text":"<p>```python id=\"Nn_7UddRdjHs\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#build-the-model-architecture-with-maxpooling-layer_1","title":"Build the model architecture with 'MaxPooling' layer","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding=\"same\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")) model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding=\"same\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9H3oo9X4e0Qo\" outputId=\"712dfcb4-af55-4cd6-d116-92b7e17bed51\"\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#advantages-of-pooling","title":"Advantages of Pooling","text":"<p>Pooling operations offer several key advantages that contribute to the network's effectiveness in image processing tasks:</p> <ol> <li>Spatial Dimension Reduction:</li> <li> <p>Pooling reduces the spatial dimensions of input feature maps, effectively downsampling the information. This spatial reduction is crucial for managing computational load and improving efficiency in subsequent layers.   </p> </li> <li> <p>Translation Invariance:</p> </li> <li> <p>Pooling introduces a level of translation invariance by selecting the most relevant features. This means the network becomes less sensitive to small shifts or translations in the input, enhancing its ability to generalize across different positions of objects.   </p> </li> <li> <p>Enhanced Features:</p> </li> <li> <p>Pooling operations contribute to enhancing distinctive features within the input data. By selecting the most significant values from local regions, pooling helps the network focus on crucial patterns and characteristics, facilitating better feature extraction.   </p> </li> <li> <p>No Need for Training:</p> <ul> <li>Pooling is a parameter-free operation, requiring no additional training or learnable weights. This simplicity makes it easy to implement and incorporate into CNN architectures without introducing extra parameters that would need to be adjusted during the training process. The lack of trainable parameters in pooling layers adds to the efficiency and simplicity of the overall network design.</li> </ul> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#types-of-pooling","title":"Types of Pooling","text":"<p>There are several types of pooling operations used in Convolutional Neural Networks (CNNs), each serving different purposes in feature extraction and spatial dimension reduction. Three common types of pooling are MaxPooling, AveragePooling, and Global Pooling:</p> <ol> <li>Max Pooling:</li> <li>Operation: Selects the maximum value from a group of neighboring pixels in the input feature map.</li> <li>Purpose: Emphasizes the most prominent features, contributing to translation invariance and retaining salient information.</li> <li> <p>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nMax Pooling (2x2):\n[[5]]\n</code></pre></p> </li> <li> <p>Average Pooling:</p> </li> <li>Operation: Calculates the average value from a group of neighboring pixels in the input feature map.</li> <li>Purpose: Smooths the representation, reduces sensitivity to outliers, and provides a form of translation invariance.</li> <li> <p>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nAverage Pooling (2x2):\n[[3.5]]\n</code></pre></p> </li> <li> <p>Global Pooling (Global Average Pooling or Global Max Pooling):</p> </li> <li>Operation: Computes a single value (global average or global maximum) for each channel across the entire feature map.</li> <li>Purpose: Aggregates information globally, reducing the spatial dimensions to a single value per channel. Commonly used as a transition to fully connected layers in classification tasks.</li> <li>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nGlobal Average Pooling:\n[[5]]\n</code></pre></li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/04_Pooling_Layer_in_CNN/#disadvantages-of-pooling","title":"Disadvantages of Pooling","text":"<p>While pooling operations offer several advantages in Convolutional Neural Networks (CNNs), they also come with certain disadvantages. Here are some drawbacks associated with pooling:</p> <ol> <li>Loss of Spatial Information:</li> <li> <p>Pooling involves down-sampling, leading to a reduction in spatial dimensions. This reduction can result in the loss of fine-grained spatial information, making it challenging to reconstruct the exact spatial arrangement of features.</p> </li> <li> <p>Reduced Sensitivity to Small-Scale Patterns:</p> </li> <li> <p>Max pooling, in particular, focuses on the most significant features within a local region. While this is beneficial for translation invariance, it can lead to reduced sensitivity to small-scale patterns, especially if the maximum values dominate the features.</p> </li> <li> <p>Not Suitable for Image Segmentation:</p> </li> <li>In tasks such as image segmentation, where preserving spatial information is crucial, pooling may not be as suitable. Downsampling through pooling can result in a loss of fine-grained details, making it challenging for the network to precisely delineate object boundaries in segmented images.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/","title":"Cnn Architecture And Lenet 5","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#cnn-architecture-lenet-5","title":"CNN Architecture &amp; LeNet-5","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#cnn-architecture","title":"CNN Architecture","text":"<p>Convolutional Neural Networks (CNNs) have been highly successful in various computer vision tasks, such as image classification, object detection, and image segmentation. While CNN architectures can vary based on specific requirements and tasks, a typical CNN architecture consists of several key components. Here is a generic overview of a CNN architecture:</p> <ol> <li>Input Layer:</li> <li> <p>Accepts the input data, usually in the form of images. The size of the input layer corresponds to the dimensions of the input images.</p> </li> <li> <p>Convolutional Layers:</p> </li> <li>Convolutional layers are the core building blocks of CNNs. They use filters (kernels) to convolve over input feature maps, extracting hierarchical features.</li> <li> <p>Activation functions (e.g., ReLU) introduce non-linearity.</p> </li> <li> <p>Pooling Layers:</p> </li> <li>Pooling layers follow convolutional layers and downsample the spatial dimensions of feature maps. Common pooling types include Max Pooling and Average Pooling.</li> <li> <p>Pooling contributes to translation invariance and reduces computational complexity.</p> </li> <li> <p>Flatten Layer:</p> </li> <li> <p>Flatten layers transition from convolutional layers to fully connected layers. They reshape the 3D output of the convolutional/pooling layers into a 1D vector.</p> </li> <li> <p>Fully Connected Layers:</p> </li> <li> <p>Fully connected layers connect every neuron to every neuron in the previous and next layers. These layers capture global patterns and relationships in the data.</p> </li> <li> <p>Output Layer:</p> </li> <li>The output layer produces the final predictions. The number of neurons in this layer corresponds to the number of classes in a classification task.</li> <li>Activation functions vary based on the task (e.g., softmax for classification).</li> </ol> <p>This architecture is a sequential stack of layers and is commonly implemented using deep learning frameworks such as TensorFlow or PyTorch. Specific CNN architectures like LeNet-5, AlexNet, VGGNet, GoogLeNet (Inception), ResNet, and others have been influential in shaping the field. The choice of architecture depends on factors like the complexity of the task, available data, and computational resources. Researchers often modify and adapt existing architectures or design custom architectures to address specific challenges.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#lenet-5-architecture","title":"LeNet-5 Architecture","text":"<p>LeNet-5 is a pioneering convolutional neural network architecture designed for handwritten digit recognition. It was introduced by Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner in 1998. LeNet-5 played a crucial role in demonstrating the effectiveness of deep learning in computer vision tasks. </p> <p></p> <p>Below is an overview of the LeNet-5 architecture:</p> <ol> <li>Input Layer:</li> <li> <p>Accepts grayscale images of size 32x32 pixels.</p> </li> <li> <p>Convolutional Layer (C1):</p> </li> <li>Convolution with 6 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 28x28x6.</p> </li> <li> <p>Subsampling Layer (S2):</p> </li> <li>Average pooling over non-overlapping 2x2 regions.</li> <li> <p>Output feature maps: 14x14x6.</p> </li> <li> <p>Convolutional Layer (C3):</p> </li> <li>Convolution with 16 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 10x10x16.</p> </li> <li> <p>Subsampling Layer (S4):</p> </li> <li>Average pooling over non-overlapping 2x2 regions.</li> <li> <p>Output feature maps: 5x5x16.</p> </li> <li> <p>Convolutional Layer (C5):</p> </li> <li>Convolution with 120 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 1x1x120.</p> </li> <li> <p>Fully Connected Layer (F6):</p> </li> <li>Fully connected layer with 84 neurons.</li> <li> <p>Activation function: Sigmoid.</p> </li> <li> <p>Output Layer:</p> </li> <li>Fully connected layer with 10 neurons (corresponding to the 10 digits in digit recognition tasks).</li> <li>Activation function: Softmax.</li> </ol> <p>The architecture incorporates a series of convolutional and subsampling layers, followed by fully connected layers. Sigmoid activation functions were used in the original design, and average pooling was employed in the subsampling layers. LeNet-5 demonstrated the effectiveness of deep learning in pattern recognition tasks and laid the foundation for subsequent developments in convolutional neural networks.</p> <p>It's important to note that while LeNet-5 was groundbreaking at the time, more recent CNN architectures, such as those used in image classification tasks today, often involve deeper networks, different activation functions (e.g., ReLU), and other architectural innovations.</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#implementation-of-lenet-5-architecture-in-keras","title":"Implementation of LeNet-5 Architecture in Keras","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"HZj4KksAwAfa\" outputId=\"87794a1d-73b6-4e22-bac4-1b270768aea8\" import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense from keras.datasets import mnist print(tf.version) <pre><code>&lt;!-- #region id=\"bTl6KfiRwdI2\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zWQgFT_2wjU1\" outputId=\"af1b51da-8bc0-48ad-f3dd-e371e280ae97\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#build-the-lenet-5-architecture","title":"Build the <code>LeNet-5</code> Architecture","text":"<p>```python id=\"0kOoXdYuw2mw\"</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/05_CNN_Architecture_and_LeNet_5/#build-the-lenet-5-architecture_1","title":"Build the LeNet-5 architecture","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(6, kernel_size=(5, 5), padding=\"valid\", activation=\"tanh\", input_shape=(32, 32, 1))) model.add(AveragePooling2D((2, 2), strides=2, padding=\"valid\"))</p> <p>model.add(Conv2D(16, kernel_size=(5, 5), padding=\"valid\", activation=\"tanh\")) model.add(AveragePooling2D((2, 2), strides=2, padding=\"valid\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(120, activation=\"tanh\")) model.add(Dense(84, activation=\"tanh\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"dWT-qUF5yGwT\" outputId=\"e0d8a006-e373-443a-e412-3883e8cc694c\"\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/06_CNN_Vs_ANN/","title":"Cnn Vs Ann","text":""},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/06_CNN_Vs_ANN/#cnn-vs-ann","title":"CNN Vs ANN","text":"<p>Convolutional Neural Networks (CNNs) and Artificial Neural Networks (ANNs) are both types of neural networks, but they differ in their architectures, designs, and applications. Here's a brief comparison between CNNs and ANNs:</p> <p>Working Principle of ANN: </p> <p>Working Principle of CNN </p>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/06_CNN_Vs_ANN/#similarities-between-ann-and-cnn","title":"Similarities between ANN and CNN","text":"<p>The working principles of Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs) share several fundamental concepts, despite their architectural differences. Here are the key similarities in their working principles:</p> <ol> <li>Neurons and Activation Functions:</li> <li>Both ANNs and CNNs are composed of interconnected neurons.</li> <li>Neurons in both architectures process weighted inputs, apply activation functions, and produce output signals.</li> <li> <p>Activation functions introduce non-linearity, allowing the network to learn complex relationships in the data.</p> </li> <li> <p>Forward Propagation:</p> </li> <li>Both architectures employ forward propagation to process input data and generate predictions.</li> <li> <p>During forward propagation, input signals are passed through the network's layers, and weighted sums are computed at each neuron. The output is then obtained through the activation function.</p> </li> <li> <p>Loss Function and Training:</p> </li> <li>Both ANNs and CNNs are trained using a supervised learning approach.</li> <li> <p>They use a loss function to measure the difference between predicted and actual outputs. The goal is to minimize this loss during training.</p> </li> <li> <p>Backpropagation:</p> </li> <li>Both architectures use backpropagation as a learning algorithm.</li> <li> <p>Backpropagation involves calculating gradients of the loss with respect to the network's parameters and adjusting these parameters to minimize the loss.</p> </li> <li> <p>Optimization Algorithms:</p> </li> <li>Both ANNs and CNNs employ optimization algorithms, such as gradient descent and its variants (e.g., stochastic gradient descent), to update weights and biases during training.</li> <li> <p>These algorithms aim to find the optimal set of parameters that minimize the loss function.</p> </li> <li> <p>Batch Processing:</p> </li> <li>Both architectures can process input data in batches during training to improve computational efficiency.</li> <li>**Batch processing involves updating weights and biases based on the average gradient calculated over a batch of input samples.</li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/06_CNN_Vs_ANN/#quick-quiz","title":"Quick Quiz","text":"<ol> <li> <p>How many learnable parameters are there in a convolutional layer with 50 filters applied to an RGB image, where each filter has a shape of (3, 3, 3)?</p> <p>Ans: Total learnable parameters in a single filter = <code>(3 * 3 * 3) + 1 = 28</code>  where, (3, 3, 3) is the shape of a filter and 1 is for bias. Total learnable parameters of 50 filters = <code>(28 * 50) = 1400</code></p> </li> </ol>"},{"location":"deep-learning/End-to-End-Deep-Learning/02_CNN/06_CNN_Vs_ANN/#differences-between-ann-and-cnn","title":"Differences between ANN and CNN","text":"<p>The computational cost of Convolutional Neural Networks (CNNs) and Artificial Neural Networks (ANNs) varies significantly when working with image data. Here are the key differences in terms of computational cost:</p> <ol> <li> <p>Parameter Efficiency:</p> <ul> <li>Fully connected layers in ANNs have a large number of parameters, especially when working with high-dimensional data like images. The sheer number of parameters increases the computational cost during both training and inference.</li> <li>CNNs leverage parameter sharing through convolutional filters, leading to a more parameter-efficient architecture. The use of shared filters significantly reduces the number of parameters compared to ANNs when processing image data.</li> </ul> </li> <li> <p>Localized Operations:</p> <ul> <li>ANN operates on the entire input space, making it computationally intensive, especially for large images. Lacks the ability to efficiently capture localized patterns.</li> <li>CNN Employs localized operations, such as convolution and pooling, which significantly reduce the computational cost. Focuses on specific regions of the input, enhancing efficiency in capturing local patterns.</li> </ul> </li> <li> <p>Hierarchical Feature Extraction:</p> <ul> <li>Hierarchical feature extraction in ANNs involves fully connected layers, which may struggle to capture spatial hierarchies effectively. Computationally expensive due to the large number of parameters and global connectivity.</li> <li>CNNs are designed for hierarchical feature extraction through convolutional layers. Filters capture spatial hierarchies locally, making them computationally efficient in processing image data.</li> </ul> </li> <li> <p>Spatial Invariance:</p> <ul> <li>ANN lacks inherent spatial invariance, requiring extensive processing to handle variations in object position within an image. Higher computational cost in dealing with spatial transformations.</li> <li>CNN incorporates pooling layers to achieve spatial invariance, reducing the computational burden related to position variations. Robustness to translations results in computational efficiency in handling diverse spatial configurations.</li> </ul> </li> </ol>"},{"location":"deep-learning/deep-learning-from-scratch/","title":"\ud83c\udf0d DL4EO: Deep Learning from Scratch for Earth Observation","text":"<p>Welcome to dl4eo \u2014 a hands-on roadmap for learning and applying Deep Learning in Geospatial and Earth Observation (EO) applications. This repository is inspired by the idea of \u201clearning from scratch\u201d but tailored to the unique challenges of satellite imagery, multispectral/hyperspectral data, and geospatial analysis.  </p>"},{"location":"deep-learning/deep-learning-from-scratch/#goals","title":"\ud83d\udccc Goals","text":"<ul> <li>Build deep learning models from the ground up, starting with the basics.  </li> <li>Apply models to geospatial datasets: crop health, land cover, object detection, segmentation, change detection, etc.  </li> <li>Provide clean, reproducible code with step-by-step progression.  </li> <li>Serve as both a learning guide and a research playground.  </li> </ul>"},{"location":"deep-learning/deep-learning-from-scratch/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Open an issue or submit a PR if you\u2019d like to add new models, datasets, or tutorials.</p>"},{"location":"deep-learning/deep-learning-from-scratch/#license","title":"\ud83d\udcdc License","text":"<p>MIT License \u2013 feel free to use this repo for learning and research.</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B2%5D%20SRGAN/test/","title":"Test","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: geo     language: python     name: geo</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B2%5D%20SRGAN/test/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xarray as xr\nimport os\nfrom glob import glob\n\nimport torch\nfrom torch import nn\nfrom model import Generator, Discriminator\n\nDATA_DIR = '/beegfs/halder/DATA/climate_data_(kaushik)/'\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B2%5D%20SRGAN/test/#model","title":"Model","text":"<pre><code>X = torch.randn((1, 17, 128, 128))\n\ngenerator = Generator(in_channels=17, out_channels=1, num_channels=64, num_blocks=16)\n\nX_hat = generator(X)\nprint(X_hat.shape)\n\ndiscriminator = Discriminator(in_channels=1)\nX_hat = discriminator(X_hat)\nprint(X_hat.shape)\n</code></pre> <pre><code>image_dir = os.path.join(DATA_DIR, 'images')\nlabel_dir = os.path.join(DATA_DIR, 'labels')\n\nimage_paths = sorted(glob(os.path.join(image_dir, '*.nc')))\nlabel_paths = sorted(glob(os.path.join(label_dir, '*.nc')))\n\nimage = xr.open_dataset(image_paths[0])\nimage = [image[var].values.squeeze() for var in image.data_vars]\nimage = np.stack(image, axis=0)\n\nlabel = xr.open_dataset(label_paths[0])\n# label = [label[var].values.squeeze() for var in label.data_vars]\n# label = np.stack(label, axis=0)\n</code></pre> <pre><code>label\n</code></pre> <pre><code>image.shape\n</code></pre> <pre><code>plt.imshow(image[0, 12]);\n</code></pre> <pre><code>plt.imshow(label[0]);\n</code></pre> <pre><code># Extract all data variables and drop the singleton time dimension\narrays = [image[var].values.squeeze() for var in ds.data_vars]\n\n# Stack along channel axis\ndata = np.stack(arrays, axis=0)\n\nprint(\"Shape:\", data.shape)  \n# (channels, lat, lon)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/","title":"Simplified Attention Mechanism Without Trainable Weights","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#input-text","title":"Input text","text":"<pre><code>import torch\n\ninputs = torch.tensor(\n    [[0.72, 0.45, 0.31], # Dream    (x^1)\n     [0.75, 0.20, 0.55], # big      (x^2)\n     [0.30, 0.80, 0.40], # and      (x^3)\n     [0.85, 0.35, 0.60], # work     (x^4)\n     [0.55, 0.15, 0.75], # for      (x^5)\n     [0.25, 0.20, 0.85]] # it       (x^6)\n)\n\n# Corresponding words\nwords = ['Dream', 'big', 'and', 'work', 'for', 'it']\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#magnitude-of-vectors","title":"Magnitude of vectors","text":"<pre><code># Calculate the magnitude of each vector\nmagnitudes = torch.norm(inputs, dim=1)\n\n# Print the magnitudes\nprint('Magnitudes of the vectors:')\nfor word, magnitude in zip(words, magnitudes):\n    print(f'{word}: {magnitude.item():.4f}')\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#plotting-the-3d-vectors","title":"Plotting the 3D vectors","text":"<pre><code>import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Extract x, y, z coordinates\nx_coords = inputs[:, 0].numpy()\ny_coords = inputs[:, 1].numpy()\nz_coords = inputs[:, 2].numpy()\n\n# 3D plot with vectors from origin\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\ncolors = ['r', 'g', 'b', 'c', 'm', 'y']\n\nfor (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n    ax.text(x, y, z, word, fontsize=10, color=color)\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nplt.title('3D Plot of Word Embeddings with Colored Vectors')\nplt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#dot-product-between-2nd-input-token-and-all-words","title":"Dot product between 2nd input token and all words","text":"<pre><code>query = inputs[1] # 2nd input token is the query\n\nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here)\n\nprint(attn_scores_2)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#normalize-the-attention-weights","title":"Normalize the attention weights","text":"<pre><code># attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\n# print('Attention weights:', attn_weights_2_tmp)\n# print('Sum:', attn_weights_2_tmp.sum())\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#converting-to-softmax-probabilities","title":"Converting to softmax probabilities","text":"<pre><code>from torch import  nn\n\n# Convert attn_scores_2 to softmax\nsoftmax = nn.Softmax(dim=-1)\nattn_weights_2_tmp = softmax(attn_scores_2)\n\nprint('Attention weights:', attn_weights_2_tmp)\nprint('Sum:', attn_weights_2_tmp.numpy().sum())\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#attention-scores-for-all-queries","title":"Attention scores for all queries","text":"<pre><code>attn_scores = inputs @ inputs.T # @ for efficient matrix multiplication\nprint(attn_scores)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#attention-scores-converted-to-attention-weights","title":"Attention scores converted to attention weights","text":"<pre><code>attn_weights = softmax(attn_scores)\nprint(attn_weights)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#sum-of-each-row-is-1","title":"Sum of each row is 1","text":"<pre><code>row_2_sum = sum([0.1624, 0.1803, 0.1336, 0.2059, 0.1715, 0.1462])\nprint(\"Row 2 sum:\", row_2_sum)\nprint(\"All row sums:\", attn_weights.sum(dim=-1))\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#print-the-context-vector-for-all-queries","title":"Print the context vector for all queries","text":"<pre><code>all_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#context-vector-corresponding-to-the-2nd-input","title":"Context vector corresponding to the 2nd input","text":"<pre><code>context_vec_2 = all_context_vecs[1]\nprint('Previous 2nd context vector:', context_vec_2)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/00_Simplified%20attention%20mechanism%20without%20trainable%20weights/#adding-context-vector-just-for-plotting","title":"Adding context vector just for plotting","text":"<pre><code># Append context_vec_2 to inputs\ninputs = torch.cat((inputs, context_vec_2.unsqueeze(0)), dim=0)\n\n# Add 'context_vector' to the words list\nwords.append('context_vector_for_big')\n\nprint('Updated input tensor:')\nprint(inputs)\nprint('\\nUpdated input tensor:')\nprint(words)\n</code></pre> <pre><code># Extract x, y, z coordinates\nx_coords = inputs[:, 0].numpy()\ny_coords = inputs[:, 1].numpy()\nz_coords = inputs[:, 2].numpy()\n\n# 3D plot with vectors from origin\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\ncolors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n\nfor (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n    ax.text(x, y, z, word, fontsize=10, color=color)\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nplt.title('3D Plot of Word Embeddings with Colored Vectors')\nplt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/","title":"Self Attention With Trainable Weights","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#input-sequence-dream-big-and-work-for-it","title":"Input sequence: \"Dream big and work for it\"","text":"<pre><code>import torch\n\ninputs = torch.tensor(\n    [[0.72, 0.45, 0.31], # Dream    (x^1)\n     [0.75, 0.20, 0.55], # big      (x^2)\n     [0.30, 0.80, 0.40], # and      (x^3)\n     [0.85, 0.35, 0.60], # work     (x^4)\n     [0.55, 0.15, 0.75], # for      (x^5)\n     [0.25, 0.20, 0.85]] # it       (x^6)\n)\n\n# Corresponding words\nwords = ['Dream', 'big', 'and', 'work', 'for', 'it']\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#we-want-to-generate-the-context-vector-for-2nd-token","title":"We want to generate the context vector for 2nd token","text":"<pre><code>x_2 = inputs[1]\nd_in = inputs.shape[1]\nd_out = 2 #C = dimensionality of the context vector\nprint(x_2)\nprint(d_in)\nprint(d_out)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#randomly-initializing-wq-wk-wv-matrices","title":"Randomly initializing Wq, Wk, Wv matrices","text":"<pre><code>torch.manual_seed(123)\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n</code></pre> <pre><code>print(W_query)\n</code></pre> <pre><code>print(W_key)\n</code></pre> <pre><code>print(W_value)\n</code></pre> <pre><code>query_2 = x_2 @ W_query\nkey_2 = x_2 @ W_key\nvalue_2 = x_2 @ W_value\nprint(query_2)\nprint(key_2)\nprint(value_2)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#calculating-q-k-and-v-using-x-wq-wk-wv","title":"Calculating Q, K, and V using X, Wq, Wk, Wv","text":"<pre><code>keys = inputs @ W_key\nvalues = inputs @ W_value\nqueries = inputs @ W_query\n\nprint('keys.shape', keys.shape)\nprint('values.shape', values.shape)\nprint('queries.shape', queries.shape)\n\nprint('keys:', keys)\nprint('queries', queries)\nprint('values', values)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#keys-corresponding-to-second-token-and-the-attention-of-second-token-to-itself","title":"Keys corresponding to second token and the attention of second token to itself","text":"<pre><code>keys_2 = keys[1]\nattn_score_22 = query_2 @ keys_2\nprint(attn_score_22)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#all-attention-scores-for-query-number-2","title":"All attention scores for query number 2","text":"<pre><code>attn_scores_2 = query_2 @ keys.T # All attention scores for given query\nprint(attn_scores_2)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#attention-scores-not-weights-matrix","title":"Attention scores (NOT Weights) matrix","text":"<pre><code>attn_scores = queries @ keys.T # omega\nprint(attn_scores)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#scale-by-1sqrtd-and-then-take-softmax","title":"Scale by 1/sqrt(d) and then take softmax","text":"<pre><code>attn_scores_2 = query_2 @ keys.T # All attention scores for given query\nprint(attn_scores_2)\nd_k = keys.shape[-1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)\n\nattn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\nprint(attn_weights)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#softmax-peaks-when-the-numbers-are-scaled","title":"Softmax peaks when the numbers are scaled","text":"<pre><code>import torch\n# Define the tensor\ntensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n\n# Apply softmax\n# Apply softmax without scaling\nsoftmax_result = torch.softmax(tensor, dim=-1)\nprint(\"Softmax without scaling:\", softmax_result)\n\n# Multiply the tensor by 8 and the apply softmax\nscaled_tensor = tensor * 8\nsoftmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\nprint(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#scaling-has-to-be-such-that-the-variance-of-qkt-is-close-to-1","title":"Scaling has to be such that the variance of Q*K.T is close to 1","text":"<pre><code>import numpy as np\n\n# Function to compute variance before and after scaling\ndef compute_variance(dim, num_trials=1000):\n    dot_products = []\n    scaled_dot_products = []\n\n    # Generate multiple random vectors and compute dot products\n    for _ in range(num_trials):\n        q = np.random.randn(dim)\n        k = np.random.randn(dim)\n\n        # Compute dot product\n        dot_product = q @ k\n        dot_products.append(dot_product)\n\n        # Scale the dot product by sqrt (dim)\n        scaled_dot_product = dot_product / (dim)\n        scaled_dot_products.append(scaled_dot_product)\n\n    # Calculate the variance of the dot products\n    variance_before_scaling = np.var(dot_products)\n    variance_after_scaling = np.var(scaled_dot_products)\n\n    return variance_before_scaling, variance_after_scaling\n\n# For dimension 5\nvariance_before_5, variance_after_5 = compute_variance(5)\nprint(f\"Variance before scaling (dim=5): {variance_before_5}\")\nprint(f\"Variance after scaling (dim=5): {variance_after_5}\")\n\n# For dimension 100\nvariance_before_100, variance_after_100 = compute_variance(100)\nprint(f\"Variance before scaling (dim=100): {variance_before_100}\")\nprint(f\"Variance after scaling (dim=100): {variance_after_100}\")\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#context-vector-corresponding-to-2nd-input-token","title":"Context vector corresponding to 2nd input token","text":"<pre><code>context_vec_2 = attn_weights_2 @ values\ncontext_vec = attn_weights @ values\nprint(context_vec_2)\nprint(context_vec)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/01_Self%20attention%20with%20trainable%20weights/#python-class-for-doing-this-whole-operation","title":"Python class for doing this whole operation","text":"<pre><code>import torch.nn as nn\n\nclass SelfAttention_v1(nn.Module):\n\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n\n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n</code></pre> <pre><code>torch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))\n</code></pre> <pre><code>class SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)  \n        queries = self.W_query(x) \n        values = self.W_value(x)  \n\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n        context_vec = torch.matmul(attn_weights, values)  # [batch, seq_len, d_out]\n        return context_vec\n\n# Example test\ntorch.manual_seed(123)\nsa_v2 = SelfAttention_v1(d_in, d_out)\nprint(sa_v2(inputs))\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/","title":"Causal Attention","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#input-sequence-dream-big-and-work-for-it","title":"Input sequence: \"Dream big and work for it\"","text":"<pre><code>import torch\n\ninputs = torch.tensor(\n    [[0.72, 0.45, 0.31], # Dream    (x^1)\n     [0.75, 0.20, 0.55], # big      (x^2)\n     [0.30, 0.80, 0.40], # and      (x^3)\n     [0.85, 0.35, 0.60], # work     (x^4)\n     [0.55, 0.15, 0.75], # for      (x^5)\n     [0.25, 0.20, 0.85]] # it       (x^6)\n)\n\n# Corresponding words\nwords = ['Dream', 'big', 'and', 'work', 'for', 'it']\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#class-for-implementing-self-attention","title":"Class for implementing self attention","text":"<pre><code>from torch import nn\n\nclass SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        queries = self.W_query(x)\n        keys = self.W_key(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#context-vectors-corresponding-to-inputs","title":"Context vectors corresponding to inputs","text":"<pre><code>d_in = inputs.shape[-1]\nd_out = 2\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#final-attention-weights","title":"Final attention weights","text":"<pre><code>queries = sa_v2.W_query(inputs)\nkeys = sa_v2.W_key(inputs)\nattn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#lower-triangular-matrix-mask","title":"Lower triangular matrix (mask)","text":"<pre><code>context_length = inputs.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#attention-weights-after-applying-mask","title":"Attention weights after applying mask","text":"<pre><code>masked_simple = attn_weights * mask_simple\nprint(masked_simple)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#attention-weights-normalized","title":"Attention weights normalized","text":"<pre><code>row_sums = masked_simple.sum(dim=-1, keepdim=True)\nprint(row_sums.shape)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#attention-scores","title":"Attention scores","text":"<pre><code>print(attn_scores)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#upper-triangular-matrix","title":"Upper triangular matrix","text":"<pre><code>mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nprint(mask)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#ones-are-converted-to-ve-infinity","title":"Ones are converted to -ve infinity","text":"<pre><code>mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nprint(mask)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#attention-weights-after-taking-softmax","title":"Attention weights after taking softmax","text":"<pre><code>attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#ones-matrix","title":"Ones matrix","text":"<pre><code>example = torch.ones(inputs.shape[0], inputs.shape[0]) #B\nprint(example)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#random-dropout-with-50-probability","title":"Random dropout with 50% probability","text":"<pre><code>torch.manual_seed(123)\ndropout = nn.Dropout(0.5) #A\ndropout(example)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/02_Causal%20attention/#attention-weights-after-dropout-mask","title":"Attention weights after dropout mask","text":"<pre><code>torch.manual_seed(123)\nprint(dropout(attn_weights))\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/","title":"Multi Head Attention","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: PyTorch CUDA 12.1     language: python     name: pytorchcu121</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#input-sequence-dream-big-and-work-for-it","title":"Input sequence: \"Dream big and work for it\"","text":"<pre><code>import torch\n\ninputs = torch.tensor(\n    [[0.72, 0.45, 0.31], # Dream    (x^1)\n     [0.75, 0.20, 0.55], # big      (x^2)\n     [0.30, 0.80, 0.40], # and      (x^3)\n     [0.85, 0.35, 0.60], # work     (x^4)\n     [0.55, 0.15, 0.75], # for      (x^5)\n     [0.25, 0.20, 0.85]] # it       (x^6)\n)\n\n# Corresponding words\nwords = ['Dream', 'big', 'and', 'work', 'for', 'it']\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#class-for-implementing-causal-attention","title":"Class for implementing causal attention","text":"<pre><code>from torch import nn\n\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ values.transpose(1, 2)\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n        )\n\n        attn_weights = self.dropout(attn_weights)\n\n        context_vec = attn_weights @ values\n        return context_vec\n</code></pre> <pre><code>d_in = inputs.shape[-1]\nd_out = 2\nprint(d_in, d_out)\n</code></pre> <pre><code>batch = torch.stack((inputs,), dim=0)\nprint(batch.shape)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#class-for-implementing-multi-head-attention","title":"Class for implementing multi-head attention","text":"<pre><code>class MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList([\n            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n        ])\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n</code></pre> <pre><code>torch.manual_seed(123)\ncontext_length = batch.shape[1]\nd_in, d_out = 3, 2\nmha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n</code></pre> <pre><code>context_vecs = mha(batch)\nprint(context_vecs)\nprint('context_vecs.shape:', context_vecs.shape)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#implementing-multi-head-attention-with-weight-splits","title":"Implementing multi-head attention with weight splits","text":"<p>Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class.</p> <ul> <li>Step 1: Reduce the projection dim to match desired output dim</li> <li>Step 2: Use a linear layer to combine head outputs</li> <li>Step 3: Tensor shape: (b, num_tokens, d_out)</li> <li>Step 4: We implicitly split the matrix by adding a num_heads dimension. Then we unroll last dim: (b, num_tokens, head_dim)</li> <li>Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)</li> <li>Step 6: Compute dot product of each head</li> <li>Step 7: Mask truncated to the number of tokens</li> <li>Step 8: Use the mask to fill attention scores</li> <li>Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)</li> <li>Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim</li> <li>Step 11: Add an optional linear projection </li> </ul> <pre><code>class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_in) # Linear layer to combine\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(\n                context_length, context_length\n            ), diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n        keys = self.W_key(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a 'num_heads' dimension\n        # Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the maks to fill attention scores\n        attn_scores.masked_fill(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec\n</code></pre> <pre><code>torch.manual_seed(123)\n\n# Define the tensor with 3 rows and 6 columns\ninputs = torch.tensor([\n    [0.43, 0.15, 0.89, 0.55, 0.87, 0.66], # Row 1\n    [0.57, 0.85, 0.64, 0.22, 0.58, 0.33], # Row 2\n    [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]  # Row 3\n])\n\nbatch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape)\nprint(batch)\n\nbatch_size, context_length, d_in = batch.shape\nd_out = 6\ncontext_length = inputs.shape[0]\ndropout = 0.0\nnum_heads = 2\n\nmha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads, True)\n\ncontext_vecs = mha(batch)\nprint(context_vecs)\nprint('context_vecs.shape:', context_vecs.shape)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#stepwise-implementation-of-multi-head-attention","title":"Stepwise implementation of multi-head attention","text":"<pre><code>import torch\n\n# Input: (batch=1, seq_len=3, d_model=6)\nx = torch.tensor([[\n    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], # The \n    [6.0, 5.0, 4.0, 3.0, 2.0, 1.0], # Kid\n    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # Smiles\n]])\n\nbatch_size, seq_len, d_model = x.shape\n</code></pre> <pre><code># Define 6x6 projection matrices (d_model x d_model)\ntorch.manual_seed(0) # for reprducibility\nWq = torch.randn(d_model, d_model)\nWk = torch.randn(d_model, d_model)\nWv = torch.randn(d_model, d_model)\n\n# Compute Q, K, V\n# Shape logic: (B, T, d_model) @ (d_model, d_model) -&gt; (B, T, d_model)\nQ = x @ Wq\nK = x @ Wk\nV = x @ Wv\n\n# Print Q, K, and V\nprint(\"Q:\\n\", Q)\nprint(\"K:\\n\", K)\nprint(\"V:\\n\", V)\n\n# Print dimensionalities\nprint('X shape:', x.shape)      # (1, 3, 6)\nprint('Wq shape:', Wq.shape)    # (6, 6)\nprint('Wk shape:', Wk.shape)    # (6, 6)\nprint('Wv shape:', Wv.shape)    # (6, 6)\nprint('Q shape:', Q.shape)      # (1, 3, 6)\nprint('K shape:', K.shape)      # (1, 3, 6)\nprint('V shape:', V.shape)      # (1, 3, 6)\n</code></pre> <pre><code># Print values\ntorch.set_printoptions(precision=2)\nprint(\"\\nWq:\\n\", Wq)\nprint(\"\\nWk:\\n\", Wk)\nprint(\"\\nWv:\\n\", Wv)\n\nprint(\"\\nQ:\\n\", Q)\nprint(\"\\nK:\\n\", K)\nprint(\"\\nV:\\n\", V)\n</code></pre> <pre><code>num_heads = 2\nhead_dim = 3\n\nQ = Q.view(1, 3, num_heads, head_dim)\nK = K.view(1, 3, num_heads, head_dim)\nV = V.view(1, 3, num_heads, head_dim)\n\nprint('Q after unrolling:', Q)\nprint('K after unrolling:', K)\nprint('V after unrolling:', V)\n</code></pre> <pre><code>Q = Q.transpose(1, 2)\nK = K.transpose(1, 2)\nV = V.transpose(1, 2)\n\nprint('Q after grouping by heads:', Q)\nprint('K after grouping by heads:', K)\nprint('V after grouping by heads:', V)\n</code></pre> <pre><code>K_T = K.transpose(2, 3)\nprint('K_T shape:', K_T)\n</code></pre> <pre><code>attn_scores = Q @ K_T\nprint('Attention scores shape:', attn_scores.shape)\nprint('Attention scores:\\n', attn_scores)\n</code></pre> <pre><code>mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\nprint('Causal mask:\\n', mask)\n\nattn_scores.masked_fill_(mask, -torch.inf)\nprint('Attention scores after masking:\\n', attn_scores)\n</code></pre> <pre><code>torch.set_printoptions(precision=3, sci_mode=False)\nhead_dim = 3\nattn_weights = torch.softmax(attn_scores / head_dim**0.5, dim=-1)\nprint('Attention weights shape:', attn_weights.shape)\nprint('Attention weights:\\n', attn_weights)\n</code></pre> <pre><code>dropout = torch.nn.Dropout(0.1)\nattn_weights = dropout(attn_weights)\nprint('Attention weights after dropout:\\n', attn_weights)\n</code></pre> <pre><code>context_vec = attn_weights @ V\nprint('Context vector shape:', context_vec.shape)\nprint('Context vec:\\n', context_vec)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B3%5D%20Transformers/03_Multi-head%20attention/#step-11-reformat-and-concatenate","title":"Step 11: Reformat and concatenate","text":"<pre><code>context_vec = context_vec.transpose(1, 2)\nprint('Context vector after swapping dimensions 1 and 2:', context_vec.shape)\nprint('Context vector:\\n', context_vec)\n</code></pre> <pre><code>context_vec = context_vec.reshape(batch_size, seq_len, num_heads * head_dim)\nprint('Context vector after concatenating heads:', context_vec.shape)\nprint('Context vector:\\n', context_vec)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/","title":"Coding Vit From Scratch","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p> <p></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#import-libraires","title":"Import libraires","text":"<p>```python executionInfo={\"elapsed\": 72, \"status\": \"ok\", \"timestamp\": 1764507984616, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"zyZwqi9ig3qa\" import torch from torch import nn import torchvision from torchvision.datasets import MNIST from torchvision.transforms import transforms from torch.utils.data import DataLoader <pre><code>&lt;!-- #region id=\"gJQiiGHriU0q\" --&gt;\n## Define the transformation\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 9, \"status\": \"ok\", \"timestamp\": 1764507984992, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"H-UMiRnuiZJj\"\n# Transformation for PIL to tensor format\ntransform = transforms.Compose(\n    [transforms.ToTensor()]\n)\n</code></pre></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#download-the-data","title":"Download the data","text":"<p>```python executionInfo={\"elapsed\": 62, \"status\": \"ok\", \"timestamp\": 1764507985390, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"3hG2tdouiE8j\" train_dataset = MNIST(root='./data', train=True, transform=transform, download=True) val_dataset = MNIST(root='./data', train=False, transform=transform, download=False) <pre><code>&lt;!-- #region id=\"B0okpNADjbOV\" --&gt;\n## Define the parameters\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 37, \"status\": \"ok\", \"timestamp\": 1764507985687, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"t_JALZ9Xjesk\"\nnum_classes = 10\nbatch_size = 64\nnum_channels = 1\nimg_size = 28\npatch_size = 7\nnum_patches = (img_size // patch_size) ** 2\nembedding_dim = 64\nattention_heads = 4\ntransformer_blocks = 4\nmlp_hidden_nodes = 128\nlearning_rate = 0.001\nepochs = 5\n</code></pre></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#define-the-dataloader","title":"Define the dataloader","text":"<p>```python executionInfo={\"elapsed\": 46, \"status\": \"ok\", \"timestamp\": 1764507986033, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"BHZGRdmTjRvk\" train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) <pre><code>&lt;!-- #region id=\"44aj-sqYlAql\" --&gt;\n## Part 1: Patch embedding\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 41, \"status\": \"ok\", \"timestamp\": 1764507986386, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"IPxvmWmElE9E\"\nclass PatchEmbedding(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.patch_embed = nn.Conv2d(num_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n\n        return x\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 22, \"status\": \"ok\", \"timestamp\": 1764507986495, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"VGgBf_u5mY41\" outputId=\"b4e5a15d-be14-4b9f-d331-be4fd61089eb\"</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#sample-a-data-point-from-the-train_loader","title":"Sample a data point from the train_loader","text":"<p>data_point, label = next(iter(train_loader)) print(\"Shape of data point:\", data_point.shape)</p> <p>patch_embed = nn.Conv2d(num_channels, embedding_dim, kernel_size=patch_size, stride=patch_size) patch_embed_output = patch_embed(data_point) print(patch_embed(data_point).shape)</p> <p>patch_embed_output_flatten = patch_embed_output.flatten(2) print(patch_embed_output_flatten.shape)</p> <p>patch_embed_output_flatten_transpose = patch_embed_output_flatten.transpose(1, 2) print(patch_embed_output_flatten_transpose.shape) <pre><code>&lt;!-- #region id=\"Zjx1EpGmn27m\" --&gt;\n## Part 2: Transformer encoder\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 4, \"status\": \"ok\", \"timestamp\": 1764507986865, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"ifbBeSn4pvCs\"\nclass TransformerEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, batch_first=True)\n        self.mlp = nn.Sequential(\n            nn.Linear(embedding_dim, mlp_hidden_nodes),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_nodes, embedding_dim)\n        )\n\n    def forward(self, x):\n        residual1 = x\n        x = self.layer_norm1(x)\n        x = self.multihead_attention(query=x, key=x, value=x)[0]\n        x = x + residual1\n\n        residual2 = x\n        x = self.layer_norm2(x)\n        x = self.mlp(x)\n        x = x + residual2\n\n        return x\n</code></pre></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#part-3-mlp-head","title":"Part 3: MLP head","text":"<p>```python executionInfo={\"elapsed\": 35, \"status\": \"ok\", \"timestamp\": 1764507987593, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"3iKf7XvIsq3T\" class MLP_head(nn.Module):     def init(self):         super().init()         self.layer_norm1 = nn.LayerNorm(embedding_dim)         self.mlp_head = nn.Linear(embedding_dim, num_classes)</p> <pre><code>def forward(self, x):\n    x = self.layer_norm1(x)\n    x = self.mlp_head(x)\n\n    return x\n</code></pre> <p>```  </p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#part-4-vision-transformer","title":"Part 4: Vision Transformer","text":"<p> <code>python executionInfo={\"elapsed\": 3, \"status\": \"ok\", \"timestamp\": 1764507988352, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"ExzeQqW2thmb\" class VisionTransformer(nn.Module):     def __init__(self):         super().__init__()         self.patch_embed = PatchEmbedding()         self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))         self.position_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))         self.transformer_blocks = nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])         self.mlp_head = MLP_head()      def forward(self, x):         x = self.patch_embed(x)         B = x.size(0)         class_tokens = self.cls_token.expand(B, -1, -1)         x = torch.cat((class_tokens, x), dim=1)         x = x + self.position_embedding         x = self.transformer_blocks(x)         x = x[:, 0]         x = self.mlp_head(x)          return x</code></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#training","title":"Training","text":"<p>```python executionInfo={\"elapsed\": 18, \"status\": \"ok\", \"timestamp\": 1764507989567, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"OX3VVVRBxYBV\" device = 'cuda' if torch.cuda.is_available() else 'cpu' model = VisionTransformer().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) criterion = nn.CrossEntropyLoss() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 82519, \"status\": \"ok\", \"timestamp\": 1764508072628, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"km1OSgV5xvd9\" outputId=\"ff5a1b85-3a13-4076-e833-a8c0c852bae2\"\n# Training\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    correct_epoch = 0\n    total_epoch = 0\n\n    print(f'\\nEpoch {epoch+1}')\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        correct = (preds == labels).sum().item()\n        accuracy = 100.0 * (correct / labels.size(0))\n\n        correct_epoch += correct\n        total_epoch += labels.size(0)\n\n        if batch_idx % 100 ==0:\n            print(f'Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%')\n\n    epoch_acc = 100.0 * correct_epoch / total_epoch\n    print(f'==&gt; Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%')\n</code></pre></p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#validation","title":"Validation","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 1439, \"status\": \"ok\", \"timestamp\": 1764508083819, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"E4Nyn3iO4Gl2\" outputId=\"6de047f8-5564-4c3e-862e-13dbbe73d284\"</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B4%5D%20Vision%20Transformer%20%28ViT%29/Coding%20ViT%20from%20scratch/#validation_1","title":"Validation","text":"<p>model.eval() val_loss = 0 correct_val = 0 total_val = 0</p> <p>with torch.no_grad():     for images, labels in val_loader:         images, labels = images.to(device), labels.to(device)</p> <pre><code>    outputs = model(images)\n    loss = criterion(outputs, labels)\n    val_loss += loss.item()\n\n    preds = outputs.argmax(dim=1)\n    correct_val += (preds == labels).sum().item()\n    total_val += labels.size(0)\n</code></pre> <p>val_acc = 100.0 * correct_val / total_val avg_val_loss = val_loss / len(val_loader)</p> <p>print(f'&gt;&gt; Validation: Loss = {avg_val_loss:.4f}, Accuracy = {val_acc:.2f}%')</p> <pre><code>&lt;!-- #region id=\"S4SSAPFM4bG-\" --&gt;\n## Plotting\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 582} executionInfo={\"elapsed\": 679, \"status\": \"ok\", \"timestamp\": 1764508208261, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"5oD_ISZY4al-\" outputId=\"6f5efe38-b162-49e9-e00f-614774265c63\"\nimport matplotlib.pyplot as plt\nimport random\n\ndef show_random_predictions(model, dataloader, class_names=None, num_images=10):\n    model.eval()\n    images_shown = 0\n\n    # grab one big batch so we can pick random samples from it\n    images, labels = next(iter(dataloader))\n    images, labels = images.to(device), labels.to(device)\n\n    # run through model\n    with torch.no_grad():\n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n\n    # randomly pick indices\n    idxs = random.sample(range(len(images)), num_images)\n\n    plt.figure(figsize=(12, 6))\n    for i, idx in enumerate(idxs):\n        img = images[idx].cpu().squeeze(0)  # MNIST-like (1, H, W)\n        pred = preds[idx].item()\n        true = labels[idx].item()\n\n        plt.subplot(2, num_images // 2, i + 1)\n        plt.imshow(img, cmap=\"gray\")\n        plt.title(f\"Pred: {pred}\\nTrue: {true}\",\n                  color=\"green\" if pred == true else \"red\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nshow_random_predictions(model, val_loader)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/","title":"Build Nanovlm From Scratch","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#import-libraries","title":"Import libraries","text":"<pre><code>import math\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#define-variables","title":"Define variables","text":"<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\nIMG_SIZE = 32\nEMBED_DIM = 64\nATTENTION_HEADS = 4\nBATCH_SIZE = 12\nEPOCHS = 10\nLR = 3e-4\nTEMPERATURE = 0.07\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#synthetic-dataset-properties","title":"Synthetic dataset properties","text":"<pre><code>colors = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray']\nshapes = ['square', 'circle', 'triangle']\npositions = ['left', 'center', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right']\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#drawing-image-shapes","title":"Drawing image shapes","text":"<pre><code>def draw_sample(color, shape, position, img_size=IMG_SIZE):\n    img = Image.new('RGB', (img_size, img_size), 'white')\n    draw = ImageDraw.Draw(img)\n\n    margin = 6\n    h = w = img_size - 2 * margin\n\n    # Calculate x coordinates\n    if 'left' in position:\n        x0 = margin\n        x1 = margin + w // 2\n    elif 'top-left' in position:\n        x0 = margin\n        x1 = margin + w // 2\n    elif 'bottom-left' in position:\n        x0 = margin\n        x1 = margin + w // 2\n    elif 'right' in position:\n        x0 = margin + w // 2\n        x1 = img_size - margin\n    elif 'top-right' in position:\n        x0 = margin + w // 2\n        x1 = img_size - margin\n    elif 'bottom-right' in position:\n        x0 = margin + w // 2\n        x1 = img_size - margin\n    else: # Center or vertical positions\n        x0 = margin + w // 4\n        x1 = margin + 3 * w // 4\n\n     # Calculate y coordinates\n    if 'top' in position:\n        y0 = margin\n        y1 = margin + h // 2\n    elif 'top-left' in position:\n        y0 = margin\n        y1 = margin + h //  2\n    elif 'top-right' in position:\n        y0 = margin\n        y1 = margin + h // 2\n    elif 'bottom' in position:\n        y0 = margin + h // 2\n        y1 = img_size - margin\n    elif 'bottom-left' in position:\n        y0 = margin + h // 2\n        y1 = img_size - margin\n    elif 'bottom-right' in position:\n        y0 = margin + h // 2\n        y1 = img_size - margin\n    else: # Center or horizontal positions\n        y0 = margin + h // 4\n        y1 = margin + 3 * h // 4\n\n    if shape=='square':\n        draw.rectangle([x0, y0, x1, y1], fill=color, outline='black')\n    elif shape=='circle':\n        draw.ellipse([x0, y0, x1, y1], fill=color, outline='black')\n    else: # triangle\n        draw.polygon([(x0+(x1-x0)//2, y0), (x0, y1), (x1, y1)], fill=color, outline='black')\n\n    return img\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#class-for-building-our-dataset","title":"Class for building our dataset","text":"<pre><code>class ShapeDataset(Dataset):\n    def __init__(self):\n        self.images = []\n        self.captions = []\n\n        for c in colors:\n            for s in shapes:\n                for p in positions:\n                    img = draw_sample(c, s, p)\n                    cap = f\"{c} {s} {p}\"\n\n                    self.images.append(torch.from_numpy(np.asarray(img)).permute(2,0,1).float()/255.0)\n                    self.captions.append(cap)\n\n        self.vocab, self.word2idx = self.build_vocab(self.captions)\n\n    def build_vocab(self, texts):\n        words = sorted({w for t in texts for w in t.split()})\n        vocab = ['[CLS]'] + words\n        w2i = {w:i for i, w in enumerate(vocab)}\n        return vocab, w2i\n\n    def encode_text(self, text):\n        toks = [self.word2idx['[CLS]']] + [self.word2idx[w] for w in text.split()]\n        return torch.tensor(toks, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, i):\n        return self.images[i], self.encode_text(self.captions[i]), self.captions[i]\n\n# Craete the full dataset\nfull_dataset = ShapeDataset()\nVOCAB_SIZE = len(full_dataset.vocab)\nprint(VOCAB_SIZE)\nprint(full_dataset.vocab)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#train-val-data-creation","title":"Train-val data creation","text":"<pre><code>train_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#dataloader","title":"Dataloader","text":"<pre><code>train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#display-sample-data-points","title":"Display sample data points","text":"<pre><code>imgs, _, caps = next(iter(train_loader))\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 9))\naxes = axes.flatten()\n\nfor ax in axes:\n    rand_id = random.randint(0, len(imgs)-1)\n    img = imgs[rand_id].permute(1, 2, 0).numpy()\n    cap = caps[rand_id]\n    ax.imshow(img)\n\n    ax.set_title(cap, fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#image-encoder","title":"Image Encoder","text":"<pre><code>class ImageEncoder(nn.Module):\n    def __init__(self, embed_dim=EMBED_DIM):\n        super().__init__()\n        self.convolutions = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n\n        self.projection = nn.Linear(256, embed_dim)\n        self.layernorm1 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.convolutions(x)\n        x = x.mean(dim=[2, 3])\n        x = self.projection(x)\n        x = F.normalize(self.layernorm1(x), dim=-1)\n        return x\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B5%5D%20NanoVLM/Build%20NanoVLM%20from%20scratch/#text-encoder","title":"Text Encoder","text":""},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/","title":"Coding Vae From Scratch","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#coding-variational-autoencoder-vae-from-scratch","title":"Coding Variational AutoEncoder (VAE) from scratch","text":""},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#import-libraries","title":"Import libraries","text":"<pre><code>import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#load-mnist-dataset","title":"Load MNIST dataset","text":"<pre><code>transform = transforms.ToTensor()\n\ntrain_dataset = datasets.MNIST(root=\"/.data\", train=True, transform=transform, download=True)\ntest_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n\nbatch_size = 128\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(\"Train samples:\", len(train_dataset))\nprint(\"Test samples:\", len(test_dataset))\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#define-the-variational-autoencoder-model","title":"Define the Variational AutoEncoder model","text":"<pre><code>class VAE(nn.Module):\n    def __init__(self, latent_dim=2):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28*28, 400),\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(400, latent_dim)\n        self.fc_logvar = nn.Linear(400, latent_dim)\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 400),\n            nn.ReLU(),\n            nn.Linear(400, 28*28),\n            nn.Sigmoid()\n        )\n\n    def encode(self, x):\n        h = self.encoder(x)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        x_hat = self.decoder(z)\n        return x_hat.view(-1, 1, 28, 28)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decode(z)\n        return x_hat, mu, logvar\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#instantiate-model-and-optimizer","title":"Instantiate model and optimizer","text":"<pre><code>latent_dim = 2\nmodel = VAE(latent_dim=latent_dim).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#define-the-loss-function","title":"Define the loss function","text":"<p>Evidence Lower BOund (ELBO) Loss = Reconstruction Loss + KL Divergence</p> <pre><code>def criterion(x_hat, x, mu, logvar):\n    x = x.view(x.size(0), -1)\n    x_hat = x_hat.view(x_hat.size(0), -1)\n\n    # Reconstruction loss\n    recon_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')\n\n    # KL divergence\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    return recon_loss + kl_loss, recon_loss, kl_loss\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#training-loop","title":"Training loop","text":"<pre><code>def train_epoch(epoch):\n    model.train()\n    total_loss = total_recon = total_kl = 0\n\n    for x, _ in train_loader:\n        x = x.to(device)\n        optimizer.zero_grad()\n\n        x_hat, mu, logvar = model(x)\n        loss, recon, kl = criterion(x_hat, x, mu, logvar)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_recon += recon.item()\n        total_kl += kl.item()\n\n    n = len(train_dataset)\n    print(f\"Epoch {epoch:02d} | Loss {total_loss/n:.4f} | Recon {total_recon/n:.4f} | KL {total_kl/n:.4f}\")\n\ndef test_epoch(epoch):\n    model.eval()\n    total_loss = total_recon = total_kl = 0\n\n    with torch.no_grad():\n        for x, _ in test_loader:\n            x = x.to(device)\n            x_hat, mu, logvar = model(x)\n            loss, recon, kl = criterion(x_hat, x, mu, logvar)\n\n            total_loss += loss.item()\n            total_recon += recon.item()\n            total_kl += kl.item()\n\n    n = len(test_dataset)\n    print(f\"[Test] Loss {total_loss/n:.4f} | Recon {total_recon/n:.4f} | KL {total_kl/n:.4f}\")\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#train-the-model","title":"Train the model","text":"<pre><code>num_epochs = 10\n\nfor epoch in range(1, num_epochs + 1):\n    train_epoch(epoch)\n    test_epoch(epoch)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#visualize-reconstructions","title":"Visualize reconstructions","text":"<pre><code>def show_reconstructions():\n    model.eval()\n    x, _ = next(iter(test_loader))\n    x = x.to(device)\n\n    with torch.no_grad():\n        x_hat, _, _ = model(x)\n\n    x = x.cpu().numpy()\n    x_hat = x_hat.cpu().numpy()\n\n    n = 8\n    plt.figure(figsize=(12, 3))\n\n    for i in range(n):\n        plt.subplot(2, n, i+1)\n        plt.imshow(x[i][0], cmap=\"gray\")\n        plt.axis(\"off\")\n\n        plt.subplot(2, n, n+i+1)\n        plt.imshow(x_hat[i][0], cmap=\"gray\")\n        plt.axis(\"off\")\n\n    plt.suptitle(\"Original (top) vs Reconstruction (bottom)\")\n    plt.show()\n\nshow_reconstructions()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#plot-2d-latent-space","title":"Plot 2D latent space","text":"<pre><code>def plot_latent_space(num_batches=30):\n    model.eval()\n    zs, labels = [], []\n\n    with torch.no_grad():\n        for i, (x, y) in enumerate(test_loader):\n            if i &gt;= num_batches:\n                break\n            x = x.to(device)\n            mu, _ = model.encode(x)\n            zs.append(mu.cpu().numpy())\n            labels.append(y.numpy())\n\n    zs = np.concatenate(zs, axis=0)\n    labels = np.concatenate(labels, axis=0)\n\n    plt.figure(figsize=(6, 6))\n    scatter = plt.scatter(zs[:, 0], zs[:, 1], c=labels, cmap=\"tab10\", s=10)\n    plt.colorbar(scatter)\n    plt.xlabel(\"z1\")\n    plt.ylabel(\"z2\")\n    plt.title(\"2D Latent Space of MNIST\")\n    plt.grid(True)\n    plt.show()\n\nplot_latent_space()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#sampling-from-latent-space-grid","title":"Sampling from latent space grid","text":"<pre><code>def sample_latent_grid():\n    model.eval()\n    n = 5\n    grid_x = np.linspace(-3, 3, n)\n    grid_y = np.linspace(-3, 3, n)\n\n    plt.figure(figsize=(6, 6))\n\n    with torch.no_grad():\n        for i, y in enumerate(grid_y):\n            for j, x in enumerate(grid_x):\n                z = torch.tensor([[x, y]], dtype=torch.float32).to(device)\n                x_hat = model.decode(z)[0, 0].cpu().numpy()\n                plt.subplot(n, n, i*n + j + 1)\n                plt.imshow(x_hat, cmap=\"gray\")\n                plt.axis(\"off\")\n\n    plt.suptitle(\"Sampling from the 2D Latent Space\")\n    plt.show()\n\nsample_latent_grid()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#collect-latent-space-per-epoch","title":"Collect latent space per epoch","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sqrt, pi, exp\n\ndef collect_latent_samples(model, data_loader, num_batches=50, use_mu=True):\n    \"\"\"\n    Go through a few batches and collect latent vectors z (or mu).\n    - use_mu=True: use the mean \u03bc(x) of q(z|x)\n    - use_mu=False: sample z via reparameterization\n    \"\"\"\n    model.eval()\n    zs = []\n\n    with torch.no_grad():\n        for i, (x, _) in enumerate(data_loader):\n            if i &gt;= num_batches:\n                break\n            x = x.to(device)\n            mu, logvar = model.encode(x)\n            if use_mu:\n                z = mu\n            else:\n                # sample z = mu + sigma * eps\n                std = torch.exp(0.5 * logvar)\n                eps = torch.randn_like(std)\n                z = mu + std * eps\n            zs.append(z.cpu().numpy())\n\n    if len(zs) == 0:\n        return None\n\n    zs = np.concatenate(zs, axis=0)   # shape [N, latent_dim]\n    return zs\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#plot-latent-distribution-vs-gaussian","title":"Plot latent distribution vs Gaussian","text":"<pre><code>def normal_pdf(x):\n    # PDF of N(0,1)\n    return (1.0 / sqrt(2 * pi)) * np.exp(-0.5 * x**2)\n\ndef plot_latent_hist(z_samples, epoch, bins=40):\n    \"\"\"\n    z_samples: numpy array [N, 2]\n    Plots:\n      - Histogram of z1 and z2\n      - Overlaid standard normal PDF\n    \"\"\"\n    if z_samples is None:\n        print(\"No latent samples to plot.\")\n        return\n\n    z1 = z_samples[:, 0]\n    z2 = z_samples[:, 1]\n\n    mean_z1, std_z1 = np.mean(z1), np.std(z1)\n    mean_z2, std_z2 = np.mean(z2), np.std(z2)\n\n    print(f\"[Epoch {epoch}] z1 mean={mean_z1:.3f}, std={std_z1:.3f} | \"\n          f\"z2 mean={mean_z2:.3f}, std={std_z2:.3f}\")\n\n    xs = np.linspace(-4, 4, 400)\n    pdf = normal_pdf(xs)\n\n    plt.figure(figsize=(12, 4))\n\n    # z1 histogram\n    plt.subplot(1, 3, 1)\n    plt.hist(z1, bins=bins, density=True, alpha=0.6)\n    plt.plot(xs, pdf, linewidth=2)\n    plt.title(f\"z1 distribution (epoch {epoch})\")\n    plt.xlabel(\"z1\")\n    plt.ylabel(\"density\")\n\n    # z2 histogram\n    plt.subplot(1, 3, 2)\n    plt.hist(z2, bins=bins, density=True, alpha=0.6)\n    plt.plot(xs, pdf, linewidth=2)\n    plt.title(f\"z2 distribution (epoch {epoch})\")\n    plt.xlabel(\"z2\")\n    plt.ylabel(\"density\")\n\n    # 2D scatter\n    plt.subplot(1, 3, 3)\n    plt.scatter(z1, z2, s=4, alpha=0.5)\n    plt.title(\"2D latent scatter\")\n    plt.xlabel(\"z1\")\n    plt.ylabel(\"z2\")\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B6%5D%20Variational%20AutoEncoders/Coding%20VAE%20from%20scratch/#train-monitor-latent-gaussianity-over-epochs","title":"Train + monitor latent Gaussianity over epochs","text":"<pre><code>latent_stats = {\n    \"epoch\": [],\n    \"mean_z1\": [],\n    \"std_z1\": [],\n    \"mean_z2\": [],\n    \"std_z2\": [],\n}\n\nnum_epochs = 7  \nnum_batches_for_latent = 50\n\nfor epoch in range(1, num_epochs + 1):\n    train_epoch(epoch)\n    test_epoch(epoch)\n\n    # collect latent samples from test set (you can switch to train_loader)\n    z_samples = collect_latent_samples(model, test_loader,\n                                       num_batches=num_batches_for_latent,\n                                       use_mu=True)  # or False for sampled z\n\n    if z_samples is not None:\n        z1 = z_samples[:, 0]\n        z2 = z_samples[:, 1]\n        m1, s1 = np.mean(z1), np.std(z1)\n        m2, s2 = np.mean(z2), np.std(z2)\n\n        latent_stats[\"epoch\"].append(epoch)\n        latent_stats[\"mean_z1\"].append(m1)\n        latent_stats[\"std_z1\"].append(s1)\n        latent_stats[\"mean_z2\"].append(m2)\n        latent_stats[\"std_z2\"].append(s2)\n\n        # Visual inspection each epoch\n        plot_latent_hist(z_samples, epoch)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/","title":"Ddpm Forward Diffusion","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/#create-a-simple-letter-image","title":"Create a simple letter image","text":"<pre><code># Image size\nH, W = 64, 64\n\n# Background = -1 (black), foreground = +1 (white)\nx0 = -np.ones((H, W), dtype=np.float32)\n\n# Draw a sample 'T'\nx0[10:54, 30:34] = 1.0  # vertical stroke\nx0[10:14, 18:46] = 1.0  # top stroke\n\n# Plot the image\nplt.imshow(x0);\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/#define-the-diffusion-schedule-beta-alpha","title":"Define the diffusion schedule (beta &amp; alpha)","text":"<pre><code>T = 100 # number of diffusion steps\n\n# IMPORTANT: beta must be in (0, 1)\nbeta = np.linspace(0.01, 0.30, T).astype(np.float32)\n\n# \u03b1 = \u221a(1 \u2212 \u03b2\u00b2)\nalpha = np.sqrt(1.0 - beta**2).astype(np.float32)\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/#step-by-step-forward-diffusion-markov-process","title":"Step-by-step forward diffusion (Markov process)","text":"<pre><code>xt = x0.copy()\nsnapshots = {0: x0.copy()}\n\nkeep_steps = [1, 5, 10, 20, 40, 60, 80, 100]\n\nfor i in range(1, T + 1):\n    b = beta[i - 1]\n    a = alpha[i - 1]\n\n    z = np.random.randn(H, W).astype(np.float32) # z_i ~ N(0, I)\n    xt = a * xt + b * z\n\n    if i in keep_steps:\n        snapshots[i] = xt.copy()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/00_DDPM_Forward_Diffusion/#visualize-how-it-becomes-noise","title":"Visualize how it becomes noise","text":"<pre><code>steps = sorted(snapshots.keys())\n\nplt.figure(figsize=(2.2 * len(steps), 2.2))\nfor k, t in enumerate(steps):\n    ax = plt.subplot(1, len(steps), k + 1)\n    ax.imshow(np.clip(snapshots[t], -1, 1), cmap=\"gray\", vmin=-1, vmax=1)\n    ax.set_title(f\"i={t}\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/","title":"Ddpm Reverse Transition Kernel","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\ndevice = 'cpu'\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#load-a-single-image-from-mnist","title":"Load a single image from MNIST","text":"<pre><code>transform = transforms.Compose([transforms.ToTensor()])\ndataset = datasets.MNIST(root=\"../data\", train=True, download=True, transform=transform)\n\n# Pick a specific image\nx_0, _ = dataset[0]\nx_0 = x_0.to(device)\n\n# Function to show image\ndef show_image(tensor, title=\"\"):\n    img = tensor.cpu().squeeze().numpy()\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(title)\n    plt.axis(\"off\")    \n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#define-the-noise-schedule","title":"Define the Noise Schedule","text":"<pre><code>TIMESTEPS = 200\n\n# Define beta (\u03b2)\nbeta = torch.linspace(0.0001, 0.02, TIMESTEPS).to(device)\n\n# Calculate alpha (\u03b1)\nalpha = 1.0 - beta\n\n# Calculate alpha bar (cumulative product of \u03b1)\nalpha_bar = torch.cumprod(alpha, dim=0)\n\nprint(f\"Schedule created for {TIMESTEPS} timesteps.\")\nprint(f\"Alpha bar at last step: {alpha_bar[-1]:.4f}\")\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#the-forward-diffusion","title":"The Forward Diffusion","text":"<pre><code>def forward_diffusion(x_0, t):\n    \"\"\"\n    Takes clean image x_0 and returns noisy image x_t at timestep t.\n    \"\"\"\n    noise = torch.randn_like(x_0)\n\n    # Gather alpha_bar for timesteps t\n    sqrt_alpha_bar_t = torch.sqrt(alpha_bar[t])\n    sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar[t])\n\n    # The forward process equation\n    x_t = (sqrt_alpha_bar_t * x_0) - (sqrt_one_minus_alpha_bar_t * noise)\n    return x_t\n\n# Let's generate a noisy image at step i = 100\ni_step = 100\nx_i = forward_diffusion(x_0, i_step)\n\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1); show_image(x_0, \"Original (x)\")\nplt.subplot(1, 2, 2); show_image(x_i, f\"Noisy Input (x_i) at step {i_step}\")\nplt.show()\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#the-reverse-process","title":"The Reverse Process","text":"<pre><code>def get_posterior_mean_variance(x_i, x_0, t):\n    \"\"\"\n    Calculates the mean and variance for p(x_{i-1} | x_i, x_0)\n    using the Lemma 2.2.2 formula.\n    \"\"\"\n    # Note: t is the index, so t corresponds to step i\n    # We need t-1 for the previous alpha_bar\n\n    # 1. Gather terms\n    alpha_t = alpha[t]\n    alpha_bar_t = alpha_bar[t]\n    alpha_bar_prev = alpha_bar[t-1] if t &gt; 0 else torch.tensor(1.0).to(device)\n    beta_t = beta[t]\n\n    # 2. Calculate Coefficient 1 (for x_0)\n    # Formula: (sqrt(alpha_bar_prev) * beta_t) / (1 - alpha_bar_t)\n    coeff_1 = (torch.sqrt(alpha_bar_prev) * beta_t) / (1 - alpha_bar_t)\n\n    # 3. Calculate Coefficient 2 (for x_i)\n    # Formula: (sqrt(alpha_t) * (1 - alpha_bar_prev)) / (1 - alpha_bar_t)\n    coeff_2 = (torch.sqrt(alpha_t) * (1 - alpha_bar_prev)) / (1 - alpha_bar_t)\n\n    # 4. Calculate Mean mu\n    mu = (coeff_1 * x_0) + (coeff_2 * x_i)\n\n    # 5. Calculate Posterior Variance (sigma^2)\n    # Formula: ((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * beta_t\n    posterior_variance = ((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * beta_t\n\n    return mu, posterior_variance\n\n# Run the reverse transition\n# We are at step i_step, we want to guess i_step - 1\nmu_prev, var_prev = get_posterior_mean_variance(x_i, x_0, i_step)\n\nprint(\"Posterior Mean calculated.\")\n</code></pre>"},{"location":"deep-learning/deep-learning-from-scratch/%5B7%5D%20Diffusion%20Models/01_DDPM_Reverse_Transition_Kernel/#visualization","title":"Visualization","text":"<pre><code># 1. Start at High Noise (Step 100)\n# We use forward_diffusion to get our starting point\nstart_step = 50\nx_current = forward_diffusion(x_0, start_step)\n\n# Store the starting point for comparison\nx_start_img = x_current.clone()\n\n# 2. Run the Reverse Transition Loop (100 -&gt; 1)\n# We apply the Gaussian formula repeatedly\nprint(f\"Denoising from step {start_step} down to 1...\")\nfor t in range(start_step, 0, -1):\n    # Calculate mean and variance for the previous step (t-1)\n    mu, var = get_posterior_mean_variance(x_current, x_0, t)\n\n    # Sample from the Gaussian distribution\n    noise = torch.randn_like(x_0)\n    x_current = mu + (torch.sqrt(var) * noise)\n\n# 3. Visualization\nplt.figure(figsize=(12, 4))\n\n# Plot 1: The Clean Original (Ground Truth)\nplt.subplot(1, 4, 1)\nshow_image(x_0, \"Original (x)\")\n\n# Plot 2: Where we started (Step 100) - Very Noisy\nplt.subplot(1, 4, 2)\nshow_image(x_start_img, f\"Start: Step {start_step}\\n(High Noise)\")\n\n# Plot 3: The Result of the Reverse Process (Step 1)\nplt.subplot(1, 4, 3)\nshow_image(x_current, f\"End: Step 1\\n(Recovered)\")\n\n# Plot 4: The Difference (Noise Removed)\n# Showing what was subtracted\ndiff = x_start_img - x_current\nplt.subplot(1, 4, 4)\nshow_image(diff, \"Noise Removed\\n(Difference)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/","title":"PyTorch for Deep Learning and Machine Learning","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/","title":"Pytorch Fundamentals","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#pytorch-fundamentals","title":"PyTorch Fundamentals","text":"<p>PyTorch is an open-source machine learning library primarily developed by Facebook's AI Research lab (FAIR). It is widely used for deep learning applications and provides a flexible and efficient platform for building and training neural networks. PyTorch is known for its dynamic computation graph, which allows developers to change the architecture of their models on the fly, making it highly intuitive and user-friendly for research and development.</p> <p></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"qntbaKpECu5K\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724564946179, \"user_tz\": -330, \"elapsed\": 9641, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"79d9257f-3a1f-472d-8acb-26acf2ab160d\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import torch</p> <p>print(torch.version) <pre><code>```python id=\"Qu6bDCxbDBw8\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567650925, \"user_tz\": -330, \"elapsed\": 446, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}}\n# Check the GPU available\n# !nvidia-smi\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#introduction-to-tensors","title":"Introduction to Tensors","text":"<p>A PyTorch Tensor is a multi-dimensional array or matrix that holds elements of a single data type. It is similar to a NumPy array but with additional capabilities that make it more suitable for deep learning and high-performance computing.</p> <p>Here are some key features of a PyTorch Tensor:</p> <ol> <li> <p>Data Storage: Tensors can store data in various dimensions (1D, 2D, 3D, etc.). For example, a 1D tensor is like a vector, a 2D tensor is like a matrix, and higher-dimensional tensors can represent more complex data structures.</p> </li> <li> <p>GPU Acceleration: Unlike NumPy arrays, PyTorch Tensors can be moved to GPU memory, allowing for faster computation, especially in deep learning tasks.</p> </li> <li> <p>Automatic Differentiation: PyTorch provides a feature called Autograd that allows for automatic computation of gradients. This is particularly useful for training neural networks, where backpropagation requires the calculation of gradients.</p> </li> <li> <p>Data Types: PyTorch Tensors support various data types such as <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, etc. The data type can be specified when creating a tensor.</p> </li> <li> <p>Operations: PyTorch provides a wide range of operations that can be performed on Tensors, such as mathematical operations, linear algebra, and random sampling.</p> </li> </ol> <p></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#creating-tensors","title":"Creating Tensors","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#scalar","title":"Scalar","text":"<p>A scalar is a single number or a single value that represents a quantity. In mathematics and computer science, it is a simple, one-dimensional value without any additional structure, like direction or components.</p> <p>Examples of Scalars: - A single integer: <code>5</code> - A floating-point number: <code>3.14</code> - A real number: <code>-2.7</code></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"e_QV8yu7FfPc\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565055696, \"user_tz\": -330, \"elapsed\": 4, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"e705b0c8-21ce-4676-8b27-8796f7be2b87\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#scalar_1","title":"scalar","text":"<p>scalar = torch.tensor(7) scalar <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"nWCccAG8Fm8m\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565070441, \"user_tz\": -330, \"elapsed\": 459, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"431150b6-b472-4358-88f2-3751ace2c57b\"\nscalar.ndim\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"jc8nwQRcFooh\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565091357, \"user_tz\": -330, \"elapsed\": 478, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"12b69a90-3dd8-42cf-f547-09ddc5fc6414\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#get-tensor-back-as-python-list","title":"Get tensor back as Python list","text":"<p>scalar.item() <pre><code>&lt;!-- #region id=\"E2E68vq3Fvix\" --&gt;\n#### **Vector**\nA vector is a mathematical object that has both magnitude (size) and direction. Vectors are used to represent quantities that involve both of these properties, such as velocity, force, and displacement. In PyTorch, a vector can be represented as a 1-dimensional tensor or array.\n\n**Key Characteristics of Vectors:**\n- **Magnitude**: The length or size of the vector.\n- **Direction**: The direction in which the vector is pointing.\n\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"TKR2j3t4GR9i\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565275098, \"user_tz\": -330, \"elapsed\": 436, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"9ab9060a-c1d1-4e96-e515-7b69d423a17a\"\n# vector\nvector = torch.tensor([7, 7])\nvector\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Zd4SRe6SGc7L\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565285070, \"user_tz\": -330, \"elapsed\": 3, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"a6b0c979-7e6e-4cdd-89b6-02c9aaa6c719\" vector.ndim <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IztBO2vHGelS\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565293199, \"user_tz\": -330, \"elapsed\": 456, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"84b1f1a5-d44a-4343-9968-6f6dd8aeaf76\"\nvector.shape\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#matrix","title":"Matrix","text":"<p>A matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. Matrices are fundamental objects in mathematics and are widely used in various fields such as linear algebra, computer graphics, machine learning, and physics.</p> <p>Structure of a Matrix: - Rows and Columns: A matrix is organized into rows (horizontal lines) and columns (vertical lines). - Elements: Each element in a matrix is identified by its position within the matrix, typically denoted as \\(a_{ij}\\), where \\(i\\) is the row index and \\(j\\) is the column index.</p> <p>Example of a 2x3 Matrix: $$ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\end{pmatrix} $$ This is a matrix with 2 rows and 3 columns.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"68aKwdobHMlZ\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565502968, \"user_tz\": -330, \"elapsed\": 470, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"cbf6ed44-8c9c-469b-b5b0-cf19d4b7519b\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#matrix_1","title":"MATRIX","text":"<p>MATRIX = torch.tensor([[3, 4],                        [6, 7]]) MATRIX <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"n1pTufPdHTya\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565510304, \"user_tz\": -330, \"elapsed\": 480, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"aa7aeeba-20dd-4eb0-d84d-eb0eaa62afd3\"\nMATRIX.ndim\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"WS0Tw4nyHVhx\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724565517378, \"user_tz\": -330, \"elapsed\": 3, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"42a47b4a-03a5-4c38-a99a-c57f9379bbdb\" MATRIX.shape <pre><code>&lt;!-- #region id=\"4gASZ-QZHbDi\" --&gt;\n#### **Tensor**\nA tensor is a generalization of scalars, vectors, and matrices to higher dimensions. Tensors are multi-dimensional arrays that can store data in more than two dimensions, making them a fundamental concept in various fields like physics, machine learning, and deep learning. In PyTorch, tensors are implemented as multi-dimensional arrays. PyTorch tensors are similar to NumPy arrays but come with additional features like GPU support and automatic differentiation.\n\n**Key Characteristics of Tensors:**\n- **Dimensions (or Rank)**: The number of indices required to specify an element in the tensor. This is also known as the tensor's rank or order.\n  - **Scalar**: A 0-dimensional tensor (a single number).\n  - **Vector**: A 1-dimensional tensor (a list of numbers).\n  - **Matrix**: A 2-dimensional tensor (a grid of numbers).\n  - **Higher-Dimensional Tensor**: A tensor with three or more dimensions.\n\n\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"RhmLKUNYJI5e\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566104693, \"user_tz\": -330, \"elapsed\": 494, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"aef6a14b-a96c-46f8-e79c-c4ac33e36816\"\n# TENSOR\nTENSOR = torch.tensor([[[2, 3, 4],\n                        [5, 6, 7]],\n\n                       [[8, 9, 6],\n                        [1, 2, 3]]])\nTENSOR\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"_tAqn5KxJmqa\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566118440, \"user_tz\": -330, \"elapsed\": 459, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"7c4e5273-bf98-4a1b-ada7-0b933ae3d429\" TENSOR.ndim <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Ltv8j0UwJqJi\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566128293, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"fb032792-2245-464f-d19e-75f29b074f22\"\nTENSOR.shape\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#random-tensors","title":"Random Tensors","text":"<p>A random tensor in PyTorch is a tensor whose elements are generated randomly according to a specified probability distribution. PyTorch provides several functions to create tensors filled with random values, each drawn from different distributions like uniform, normal (Gaussian), and others.</p> <p>Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.</p> <p><code>Start with random numbers -&gt; look at the data -&gt; update random numbers -&gt; look at the data -&gt; update random numbers</code></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7f2K8WKgKR-9\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566588970, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"99090beb-9f13-4e06-8f2a-967ad3654775\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#create-a-random-tensor-of-size-2-3-4","title":"Create a random tensor of size (2, 3, 4)","text":"<p>random_tensor = torch.rand(2, 3, 4) random_tensor <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"mdpVmCz2Lf0m\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566610485, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"c5c5e9de-738a-4d1b-f482-2da4b46e743f\"\nrandom_tensor.ndim\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"mEIjxTdjLicQ\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566617900, \"user_tz\": -330, \"elapsed\": 472, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"98f3d965-bf54-4690-b2ef-f967be5398e1\" random_tensor.shape <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1mYF_l4sLsp6\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724566834479, \"user_tz\": -330, \"elapsed\": 509, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"4b292b85-9e62-41f8-8998-56c2fb1a549c\"\n# Create a random tensor with similar shape to an image tensor\nrandom_image_size_tensor = torch.rand(size=(3, 224, 224)) # colour channel, height, width\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#zeros-and-ones","title":"Zeros and Ones","text":"<p>In PyTorch, <code>zeros</code> and <code>ones</code> are functions used to create tensors filled entirely with zeros or ones, respectively. These functions are often used to initialize tensors in various machine learning tasks, such as setting up initial weights, biases, or input data.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ek1M-UUpM88S\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567021610, \"user_tz\": -330, \"elapsed\": 456, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"cb78cee2-c0fc-4725-aa7f-38e186fa7e61\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#create-a-tensor-of-all-zeros","title":"Create a tensor of all zeros","text":"<p>zeros = torch.zeros(size=(3, 4)) zeros <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"qa7uwwOnNGws\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567052143, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"f267c342-b35f-4ee9-b14a-5f9a4293bdf8\"\n# Create a tensor of all ones\nones = torch.ones(size=(3, 4))\nones\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"uHVbWz0ANQe9\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567071066, \"user_tz\": -330, \"elapsed\": 3, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"4d8d9761-acfa-45b3-fdc3-e882b4627385\" ones.dtype <pre><code>&lt;!-- #region id=\"40V57OLZOB15\" --&gt;\n### **Creating a range of tensors and tensors-like**\nIn PyTorch, you can create a range of tensors using functions like `torch.arange` and `torch.linspace`, and you can create tensors with the same shape and properties as another tensor using functions like `torch.zeros_like`, `torch.ones_like`, and `torch.full_like`.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ThOnmODWOIM6\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567426892, \"user_tz\": -330, \"elapsed\": 4, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"6c41c6e3-d8e4-4f2f-bea9-30dbd478c6a2\"\n# Use torch.arange()\none_to_ten = torch.arange(start=1, end=11, step=1) # default step is 1\none_to_ten\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ISZ5pZJuOp2-\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724567463067, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"506258f7-ed6b-4611-8ca8-0e79c5228cd5\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/00_pytorch_fundamentals/#creating-tensor-like","title":"Creating tensor like","text":"<p>ten_zeros = torch.zeros_like(input=one_to_ten) ten_zeros <pre><code>&lt;!-- #region id=\"NpBmimE4O_yo\" --&gt;\n### **Tensor Datatypes**\nIn PyTorch, tensors can have different data types (also known as `dtype`), which determine the type of data (e.g., integers, floating-point numbers) that the tensor can hold. The data type of a tensor is crucial for performance and accuracy, especially in computations involving large datasets or deep learning models.\n\n**Common Tensor Data Types in PyTorch:**\n\n1. **Floating-Point Types**:\n   - **`torch.float32`** (`torch.float`): 32-bit floating-point (single precision). This is the most commonly used type for training neural networks.\n   - **`torch.float64`** (`torch.double`): 64-bit floating-point (double precision). Used when more precision is needed.\n   - **`torch.float16`** (`torch.half`): 16-bit floating-point (half precision). Often used in deep learning for reducing memory usage and speeding up computations, particularly on GPUs.\n\n2. **Integer Types**:\n   - **`torch.int8`**: 8-bit signed integer.\n   - **`torch.uint8`**: 8-bit unsigned integer.\n   - **`torch.int16`** (`torch.short`): 16-bit signed integer.\n   - **`torch.int32`** (`torch.int`): 32-bit signed integer. Commonly used for general-purpose integer data.\n   - **`torch.int64`** (`torch.long`): 64-bit signed integer. Often used for indexing and large-range integers.\n\n3. **Boolean Type**:\n   - **`torch.bool`**: Boolean type, where each element is either `True` or `False`.\n\n4. **Complex Types**:\n   - **`torch.complex64`**: 64-bit complex number, with 32 bits for the real part and 32 bits for the imaginary part.\n   - **`torch.complex128`**: 128-bit complex number, with 64 bits for the real part and 64 bits for the imaginary part.\n\n5. **Quantized Types** (used in model quantization):\n   - **`torch.qint8`**: 8-bit signed integer used in quantized models.\n   - **`torch.quint8`**: 8-bit unsigned integer used in quantized models.\n   - **`torch.qint32`**: 32-bit signed integer used in quantized models.\n\n\n\ud83e\udd14**Note:**&lt;br&gt;\nTensor datatypes is one of the 3 big errors you'll run into with PyTorch &amp; Deep Learning:\n   1. Tensors not right datatype\n   2. Tensors not right shape\n   3. Tensors not on the right device\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"S49cfA5PPFN4\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724568117174, \"user_tz\": -330, \"elapsed\": 425, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"78ef8e81-0ce5-4a5a-8fcf-c4082bb64571\"\n# Float 32 tensor\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # what datatype is the tensor (e.g., float32 or float64)\n                               device=None, # What device is your tensor on\n                               requires_grad=False) # Whether or not to track gradients with this tensors operations\nfloat_32_tensor\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"JH-GaQrcRUk6\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724568139395, \"user_tz\": -330, \"elapsed\": 2, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"4d609db7-4e82-40c8-e459-506a4973a2ff\" float_32_tensor.dtype <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GyeNGTGrRXhB\" executionInfo={\"status\": \"ok\", \"timestamp\": 1724568175264, \"user_tz\": -330, \"elapsed\": 3, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}} outputId=\"8013f3bb-988f-4181-d88e-bb54d8f5c55c\"\nfloat_16_tensor = float_32_tensor.type(torch.float16)\nfloat_16_tensor\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/","title":"Pytorch Workflow","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#pytorch-workflow","title":"PyTorch Workflow","text":"<p>A typical workflow for a machine learning project using PyTorch involves several key steps, from data preparation to model deployment. Below is an outline of a common PyTorch workflow:</p> <ol> <li> <p>Get Data Ready (Turn into Tensors):     The first step involves preparing your dataset. This includes loading your data and transforming it into a format that PyTorch can work with, specifically tensors. Tensors are multidimensional arrays that are the basic building blocks in PyTorch, allowing for efficient computation on GPUs.</p> </li> <li> <p>Build or Pick a Pretrained Model (to Suit Your Problem)     In this step, you either build a custom model from scratch or select a pretrained model that fits the task at hand. Pretrained models can be especially useful when working with large, complex datasets like images or text. This step also involves:</p> <ul> <li>Pick a Loss Function &amp; Optimizer: Selecting an appropriate loss function that the model will try to minimize and choosing an optimizer that will update the model parameters during training.</li> <li>Build a Training Loop: Setting up a loop that will iterate over the data in batches, feed it through the model, compute the loss, and adjust the model's parameters to minimize the loss.</li> </ul> </li> <li> <p>Fit the Model to the Data and Make a Prediction     In this phase, the model is trained on the prepared data. The training loop defined earlier is executed, allowing the model to learn from the data by minimizing the loss function. Once the model has been trained, it can make predictions on new, unseen data.</p> </li> <li> <p>Evaluate the Model     After training, the model's performance is assessed on a validation or test dataset. This step determines how well the model has learned and whether it generalizes well to new data. The evaluation results help identify any issues, such as overfitting or underfitting.</p> </li> <li> <p>Improve Through Experimentation     Based on the evaluation results, the model may need to be improved. This could involve experimenting with different model architectures, hyperparameters, or data preprocessing techniques. The goal is to fine-tune the model for better performance.</p> </li> <li> <p>Save and Reload Your Trained Model    Once the model has been trained and evaluated successfully, it is saved to disk. This allows you to reload the model later for further training, fine-tuning, or deployment. Saving the model also ensures that you don't need to retrain it every time you want to make predictions.</p> </li> </ol> <p>This workflow is iterative, meaning that based on the results from the evaluation and improvement steps, you may need to loop back and refine earlier stages.</p> <p></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"WLBC4RYrZXUl\" outputId=\"0fe53de5-ad96-4450-c831-f351611923be\" import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import numpy as np import matplotlib.pyplot as plt</p> <p>plt.rcParams[\"font.family\"] = \"deJavu Serif\" plt.rcParams[\"font.serif\"] = \"Times New Roman\"</p> <p>torch.version <pre><code>&lt;!-- #region id=\"XbbrFOCzZ-05\" --&gt;\n## **1. Data Preparation and Loading**\nData can be almost anything... in machine learning.\n- Excel spreadsheet\n- Images of any kind\n- Videos (YouTube has lots of data)\n- Audio like songs or podcasts\n- DNA\n- Text\n\nMachine learning is a game of two parts:\n1. Get data into a numerical representation.\n2. Build a model to learn patterns in that numerical representation.\n\nTo showcase this, let's create some *known* data using the linear regression formula.\n\nWe'll use a linear regression formula to make a straight line with known **parameters**.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"r6_NuArnaUUj\" outputId=\"70c2a1a2-101a-4c9c-cdfa-45a012e88179\"\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"okOgdLC2cfcS\" outputId=\"94e56b6f-5455-465c-cd30-bf7cb6b25019\" len(X), len(y) <pre><code>&lt;!-- #region id=\"t52dbu_tchz6\" --&gt;\n## **Splitting Data into Training and Test Sets**\n\n(One of the most important concept in machine learning.)\n\nLet's create a training and test set with our data.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"jyAJ_BADheCt\" outputId=\"84ddabf2-2e40-42b2-9d99-120eb1f40719\"\n# Create a train/test split\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</code></pre></p> <p>How might we better visualize our data?</p> <p>This is where the data explorer's motto comes in!</p> <p>\"Visualize, visualize, visualize!\"</p> <p>```python id=\"lmR8SHtGiXfE\" def plot_predictions(train_data=X_train,                      train_labels=y_train,                      test_data=X_test,                      test_labels=y_test,                      predictions=None):     \"\"\"     Plots training data, test data and compares predictions.     \"\"\"     plt.figure(figsize=(8, 5))</p> <pre><code># Plot the training data in plue\nplt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n# Plot the testing data in green\nplt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n# Are there predictions?\nif predictions is not None:\n    # Plot the predictions if they exis\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n# Show the legend\nplt.legend(prop={\"size\": 14})\n</code></pre> <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 445} id=\"RxvTf6Voj24n\" outputId=\"e0faedbf-2a19-484c-928a-7069efa394d2\"\nplot_predictions()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#2-build-the-model","title":"2. Build the Model","text":"<p>Our first PyTorch model!</p> <p>This is very exciting... let's do it!</p> <p>What our model does: - Start with random values (weight &amp; bias) - Look at the training data and adjust the random values to better represent (or get closer to) the ideal values (the weight and bias values we used to create the data)</p> <p>How does it do so?</p> <p>Through two mail algorithms: 1. Gradient descent 2. Backpropagation</p> <p>```python id=\"ZJ7KRSUPkqv8\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#create-linear-regression-model-class","title":"Create linear regression model class","text":"<p>class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch inherits from nn.Module     def init(self):         super().init()         self.weights = nn.Parameter(torch.randn(1, # &lt;- start with a random weight and try to adjust it to the ideal weight                                                 requires_grad=True, # &lt;- can this parameter be updated via gradient descent?                                                 dtype=torch.float)) # &lt;- PyTorch loves the datatype torch.float32</p> <pre><code>    self.bias = nn.Parameter(torch.randn(1, # &lt;- start with a random weight and try to adjust it to the ideal BIAS\n                                         requires_grad=True, # &lt;- can this parameter be updated via gradient descent?\n                                         dtype=torch.float)) # &lt;- PyTorch loves the datatype torch.float32\n\n# Forward method to define the computation in the model\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data\n    return self.weights * x + self.bias # &lt;- this is the linear regression formula\n</code></pre> <p>```  </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#pytorch-model-building-essentials","title":"PyTorch Model Building Essentials","text":"<ul> <li><code>torch.nn</code>: contains all of the buildings for computational graphs (a neural network can be considered as a computational graph)</li> <li><code>torch.nn.Parameter</code>: what parameters should our model try and learn, often a PyTorch layer from <code>torch.nn</code> will set these for us</li> <li><code>torch.nn.Modules</code>: The base class for all neural network modules, if you subclass it, you should overwrite forward()</li> <li><code>torch.optim</code>: this is where the optimizers in PyTorch liv, they will help with gradient descent</li> <li><code>def forward()</code>: All <code>nn.Module</code> subclasses require you to overwrite <code>forward()</code>, this method defines what happens in the forward computation.  See more of these essential modules via the PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html  </li> </ul>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#checking-the-contents-of-a-pytorch-model","title":"Checking the Contents of a PyTorch Model","text":"<p>Now we've created a model, let's see what's inside...  So we can check our model parameters or what's inside our model using <code>.parameters()</code>.   ```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GaqzXBZmIjwW\" outputId=\"e7646bab-aa4e-44d1-b831-01cbd592d899\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#create-a-random-seed","title":"Create a random seed","text":"<p>torch.manual_seed(42) </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#create-an-instance-of-the-model-this-is-a-subclass-of-nnmodule","title":"Create an instance of the model (this is a subclass of nn.Module","text":"<p>model_0 = LinearRegressionModel() </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#check-out-the-parameters","title":"Check out the parameters","text":"<p>list(model_0.parameters()) ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wAXJazW7Jb09\" outputId=\"ed911d6c-c8e0-4850-e81e-675cb9bddcaf\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#list-named-parameters","title":"List named parameters","text":"<p>model_0.state_dict() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"qniuLJvpJn4W\" outputId=\"e3b6ff01-30b4-429c-b795-313bd2a9f2a3\"\nweight, bias\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#making-prediction-using-torchinference_mode","title":"Making Prediction using <code>torch.inference_mode()</code>","text":"<p>To check our model's predictive power, let's see how well it predicts <code>y_test</code> based on <code>x_test</code>.</p> <p>When we pass data through our model, it's going to run it through the <code>forward()</code> method.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"s181nXuTLSlz\" outputId=\"bbc19761-d5bf-4a67-e7b9-c36e626970b3\" y_preds = model_0(X_test) y_preds <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"RWKhKsuJKhuN\" outputId=\"656a01ea-ea6e-49fb-e07e-34be3a162463\"\n# Make predictions with model\nwith torch.inference_mode():\n    y_preds = model_0(X_test)\n\n# You can also do something similar with torch.no_grad(), however, torch.inference_mode() is preferred\nwith torch.no_grad():\n    y_preds = model_0(X_test)\n\ny_preds\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"NNE7_FKvLDH5\" outputId=\"839475a5-d286-41ae-bc7a-75e700ee51f0\" y_test <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 445} id=\"qZwUUhJ9LFJ5\" outputId=\"ed00b61f-7177-4fda-afac-af9b3a5f3268\"\nplot_predictions(predictions=y_preds)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#3-train-model","title":"3. Train Model","text":"<p>The whole idea of training is for a model to move from some unknown parameters (these may be random) to some known parameters.</p> <p>Or in other words from a poor representation of the data to a better representation of the data.</p> <p>One way to measure how poor or how wrong your models predictions are is to use a loss function.</p> <ul> <li>Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.</li> </ul> <p>Things we need to train: * Loss function: A function to measure how wrong your model's predictions are to the ideal outputs, lower is better. * Optimizer: Takes into account the loss of a model and adjusts the model's parameters (e.g., weight and bias) to improve the loss function.</p> <pre><code>* Inside the optimizer you'll often have to set two parameters:\n    - `params` - the model parameters you'd like to optimize, for example params=model_0.parameters()\n\n    - `lr (learning rate)` - the learning rate is a hyperparameter that define how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a larger `lr` results in large changes)\n</code></pre> <p>And specifcally for PyTorch, we need: * A training loop * A testing loop</p> <p>```python id=\"Yenl_ZBGE2p5\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"8217390b-90ab-499f-ef68-4756a2d13a5c\" list(model_0.parameters()) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"KxtOV01D0gIE\" outputId=\"77a7215e-191a-44ec-ae5f-b439cac332a8\"\n# Check out our model's parameters (a parameter is a value that the model sets itself)\nmodel_0.state_dict()\n</code></pre></p> <p>```python id=\"UEbZcQUi0yfc\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#setup-a-loss-function","title":"Setup a loss function","text":"<p>loss_fn = nn.L1Loss()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#setup-an-optimizer-stochastic-gradient-descent","title":"Setup an optimizer (stochastic gradient descent)","text":"<p>optimizer = torch.optim.SGD(params = model_0.parameters(),                             lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set <pre><code>&lt;!-- #region id=\"w0VsXCerT1w2\" --&gt;\n**Q:** Which loss function and optimizer should I use?\n\n**A:** This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.\n\nFor example, for a regression problem (like ours), a loss function of `nn.L1loss()` and an optimizer like `torch.optim.SGD()` will suffice.\n\nBut for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss).\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"O2y8w1ugUwl1\" --&gt;\n### **Building a Training Loop (and a Testing Loop) in PyTorch**\n\nA couple of things we need in a training loop:\n0. Loop through the data\n1. Forward pass (this involves data moving through our model's `forward()` functions) to make predictions on data - also called forward propagation.\n2. Calculate the loss (compare forward pass predictions to ground truth labels)\n3. Optimizer zero grad\n4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropagation**)\n5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"uDPqjngEWc7G\" outputId=\"5a1fbe5f-eac6-431d-b12c-9f3f5c536c16\"\nlist(model_0.parameters())\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zICzYw3UU3RM\" outputId=\"a688c341-b8bf-4b2a-f79c-483555d83e8e\" torch.manual_seed(0)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#an-epoch-is-one-loop-through-the-data","title":"An epoch is one loop through the data","text":"<p>epochs = 100</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#track-different-values","title":"Track different values","text":"<p>epoch_count = [] loss_values = [] test_loss_values = []</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#training","title":"Training","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#0-loop-through-the-data","title":"0. Loop through the data","text":"<p>for epoch in range(epochs):     # Set the model to training mode     model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients</p> <pre><code># 1. Forward pass\ny_pred = model_0(X_train)\n\n# 2. Calculate the loss\nloss = loss_fn(y_train, y_pred)\nprint(f\"Loss: {loss}\")\n\n# 3. Optimizer zero grad\noptimizer.zero_grad()\n\n# 4. Perform backpropagation on the loss with respect to the parameters of the model\nloss.backward()\n\n# 5. Step the optimizer (perform gradient descent)\noptimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above in step 3 for the next iteration of the loop\n\n### Testing\nmodel_0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\nwith torch.inference_mode():\n    # 1. Do the forward pass\n    test_pred = model_0(X_test)\n\n    # 2. Calculate the loss\n    test_loss = loss_fn(test_pred, y_test)\n\n# Print out what's happening\nif epoch % 10 == 0:\n    epoch_count.append(epoch)\n    loss_values.append(loss)\n    test_loss_values.append(test_loss)\n\n    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n    # Print out model state_dict()\n    print(model_0.state_dict())\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"vjIoD7MdiveJ\" outputId=\"b335cded-b353-4edb-829a-3792e93e8ba5\" epoch_count ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"cb2yNx_-h_Qb\" outputId=\"c615a2e1-b456-4eb1-e0c0-25a5d529fff4\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#plot-the-loss-curves","title":"Plot the loss curves","text":"<p>plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\") plt.plot(epoch_count, np.array(test_loss_values), label=\"Test loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend(); <pre><code>```python id=\"SM51R97UdEwR\"\nwith torch.inference_mode():\n    y_preds_new = model_0(X_test)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 445} id=\"rNz82diMdSgQ\" outputId=\"c02141c5-23da-4cb1-810c-25dbab6e1a82\" plot_predictions(predictions=y_preds_new) <pre><code>&lt;!-- #region id=\"i99vpGP1Ndbz\" --&gt;\n## **4. Saving a Model in Pytorch**\nThere are three main methods you should about for saving and loading models in PyTorch.\n1. `torch.save()` - allows you save a PyTorch object in Python's pickle format\n2. `torch.load()` - allows you load a saved PyTorch object\n3. `torch.nn.Module.load_state_dict()` - this allows to load a model's saved state dictionary\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"arwBMkkhNi_J\" outputId=\"31d32aeb-dc0d-4be3-f4fb-7aef5f0dc132\"\n# Saving our PyTorch model\nfrom pathlib import Path\n\n# 1. Create models directory\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path\nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(),\n           f=MODEL_SAVE_PATH)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"rcC9KGwbQGqS\" outputId=\"afaa2a17-6d66-4b46-a727-68f9df16aee3\" !ls -l models <pre><code>&lt;!-- #region id=\"tT5cJfAeQL5s\" --&gt;\n## **5. Loading a PyTorch Model**\nSince we saved our model's `state_dict()` rather than entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Khq9reUgQtgY\" outputId=\"6c0792ca-0f29-4b0f-e786-0738e7821297\"\n# To load in a saved saved state_dict we have to instantiate a new instance of our model class\nloaded_model_0 = LinearRegressionModel()\n\n# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"VWPrPAKeRmI-\" outputId=\"8d26783c-0684-4c32-dc72-d49707919322\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#make-some-preditions-with-our-loaded-model","title":"Make some preditions with our loaded model","text":"<p>loaded_model_0.eval() with torch.no_grad():     loaded_model_preds = loaded_model_0(X_test)</p> <p>loaded_model_preds <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"LuFUFO5nR66Z\" outputId=\"d8a8e206-e4af-4834-b2e6-6d7000714079\"\n# Compare the loaded model preds with original model preds\ny_preds == loaded_model_preds\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"_u5-jN_mSHCS\" outputId=\"e09cbe00-321f-4361-fe89-826ec27ea531\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#make-some-models-preds","title":"Make some models preds","text":"<p>model_0.eval()</p> <p>with torch.inference_mode():     y_preds = model_0(X_test)</p> <p>y_preds <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"8o1e_kOoStA2\" outputId=\"284d7244-bfe5-49bb-bbe1-fd601ea841fb\"\n# Compare the loaded model preds with original model preds\ny_preds == loaded_model_preds\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#6-putting-it-all-together","title":"6. Putting it all together","text":"<p>Let's go back through the steps above and see it all in one place.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"OEUr-0tHX1za\" outputId=\"2a28866f-1ac7-4b98-f595-57f213366697\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#import-pytorch-and-matplotlib","title":"Import PyTorch and matplotlib","text":"<p>import torch from torch import nn import numpy as np import matplotlib.pyplot as plt</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#check-the-pytorch-version","title":"Check the PyTorch version","text":"<p>torch.version <pre><code>&lt;!-- #region id=\"lK_GWffqYXGO\" --&gt;\nCreate device-agnostic code.\n\nThis means if we've got access to a GPU, our code will use it (for potentially faster computing).\n\nIf no GPU is available, the code will default to using CPU.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"C9-6T5IQYyhB\" outputId=\"d4586118-a2eb-42ff-ec87-69c98516cd84\"\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"5NXbg2_3ZSDE\" outputId=\"058aebd1-7207-411d-a2d7-dbb50428f1ba\" !nvidia-smi <pre><code>&lt;!-- #region id=\"TwQ5kfhlZqFF\" --&gt;\n### **6.1 Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"FjuvKOoVZvPW\" outputId=\"0675974d-ad90-4740-bd5d-c40d21fce6b6\"\n# Create some data using the linear regression formula of y = weight * X + bias\nweight = 0.7\nbias = 0.3\n\n# Create X and y (features and labels)\nX = torch.arange(start=0, end=1, step=0.02).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[: 10], y[:10]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"-YTZphXBaTrD\" outputId=\"d9323799-8089-4eb4-8921-4dbf9e45ec65\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#split-the-data","title":"Split the data","text":"<p>train_split = int(len(X) * 0.8) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]</p> <p>len(X_train), len(y_train), len(X_test), len(y_test) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 445} id=\"_C8yGfWQaxyf\" outputId=\"92349562-fa68-4d2c-ad0e-4ebf035a5f09\"\n# Plot the data\n# Note: if you don't have the plot_pedictions() function loaded, this will error\nplot_predictions(X_train, y_train, X_test, y_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#62-building-a-pytorch-linear-model","title":"6.2 Building a PyTorch Linear Model","text":"<p>```python id=\"AKoEkF1YbaME\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"ece76aa6-ce01-464b-ea72-07949cb06f81\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#create-a-linear-model-by-subclassing-nnmodule","title":"Create a linear model by subclassing nn.Module","text":"<p>class LinearRegression(nn.Module):     def init(self):         super().init()</p> <pre><code>    # Use nn.Linear() for creating the model parameters / also called: linear transform, probing layer, fully connected layer, dense layer\n    self.linear_layer = nn.Linear(in_features=1, out_features=1)\n\ndef forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n    return self.linear_layer(X)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#set-the-manual-seed","title":"Set the manual seed","text":"<p>torch.manual_seed(42) model_1 = LinearRegression() model_1, model_1.state_dict() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"-ty1bOA_msT7\" outputId=\"366452e5-3209-4180-de32-615c1cbaf86a\"\n# Check the model current device\nnext(model_1.parameters()).device\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"QBCA6tBRnaCf\" outputId=\"335d28f8-6749-46da-e8ee-042fb180df00\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#set-the-model-to-use-the-target-device","title":"Set the model to use the target device","text":"<p>model_1.to(device) next(model_1.parameters()).device <pre><code>&lt;!-- #region id=\"LMjUyylMnmgR\" --&gt;\n### **6.3 Training**\nFor training we need:\n* Loss function\n* Optimizer\n* Training Loop\n* Testing Loop\n&lt;!-- #endregion --&gt;\n\n```python id=\"Ri2MnVZCn2Ir\"\n# Setup the loss function\nloss_fn = nn.L1Loss()\n\n# Setup our optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(),\n                            lr=0.01)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"-DTU_Dl1oRQt\" outputId=\"cb5b7b56-f5c4-465a-e85b-09fab5f5fef5\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#lets-write-a-training-loop","title":"Let's write a training loop","text":"<p>torch.manual_seed(42)</p> <p>epochs = 200</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#put-data-on-the-target-device-device-agnostic-code-for-data","title":"Put data on the target device (device agnostic code for data)","text":"<p>X_train = X_train.to(device) y_train = y_train.to(device) X_test = X_test.to(device) y_test = y_test.to(device)</p> <p>for epoch in range(epochs):     # Set the model in the training mode     model_1.train()</p> <pre><code># 1. Forward pass\ny_pred = model_1(X_train)\n\n# 2. Calculate the loss\nloss = loss_fn(y_train, y_pred)\n\n# 3. Optimizer zero grad\noptimizer.zero_grad()\n\n# 4. Perform backpropagation\nloss.backward()\n\n# 5. Perform gradient descent\noptimizer.step()\n\n### Testing\nmodel_1.eval()\n\nwith torch.inference_mode():\n    test_pred = model_1(X_test)\n\n    test_loss = loss_fn(test_pred, y_test)\n\n# Print out what's happening\nif epoch % 10 == 0:\n    print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"-pJdlbZXrisE\" outputId=\"844166aa-a06c-4f9f-8b80-69c554322719\" model_1.state_dict() ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zsnOtj5krnHU\" outputId=\"d6aa7027-c16d-4f88-c9ac-ea856b5a483b\" weight, bias <pre><code>&lt;!-- #region id=\"eR2iUrb3rpNt\" --&gt;\n### **6.4 Making and Evaluating Predictions**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"WxwBoIpmsGrg\" outputId=\"d65daf7e-5db5-4856-fd68-135148a7f8da\"\n# Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\n\ny_preds\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 445} id=\"lOw-c0VHsdbT\" outputId=\"cfcac21f-e60f-473d-c5a9-1f2cde8ee11e\" plot_predictions(predictions=y_preds.cpu()) <pre><code>&lt;!-- #region id=\"Dkrvr_8GsuLM\" --&gt;\n### **6.5 Saving and Loading a Trained Model**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"A2xXgctTs1qN\" outputId=\"225bb41e-f11b-4cc3-9b36-49a60842d6b8\"\nfrom pathlib import Path\n\n# 1. Create models directory\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path\nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(),\n           f=MODEL_SAVE_PATH)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zAmF0iaotyut\" outputId=\"af64f4cf-355d-4b85-968e-846954062547\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#load-a-pytorch-model","title":"Load a PyTorch model","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#create-a-new-instance-of-linear-regression-model","title":"Create a new instance of linear regression model","text":"<p>loaded_model_1 = LinearRegression()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#load-the-saved-model_1-state-dict","title":"Load the saved model_1 state dict","text":"<p>loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH, weights_only=True))</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/01_pytorch_workflow/#put-the-loaded-model-to-device","title":"Put the loaded model to device","text":"<p>loaded_model_1.to(device) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Ed55DLAruXrB\" outputId=\"7694abfc-c1e6-4c98-95ac-8c033c85dc8e\"\nnext(loaded_model_1.parameters()).device\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9n9mO1fFur5k\" outputId=\"4060c6cc-1fd9-46f5-a659-4533b4a250e5\" loaded_model_1.state_dict() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"j8LPm5DmuuPy\" outputId=\"f91fe6c4-b222-4bf9-9438-9348221a70ae\"\n# Evaluate the loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\n\ny_preds == loaded_model_1_preds\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/","title":"Pytorch Classification","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#neural-network-classification-with-pytorch","title":"Neural Network Classification with PyTorch","text":"<p>Classification is a problem of predicting whether something is one thing or another (there can be multiple things as the outputs).</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"cAUzVso-dz0_\" outputId=\"7c4acda5-938b-4a28-f578-81c068b97444\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split import torch from torch import nn import requests from pathlib import Path</p> <p>plt.rcParams[\"font.family\"] = \"DeJavu Serif\" plt.rcParams[\"font.serif\"] = \"Times New Roman\"</p> <p>import warnings warnings.filterwarnings(\"ignore\")</p> <p>torch.version <pre><code>&lt;!-- #region id=\"z1Xfvor2dC2r\" --&gt;\n## **1. Make Classification Data and Get Ready**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9_e-EL06eUyr\" outputId=\"567fba31-0331-487f-f08b-fcd7df8e2e40\"\n# Make 1000 samples\nn_samples =1000\n\n# Create circles\nX, y = make_circles(n_samples=n_samples,\n                    noise=0.03,\n                    random_state=42)\n\nlen(X), len(y)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"PsgpYT4BexaG\" outputId=\"8d00e215-da19-46ad-8393-0510f455c093\" print(f\"First 5 samples of X:\\n {X[:5]}\") print(f\"First 5 samples of y:\\n {y[:5]}\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"OQ0fMsQjfBZP\" outputId=\"a35f94bf-36ae-4239-93c7-482c786c4695\"\n# Make DataFrame of circle data\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n                        \"X2\": X[:, 1],\n                        \"label\": y})\nprint(circles.shape)\ncircles.head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} id=\"h1h7t7UKfvkl\" outputId=\"7a83f37f-a9df-44ee-9fa4-ff534ccf79e5\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#visualize-visualize-visualize","title":"Visualize, visualize, visualize","text":"<p>sns.scatterplot(data=circles, x=\"X1\", y=\"X2\", hue=\"label\"); <pre><code>&lt;!-- #region id=\"2CyE1XD7g57V\" --&gt;\n**Note:** The data we're working is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamentals.\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"vHNI0O2mhLzv\" --&gt;\n### **1.1 Check Input and Output Shapes**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"e708iA00hUUp\" outputId=\"546b0a5a-3f10-4910-9022-8d7745739cd7\"\nX.shape, y.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"NX2Alu8BhXWA\" outputId=\"d3ca74f8-e0f7-4b45-e512-1475a1a85706\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#view-the-first-sample-example-features-and-labels","title":"View the first sample example features and labels","text":"<p>X_sample = X[0] y_sample = y[0]</p> <p>print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\") print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\") <pre><code>&lt;!-- #region id=\"sZ0B5xzwh552\" --&gt;\n### **1.2 Turn Data into Tensors and Create Train and Test Splits**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"HLtdvlJ9iCeN\" outputId=\"57a09dca-2ebc-4373-b203-f39eaa8a7d15\"\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\nX[:5], y[:5]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"UXeCPLJUidr_\" outputId=\"8a7e370d-c875-4bef-9cc8-121db057ad53\" type(X), X.dtype, y.dtype <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"MJND7cB1iou4\" outputId=\"87a1771d-52b4-4f04-a139-f02e99c9939e\"\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=42)\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#2-building-a-model","title":"2. Building a Model","text":"<p>Let's build a model to classify our blue and orange dots.</p> <p>To do so, we want to: 1. Setup device agnostic code so our code will run on an accelerator (GPU) if there is one 2. Construct a model (by subclassing <code>nn.Module</code>) 3. Define a loss function and optimizer 4. Create a training and test loop</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"-HsXsKZNj7B7\" outputId=\"f20f09b6-70cd-4785-9e29-f455c1adb4d4\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#make-device-agnostic-code","title":"Make device agnostic code","text":"<p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device <pre><code>&lt;!-- #region id=\"NcYbokQfkQU9\" --&gt;\nNow we've setup device agnostic code, let's create a model that:\n1. Subclasses `nn.Module` (almost all models in PyTorch subclass `nn.Module`)\n2. Create 2 `nn.Linear()` layers that are capable of handling that shapes of our data.\n3. Defines a `foward()` method that outlines the forward pass (or forward computation) of the model\n4. Instantiate an instance of our model class and send it to the target `device`\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"AIcAMaVzlMfD\" outputId=\"3562e5ad-4b27-4de1-b865-e8bf138a5573\"\n# 1. Construct a model that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handing the shapes of our data\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single features (same shape as y)\n\n        # self.two_linear_layers = nn.Sequential(\n        #     nn.Linear(in_features=2, out_features=5),\n        #     nn.Linear(in_features=5, out_features=1)\n        # )\n\n    # 3. Define a forward() method that outlines the forward pass\n    def forward(self, X):\n        return self.layer_2(self.layer_1(X)) # X -&gt; layer_1 -&gt; layer_2 -&gt; output\n        # return self.two_linear_layers(X)\n\n# 4. Instantiate an instance of our model class and send it to the target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"uAFz8UVmnG5W\" outputId=\"9d0c91dc-6bfc-494b-86ef-c02ecf53ae3d\" device <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"t0q3up4JoVwB\" outputId=\"74f1dfab-4e61-441e-fd61-3d4c26375e2e\"\nnext(model_0.parameters()).device\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"OvUGh9NlocHy\" outputId=\"ec2ee29b-c5ea-4e9d-99a5-bc0c71efdb0e\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#lets-replicate-the-model-above-using-nnsequential","title":"Let's replicate the model above using nn.Sequential()","text":"<p>model_0 = nn.Sequential(     nn.Linear(in_features=2, out_features=5),     nn.Linear(in_features=5, out_features=1) ).to(device)</p> <p>model_0 <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"X03vbWwJplWC\" outputId=\"be4aba39-7169-42a9-8351-0b9d533fc8b0\"\nmodel_0.state_dict()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cFy0-wp-p55D\" outputId=\"856c58ca-cc91-48fb-db6c-5e03d1a7dadf\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#make-predictions","title":"Make predictions","text":"<p>with torch.inference_mode():     untrained_preds = model_0(X_test.to(device)) print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\") print(f\"Length of test samples: {len(X_test)}, Shape: {X_test.shape}\") print(f\"\\nFirst 10 predictions: \\n{torch.round(untrained_preds[:10])}\") print(f\"\\nFirst 10 labels:\\n{y_test[:10]}\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"4rwJc-c_q06s\" outputId=\"3fd8dd92-4695-44ba-dfb7-6b2bae14bedc\"\nX_test[:10], y_test[:10]\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#21-setup-loss-functions-and-optimizer","title":"2.1 Setup Loss Functions and Optimizer","text":"<p>Which loss function or optimizer should you use?</p> <p>Again... this is problem specific.</p> <p>For example for regression you might want MAE or MSE (Mean Absolute Error or Mean Squared Error).</p> <p>For classification you might want binary cross entropy or categorical cross entropy (cross entropy).</p> <p>As a reminder, the loss function measures how wrong your models predictions are.</p> <p>And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in optins.</p> <ul> <li>For the loss function we're going to use <code>totch.nn.BCEWithLogitLoss()</code>.</li> <li>For different optimizers see <code>torch.optim</code></li> </ul> <p>```python id=\"yVqmpVaxrXOu\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#setup-the-loss-function","title":"Setup the loss function","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#loss_fn-nnbceloss-bceloss-requires-input-to-have-gone-through-the-sigmoid-activation-function-prior-to-input-to-bceloss","title":"loss_fn = nn.BCELoss() # BCELoss = requires input to have gone through the sigmoid  activation function prior to input to BCELoss","text":"<p>loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitloss = sigmoid activation function built-in</p> <p>optimizer = torch.optim.SGD(params=model_0.parameters(),                             lr=0.1) <pre><code>```python id=\"OFYy-JrWvgdl\"\n# Calculate accuracy - out of 100 examples, what percentage does our model get right?\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item()\n    acc = (correct / len(y_pred)) * 100\n    return acc\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#3-train-model","title":"3. Train Model","text":"<p>To train our model, we're going to need to build a training loop:</p> <ol> <li>Forward pass</li> <li>Calculate the loss</li> <li>Optimizer zero grad</li> <li>Loss backward (backpropagation)</li> <li>Optimizer step (gradient descent)</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#31-going-from-raw-logits-prediction-probabilities-prediction-labels","title":"3.1 Going from raw logits -&gt; prediction probabilities -&gt; prediction labels","text":"<p>Our model outputs are going to be raw logits.</p> <p>We can convert these logits into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary classification and softmax for multiclass classification).</p> <p>Then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the <code>argmax()</code>.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"CZUxJoWfwwN9\" outputId=\"d630f153-af15-4247-97d2-0e98003dbf53\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#view-the-first-5-outputs-of-the-forward-pass-on-the-test-data","title":"View the first 5 outputs of the forward pass on the test data","text":"<p>model_0.eval() with torch.inference_mode():     y_logits = model_0(X_test.to(device))[:5]</p> <p>y_logits <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"BuvIH87Xx6wd\" outputId=\"62826887-5964-4d5d-ec23-88eba045685d\"\ny_test[:5]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"W-WLTSJax9qN\" outputId=\"1a056582-7075-4730-fdcc-d47f89804730\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#use-the-sigmoid-activation-function-on-our-model-logits-to-turn-them-into-prediction-probabilities","title":"Use the sigmoid activation function on our model logits to turn them into prediction probabilities","text":"<p>y_pred_probs = torch.sigmoid(y_logits) y_pred_probs <pre><code>&lt;!-- #region id=\"S8MfLO16yWBx\" --&gt;\nFor our prediction probability values, we need to perform a range-style rounding on them:\n* `y_pred_probs` &gt;= 0.5, `y=1` (class 1)\n* `y_pred_probs` &lt; 0.5, `y=0` (class 0)\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"WROvVHmByPA3\" outputId=\"9a8fd874-418b-4e3a-ac39-d247ec4aca3c\"\n# Find the predicted labels\ny_preds = torch.round(y_pred_probs)\n\n# In full (logits -&gt; pred probs -&gt; pred labels)\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"EuJB4U1ezq_h\" outputId=\"5ba4823f-65b2-4ea7-bc53-4a0846044157\" y_test[:5] <pre><code>&lt;!-- #region id=\"ssuG4DFcnv2Q\" --&gt;\n### **3.2 Building a Training and Testing Loop**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Lj2dxp1Hn6_R\" outputId=\"9de4ecbf-4c52-4124-d306-b44c4c66d633\"\n# Set the manual seed\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put the data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass\n    y_logits = model_0(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labels\n\n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input\n    #                y_train)\n    loss = loss_fn(y_logits, # nn.BCEWithLogitLoss expects raw logits as input\n                   y_train)\n    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward (backpropagation)\n    loss.backward()\n\n    # 5. Optimizer step (gradient descent)\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze()\n        test_pred = torch.round(torch.sigmoid(test_logits))\n\n        # 2. Calculate test loss/acc\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n        # Print out what's happenin\n        if epoch % 10 == 0:\n            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#4-make-predictions-and-evaluate-the-model","title":"4. Make Predictions and Evaluate the Model","text":"<p>From the metrics it looks like our model isn't learning anything...</p> <p>So to inspect it let's make some predictions and make them visual!</p> <p>In othre words, \"Visualize, visualize, visualize!\"</p> <p>To do so, we're going to import a function called <code>plot_decision_boundary()</code></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wKTvOIZbpah6\" outputId=\"2682afec-8169-41b3-a862-584ab793d6c1\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#download-helper-functions-from-learn-pytorch-repo-if-its-not-already-downloaded","title":"Download helper functions from Learn PyTorch repo (if it's not already downloaded)","text":"<p>if Path(\"helper_functions.py\").is_file():     print(\"helper_functions.py already exists, skipping download\") else:     print(\"Downloading helper_functions.py\")     request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")     with open(\"helper_functions.py\", \"wb\") as f:         f.write(request.content)</p> <p>from helper_functions import plot_predictions, plot_decision_boundary <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 544} id=\"jt5h1qfitBWL\" outputId=\"5e0509f5-7b9d-43fa-f000-f2ea5caa4e44\"\n# Plot the decision boundary of the model\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#5-improving-a-model-from-a-model-perspective","title":"5. Improving a Model (from a Model Perspective)","text":"<ul> <li>Add more layers - give the model more chances to learn about patterns in the data</li> <li>Add more hidden units - go from 5 hidden units to 10 hidden units</li> <li>Fit for longer</li> <li>Changing the activation functions</li> <li>Change the learning rate</li> <li>Change the loss function</li> </ul> <p>These options are all from a model's perspective because they deal directly with the model, rather than the data.</p> <p>And beacuse these options are all values we (as machine learning engineers and data scientists) can change, they are referred as hyperparameters.</p> <p>Let's try and improve our model by: * Adding more hidden units: 5 -&gt; 10 * Increase the number of layers: 2 -&gt; 3 * Increase the number of epochs: 100 -&gt; 1000</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"UcG-bMtvvqjO\" outputId=\"17551b92-bd4c-48db-9023-b50b75048000\" class CircleModelV1(nn.Module):     def init(self):         super().init()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10)         self.layer_3 = nn.Linear(in_features=10, out_features=1)</p> <pre><code>def forward(self, X):\n    # z = self.layer_1(X)\n    # z = self.layer_2(z)\n    # z = self.layer_3(z)\n    return self.layer_3(self.layer_2(self.layer_1(X))) # this way of writing operations leverages speed ups where possible behind the scene\n</code></pre> <p>model_1 = CircleModelV1().to(device) model_1 <pre><code>```python id=\"eTVkJVXmxwi-\"\n# Create a loss function\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2um5kRcUyGBA\" outputId=\"c6a3dabe-3e46-403b-b631-4163c6ff4a66\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#write-a-training-and-evaluation-loop-for-model_1","title":"write a training and evaluation loop for model_1","text":"<p>epochs = 1000</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#put-data-on-the-target-device","title":"Put data on the target device","text":"<p>X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)</p> <p>for epoch in range(epochs):     ### Training     model_1.train() # Set the model in training model</p> <pre><code># 1. Forward pass\ny_logits = model_1(X_train).squeeze()\ny_pred = torch.round(torch.sigmoid(y_logits))\n\n# 2. Calculate the loss\nloss = loss_fn(y_logits,\n               y_train)\nacc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n\n# 3. Optimizer zero grad\noptimizer.zero_grad()\n\n# 4. Perform backpropagation\nloss.backward()\n\n# 5. Perform gradient descent\noptimizer.step()\n\n### Testing\nwith torch.inference_mode():\n    model_1.eval() # Set the model the the eval mode\n\n    # Forward pass\n    test_logits = model_1(X_test).squeeze()\n    test_preds = torch.round(torch.sigmoid(test_logits))\n\n    # Calculate test loss/acc\n    test_loss = loss_fn(test_logits,\n                        y_test)\n    test_acc = accuracy_fn(y_true=y_test, y_pred=test_preds)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 544} id=\"P6rpQtiO3JXT\" outputId=\"4109177a-8c26-4af6-ec10-24f3f1750f35\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#plot-the-decision-boundary-of-the-model","title":"Plot the decision boundary of the model","text":"<p>plt.figure(figsize=(12, 6))  plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train)  plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_1, X_test, y_test) ```</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#51-preparing-data-to-see-if-our-model-can-fit-a-straight-line","title":"5.1 Preparing Data to See if Our Model can Fit a Straight Line","text":"<p>One way to troubleshoot to a larger problem is to test out a smaller problem.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xHjua31I39PY\" outputId=\"db265a3c-d8b8-4ca8-bd36-f00ffa3ca77e\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-some-data","title":"Create some data","text":"<p>weight = 0.7 bias = 0.3</p> <p>start = 0 end = 1 step = 0.01</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-data","title":"Create data","text":"<p>X_regression = torch.arange(start, end, step).unsqueeze(dim=1) y_regression = weight * X_regression + bias # Linear regression formula (without epsilon)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#check-the-data","title":"Check the data","text":"<p>print(len(X_regression)) X_regression[:5], y_regression[:5] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IuiR77zr47nv\" outputId=\"c0cd7a81-14e0-4133-b277-01449994f238\"\n# Create train and test splits\ntrain_split = int(len(X_regression) * 0.8)\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each\nlen(X_train_regression), len(y_train_regression), len(X_test_regression), len(y_test_regression)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 599} id=\"MsxlF8Ki56rn\" outputId=\"9b9127fd-aa31-4482-b297-ad0ee1f448ea\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#plot-the-data","title":"Plot the data","text":"<p>plot_predictions(train_data=X_train_regression,                  train_labels=y_train_regression,                  test_data=X_test_regression,                  test_labels=y_test_regression) <pre><code>&lt;!-- #region id=\"sbcj-c_76ejZ\" --&gt;\n### **5.2 Adjusting `model_1` to Fit a Straight Line**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"nOQUDPq466-l\" outputId=\"e113d9e9-20b9-417e-878b-9b85ae944048\"\n# Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</code></pre></p> <p>```python id=\"pIbPEuHw7ZB5\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#loss-and-optimizer","title":"Loss and Optimizer","text":"<p>loss_fn = nn.L1Loss() # MAE loss with regression data optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.01) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"_2r4dHD97n7J\" outputId=\"cc67b360-1cb1-4ce8-e349-a69d98833148\"\n# Train the model\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put the data on the target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\n### Training\nfor epoch in range(epochs):\n    model_2.train()\n\n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n\n    # 2. Calculate the loss\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Perform backpropagation\n    loss.backward()\n\n    # 5. Perform gradient descent\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n\n    with torch.inference_mode():\n        test_preds = model_2(X_test_regression)\n\n    test_loss = loss_fn(test_preds, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.4f} | Test Loss: {test_loss:.4f}\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 599} id=\"LF9-_iG5-0VY\" outputId=\"79d24c6f-2d3d-4f17-9789-e7f1671dada0\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#turn-on-evaluation-mode","title":"Turn on evaluation mode","text":"<p>model_2.eval()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#make-predictions-inference","title":"Make predictions (inference)","text":"<p>with torch.inference_mode():     y_preds = model_2(X_test_regression)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#plot-data-and-predictions","title":"Plot data and predictions","text":"<p>plot_predictions(train_data=X_train_regression.cpu(),                  train_labels=y_train_regression.cpu(),                  test_data=X_test_regression.cpu(),                  test_labels=y_test_regression.cpu(),                  predictions=y_preds.cpu()) <pre><code>&lt;!-- #region id=\"R-IC5X96idrS\" --&gt;\n## **6. The Missing Piece: Non-Linearity**\n\n\"What patterns could you draw if you were given an infinite amount of a straight and non-straight lines?\"\n\nOr in machine learning terms, an infinite (but really it is finite) or linear and non-linear functions?\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"XloUXWUAxjMF\" --&gt;\n### **6.1 Recreating Non-Linear Data (Orange and Blue Circles)**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"r48X73nfxsp2\" outputId=\"d43184ea-c1ed-462d-a1f3-ebae53e04bad\"\n# Make and plot data\nn_samples = 1000\n\nX, y = make_circles(n_samples,\n                    noise=0.03,\n                    random_state=42)\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"uN9sDBZzyKZB\" outputId=\"2432a0ae-3766-42e9-e9e6-4b652b01f702\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#convert-data-to-tensors-and-then-to-train-and-test-splits","title":"Convert data to tensors and then to train and test splits","text":"<p>X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)</p> <p>X_train, X_test, y_train, y_test = train_test_split(X,                                                     y,                                                     test_size=0.2,                                                     random_state=42)</p> <p>X_train[:5], y_train[:5] <pre><code>&lt;!-- #region id=\"YHNwEng7y8It\" --&gt;\n### **6.2 Building a Model with Non-Linearity**\n* Linear = Straight Line\n* Non-Linear = Non-Straight Lines\n\nArtifical neural networks are a large combination of linear (straight) amd non-straight (non-linear) functions which are potentiallly able to find patterns in the data.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"X19rKkr8zdO7\" outputId=\"04623d65-3b7e-4403-ecfa-a03920f76a6a\"\n# Build a model with non-linear activation function\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # relu is a non-linear activation function\n\n    def forward(self, X):\n        # Where should we put our non-linear activation functions?\n        return self.layer_3(self.relu(self.layer_2(self.layer_1(X))))\n\nmodel_3 = CircleModelV2().to(device)\nmodel_3\n</code></pre></p> <p>```python id=\"p4xe0Hj80yTl\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#setup-loss-and-optimizer","title":"Setup loss and optimizer","text":"<p>loss_fn = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1) <pre><code>&lt;!-- #region id=\"uAr7yTSI2PtP\" --&gt;\n### **6.3 Train a Model with Non-Linearity**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"YX0kIvep2W-4\" outputId=\"219e5d00-4351-47c2-dcab-b676b4d366c9\"\n# Random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Put all the data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Loop through the data\nepochs = 1000\n\nfor epoch in range(epochs):\n    ### Training\n    model_3.train()\n\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits))\n\n    # 2. Calculate the loss\n    loss = loss_fn(y_logits,\n                   y_train)\n\n    acc = accuracy_fn(y_train, y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Perform backpropagation\n    loss.backward()\n\n    # 5. Perform gradient descent\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n        test_logits = model_3(X_test).squeeze()\n        test_preds = torch.round(torch.sigmoid(test_logits))\n\n    test_loss = loss_fn(test_logits, y_test)\n    test_acc = accuracy_fn(y_test, test_preds)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.4f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#64-evaluating-a-model-trained-with-non-linear-activation-functions","title":"6.4 Evaluating a Model Trained with Non-Linear Activation Functions","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Y-TW5j196-Jo\" outputId=\"6dadce69-d778-4435-d168-1c52126c42c1\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#make-predictions_1","title":"Make predictions","text":"<p>model_3.eval() with torch.inference_mode():     y_preds = torch.round(torch.sigmoid(model_3(X_test).squeeze()))</p> <p>y_preds[:10], y_test[:10] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 544} id=\"_F-qSSnm7Ttz\" outputId=\"1a2768b3-877f-480e-98e3-7e4b0e53a01f\"\n# Plot the decision boundaries\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_3, X_train, y_train)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#7-relicating-non-linear-activation-functions","title":"7. Relicating Non-Linear Activation Functions","text":"<p>Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in the data and it tries to figure out the patterns on its own.</p> <p>And these tools are linear &amp; non-linear functions.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"g1nyY6bL8Gxp\" outputId=\"68b7ca5d-0093-4c06-9337-506ed9ee0388\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-a-tensor","title":"Create a tensor","text":"<p>A = torch.arange(-10, 10, 1, dtype=torch.float32) A.dtype <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"3vBuE4pH8nCT\" outputId=\"3d6a91b0-cfef-44fa-8651-8495a9bb2a60\"\n# Visualize the tensor\nplt.plot(A);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"MwrBBp9f8rgE\" outputId=\"edcb96de-1346-4f2e-80fb-762298a1abed\" plt.plot(torch.relu(A)); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ahgMxlBM9CLu\" outputId=\"e48ae914-48b1-4169-9061-b6c3fa89d1df\"\ndef relu(X: torch.Tensor) -&gt; torch.Tensor:\n    return torch.maximum(torch.tensor(0), X) # input must be a tensor\n\nrelu(A)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"EYoz6X6S9SKo\" outputId=\"d4c6a5ca-1734-4375-b8cb-480af8d01de6\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#plot-relu-activation-function","title":"Plot ReLU activation function","text":"<p>plt.plot(relu(A)); <pre><code>```python id=\"N5OgiYVK9a1q\"\n# Now let's do the same for sigmoid\ndef sigmoid(X: torch.Tensor) -&gt; torch.Tensor:\n    return 1 / (1 + torch.exp(-X))\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"V1jb1n6I9vkj\" outputId=\"f9dc3f85-5d31-48fb-b395-c854fa8ee027\" plt.plot(torch.sigmoid(A)); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"_CkrTjs2901M\" outputId=\"c03e700a-441d-4f4a-9e49-0918dc89f783\"\nplt.plot(sigmoid(A));\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#8-putting-it-all-together-with-a-multi-class-classification-problem","title":"8. Putting it all Together with a Multi-Class Classification Problem","text":"<ul> <li>Binary Classification = One thing or another (cat vs. dog, spam vs. not spam, fraud or not fraud)</li> <li>Multi-class classification = More than one thing or another (cat vs. dog vs. chicken)</li> </ul>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#81-creating-a-toy-multi-class-dataset","title":"8.1 Creating a Toy Multi-Class Dataset","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"gvtgci1Kq1kh\" outputId=\"4d12c320-21a9-4b01-f8e4-263b5026cb36\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#import-dependencies","title":"Import dependencies","text":"<p>import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split import torch from torch import nn</p> <p>plt.rcParams[\"font.family\"] = \"DeJavu Serif\" plt.rcParams[\"font.serif\"] = \"Times New Roman\"</p> <p>torch.version <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 465} id=\"beyq0_bgrm7W\" outputId=\"1093ecc7-809f-405f-fb10-03ed56cdccea\"\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n                            n_features=NUM_FEATURES,\n                            centers=NUM_CLASSES,\n                            cluster_std=1.5, # give the clusters a little shake up\n                            random_state=RANDOM_SEED)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n\n# 3. Split the data into train and test\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n                                                                        y_blob,\n                                                                        test_size=0.2,\n                                                                        random_state=RANDOM_SEED)\n\n# 4. Plot data (visualize, visualize, visualize)\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x=X_blob[:, 0], y=X_blob[:, 1], hue=y_blob, palette=\"Set1\", edgecolor=\"k\");\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#82-building-a-multi-class-classification-model-in-pytorch","title":"8.2 Building a Multi-Class Classification Model in PyTorch","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"2PupUWRwrlov\" outputId=\"b0a7f2ae-77f2-48b5-9195-ec9e2f371988\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-device-agnostic-code","title":"Create device agnostic code","text":"<p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"o6JHL6C4ugw2\" outputId=\"df4ea65d-0f6a-42bd-f4e5-35c6525e1597\"\n# Build a multi-class classification model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"Initializes multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model\n            output_features (int): Number of output features (number of output classes)\n            hidden units (int): Number of hidden units between layers, default 8\n\n        Returns:\n\n        Example:\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_features)\n        )\n\n    def forward(self, X):\n        return self.linear_layer_stack(X)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=2,\n                    output_features=4,\n                    hidden_units=8).to(device)\n\nmodel_4\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#83-create-a-loss-function-and-optimizer-for-a-multi-class-classification-model","title":"8.3 Create a Loss Function and Optimizer for a Multi-Class Classification Model","text":"<p>```python id=\"sG8UD6rGxlo5\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-a-loss-function-for-multi-class-classification","title":"Create a loss function for multi-class classification","text":"<p>loss_fn = nn.CrossEntropyLoss()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#create-an-optimizer-for-multi-class-classification","title":"Create an optimizer for multi-class classification","text":"<p>optimizer = torch.optim.SGD(params=model_4.parameters(),                             lr=0.1) # learning rate is a hyperparameters you can change <pre><code>&lt;!-- #region id=\"PCh0fj2MyA5U\" --&gt;\n### **8.4 Getting Prediction Probabilities for a Multi-Class Pytorch Model**\n\nIn order to evaluate and train and test our model, we need to convert our model's output (logits) to prediction probabilties and then to predcition labels.\n\nLogits (raw output of the model) -&gt; Pred probs (use torch.softmax) -&gt; Pred labels (take the argmax of the prediction probabilities)\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xxwcG8DJyPZV\" outputId=\"e696b272-3f59-44df-c650-5aeb1e3ed621\"\n# Put the data on the target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\n# Let's get some raw outputs of our model (logits)\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\ny_logits[:10]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"tgUVg1A5y_BT\" outputId=\"9acbf7fa-c65d-42da-900c-947c354d25c1\" y_blob_test[:10] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"lPYLHTHBzdiP\" outputId=\"040e1643-6d57-497f-f912-ab54b253dd40\"\n# Convert our model's logit outputs to prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"sEReT3ie0CJz\" outputId=\"ddc7f695-a574-4ab6-cc18-d6b895385d14\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#convert-our-models-prediction-probabilities-to-prediction-labels","title":"Convert our model's prediction probabilities to prediction labels","text":"<p>y_preds = torch.argmax(y_pred_probs, dim=1) y_preds <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"x-5AbB8K0GzP\" outputId=\"37993381-8203-4d04-f552-95c3c63db98c\"\ny_blob_test\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#85-creating-a-training-loop-and-testing-loop-for-a-multi-class-classification","title":"8.5 Creating a Training Loop and Testing Loop for a Multi-Class Classification","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"tCOgCv1Z1ENR\" outputId=\"ba8368c8-f9a9-49ef-f178-5db33931ba2d\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#fit-the-multi-class-model-to-the-data","title":"Fit the multi-class model to the data","text":"<p>torch.manual_seed(42) torch.cuda.manual_seed(42)</p> <p>epochs = 100</p> <p>for epoch in range(epochs):     ### Training     model_4.train()</p> <pre><code># 1. Forward pass\ny_logits = model_4(X_blob_train)\ny_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n\n# 2. Calculate the loss\nloss = loss_fn(y_logits,\n               y_blob_train)\n\nacc = accuracy_fn(y_blob_train,\n                  y_pred)\n\n# 3. Optimizer zero grad\noptimizer.zero_grad()\n\n# 4. Perform backpropagation\nloss.backward()\n\n# 5. Perform gradient descent\noptimizer.step()\n\n### Testing\nmodel_4.eval()\n\n# Forward pass\nwith torch.inference_mode():\n    test_logits = model_4(X_blob_test)\n    test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n\n    # Calculate the loss and accuracy\n    test_loss = loss_fn(test_logits,\n                        y_blob_test)\n\n    test_acc = accuracy_fn(y_blob_test, test_preds)\n\n# Print out what's happening\nif epoch % 10 == 0:\n    print(f\"Epoch: {epoch} | Train Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n</code></pre> <p>```  </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#86-making-and-evaluating-predictions-with-a-pytorch-multi-class-model","title":"8.6 Making and Evaluating Predictions with a PyTorch Multi-Class Model","text":"<p>  ```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"p1COdl8B5zpr\" outputId=\"f54e6f7c-d2c6-4746-8dd4-81c5181b56e9\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#make-predictions_2","title":"Make predictions","text":"<p>model_4.eval() with torch.inference_mode():     y_logits = model_4(X_blob_test)     y_preds = torch.softmax(y_logits, dim=1).argmax(dim=1) # Logits -&gt; Probabilities -&gt; Class </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#view-the-first-10-predictions","title":"View the first 10 predictions","text":"<p>y_preds[:10] ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"nYahYVDy6ODl\" outputId=\"8f768696-3a7c-4dd0-e015-7485b6b1072c\" y_blob_test[:10] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 544} id=\"Ns85ZKjo5a_x\" outputId=\"7282b87f-1253-4608-c739-687782b2ec51\"\n# Plot the decision boundary\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/02_pytorch_classification/#9-a-few-more-classification-metrics-to-evaluate-our-classification-model","title":"9. A Few More Classification Metrics... (to Evaluate our Classification Model)","text":"<ul> <li>Accuracy: out of 100 samples, how many does our model get right?</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> <li>Confusion matrix</li> <li>Classification report</li> </ul> <p>If you want access to a lot of PyTorch metrics, see TorchMetrics - https://lightning.ai/docs/torchmetrics/stable/</p> <p>```python id=\"lQ6UfsrH850g\" !pip install torchmetrics <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"dFa4m03Y81NB\" outputId=\"bdc5884f-820a-4188-e2f7-75c15cecc799\"\nfrom torchmetrics import Accuracy\n\n# Setup metric\ntorchmetric_accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n\n# Calculate accuracy\ntorchmetric_accuracy(y_preds, y_blob_test)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/","title":"Pytorch Computer Vision","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#pytorch-computer-vision","title":"PyTorch Computer Vision","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#0-computer-vision-libraries-in-pytorch","title":"0. Computer Vision Libraries in PyTorch","text":"<ul> <li><code>torchvision</code> - base domain library in PyTorch computer vision</li> <li><code>torchvision.datasets</code> - get datasets and data loading functions for computer vision here</li> <li><code>torchvision.models</code> - get pretrained computer vision models that you can leverage for your own problems</li> <li><code>torchvision.transforms</code> - functions for manipulating your vision data (images) to be suitable for use with an ML model</li> <li><code>torch.utils.data.Dataset</code> - base dataset class for PyTorch</li> <li><code>torch.utils.data.DataLoader</code> - creates a Python iterable over a dataset</li> </ul> <p>```python id=\"fHaIpITTNRJL\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"600e2093-1b9c-439e-b5a8-56b5a32f95a8\" %pip install torchmetrics <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"pTjoZNP5qkuf\" outputId=\"c08a2f9f-1d42-4ce2-f204-c32fec9ef865\"\nimport numpy as np\nimport pandas as pd\nimport random\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom pathlib import Path\n\n# Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import Accuracy\nfrom torchmetrics import ConfusionMatrix\nfrom timeit import default_timer as timer\nfrom tqdm.auto import tqdm\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams['font.serif'] = \"Times New Roman\"\n\n# Check versions\nprint(torch.__version__)\nprint(torchvision.__version__)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#1-getting-a-dataset","title":"1. Getting a Dataset","text":"<p>The dataset we'll be using is FashionMNIST from <code>torchvision.datasets</code> - https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"hCoVFgUBrg-W\" outputId=\"9c5d8550-56d5-41ee-eba0-5dbf6f79a66f\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-training-data","title":"Setup training data","text":"<p>train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # do we want the training dataset?     download=True, # do we want to download yes/no?     transform=transforms.ToTensor(), # how do we want to transform the data?     target_transform=None # how do we want to transform the labels/targets?     )</p> <p>test_data = datasets.FashionMNIST(     root=\"data\",     train=False,     download=True,     transform=transforms.ToTensor(),     target_transform=None ) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"CbUJkiJ_6Yq-\" outputId=\"c1548499-72ae-4f41-bf16-645f9340d21a\"\nlen(train_data), len(test_data)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wD9t44dm6jsp\" outputId=\"9e827681-0c5d-42d6-f7ea-2ac902336f2e\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#see-the-first-training-example","title":"See the first training example","text":"<p>image, label = train_data[0] image, label <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"dNcmQrV27MVV\" outputId=\"f36bd750-54fc-4cd6-d1b5-13e6affcb363\"\nclass_names = train_data.classes\nclass_names\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"MfJRraIF7U2e\" outputId=\"5d1f1d11-ee8b-48f1-f578-8e6a32ca2a35\" class_to_idx = train_data.class_to_idx class_to_idx <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"rqSIorEg7ek-\" outputId=\"e6a0aa84-6a2a-48f5-8886-b680b5b5e2c8\"\ntrain_data.targets\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#11-check-the-input-and-output-shape-of-the-data","title":"1.1 Check the Input and Output Shape of the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"W6JCsMuc7h3e\" outputId=\"df3d9991-5591-41eb-b439-3243de0c74e3\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#check-the-shape-of-our-image","title":"Check the shape of our image","text":"<p>print(f\"Image shape: {image.shape} -&gt; [color_channels, height, width]\") print(f\"Image label: {class_names[label]}\") <pre><code>&lt;!-- #region id=\"w8LtYvnU7z5B\" --&gt;\n### **1.2 Visualizing Our Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 468} id=\"DIOiXtt08O2r\" outputId=\"b5067966-a3d9-4a23-f177-6cefa5176234\"\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze())\nplt.title(label);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 427} id=\"oFnLzf5J8psG\" outputId=\"15fbb556-7e89-42f2-f83a-42896af9f76d\" plt.imshow(image.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(False); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 751} id=\"RA3ek0l985zS\" outputId=\"263c611a-cc47-4b3d-f4ab-5a82c86bb834\"\n# Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows*cols+1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n</code></pre></p> <p>Do you think these items of clothing (images) could be modelled with pure linear lines? Or do you think we'll need non-linearities?</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"yXHRzDzH_DH4\" outputId=\"28ce171f-b7f2-4587-aab3-564f50a4634a\" train_data, test_data <pre><code>&lt;!-- #region id=\"NYw97c8C-_7A\" --&gt;\n## **2. Prepare DataLoader**\n\nRight now, our data is in the form of PyTorch Datasets.\n\nDataLoader turns our dataset into a Python iterable.\n\nMore specifically, we want to turn our data into batches (or mini-batches).\n\nWhy would we do this?\n\n1. It is more computationally efficient, as in, your computing hardware may not be able to look (store in memory) at 60000 images in one hit. So we break it down to 32 images at a time (batch size of 32).\n\n2. It gives our neural network more chances to update its gradients per epoch.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Vdt_Al_l_IUo\" outputId=\"4ee4962d-4626-4500-ba99-69ae2a603967\"\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(dataset=train_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\ntest_dataloader = DataLoader(dataset=test_data,\n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\ntrain_dataloader, test_dataloader\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"MAc4_y4jBYH4\" outputId=\"fd06c2b0-f65d-4aeb-d5fc-45bc05fb5416\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#lets-check-out-what-weve-created","title":"Let's check out what we've created","text":"<p>print(f\"DataLoaders: {train_dataloader, test_dataloader}\") print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...\") print(f\"Length of test_dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}...\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ns8ovbbeCFmO\" outputId=\"468e93d1-9d16-4f63-d510-21c2f5722d78\"\n# Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 462} id=\"xRCKN3HVB-9d\" outputId=\"9fb737e5-ab4b-40ec-899a-3ae80fd0cdea\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#show-a-sample","title":"Show a sample","text":"<p>torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(False) print(f\"Image Size: {img.shape}\") print(f\"Label: {label}, Label Size: {label.shape}\"); <pre><code>&lt;!-- #region id=\"B82cO22eDpvp\" --&gt;\n## **3. Model 0: Build a Baseline Model**\n\nWhen starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model.\n\nA baseline model is a simple model you will try to improve upon with subsequent models/experiments.\n\nIn other words: start simply and add complexity when necessary.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"j51m2jBAELHc\" outputId=\"696a0951-b34b-4830-ab17-6261a3a190d7\"\n# Create a flatten layer\nflatten_model = nn.Flatten()\n\n# Get a single sample\nX = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(X) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {X.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n</code></pre></p> <p>```python id=\"UqWo9KKDFDIy\" class FashionMNISTV0(nn.Module):     def init(self,                  input_shape: int,                  hidden_units: int,                  output_shape: int):         super().init()         self.layer_stack = nn.Sequential(             nn.Flatten(),             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.Linear(in_features=hidden_units, out_features=output_shape)         )</p> <pre><code>def forward(self, X):\n    return self.layer_stack(X)\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"76rkQPT0GJlS\" outputId=\"0e6469e0-3542-4b8d-985a-3f66bfa7e3d9\" torch.manual_seed(42) </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-model-with-input-parameters","title":"Setup model with input parameters","text":"<p>model_0 = FashionMNISTV0(     input_shape=2828, # this is 2828     hidden_units=10, # how many units in the hidden layer     output_shape=len(class_names) # one for every class ).to(\"cpu\")  model_0 ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Pwq89ce5HjqU\" outputId=\"830ae897-a36f-4bb8-dc9c-933b39800b63\" dummy_X = torch.rand([1, 1, 28, 28]) model_0(dummy_X) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7gT2Zs_ZLu8I\" outputId=\"24c93927-3d2b-4bb2-f2ea-a578f7c25668\"\nmodel_0.state_dict()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#31-setup-loss-optimizer-and-evauation-metrics","title":"3.1 Setup Loss, Optimizer and Evauation Metrics","text":"<ul> <li>Loss function - since we're working with multi-class data, our loss function will be <code>nn.CrossEntropyLoss()</code></li> <li>Optimizer - our optimizer <code>toch.optim.SGD()</code> (stochastic gradient descent)</li> <li>Evaluation metric - since we're working on a classification problem, let's use accuracy as our evaluation metric</li> </ul> <p>```python id=\"ntGsZBiUMaq_\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-accuracy-function-using-torchmetrics","title":"Setup accuracy function using torchmetrics","text":"<p>accuracy_fn = Accuracy(task=\"multiclass\", num_classes=len(class_names))</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-loss-function-and-optimizer","title":"Setup loss function and optimizer","text":"<p>loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) <pre><code>&lt;!-- #region id=\"5qGDK50jOY9B\" --&gt;\n### **3.2 Creating a Function to Time Our Experiments**\n\nMachine learning is very experimental.\n\nTwo of the main things you'll often want to tracj are:\n1. Model's performance (loss and accuracy values etc)\n2. How fast it runs\n&lt;!-- #endregion --&gt;\n\n```python id=\"A-NSoTJTOq-1\"\ndef print_train_time(start: float,\n                     end: float,\n                     device: torch.device = None):\n    \"\"\"Prints difference between start and end time.\"\"\"\n    total_time = end - start\n    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"BOjZs9xZPiXL\" outputId=\"359a680a-a009-4116-e1b5-1709cfa9c678\" start_time = timer()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#some-code","title":"some code...","text":"<p>end_time = timer() print_train_time(start=start_time, end=end_time, device=\"cpu\") <pre><code>&lt;!-- #region id=\"PpNHJnDQQA5_\" --&gt;\n### **3.3 Creating a Training Loop and Training a Model on Batches of Data**\n\n1. Loop through epochs.\n2. Loop through training batches, perform training steps, calculate the train loss per batch.\n3. Loop through testing batches, perform testing steps, calcualte the loss per batch.\n4. Print out what's happening.\n5. Time it all (for fun).\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 552, \"referenced_widgets\": [\"46fb79d7ff8e46468a66763661f667f8\", \"fb3f54468b984eff9a07cc9ea336ce86\", \"92613ab9ec5942be99ff53a55ea51fa8\", \"368597cc83f64b54b427f3d96bb47ffb\", \"c1a3d6b82cfa49dfa5e9c3f91ddb7bf3\", \"f2b0a6b5a1c241c8bd66b31bf7b24ba2\", \"a1e9c66d43fc4170b1166ae96ad6f0dc\", \"45d0ce1ea2344e6780e049616ac219d7\", \"e2be8d90a71c4098938d9514e43312bb\", \"78cbed12396e4362ae9c1872eb18e3eb\", \"37226f2a127e4a2a9b47971f2c417039\"]} id=\"WxmprMsZQqIa\" outputId=\"0ffc84b0-bc17-4c4f-be27-fa2621d36ca2\"\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training time)\nepochs = 3\n\n# Create training and test loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n------\")\n\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through the training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train()\n\n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate the loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulate train loss\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Perform backpropagation\n        loss.backward()\n\n        # 5. Perform gradient descent\n        optimizer.step()\n\n        # Print out what's happening\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n\n    # Divide total train loss by length of train dataloader\n    train_loss /= len(train_dataloader)\n\n    ### Testing\n    test_loss, test_acc = 0, 0\n    model_0.eval()\n\n    with torch.inference_mode():\n        for X_test, y_test in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X_test)\n\n            # 2. Calculate loss (accumulatively)\n            test_loss += loss_fn(test_pred, y_test)\n\n            # 3. Calculate the accuracy\n            test_acc += accuracy_fn(test_pred.argmax(dim=1), y_test).item()\n\n        # Calculate the test loss average per batch\n        test_loss /= len(test_dataloader)\n\n        # Calculate the test acc average per batch\n        test_acc /= len(test_dataloader)\n\n    # Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {(test_acc*100):.4f}%\")\n\n# Calculate training time\ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n                                            end=train_time_end_on_cpu,\n                                            device=str(next(model_0.parameters()).device))\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#4-make-predictions-and-get-model-results","title":"4. Make Predictions and Get Model Results","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 101, \"referenced_widgets\": [\"8e35ab42007b4f4f9a3464ca57f730b7\", \"0a014f2a3b2d4e4c8a6f58665a2d0656\", \"5b0557124d5a41f994f2eaf50254420c\", \"a0eca6f670c94c38b8e30f840c80f8c3\", \"8bb7c0c913da4124b7a8688c3a5b204a\", \"96537e09e8c94b9bab320ba5d07de221\", \"75e20440e5724e88be3fa154fd939a1f\", \"b27a01528eef4ceeba6369ca9f920982\", \"ef50dd6d42a14a9b895e3c34d7d226f1\", \"f783cd87f72840269bfa7f46ecb92003\", \"589d1f95af784094af0414da06047767\"]} id=\"fv_3AYSUWJc5\" outputId=\"6f7dc14e-0ca4-4606-c765-2e16a9518a9a\" torch.manual_seed(42) def eval_model(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                accuracy_fn,                device):     \"\"\"Returns a dictionary containing the results of model predicting on data loader.\"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in tqdm(data_loader):             # Make our code device-agnostic             X, y = X.to(device), y.to(device)             # Make prediction             y_pred = model(X)</p> <pre><code>        # Accumulate the loss and acc values per batch\n        loss += loss_fn(y_pred, y)\n        acc += accuracy_fn(y_pred.argmax(dim=1), y)\n\n    # Scale loss and acc to find the average loss/acc per batch\n    loss /= len(data_loader)\n    acc /= len(data_loader)\n\nreturn {\"model_name\": model.__class__.__name__, # only works when model was created with a class,\n        \"model_loss\": loss.item(),\n        \"model_acc\": acc.item()}\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#calculate-model-0-results-on-test-dataset","title":"Calculate model 0 results on test dataset","text":"<p>model_0_results = eval_model(model=model_0,                              data_loader=test_dataloader,                              loss_fn=loss_fn,                              accuracy_fn=accuracy_fn,                              device=\"cpu\") model_0_results <pre><code>&lt;!-- #region id=\"0P4IGS1QzuBh\" --&gt;\n## **5. Setup Device-Agnostic Code (for using a GPU if there is one)**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 36} id=\"VT5i3zytz6gT\" outputId=\"f2588d2f-390c-4130-9889-21e60083e20c\"\n# Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"u5p2om29z3g0\" outputId=\"5e53528c-72e2-428b-a5b9-062ee41f833d\" !nvidia-smi <pre><code>&lt;!-- #region id=\"rQ0QEj730FtT\" --&gt;\n## **6. Model 1: Building a Better Model with Non-linearity**\n&lt;!-- #endregion --&gt;\n\n```python id=\"5bYqE1Qb0lsD\"\n# Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self,\n                 input_shape: int,\n                 hidden_units: int,\n                 output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into a single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n\n    def forward(self, X: torch.Tensor):\n        return self.layer_stack(X)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Dq9KK-hj2VZd\" outputId=\"cab83a6a-c22d-467d-d5e1-c237eaff3e92\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-an-instance-of-model_1","title":"Create an instance of model_1","text":"<p>torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # this is the output of the flatten after our 28*28 image goes in                               hidden_units=10,                               output_shape=len(class_names)).to(device) # send to the GPU if it's available</p> <p>next(model_1.parameters()).device <pre><code>&lt;!-- #region id=\"fvY-lJO93LMj\" --&gt;\n### **6.1 Setup Loss, Optimizer, and Evaluation Metrics**\n&lt;!-- #endregion --&gt;\n\n```python id=\"0RvBNi_d3SM1\"\n# Setup the loss function and the optimizer\nloss_fn = nn.CrossEntropyLoss() # measure how wrong our model is\noptimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1) # tries to update our model's parameter to reduce the loss\n\n# Setup the accuracy function\naccuracy_fn = Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#62-functionizing-training-and-evaluationtesting-loops","title":"6.2 Functionizing Training and Evaluation/Testing Loops","text":"<p>Let's create a function for: * training loop - <code>train_step()</code> * testing loop - <code>test_step()</code></p> <p>```python id=\"4UUJeg3R4cak\" def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     \"\"\"Performs a training with model trying to learn on data loader.\"\"\"</p> <pre><code>### Training\nmodel.train()\n\ntrain_loss, train_acc = 0, 0\n\nfor batch, (X, y) in enumerate(data_loader):\n    # Put the data on target device\n    X, y = X.to(device), y.to(device)\n\n    # 1. Forward pass\n    y_pred = model(X)\n\n    # 2. Calculate the loss and accuracy (per batch)\n    loss = loss_fn(y_pred, y)\n    train_loss += loss\n    acc = accuracy_fn(y_pred.argmax(dim=1), # go from logits -&gt; prediction labels\n                      y).item()\n    train_acc += acc\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Perform backpropagation\n    loss.backward()\n\n    # 5. Perform gradient descent (update the model's parameter once *per batch*)\n    optimizer.step()\n\n# Divide total train loss and acc by length of train dataloader\ntrain_loss /= len(train_dataloader)\ntrain_acc /= len(train_dataloader)\n\n# Print out what's happening\nprint(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")\n</code></pre> <p><code></code>python id=\"8eg8ZGUTCJ_q\" def test_step(model: torch.nn.Module,               data_loader: torch.utils.data.DataLoader,               loss: torch.nn.Module,               accuracy_fn,               device: torch.device = device):      \"\"\"Performs a testing loop step on model going over data_loader\"\"\"      test_loss, test_acc = 0, 0      # Put the model in eval mode     model.eval()      #  Turn on inference model context manager     with torch.inference_mode():         for X, y in data_loader:             # Send the data to the target device             X, y = X.to(device), y.to(device)              # 1. Forward pass (output raw logits)             test_pred = model(X)              # 2. Calculate the loss/acc             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(test_pred.argmax(dim=1), y).item() # go from logits -&gt; prediction labels          # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\") ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 292, \"referenced_widgets\": [\"1c8f6270216848ec9145e2ebd4da258e\", \"707103e56fc9416bbe85c71514e49b44\", \"b410aedd46764c9881b9c6307169cd4a\", \"55b0531aa186458699d0f29912d5568b\", \"de1185f1e5ab4093aae8d5cb4868e7ed\", \"36bb51bcba534b8abd0c02874d3a4a8d\", \"c7ac0a9fb01545829b164b8ea1ba0f63\", \"d8d66eefc8724408baa8540affa0a948\", \"9e8e563ade054a32969f93ed3a61d62f\", \"bfec5ef55da3423285cf6de35ab8757f\", \"9b834ccd7d2d46be8198156361413c79\"]} id=\"Bv7vH5nxYSGM\" outputId=\"3b394f0c-bcaf-42bd-f070-ea73068f31b2\" torch.manual_seed(42)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#measure-time","title":"Measure time","text":"<p>train_time_start_on_gpu = timer()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#set-epochs","title":"Set epochs","text":"<p>epochs = 3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-an-optimization-and-evaluation-loop-using-train_step-and-test_step","title":"Create an optimization and evaluation loop using train_step() and test_step()","text":"<p>for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(model=model_1,                data_loader=train_dataloader,                loss=loss_fn,                optimizer=optimizer,                accuracy_fn=accuracy_fn,                device=device)     test_step(model=model_1,               data_loader=test_dataloader,               loss=loss_fn,               accuracy_fn=accuracy_fn,               device=device)</p> <p>train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <pre><code>&lt;!-- #region id=\"Gh9xIcENbV2V\" --&gt;\n**Note**: Sometimes, depending on your data/hardware you might find that your model trains faster on CPU than GPU.\n\nWhy is this?\n1. It copuld be that the overhead for copying data/model to and from the CPU outweighs the compute benefits offered by the GPU.\n2. The hardware you're using has a better CPU in terms compute capability than the GPU.\n3. For more on how to make your models compute faster, see here: https://horace.io/brrr_intro.html\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"n6uRO_EOb734\" outputId=\"536f2412-3289-4e13-d01b-7ee4ef005f0a\"\nmodel_0_results\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 101, \"referenced_widgets\": [\"6491d02bc72d453cacc41e694631d60a\", \"138fd122fe36440998444261a33dc2a4\", \"ea2613791cc44e0b9ffc77f2038bb158\", \"e523d985fe09442aaf92cab08ef0aaf0\", \"763f97d20e8d47f2811063c41650de7f\", \"7ac4157b12324542af41399365270b57\", \"8b329f5c6e08420d95b771f75c865906\", \"e1ed827543a04419b53d8062dbdedeac\", \"8dd86df2f96c4b5aafc12c932b1b6c03\", \"c50b2f333648465e8829b04bd30cd3f8\", \"fc92225652464039abff56f17e8e9152\"]} id=\"tv4Wt1S7cfx6\" outputId=\"22369434-1091-4800-f819-64880e8a62b6\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#get-model_1-result-dictionary","title":"Get model_1 result dictionary","text":"<p>model_1_results = eval_model(model=model_1,                              data_loader=test_dataloader,                              loss_fn=loss_fn,                              accuracy_fn=accuracy_fn,                              device=device) model_1_results <pre><code>&lt;!-- #region id=\"3vkkYa7ndYxp\" --&gt;\n## **7. Model 2: Building a Convolutional Neural Network (CNN)**\nCNN's are also known ConvNets.\n\nCNN's are known for their capabilities to find patterns in visual data.\n\nTo find out what's happening inside a CNN, see this website: https://poloclub.github.io/cnn-explainer/\n&lt;!-- #endregion --&gt;\n\n```python id=\"PC6Qx5Pmhqto\"\n# Create a convolutional neural network\nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture that replicates the TinyVGG\n    model from CNN explainer website\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape,\n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1), # values we can set ourselves in our NN's are called hyperparameters\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units,\n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=hidden_units,\n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units,\n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=hidden_units*7*7, # there's a trick to calculating this.\n                      out_features=output_shape)\n        )\n\n    def forward(self, X):\n        X = self.conv_block_1(X)\n        # print(f\"Output shape of conv_block_1: {X.shape}\")\n        X = self.conv_block_2(X)\n        # print(f\"Output shape of conv_block_2: {X.shape}\")\n        X = self.classifier(X)\n        # print(f\"Output shape of classifier: {X.shape}\")\n        return X\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2ct9e1RElVh4\" outputId=\"eda3d745-854b-4303-f883-1f904adf7925\" image.shape <pre><code>```python id=\"DSeWRDY_iAeF\"\ntorch.manual_seed(42)\n\nmodel_2 = FashionMNISTModelV2(input_shape=1,\n                              hidden_units=10,\n                              output_shape=len(class_names)).to(device)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"OXhpuhuR6oMc\" outputId=\"c8808efd-aece-4189-def7-fa0832fa9081\" plt.imshow(image.squeeze(), cmap=\"gray\"); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"BpK6OOpq67b-\" outputId=\"ba09ea01-86cb-421e-aebc-2208b6a17e2c\"\nrand_image_tensor = torch.randn(size=(1, 28, 28))\nrand_image_tensor.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"G4weVyG77FZH\" outputId=\"42b994c3-af9f-40b8-c4d4-08bbb5dcde7e\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#pass-image-through-model","title":"Pass image through model","text":"<p>model_2(rand_image_tensor.unsqueeze(dim=1).to(device)) <pre><code>```python id=\"c8M3lzxczqnv\"\nmodel_2.state_dict()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#71-stepping-through-nnconv2d","title":"7.1 Stepping through <code>nn.Conv2d()</code>","text":"<p>See the documentation for nn.Conv2d() here - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"FjABAIB3zIIb\" outputId=\"d4e88367-9b97-43d1-9556-9ca934d6d718\" torch.manual_seed(42)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-a-batch-of-images","title":"Create a batch of images","text":"<p>images = torch.randn(size=(32, 3, 64, 64)) test_image = images[0]</p> <p>print(f\"Image batch shape: {images.shape}\") print(f\"Single image shape: {test_image.shape}\") print(f\"Test image:\\n {test_image}\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"3BF2HvQM0EVR\" outputId=\"ae3108d9-2f88-4b0e-c9af-8fbaf48e6961\"\ntest_image.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"BxCNS36x0IHy\" outputId=\"7998ae41-0173-478f-d843-e2052d1daf73\" torch.manual_seed(42)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-a-single-conv2d-layer","title":"Create a single conv2d layer","text":"<p>conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=(3, 3),                        stride=1,                        padding=1)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#pass-the-data-through-the-convolutional-layer","title":"Pass the data through the convolutional layer","text":"<p>conv_output = conv_layer(test_image) conv_output.shape <pre><code>&lt;!-- #region id=\"HJw1ZVgZ2GNz\" --&gt;\n### **7.2 Stepping through `nn.MaxPool2d()`**\nSee the documentation for `nn.MaxPool2d()` here - https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"reMCtC3f2iTZ\" outputId=\"956601ea-2369-42d2-9ca9-71a405debdb6\"\ntest_image.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Cb6vuATq2lfm\" outputId=\"a0466944-c966-4094-8d2a-cb022a729a6c\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#print-out-original-image-shape","title":"Print out original image shape","text":"<p>print(f\"Test image original shape: {test_image.shape}\")</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-a-sample-nnmaxpool2d-layer","title":"Create a sample nn.MaxPool2d layer","text":"<p>max_pool_layer = nn.MaxPool2d(kernel_size=2)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#pass-data-through-just-the-conv_layer","title":"Pass data through just the conv_layer","text":"<p>test_image_through_conv = conv_layer(test_image) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#pass-the-data-through-max-pool-layer","title":"Pass the data through max pool layer","text":"<p>test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"41tVqBcO4qvv\" outputId=\"ab9b0a94-b2c5-499f-8046-3a615898372a\"\ntorch.manual_seed(42)\n\n# Create a random tensor with a similar number of dimension to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"\\nRandom tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n {max_pool_tensor}\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\nrandom_tensor\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#73-setup-a-loss-function-and-optimizer-for-model_2","title":"7.3 Setup a Loss Function and Optimizer for <code>model_2</code>","text":"<p>```python id=\"Z7f88Zq5-R5E\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-loss-functioneval_metricsoptimizer","title":"Setup loss function/eval_metrics/optimizer","text":"<p>loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1) accuracy_fn = Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device) <pre><code>&lt;!-- #region id=\"hWgYMeX4-233\" --&gt;\n### **7.4 Training and Testing `model_2` using our Training and Test Functions**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 292, \"referenced_widgets\": [\"3f1a35f51a5c45c9afd8ba9b4d644aea\", \"bc6c2812690a430e820ca95afd3cbd03\", \"711a0e22bd7b4cdea856a3d25c6c1c35\", \"a0a145406acd4b60b1a1d385d9765cc4\", \"7b3e3f28b1eb47e2b11fc284cf09c721\", \"0368c8ebd5cf4a599550e64fecfeb643\", \"258904cd5f91469babe6332808b74265\", \"18b3a6ea380349df8549c281c1c83486\", \"a56412a4b5514a83845512b5f325164e\", \"3586c44fd8054fc39906d6ab74a46d24\", \"29ba4913a10447949d6978fb07fecbd2\"]} id=\"t2CZLCuT_UGD\" outputId=\"c2bc4ab4-a5a5-4291-a8de-168a2035587f\"\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as time\ntrain_time_start_model_2 = timer()\n\n# Train and test model\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    train_step(model=model_2,\n               data_loader=train_dataloader,\n               loss=loss_fn,\n               optimizer=optimizer,\n               accuracy_fn=accuracy_fn,\n               device=device)\n\n    test_step(model=model_2,\n              data_loader=test_dataloader,\n              loss=loss_fn,\n              accuracy_fn=accuracy_fn,\n              device=device)\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                            end=train_time_end_model_2,\n                                            device=device)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 101, \"referenced_widgets\": [\"a307aba396d74abea8d1902b342fc1fd\", \"d308807dbf7942c18e7507e89925c0e7\", \"ece2cf07559141e290b92f6a59cecb75\", \"3c855d88b24541ae8c07a34d8dad69f0\", \"32666e0c42fe48c08e4e87db3a8b1c6e\", \"8f44016a44ae4059b37dba8a0622fb06\", \"e12d6d8f4dd94713b1dc438c3131f0fb\", \"acfbc38ab24849ec8f8e13cf913b0857\", \"6502f00dd0494eceb3507b30a9388169\", \"cb444794ad204cb696055a694b7d30c7\", \"7384214fa5e34883bdd8965d66a5587a\"]} id=\"ulHwmEy9B_OP\" outputId=\"11f5df05-7309-41da-d8c0-eb9beaa2743c\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#get-model_2-results","title":"Get model_2 results","text":"<p>model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn,     device=device )</p> <p>model_2_results <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xB-KrzAICz0M\" outputId=\"a346e1ca-2037-4ec3-f8a4-fd37b4609f4b\"\nmodel_0_results\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#8-compare-the-model-results-and-training-time","title":"8. Compare the Model Results and Training Time","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 143} id=\"7_4rUmrlC_6a\" outputId=\"dc32ad31-a39b-449a-ec7f-f438b25ade6d\" compare_results = pd.DataFrame([model_0_results,                                 model_1_results,                                 model_2_results])</p> <p>compare_results <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 143} id=\"R5YdbGI3Dls2\" outputId=\"df6f3117-50fc-4748-f9a2-d7810b8b1263\"\n# Add training time to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results[\"model_acc\"] = compare_results[\"model_acc\"] * 100\ncompare_results\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 450} id=\"Q17OPhbfLY79\" outputId=\"f7a5421d-f3e3-4922-86de-cc747f83028a\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#visualize-our-model-results","title":"Visualize our model results","text":"<p>sns.barplot(data=compare_results, x=\"model_acc\", y=\"model_name\", width=0.5) plt.xlabel(\"Accuracy (%)\") plt.ylabel(\"Model\"); <pre><code>&lt;!-- #region id=\"P-ItYRmAMSbZ\" --&gt;\n## **9. Make and Evaluate Random Predictions with Best Model**\n&lt;!-- #endregion --&gt;\n\n```python id=\"tS0DXzzmMYnI\"\ndef make_predictions(model: torch.nn.Module,\n                     data: list,\n                     device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare the sample (add a batch dimension and pass to target device)\n            sample = torch.unsqueeze(sample, dim=0).to(device)\n\n            # Forward pass (model outputs raw logits)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # Get pred_prob off the GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n\n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"6kXCP8A2NipI\" outputId=\"99487a1b-47ef-44d2-f827-5f9ab19efcd2\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#randomseed42","title":"random.seed(42)","text":"<p>test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#view-the-first-sample-shape","title":"View the first sample shape","text":"<p>test_samples[0].shape <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 451} id=\"2qMIMGGDO00_\" outputId=\"f26a800d-9e3a-47fc-d12f-236b56b86c75\"\nplt.imshow(test_samples[0].squeeze(), cmap=\"gray\")\nplt.title(class_names[test_labels[0]]);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"POagx165PDi5\" outputId=\"2492a0cf-304e-4491-93e4-679c8f0bb532\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#make-predictions","title":"Make predictions","text":"<p>pred_probs = make_predictions(model=model_2,                                data=test_samples)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#view-first-two-prediction-probabilities","title":"View first two prediction probabilities","text":"<p>pred_probs[:2] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"D4kUThLfPXar\" outputId=\"9c69a142-efd6-43ba-d33c-5399ad0bbdac\"\n# Convert prediction probabilities to labels\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"XhWID5SNPhWU\" outputId=\"7a7bd6c9-f998-4ba0-bc6b-cd000c298729\" test_labels <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 749} id=\"fY3wjfrwPrls\" outputId=\"bee9ad67-2028-489e-ce50-ef87348b39e4\"\n# Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n    # Create subplot\n    plt.subplot(nrows, ncols, i+1)\n\n    # Plot the target image\n    plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n    # Find the prediction (in text form, e.g \"Sandal\")\n    pred_label = class_names[pred_classes[i]]\n\n    # Get the truth label (in text form)\n    truth_label = class_names[test_labels[i]]\n\n    # Create a title for the plot\n    title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n\n    # Check for equality between pred and truth and change color of title text\n    if pred_label == truth_label:\n        plt.title(title_text, fontsize=10, c=\"g\")\n    else:\n        plt.title(title_text, fontsize=10, c=\"r\")\n\n    plt.axis(False)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluation","title":"10. Making a Confusion Matrix for Further Prediction Evaluation","text":"<p>A confusion matrix is a fantastic way of evaluating your classification models visually: 1. Make predictions with our trained model on the test dataset 2. Make a confusion matrix <code>torchmetrics.ConfusionMatrix</code> 3. Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 66, \"referenced_widgets\": [\"022d24263bbc4d848afa125ccae25a5b\", \"acf78b19fd734a438a9fa80cfd0ba7f5\", \"e2e473f90eca47bc825b58248e96c499\", \"8aeb8dd41234409a890d1cc4f70b2dd4\", \"2345eb5376034efda4319b939d2c6a9e\", \"cd28069fbb68491db96bd83841de16ad\", \"895ae70cd7044019a4e80b4b6b74ecb2\", \"3afbff32079f4e1f82b570b402b9d10e\", \"2331b8d427c943bb81bccd97b824f74a\", \"236bd6d190134df2950b1abfbe3d5098\", \"6d0e7fcdad1b45de9a7bc63e5cca6071\"]} id=\"AcjDJmOwShvy\" outputId=\"a909d7fc-3285-4f16-d596-5167a441d3da\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#make-predictions-with-trained-model","title":"Make predictions with trained model","text":"<p>y_preds = [] model_2.eval()</p> <p>with torch.inference_mode():     for X, y in tqdm(test_dataloader, desc=\"Making predictions...\"):         # Send the data andtargets to target device         X, y = X.to(device), y.to(device)         # Forward pass         y_logit = model_2(X)         # Turn predictions from logits -&gt; prediction probabilities -&gt; prediction labels         y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)         # Put prediction on CPU for evaluation         y_preds.append(y_pred.cpu())</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#concatenate-list-of-predictions-into-a-tensor","title":"Concatenate list of predictions into a tensor","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#printy_preds","title":"print(y_preds)","text":"<p>y_pred_tensor = torch.cat(y_preds) y_pred_tensor <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2EGOAPV2T0J1\" outputId=\"d87774ee-e2c1-4344-8c96-48e58a33b3a4\"\nlen(y_pred_tensor)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 668} id=\"NZNW5NbST2pc\" outputId=\"218a9acb-2500-4b49-91af-838ab11ed52d\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#setup-confusion-instance-and-compare-prediction-to-targets","title":"Setup confusion instance and compare prediction to targets","text":"<p>confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names)) confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#plot-the-confusion-matrix","title":"Plot the confusion matrix","text":"<p>fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy     class_names=class_names,     figsize=(10, 7) ) <pre><code>&lt;!-- #region id=\"Pjbg3ol-WQIE\" --&gt;\n## **11. Save and Load Best Performing Model**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"mZXS8PdoWZwM\" outputId=\"5bf4da1a-2603-41a3-e418-db7afcbd4796\"\n# Create model directory path\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True,\n                 exist_ok=True)\n\n# Create model save\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(),\n           f=MODEL_SAVE_PATH)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"N18TPPs4Xs7c\" outputId=\"a67dbb25-cf33-4028-dc45-490fcfa34a34\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#create-a-new-instance","title":"Create a new instance","text":"<p>torch.manual_seed(42)</p> <p>loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10,                                      output_shape=len(class_names))</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#load-in-the-save_dict","title":"Load in the save_dict()","text":"<p>loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#send-the-model-to-the-target-device","title":"Send the model to the target device","text":"<p>loaded_model_2.to(device) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"OvZ9lN51Ym6L\" outputId=\"4b152d41-6be8-423d-fb9e-444c0a2981df\"\nmodel_2_results\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 101, \"referenced_widgets\": [\"dcb21b8949de46cbb8990bf1dd65237e\", \"277d758935cc45f3a0e9810963bb3a6e\", \"d4de2fe3bbb54c929a0581b7063620fa\", \"026fa826a2dc46b382b1857f173505ad\", \"6d286f91810047a4b886bbfd1abc8750\", \"b510869e4e484648b46f79563debd832\", \"8c1801dba96b412dac4de19defff672f\", \"d899ff8da4064d65b00e210f2aad8080\", \"9409cc7fb0b345008ccb60101ef7feaf\", \"ba630d6c7cf54ceeba0c3bfd3da23f76\", \"11e8b22a81694c2498822ca4684f46f9\"]} id=\"HxMCX6fUYpID\" outputId=\"a16d1a4b-5e03-46a1-ff13-0e25650efc76\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/03_pytorch_computer_vision/#evaluate-the-loaded-model","title":"Evaluate the loaded model","text":"<p>torch.manual_seed(42)</p> <p>loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn,     device=device )</p> <p>loaded_model_2_results <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"WloZtkejZLjt\" outputId=\"abe1e8ec-5377-402b-c127-594bd26daa2c\"\n# Check if model results are close to each other\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]),\n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-02)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/","title":"Introduction To Cnn","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#introduction-to-convolutional-neural-network-cnn","title":"Introduction to Convolutional Neural Network (CNN)","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#what-is-cnn","title":"What is CNN?","text":"<p>Convolutional neural network, also known as convnet, or CNNs, are a special kind of neural network for processing data that has a known grid-like topology like time series data(1D) or images(2D).</p> <p>The key feature of CNNs is the use of convolutional layers, which apply convolution operations to input data. Convolution involves sliding a small filter (also called a kernel) over the input data, performing element-wise multiplication and summing the results to produce a feature map. CNNs also typically include pooling layers to downsample the spatial dimensions of the data, reducing computational complexity and making the network more robust to variations in input. Additionally, fully connected layers are often used at the end of the network to make final predictions based on the extracted features.</p> <p>CNNs have been highly successful in various computer vision tasks, and their architecture is designed to mimic the visual processing that occurs in the human brain. They have played a crucial role in advancing the field of deep learning and have been widely used in applications like image and video recognition, medical image analysis, and more.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#history-of-cnn","title":"History of CNN","text":"<p>Convolutional Neural Networks (CNNs) have evolved over several decades, with key contributions from researchers in the field of computer vision and artificial intelligence. The development of CNNs can be traced back to the 1960s, but they gained significant prominence in the 2010s. Here's a brief overview of their evolution:</p> <ol> <li> <p>Hubel and Wiesel's Discoveries (1962): In the early 1960s, David Hubel and Torsten Wiesel conducted groundbreaking experiments on the visual cortex of cats. They discovered that neurons in the visual cortex have receptive fields that respond to specific features in the visual stimulus, such as edges and corners. This laid the foundation for understanding hierarchical visual processing. </p> </li> <li> <p>Neocognitron (1980s): Kunihiko Fukushima proposed the Neocognitron, an artificial neural network architecture inspired by the findings of Hubel and Wiesel. Neocognitron was designed for handwritten character recognition and featured alternating layers of convolutional and subsampling (pooling) layers. It introduced the concept of local receptive fields and shared weights, which are fundamental to modern CNNs.</p> </li> <li> <p>LeNet-5 (1998): Yann LeCun, along with collaborators, developed LeNet-5, a convolutional neural network for handwritten digit recognition. It consisted of convolutional layers, pooling layers, and fully connected layers. LeNet-5 demonstrated the effectiveness of CNNs in image classification tasks and laid the groundwork for future developments.</p> </li> <li> <p>AlexNet (2012): AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, marked a significant breakthrough in the use of CNNs. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, showcasing the power of deep learning for image classification. AlexNet's success spurred increased interest and research in deep neural networks.</p> </li> <li> <p>VGGNet (2014): The Visual Geometry Group (VGG) introduced VGGNet, which demonstrated that increasing the depth of CNNs could improve performance. VGGNet had a simple and uniform architecture, with small convolutional filters stacked on top of each other. This idea of deeper networks contributed to subsequent developments.</p> </li> <li> <p>GoogLeNet (Inception) (2014): Google's Inception model, also known as GoogLeNet, introduced the concept of inception modules, which allowed the network to capture features at multiple scales. This architecture aimed to address the trade-off between depth and computational efficiency.</p> </li> <li> <p>ResNet (2015): Microsoft Research introduced Residual Networks (ResNet), which employed residual learning to handle the challenges of training very deep networks. The use of residual connections allowed the network to skip certain layers during training, facilitating the training of extremely deep models.</p> </li> </ol> <p>These milestones and innovations have collectively shaped the development of CNNs, making them a cornerstone in computer vision and deep learning research. Today, CNNs are widely used in various applications, ranging from image recognition and object detection to medical image analysis and autonomous vehicles.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#why-not-use-ann","title":"Why not use ANN?","text":"<p>Artificial Neural Networks (ANNs), including traditional feedforward neural networks, are a powerful class of models used in various machine learning tasks. However, there are specific reasons why Convolutional Neural Networks (CNNs) are preferred over traditional ANNs for certain types of tasks, particularly in the field of computer vision. Here are some key reasons:</p> <ol> <li>High Computational Cost:</li> <li>Traditional ANNs: Fully connected layers in traditional artificial neural networks (ANNs) have a large number of parameters, leading to a high computational cost. As the network scales, the number of parameters grows significantly, making training and inference computationally intensive.</li> <li> <p>CNNs: Convolutional Neural Networks (CNNs) reduce the computational cost by using shared weights and local connectivity in convolutional layers. The use of pooling layers also helps downsample the data, decreasing the overall computational load.</p> </li> <li> <p>Overfitting:</p> </li> <li>Traditional ANNs: ANNs, especially with a large number of parameters, are prone to overfitting. Overfitting occurs when the model learns the training data too well, including noise and outliers, and fails to generalize to unseen data.</li> <li> <p>CNNs: CNNs are designed to mitigate overfitting through techniques like parameter sharing, which reduces the number of parameters, and the use of dropout and regularization. Additionally, pooling layers contribute to a form of implicit regularization by focusing on the most salient features.</p> </li> <li> <p>Loss of Spatial Arrangement of Pixels:</p> </li> <li>Traditional ANNs: When using traditional ANNs for tasks like image processing, the spatial arrangement of pixels is often lost. Flattening the input image into a vector discards the spatial relationships between pixels, making it challenging to capture local patterns.</li> <li>CNNs: CNNs excel in preserving spatial information through convolutional layers that capture local patterns and hierarchies. This is essential for tasks like image recognition, where the spatial arrangement of features is crucial for understanding the content of an image.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#cnn-intuition","title":"CNN Intuition","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/00_introduction_to_cnn/#applications-of-cnn","title":"Applications of CNN","text":"<p>Convolutional Neural Networks (CNNs) have found widespread applications across various domains due to their ability to effectively handle grid-like data, particularly in image processing. Here is a brief overview of some key applications of CNNs:</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/01_cnn_vs_visual_cortex/","title":"Cnn Vs Visual Cortex","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/01_cnn_vs_visual_cortex/#cnn-vs-visual-cortex","title":"CNN Vs Visual Cortex","text":"<p>Convolutional Neural Networks (CNNs) draw inspiration from the human visual system, particularly the visual cortex. While CNNs attempt to mimic certain aspects of visual processing in the brain, it's important to note that they are simplified models and do not replicate the full complexity of the biological visual system.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/01_cnn_vs_visual_cortex/#human-visual-cortex","title":"Human Visual Cortex","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/01_cnn_vs_visual_cortex/#hubel-and-wiesel-cat-experiment","title":"Hubel and Wiesel Cat Experiment","text":"<p>David Hubel and Torsten Wiesel conducted groundbreaking experiments in the early 1960s that significantly advanced our understanding of the visual system. Their work, which focused on the cat's visual cortex, provided key insights into how neurons respond to visual stimuli and laid the foundation for our understanding of feature detection and processing in the brain.</p> <p></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/","title":"Convolution Operation","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#convolution-operation","title":"Convolution Operation","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#introduction","title":"Introduction","text":"<p>Convolutional Neural Networks (CNNs) consist of multiple layers, each designed to perform specific operations to extract hierarchical features from input data, especially images. The key layers in a CNN include:</p> <ol> <li>Input Layer:</li> <li>Represents the raw data, often an image with pixel values.</li> <li> <p>The size of the input layer corresponds to the dimensions of the input data, such as the height, width, and number of color channels.</p> </li> <li> <p>Convolutional Layer:</p> </li> <li>Applies convolutional operations to the input data using filters or kernels.</li> <li>Each filter captures specific features or patterns within local receptive fields.</li> <li> <p>Activations from this layer form feature maps that represent learned features.</p> </li> <li> <p>Pooling (Subsampling) Layer:</p> </li> <li>Performs downsampling to reduce spatial dimensions and computational complexity.</li> <li> <p>Common pooling operations include max pooling or average pooling, which retain the most salient features within local regions.</p> </li> <li> <p>Fully Connected (Dense) Layer:</p> </li> <li>Neurons in this layer are connected to all neurons from the previous layer.</li> <li>Transforms high-level features into predictions or class scores.</li> <li> <p>Commonly used in the final layers of the network.</p> </li> <li> <p>Output Layer:</p> </li> <li>Produces the final output of the network.</li> <li>The number of neurons in this layer corresponds to the number of classes in a classification task.</li> <li>The activation function is chosen based on the nature of the task (e.g., softmax for classification).</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#basics-of-image","title":"Basics of Image","text":"<p>In CNNs, images are fundamental inputs that undergo processing through various layers to extract hierarchical features. The nature of the image, whether grayscale or RGB (Red, Green, Blue), affects the input dimensions and the way the network processes the information.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#grayscale-images","title":"Grayscale Images:","text":"<ol> <li>Representation:</li> <li>Grayscale images are represented using a single channel, where each pixel has a single intensity value (typically ranging from 0 to 255).</li> <li> <p>In CNNs, a grayscale image is usually treated as a 2D matrix, where each element represents the intensity of a pixel.</p> </li> <li> <p>Input Dimensions:</p> </li> <li>For a grayscale image of size H x W, the input tensor to the CNN would have dimensions (H, W, 1).</li> <li>The third dimension (1) signifies the single channel.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#rgb-images","title":"RGB Images:","text":"<ol> <li>Representation:</li> <li>RGB images are represented using three channels (Red, Green, Blue), where each channel represents the intensity of a specific color.</li> <li> <p>Each pixel has three intensity values, forming a 3D matrix.</p> </li> <li> <p>Input Dimensions:</p> </li> <li>For an RGB image of size H x W, the input tensor to the CNN would have dimensions (H, W, 3).</li> <li>The three channels correspond to Red, Green, and Blue.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#edge-detection-convolutional-operation","title":"Edge Detection (Convolutional Operation)","text":"<p>Edge detection is a fundamental operation in image processing and computer vision that aims to identify boundaries within an image. One common approach to edge detection involves using convolutional filters, such as the Sobel filters, to highlight changes in intensity that correspond to edges. The Sobel filters are particularly effective for detecting edges in both the vertical and horizontal directions.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#example-of-a-vertical-edge-detection","title":"Example of a Vertical Edge Detection","text":"<p>Demo: Click on this link.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#calculation-of-feature-map-size","title":"Calculation of Feature Map Size","text":"<p>If the stride is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after a convolution operation simplifies to:</p> \\[ \\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 \\] <p>Here, the terms are:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> </ul> <p>Example: $\\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 $</p> <p>Suppose you have a grayscale image with an input size of 28x28 and a filter size of 3x3:</p> <p>$\\text{Output Size} = 28 - 3 + 1 = 26 $</p> <p>So, in this case, the feature map size after the convolution operation would be 26x26. If you are not using any stride $\\text{Stride} = 1 $, this simplified formula is applicable.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/02_convolution_operation/#working-with-rgb-images","title":"Working with RGB Images","text":"<p>Convolution Operation on RGB Images:</p> <p>Applying Multiple Filters on same RGB Image: </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/","title":"Padding And Strides In Cnn","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#padding-and-strides-in-cnn","title":"Padding and Strides in CNN","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#what-is-padding","title":"What is Padding?","text":"<p>Padding is a technique used to preserve spatial information during the convolutional and pooling operations. It involves adding extra pixels (usually with a value of zero) around the borders of an input feature map or image.</p> <p>The main purposes of padding are:</p> <ol> <li>Preserving Spatial Information:</li> <li>Without padding, the spatial dimensions of the feature map decrease with each convolutional layer, potentially leading to a significant reduction in information at the edges.</li> <li> <p>Padding helps maintain the spatial size, ensuring that information near the borders is given proper consideration.</p> </li> <li> <p>Mitigating the Loss of Information:</p> </li> <li>In the absence of padding, the pixels at the edges of the feature map are involved in fewer convolution operations, leading to a loss of information.</li> <li> <p>Padding ensures that each pixel in the input has the opportunity to be the center of the receptive field for convolutional filters.</p> </li> <li> <p>Handling Stride and Filter Size:</p> </li> <li>Padding becomes especially useful when using larger filter sizes or strides greater than 1. Without padding, the spatial size reduction becomes more pronounced.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#types-of-padding-in-keras","title":"Types of Padding in Keras","text":"<p>In Keras, a popular deep learning library, you can specify different types of padding for convolutional layers. The main types of padding available in Keras are:</p> <ol> <li>Valid Padding (No Padding):</li> <li>This is the default setting in Keras.</li> <li>No padding is added to the input feature map.</li> <li>The convolution operation is applied only to the valid part of the input.</li> </ol> <ol> <li>Same Padding (Zero Padding):</li> <li>Padding is added to the input feature map to ensure that the spatial dimensions of the output feature map remain the same as the input.</li> <li>The padding is distributed evenly on all sides.</li> <li>Useful for preserving spatial information and handling larger filter sizes.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#calculation-of-feature-map-size","title":"Calculation of Feature Map Size","text":"<p>If the stride \\((\\text{Stride})\\) is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after padding simplifies further. For \"same\" padding, the formula becomes:</p> \\[\\text{Output Size} = {{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size} + 1}}\\] <p>Here, the terms are the same as in the previous formula:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation with padding and stride set to 1.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> <li>\\(\\text{Padding}\\): The number of zero-padding pixels added to the input on each side.</li> </ul>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#implementation-of-padding-in-keras","title":"Implementation of Padding in Keras","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"FNfPCR7sZzlY\" outputId=\"7f434a71-6d4e-4769-9e59-0c783c61adb9\" import tensorflow from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, Dense, Flatten from keras.datasets import mnist print(tensorflow.version) <pre><code>&lt;!-- #region id=\"kiB8qW3zbJ4A\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python id=\"oiGjkWo3bOrI\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#build-the-model-architecture-with-valid-padding","title":"Build the Model Architecture with <code>valid</code> Padding","text":"<p>```python id=\"W5XaAUCmbvw8\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#build-the-model-architecture-with-valid-padding-in-the-convolution-layers","title":"Build the model architecture with 'valid' padding in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"H9B2_Kr_h4Ls\" outputId=\"b023cf54-25a4-4ab0-8357-72ecf777166b\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#build-the-model-architecture-with-samezero-padding","title":"Build the Model Architecture with <code>same/zero</code> Padding","text":"<p>```python id=\"NhUEadZajEXr\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#build-the-model-architecture-with-zero-padding-in-the-convolution-layers","title":"Build the model architecture with 'zero' padding in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GR1cxM2GkKaL\" outputId=\"d22b576a-47eb-433f-8881-4890795b9cee\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#what-is-strides","title":"What is Strides?","text":"<p>In the context of CNNs, \"strides\" refer to the step size or the number of pixels the convolutional filter (kernel) moves at each step during the convolution operation. The stride parameter determines the distance between consecutive applications of the filter to the input, influencing the spatial dimensions of the output feature map. <code>Strided convolution</code> involves using a stride value greater than 1, meaning that the convolutional filter moves more than one pixel at a time while scanning the input.</p> <p>Key points about strides:</p> <ol> <li>Stride Value:</li> <li>Strides are usually set as positive integers.</li> <li>Common values are 1, indicating that the filter moves one pixel at a time, and 2, indicating that the filter moves two pixels at a time.</li> <li> <p>Larger stride values result in a more aggressive reduction of the spatial dimensions.</p> </li> <li> <p>Effect on Output Size:</p> </li> <li>Increasing the stride reduces the spatial dimensions of the output feature map.</li> <li> <p>Smaller strides lead to larger feature maps but may increase computational complexity.</p> </li> <li> <p>Strides and Subsampling:</p> </li> <li>Strides can be used to achieve subsampling or down-sampling by skipping pixels during the convolution.</li> <li>Subsampling can be beneficial for reducing the computational load and focusing on important features.</li> </ol> <p>Example of a Convolution Operation when the Stride is set to 2: </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#calculation-of-feature-map-size_1","title":"Calculation of Feature Map Size","text":"<p>The effect of strides on the output size can be described by the following formula:</p> \\[\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{{\\text{Stride}}} + 1\\] <p>Here are the terms in the formula:</p> <ul> <li>\\(\\text{Output Size}\\): The size of the feature map after the convolution operation.</li> <li>\\(\\text{Input Size}\\): The size of the input (or previous layer's feature map).</li> <li>\\(\\text{Filter Size}\\): The size of the convolutional filter (kernel).</li> <li>\\(\\text{Padding}\\): The number of zero-padding pixels added to the input on each side.</li> <li>\\(\\text{Stride}\\): The step size or the number of pixels the filter moves at each step during convolution.</li> </ul>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#why-strides-are-required","title":"Why Strides are required?","text":"<p>Strides in convolutional neural networks (CNNs) are required for several reasons:</p> <ol> <li>Downsampling and Efficiency:</li> <li>Strides enable downsampling of the input, reducing the spatial dimensions of the feature maps.</li> <li> <p>Downsampling is crucial for efficiency, reducing computational complexity and memory requirements.</p> </li> <li> <p>Feature Extraction:</p> </li> <li>Larger strides skip pixels during convolution, allowing the network to focus on more significant features and patterns.</li> <li> <p>This can be beneficial for capturing high-level features and reducing the spatial size of the feature maps.</p> </li> <li> <p>Control Over Model Complexity:</p> </li> <li>Strides provide a way to control the complexity of the model by influencing the spatial dimensions of the feature maps.</li> <li>They allow practitioners to balance between capturing fine-grained details and computational efficiency.</li> </ol> <p>In summary, strides are essential for controlling the trade-off between computational efficiency and feature representation in CNNs. They allow practitioners to tailor the network architecture to the specific requirements of the task at hand, ensuring effective feature extraction and model efficiency.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#implementation-of-strides-in-keras","title":"Implementation of Strides in Keras","text":"<p>```python id=\"-S7iEUyupiYd\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/03_padding_and_strides_in_cnn/#build-the-model-architecture-with-a-2-2-strides-in-the-convolution-layers","title":"Build the model architecture with a (2, 2) strides in the convolution layers","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\")) model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DIsVx4g5qWvD\" outputId=\"58a35826-4714-432a-9b20-d3c0dc219be0\"\n# Print the model's summary\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/","title":"Pooling Layer In Cnn","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#pooling-layer-in-cnn","title":"Pooling Layer in CNN","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#the-problem-with-convolution","title":"The Problem with Convolution","text":"<p>The convolution operation, while highly effective in capturing spatial hierarchies and features in data, comes with certain challenges. Two notable challenges associated with convolution are memory issues and translation variance.</p> <p>The convolution operation, while highly effective in capturing spatial hierarchies and features in data, comes with certain challenges. Two notable challenges associated with convolution are memory issues and translation variance.</p> <ol> <li>Memory Issues:</li> <li> <p>High Computational Cost:      Convolutional operations, especially with large filter sizes and deep networks, can be computationally expensive. The number of parameters and computations increases, leading to higher memory and processing requirements.</p> <p></p> </li> <li> <p>Translation Variance:</p> </li> <li>Positional Sensitivity:      Traditional convolution is sensitive to the absolute position of features in the input. This means that the network may not recognize an object if it appears in a different position within the input.</li> <li> <p>Lack of Invariance:      Convolutional networks may lack translation invariance, meaning that slight shifts or translations in the input can significantly affect the network's ability to recognize patterns.</p> <p></p> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#what-is-pooling","title":"What is Pooling?","text":"<p>Pooling, in CNNs, is a down-sampling operation that reduces the spatial dimensions of the input feature maps while retaining essential information. Pooling is typically applied after convolutional layers to progressively reduce the spatial size, decrease the computational load, and capture the most important features.</p> <p>Two common types of pooling operations are used in CNNs:</p> <ol> <li>Max Pooling:</li> <li> <p>Max pooling involves selecting the maximum value from a group of neighboring pixels in the input feature map.</p> </li> <li> <p>Average Pooling:</p> </li> <li>Average pooling calculates the average value of a group of neighboring pixels in the input feature map.</li> </ol> <p>Key points about pooling:</p> <ul> <li> <p>Spatial Reduction:   Pooling reduces the spatial dimensions of the feature map, effectively downsampling the information.</p> </li> <li> <p>Translation Invariance:   Pooling contributes to translation invariance by selecting the most relevant features and reducing sensitivity to small shifts or translations in the input. </p> </li> </ul> <p>Demo: Click on this link.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#pooling-on-volumes","title":"Pooling on Volumes","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#implementation-of-pooling-in-keras","title":"Implementation of Pooling in Keras","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"N6P9czPHclyl\" outputId=\"b2afcc81-8b44-40ce-e85b-eb8ce3ae3e6b\" import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from keras.datasets import mnist print(tf.version) <pre><code>&lt;!-- #region id=\"yZBFyQQAdB_P\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"yNSrQsoNdGeZ\" outputId=\"64309f2a-c4b1-4a1a-aa29-67656b189757\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#build-the-model-architecture-with-maxpooling-layer","title":"Build the Model Architecture with <code>MaxPooling</code> Layer","text":"<p>```python id=\"Nn_7UddRdjHs\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#build-the-model-architecture-with-maxpooling-layer_1","title":"Build the model architecture with 'MaxPooling' layer","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding=\"same\")) model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")) model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding=\"same\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(128, activation=\"relu\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9H3oo9X4e0Qo\" outputId=\"712dfcb4-af55-4cd6-d116-92b7e17bed51\"\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#advantages-of-pooling","title":"Advantages of Pooling","text":"<p>Pooling operations offer several key advantages that contribute to the network's effectiveness in image processing tasks:</p> <ol> <li>Spatial Dimension Reduction:</li> <li> <p>Pooling reduces the spatial dimensions of input feature maps, effectively downsampling the information. This spatial reduction is crucial for managing computational load and improving efficiency in subsequent layers.   </p> </li> <li> <p>Translation Invariance:</p> </li> <li> <p>Pooling introduces a level of translation invariance by selecting the most relevant features. This means the network becomes less sensitive to small shifts or translations in the input, enhancing its ability to generalize across different positions of objects.   </p> </li> <li> <p>Enhanced Features:</p> </li> <li> <p>Pooling operations contribute to enhancing distinctive features within the input data. By selecting the most significant values from local regions, pooling helps the network focus on crucial patterns and characteristics, facilitating better feature extraction.   </p> </li> <li> <p>No Need for Training:</p> <ul> <li>Pooling is a parameter-free operation, requiring no additional training or learnable weights. This simplicity makes it easy to implement and incorporate into CNN architectures without introducing extra parameters that would need to be adjusted during the training process. The lack of trainable parameters in pooling layers adds to the efficiency and simplicity of the overall network design.</li> </ul> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#types-of-pooling","title":"Types of Pooling","text":"<p>There are several types of pooling operations used in Convolutional Neural Networks (CNNs), each serving different purposes in feature extraction and spatial dimension reduction. Three common types of pooling are MaxPooling, AveragePooling, and Global Pooling:</p> <ol> <li>Max Pooling:</li> <li>Operation: Selects the maximum value from a group of neighboring pixels in the input feature map.</li> <li>Purpose: Emphasizes the most prominent features, contributing to translation invariance and retaining salient information.</li> <li> <p>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nMax Pooling (2x2):\n[[5]]\n</code></pre></p> </li> <li> <p>Average Pooling:</p> </li> <li>Operation: Calculates the average value from a group of neighboring pixels in the input feature map.</li> <li>Purpose: Smooths the representation, reduces sensitivity to outliers, and provides a form of translation invariance.</li> <li> <p>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nAverage Pooling (2x2):\n[[3.5]]\n</code></pre></p> </li> <li> <p>Global Pooling (Global Average Pooling or Global Max Pooling):</p> </li> <li>Operation: Computes a single value (global average or global maximum) for each channel across the entire feature map.</li> <li>Purpose: Aggregates information globally, reducing the spatial dimensions to a single value per channel. Commonly used as a transition to fully connected layers in classification tasks.</li> <li>Example: <pre><code>Input:\n[[1, 2, 3],\n [4, 5, 6],\n [7, 8, 9]]\n\nGlobal Average Pooling:\n[[5]]\n</code></pre></li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/04_pooling_layer_in_cnn/#disadvantages-of-pooling","title":"Disadvantages of Pooling","text":"<p>While pooling operations offer several advantages in Convolutional Neural Networks (CNNs), they also come with certain disadvantages. Here are some drawbacks associated with pooling:</p> <ol> <li>Loss of Spatial Information:</li> <li> <p>Pooling involves down-sampling, leading to a reduction in spatial dimensions. This reduction can result in the loss of fine-grained spatial information, making it challenging to reconstruct the exact spatial arrangement of features.</p> </li> <li> <p>Reduced Sensitivity to Small-Scale Patterns:</p> </li> <li> <p>Max pooling, in particular, focuses on the most significant features within a local region. While this is beneficial for translation invariance, it can lead to reduced sensitivity to small-scale patterns, especially if the maximum values dominate the features.</p> </li> <li> <p>Not Suitable for Image Segmentation:</p> </li> <li>In tasks such as image segmentation, where preserving spatial information is crucial, pooling may not be as suitable. Downsampling through pooling can result in a loss of fine-grained details, making it challenging for the network to precisely delineate object boundaries in segmented images.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/","title":"Cnn Architecture And Lenet 5","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#cnn-architecture-lenet-5","title":"CNN Architecture &amp; LeNet-5","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#cnn-architecture","title":"CNN Architecture","text":"<p>Convolutional Neural Networks (CNNs) have been highly successful in various computer vision tasks, such as image classification, object detection, and image segmentation. While CNN architectures can vary based on specific requirements and tasks, a typical CNN architecture consists of several key components. Here is a generic overview of a CNN architecture:</p> <ol> <li>Input Layer:</li> <li> <p>Accepts the input data, usually in the form of images. The size of the input layer corresponds to the dimensions of the input images.</p> </li> <li> <p>Convolutional Layers:</p> </li> <li>Convolutional layers are the core building blocks of CNNs. They use filters (kernels) to convolve over input feature maps, extracting hierarchical features.</li> <li> <p>Activation functions (e.g., ReLU) introduce non-linearity.</p> </li> <li> <p>Pooling Layers:</p> </li> <li>Pooling layers follow convolutional layers and downsample the spatial dimensions of feature maps. Common pooling types include Max Pooling and Average Pooling.</li> <li> <p>Pooling contributes to translation invariance and reduces computational complexity.</p> </li> <li> <p>Flatten Layer:</p> </li> <li> <p>Flatten layers transition from convolutional layers to fully connected layers. They reshape the 3D output of the convolutional/pooling layers into a 1D vector.</p> </li> <li> <p>Fully Connected Layers:</p> </li> <li> <p>Fully connected layers connect every neuron to every neuron in the previous and next layers. These layers capture global patterns and relationships in the data.</p> </li> <li> <p>Output Layer:</p> </li> <li>The output layer produces the final predictions. The number of neurons in this layer corresponds to the number of classes in a classification task.</li> <li>Activation functions vary based on the task (e.g., softmax for classification).</li> </ol> <p>This architecture is a sequential stack of layers and is commonly implemented using deep learning frameworks such as TensorFlow or PyTorch. Specific CNN architectures like LeNet-5, AlexNet, VGGNet, GoogLeNet (Inception), ResNet, and others have been influential in shaping the field. The choice of architecture depends on factors like the complexity of the task, available data, and computational resources. Researchers often modify and adapt existing architectures or design custom architectures to address specific challenges.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#lenet-5-architecture","title":"LeNet-5 Architecture","text":"<p>LeNet-5 is a pioneering convolutional neural network architecture designed for handwritten digit recognition. It was introduced by Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner in 1998. LeNet-5 played a crucial role in demonstrating the effectiveness of deep learning in computer vision tasks. </p> <p></p> <p>Below is an overview of the LeNet-5 architecture:</p> <ol> <li>Input Layer:</li> <li> <p>Accepts grayscale images of size 32x32 pixels.</p> </li> <li> <p>Convolutional Layer (C1):</p> </li> <li>Convolution with 6 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 28x28x6.</p> </li> <li> <p>Subsampling Layer (S2):</p> </li> <li>Average pooling over non-overlapping 2x2 regions.</li> <li> <p>Output feature maps: 14x14x6.</p> </li> <li> <p>Convolutional Layer (C3):</p> </li> <li>Convolution with 16 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 10x10x16.</p> </li> <li> <p>Subsampling Layer (S4):</p> </li> <li>Average pooling over non-overlapping 2x2 regions.</li> <li> <p>Output feature maps: 5x5x16.</p> </li> <li> <p>Convolutional Layer (C5):</p> </li> <li>Convolution with 120 filters of size 5x5.</li> <li>Activation function: Sigmoid.</li> <li> <p>Output feature maps: 1x1x120.</p> </li> <li> <p>Fully Connected Layer (F6):</p> </li> <li>Fully connected layer with 84 neurons.</li> <li> <p>Activation function: Sigmoid.</p> </li> <li> <p>Output Layer:</p> </li> <li>Fully connected layer with 10 neurons (corresponding to the 10 digits in digit recognition tasks).</li> <li>Activation function: Softmax.</li> </ol> <p>The architecture incorporates a series of convolutional and subsampling layers, followed by fully connected layers. Sigmoid activation functions were used in the original design, and average pooling was employed in the subsampling layers. LeNet-5 demonstrated the effectiveness of deep learning in pattern recognition tasks and laid the foundation for subsequent developments in convolutional neural networks.</p> <p>It's important to note that while LeNet-5 was groundbreaking at the time, more recent CNN architectures, such as those used in image classification tasks today, often involve deeper networks, different activation functions (e.g., ReLU), and other architectural innovations.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#implementation-of-lenet-5-architecture-in-keras","title":"Implementation of LeNet-5 Architecture in Keras","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"HZj4KksAwAfa\" outputId=\"87794a1d-73b6-4e22-bac4-1b270768aea8\" import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense from keras.datasets import mnist print(tf.version) <pre><code>&lt;!-- #region id=\"bTl6KfiRwdI2\" --&gt;\n### **Read the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"zWQgFT_2wjU1\" outputId=\"af1b51da-8bc0-48ad-f3dd-e371e280ae97\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#build-the-lenet-5-architecture","title":"Build the <code>LeNet-5</code> Architecture","text":"<p>```python id=\"0kOoXdYuw2mw\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/05_cnn_architecture_and_lenet_5/#build-the-lenet-5-architecture_1","title":"Build the LeNet-5 architecture","text":"<p>model = Sequential()</p> <p>model.add(Conv2D(6, kernel_size=(5, 5), padding=\"valid\", activation=\"tanh\", input_shape=(32, 32, 1))) model.add(AveragePooling2D((2, 2), strides=2, padding=\"valid\"))</p> <p>model.add(Conv2D(16, kernel_size=(5, 5), padding=\"valid\", activation=\"tanh\")) model.add(AveragePooling2D((2, 2), strides=2, padding=\"valid\"))</p> <p>model.add(Flatten())</p> <p>model.add(Dense(120, activation=\"tanh\")) model.add(Dense(84, activation=\"tanh\")) model.add(Dense(10, activation=\"softmax\")) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"dWT-qUF5yGwT\" outputId=\"e0d8a006-e373-443a-e412-3883e8cc694c\"\nmodel.summary()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/06_cnn_vs_ann/","title":"Cnn Vs Ann","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/06_cnn_vs_ann/#cnn-vs-ann","title":"CNN Vs ANN","text":"<p>Convolutional Neural Networks (CNNs) and Artificial Neural Networks (ANNs) are both types of neural networks, but they differ in their architectures, designs, and applications. Here's a brief comparison between CNNs and ANNs:</p> <p>Working Principle of ANN: </p> <p>Working Principle of CNN </p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/06_cnn_vs_ann/#similarities-between-ann-and-cnn","title":"Similarities between ANN and CNN","text":"<p>The working principles of Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs) share several fundamental concepts, despite their architectural differences. Here are the key similarities in their working principles:</p> <ol> <li>Neurons and Activation Functions:</li> <li>Both ANNs and CNNs are composed of interconnected neurons.</li> <li>Neurons in both architectures process weighted inputs, apply activation functions, and produce output signals.</li> <li> <p>Activation functions introduce non-linearity, allowing the network to learn complex relationships in the data.</p> </li> <li> <p>Forward Propagation:</p> </li> <li>Both architectures employ forward propagation to process input data and generate predictions.</li> <li> <p>During forward propagation, input signals are passed through the network's layers, and weighted sums are computed at each neuron. The output is then obtained through the activation function.</p> </li> <li> <p>Loss Function and Training:</p> </li> <li>Both ANNs and CNNs are trained using a supervised learning approach.</li> <li> <p>They use a loss function to measure the difference between predicted and actual outputs. The goal is to minimize this loss during training.</p> </li> <li> <p>Backpropagation:</p> </li> <li>Both architectures use backpropagation as a learning algorithm.</li> <li> <p>Backpropagation involves calculating gradients of the loss with respect to the network's parameters and adjusting these parameters to minimize the loss.</p> </li> <li> <p>Optimization Algorithms:</p> </li> <li>Both ANNs and CNNs employ optimization algorithms, such as gradient descent and its variants (e.g., stochastic gradient descent), to update weights and biases during training.</li> <li> <p>These algorithms aim to find the optimal set of parameters that minimize the loss function.</p> </li> <li> <p>Batch Processing:</p> </li> <li>Both architectures can process input data in batches during training to improve computational efficiency.</li> <li>**Batch processing involves updating weights and biases based on the average gradient calculated over a batch of input samples.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/06_cnn_vs_ann/#quick-quiz","title":"Quick Quiz","text":"<ol> <li> <p>How many learnable parameters are there in a convolutional layer with 50 filters applied to an RGB image, where each filter has a shape of (3, 3, 3)?</p> <p>Ans: Total learnable parameters in a single filter = <code>(3 * 3 * 3) + 1 = 28</code>  where, (3, 3, 3) is the shape of a filter and 1 is for bias. Total learnable parameters of 50 filters = <code>(28 * 50) = 1400</code></p> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/06_cnn_vs_ann/#differences-between-ann-and-cnn","title":"Differences between ANN and CNN","text":"<p>The computational cost of Convolutional Neural Networks (CNNs) and Artificial Neural Networks (ANNs) varies significantly when working with image data. Here are the key differences in terms of computational cost:</p> <ol> <li> <p>Parameter Efficiency:</p> <ul> <li>Fully connected layers in ANNs have a large number of parameters, especially when working with high-dimensional data like images. The sheer number of parameters increases the computational cost during both training and inference.</li> <li>CNNs leverage parameter sharing through convolutional filters, leading to a more parameter-efficient architecture. The use of shared filters significantly reduces the number of parameters compared to ANNs when processing image data.</li> </ul> </li> <li> <p>Localized Operations:</p> <ul> <li>ANN operates on the entire input space, making it computationally intensive, especially for large images. Lacks the ability to efficiently capture localized patterns.</li> <li>CNN Employs localized operations, such as convolution and pooling, which significantly reduce the computational cost. Focuses on specific regions of the input, enhancing efficiency in capturing local patterns.</li> </ul> </li> <li> <p>Hierarchical Feature Extraction:</p> <ul> <li>Hierarchical feature extraction in ANNs involves fully connected layers, which may struggle to capture spatial hierarchies effectively. Computationally expensive due to the large number of parameters and global connectivity.</li> <li>CNNs are designed for hierarchical feature extraction through convolutional layers. Filters capture spatial hierarchies locally, making them computationally efficient in processing image data.</li> </ul> </li> <li> <p>Spatial Invariance:</p> <ul> <li>ANN lacks inherent spatial invariance, requiring extensive processing to handle variations in object position within an image. Higher computational cost in dealing with spatial transformations.</li> <li>CNN incorporates pooling layers to achieve spatial invariance, reducing the computational burden related to position variations. Robustness to translations results in computational efficiency in handling diverse spatial configurations.</li> </ul> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/08_cat-vs-dogs-image-classification/","title":"Cat Vs Dogs Image Classification","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/08_cat-vs-dogs-image-classification/#cat-vs-dogs-image-classification","title":"Cat vs Dogs Image Classification","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/08_cat-vs-dogs-image-classification/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport os\nfrom glob import glob\nimport torch\nfrom torch import nn\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom torchmetrics import Accuracy\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams[\"font.serif\"] = \"Times New Roman\"\n\ntorch.__version__\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/08_cat-vs-dogs-image-classification/#read-the-data","title":"Read the Data","text":"<pre><code># Store all the image paths inside a variable\nimage_paths = glob(r\"/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/*/*.jpg\")\nprint(\"Total number of images:\", len(image_paths))\n\n# Print the size of five random images\nrandom.seed(42)\nfor f in range(5):\n    img = cv2.imread(random.choice(image_paths))\n    print(img.shape)\n</code></pre> <pre><code># Split the file paths into train and test set\ntrain_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\nprint(\"Number of training images:\", len(train_paths))\nprint(\"Number of testing images:\", len(test_paths))\n</code></pre> <pre><code># Write a helper function to prepare images\ndef prepare_images(image_paths, size, labels):\n    # Store all the images and labels into seperate arrays\n    X, y = [], []\n\n    # Preapare testings images and labels\n    for f in tqdm(image_paths, desc=\"Preparing images...\"):\n        try:\n            img = plt.imread(f) # reading the image\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # transforming from BGR to RGB\n            img_resized = cv2.resize(img, (size, size)) # Resizing the image\n\n            if img_resized.shape == (SIZE, SIZE, 3):\n                X.append(img_resized)\n                label_str = f.split(\"/\")[-2].lower()\n                y.append(labels[label_str])\n\n            else:\n                continue\n\n        except:\n            continue\n\n    return np.array(X, dtype=np.uint8), np.array(y, dtype=np.uint8)\n</code></pre> <pre><code># Define the image size parameters\nSIZE = 128\nLABELS = {\"cat\": 0, \"dog\": 1}\n\n# Store all the images into arrays\nX_train, y_train = prepare_images(train_paths, size=SIZE, labels=LABELS)\nX_test, y_test = prepare_images(test_paths, size=SIZE, labels=LABELS)\n\nprint(X_train.shape, X_test.shape)\n</code></pre> <pre><code># Convert the channel last to channel first\nX_train, X_test = np.transpose(X_train, (0, 3, 1, 2)), np.transpose(X_test, (0, 3, 1, 2))\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n</code></pre> <pre><code># Plot some random images from the training data\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows*cols+1):\n    random_id = random.randint(0, X_train.shape[0])\n    X, y = X_train[random_id], y_train[random_id]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.transpose(X, (1, 2, 0)))\n    plt.title([key for key, value in LABELS.items() if y==value][0])\n    plt.axis(False)\n</code></pre> <pre><code># Convert the data into tensors\nX_train, y_train = torch.from_numpy(X_train).type(torch.float), torch.from_numpy(y_train).type(torch.LongTensor)\nX_test, y_test = torch.from_numpy(X_test).type(torch.float), torch.from_numpy(y_test).type(torch.LongTensor)\n\n# Apply normalization\nX_train, X_test = X_train/255, X_test/255\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/CNN/08_cat-vs-dogs-image-classification/#build-the-model-vgg-16-using-pytorch","title":"Build the Model (VGG-16) using PyTorch","text":"<p>Model Architecture: </p> <pre><code># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</code></pre> <pre><code># Build the VGG-16 architecture\nclass VGG16(nn.Module):\n    def __init__(self, in_channels=3, num_classes=2):\n        super().__init__()\n\n        # First block\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=64),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Second block\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=128),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Third block\n        self.conv_block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Fourth block\n        self.conv_block_4 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Fifth block\n        self.conv_block_5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Final linear block and the classifier\n        self.linear_block_6 = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(in_features=4*4*512, out_features=4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(in_features=4096, out_features=4096),\n            nn.ReLU(),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, X: torch.Tensor):\n        X = self.conv_block_1(X)\n        X = self.conv_block_2(X)\n        X = self.conv_block_3(X)\n        X = self.conv_block_4(X)\n        X = self.conv_block_5(X)\n        X = self.linear_block_6(X)\n        return X\n</code></pre> <pre><code># Create an instance of VGG-16 model\ntorch.manual_seed(42)\n\nmodel = VGG16(in_channels=3, num_classes=2).to(device)\nmodel\n</code></pre> <pre><code># Setup the loss function and the optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.005, momentum=0.9)\n\n# Setup the accuracy function using torchmetrics\naccuracy_fn = Accuracy(task=\"binary\", num_classes=2).to(device)\n</code></pre> <pre><code># Prepare dataloaders\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=True)\n\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</code></pre> <pre><code># Start the training loop\ntorch.manual_seed(42)\n\n# Define the epochs\nEPOCHS = 10\n\n# Store all the history in a dictionary\nhistory = {\n    \"epoch\": [],\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"test_loss\": [],\n    \"test_acc\": []\n}\n\nhighest_test_acc = 0\nbest_model_path = \"best_model.pth\" \n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch: {epoch} | \", end=\"\")\n\n    ## Training\n    train_avg_loss, train_avg_acc = 0.0, 0.0\n\n    for X, y in train_dataloader:\n        X, y = X.to(device), y.to(device)\n\n        # Set the model in training model\n        model.train()\n\n        # Perform the steps\n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n        train_avg_loss += loss.item()\n\n        # Calculate accuracy\n        acc = accuracy_fn(y_pred.argmax(dim=1), y).item()\n        train_avg_acc += acc\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Divide total train_avg_loss, train_avg_acc by the length of the train dataloader\n    train_avg_loss /= len(train_dataloader)\n    train_avg_acc /= len(train_dataloader)\n\n    # Testing\n    test_avg_loss, test_avg_acc = 0.0, 0.0\n    model.eval()\n\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            X, y = X.to(device), y.to(device)\n            test_pred = model(X)\n            test_loss = loss_fn(test_pred, y).item()\n            test_avg_loss += test_loss\n\n            test_acc = accuracy_fn(test_pred.argmax(dim=1), y).item()\n            test_avg_acc += test_acc\n\n    # Divide the test_avg_loss and test_avg_acc by length of test dataloader\n    test_avg_loss /= len(test_dataloader)\n    test_avg_acc /= len(test_dataloader)\n\n    # Print out training and testing results\n    history[\"epoch\"].append(epoch)\n    history[\"train_loss\"].append(train_avg_loss)\n    history[\"train_acc\"].append(train_avg_acc)\n    history[\"test_loss\"].append(test_avg_loss)\n    history[\"test_acc\"].append(test_avg_acc)\n\n    print(f\"Train Loss: {train_avg_loss:.4f}, Train Accuracy: {train_avg_acc:.4f} | \", end=\"\")\n    print(f\"Test Loss: {test_avg_loss:.4f}, Test Accuracy: {test_avg_acc:.4f}\")\n\n    # Save the model if test accuracy is the lowest\n    if test_avg_acc &gt; highest_test_acc:\n        highest_test_acc = test_avg_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"*** Model saved with highest test accuracy: {highest_test_acc:.4f} ***\")\n</code></pre> <pre><code>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes = axes.flatten()\n\nsns.lineplot(x=history[\"epoch\"], y=history[\"train_loss\"], label=\"Train\", ax=axes[0], marker=\"o\")\nsns.lineplot(x=history[\"epoch\"], y=history[\"test_loss\"], label=\"Test\", ax=axes[0], marker=\"o\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].grid()\n\nsns.lineplot(x=history[\"epoch\"], y=history[\"train_acc\"], label=\"Train\", ax=axes[1], marker=\"o\")\nsns.lineplot(x=history[\"epoch\"], y=history[\"test_acc\"], label=\"Test\", ax=axes[1], marker=\"o\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].grid()\n\nplt.tight_layout();\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/","title":"Pytorch Autograd","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#pytorch-autograd","title":"PyTorch Autograd","text":"<p>Autograd is PyTorch's automatic differentiation engine. It keeps track of all operations performed on tensors that have <code>requires_grad=True</code>, creating a computation graph dynamically. This graph is used to compute gradients for optimization tasks like backpropagation.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#how-does-it-work","title":"How Does It Work?","text":"<ol> <li>Computation Graph:</li> <li>When you perform operations on tensors, autograd dynamically builds a directed acyclic graph (DAG) where nodes represent operations and edges represent the flow of data.</li> <li> <p>This graph allows autograd to trace how each tensor is derived from others.</p> </li> <li> <p>Backward Pass:</p> </li> <li> <p>When you call <code>.backward()</code> on a tensor, autograd traverses the graph in reverse order (hence \"backpropagation\"), computing gradients for all tensors with <code>requires_grad=True</code>.</p> </li> <li> <p>Gradient Storage:</p> </li> <li>Gradients are stored in the <code>.grad</code> attribute of the corresponding tensor.</li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#import-dependencies","title":"Import Dependencies","text":"<p>```python executionInfo={\"elapsed\": 4689, \"status\": \"ok\", \"timestamp\": 1737239751080, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"R4cOlDb_kQwD\" import numpy as np import matplotlib.pyplot as plt import torch</p> <p>import warnings warnings.filterwarnings('ignore') <pre><code>&lt;!-- #region id=\"O86hSGhidee6\" --&gt;\n## **Calculate Gradient Manually**\n\n**Example-1:**&lt;br&gt;\nWe want to calculate the gradient of the function:\n\n$$ y = x^2 $$\n\nThe derivative of \\( y \\) with respect to \\( x \\) is:\n\n$$ \\frac{\\partial y}{\\partial x} = 2x $$\n\nUsing Python, we can compute the gradient at a specific value of \\( x \\) with the following code:\n\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 6, \"status\": \"ok\", \"timestamp\": 1737239751080, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"z3Gig1uUjqGv\" outputId=\"64b47f94-1056-4261-dd5b-c7f5da02eec0\"\n# Function to calculate the gradient of y = x^2\ndef dy_dx(x):\n    \"\"\"\n    Calculate the derivative of y = x^2 with respect to x.\n\n    Parameters:\n        x (float or int): The value of x at which the gradient is evaluated.\n\n    Returns:\n        float: The gradient (2 * x).\n    \"\"\"\n    return 2 * x\n\n# Example usage\nx = 3\ngradient = dy_dx(x)\nprint(f\"The gradient of y = x^2 at x = {x} is {gradient}.\")\n</code></pre></p> <p>Example-2: We want to calculate the gradient of the function: $$ y = x^2 $$ $$ z = \\sin {y} $$</p> <p>The derivative of \\( z \\) with respect to \\( x \\) is:</p> \\[ \\frac{\\partial z}{\\partial x} = \\frac{\u2202z}{\u2202y} \u22c5 \\frac{\u2202y}{\u2202x} $$ $$ \\frac{\\partial z}{\\partial x} = \\cos{y} \u22c5 2x $$ $$ \\frac{\\partial z}{\\partial x} = \\cos{x^2} \u22c5 2x \\] <p>Using Python, we can compute the gradient at a specific value of \\( x \\) with the following code:</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 5, \"status\": \"ok\", \"timestamp\": 1737239751080, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"SNefBd91pTt2\" outputId=\"4453ec90-eace-482c-dd23-2b6de822db66\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#function-to-calculate-the-gradient-of-z","title":"Function to calculate the gradient of z","text":"<p>import math</p> <p>def dz_dx(x):     \"\"\"     Calculate the derivative of z with respect to x.</p> <pre><code>Parameters:\n    y (float or int): The value of x at which the gradient is evaluated.\n\nReturns:\n    float: The gradient.\n\"\"\"\nreturn math.cos(x**2) * (2 * x)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#example-usage","title":"Example usage","text":"<p>x = 3 gradient = dz_dx(x) print(f\"The gradient of z at x = {x} is {gradient:.2f}.\") <pre><code>&lt;!-- #region id=\"6XAw9PBatKqg\" --&gt;\n## **Calculate Gradient using PyTorch**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 4, \"status\": \"ok\", \"timestamp\": 1737239751080, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"TjupWz-4tQSn\" outputId=\"b9333d21-a682-4de5-86c8-13c99993a5ab\"\n# Example-1\n# Define a tensor with gradient tracking enabled\nx = torch.tensor(3.0, requires_grad=True)\n\n# Define the function y = x^2\ny = x**2\n\n# Print the values of x and y\nprint(\"x:\", x)\nprint(\"y:\", y)\n\n# Perform backpropagation to compute the gradient\ny.backward()\n\n# Print the gradient of y with respect to x\nprint(\"Gradient (dy/dx):\", x.grad)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 3, \"status\": \"ok\", \"timestamp\": 1737239751080, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"0is1TmcVt_vP\" outputId=\"4f6fa99c-341f-46c1-abff-958d4123cd48\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#example-2","title":"Example-2","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#define-a-tensor-with-gradient-tracking-enabled","title":"Define a tensor with gradient tracking enabled","text":"<p>x = torch.tensor(3.0, requires_grad=True)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#define-the-function-y-x2","title":"Define the function y = x^2","text":"<p>y = x**2</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#define-the-function-z-siny","title":"Define the function z = sin(y)","text":"<p>z = torch.sin(y)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#print-the-values-of-x-and-y","title":"Print the values of x and y","text":"<p>print(\"x:\", x) print(\"y:\", y) print(\"z:\", z)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#perform-backpropagation-to-compute-the-gradient","title":"Perform backpropagation to compute the gradient","text":"<p>z.backward()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#print-the-gradient-of-y-with-respect-to-x","title":"Print the gradient of y with respect to x","text":"<p>print(\"Gradient (dy/dx):\", x.grad) <pre><code>&lt;!-- #region id=\"hnzg-HZskgJ2\" --&gt;\n## **Manual Gradient of Loss Calculation w.r.t Weight and Bias**\n\n1. Linear Transformation:\n$$ z = w \\cdot x + b $$\n2. Activation:\n$$ y_{pred} = \u03c3(z) = \\frac{1}{1 + e^{-z}} $$\n3. Loss Function (Binary Cross-Entropy Loss):\n$$ L = -[y_{target} \\cdot \\ln(y_{pred}) + (1 - y_{target}) \\cdot \\ln( - y_{pred})] $$\n\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 218, \"status\": \"ok\", \"timestamp\": 1737240113448, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"F9rVMl9OkmOO\"\n# Inputs\nx = torch.tensor(6.7) # Input feature\ny = torch.tensor(0.0) # True Label (Binary)\n\nw = torch.tensor(1.0) # Weight\nb = torch.tensor(0.0) # Bias\n</code></pre></p> <p>```python executionInfo={\"elapsed\": 208, \"status\": \"ok\", \"timestamp\": 1737240941033, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"Id8yFk5TlHRh\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#binary-cross-entropy-loss-for-scalar","title":"Binary Cross-Entropy Loss for scalar","text":"<p>def binary_cross_entropy_loss(prediction, target):     epsilon = 1e-8     prediction = torch.clamp(prediction, epsilon, 1-epsilon)     return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction)) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 220, \"status\": \"ok\", \"timestamp\": 1737241262860, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"M317Df_uoTNo\" outputId=\"da8f6ea1-5590-4de8-9bb5-143ff5721442\"\n# Forward pass\nz = w * x + b # Weighted sum (Linear Transformation)\ny_pred = torch.sigmoid(z) # Predicted Probability (Activation)\n\n# Compute binary cross-entropy loss\nloss = binary_cross_entropy_loss(y_pred, y)\nprint(loss)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 3, \"status\": \"ok\", \"timestamp\": 1737241511871, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"7ZbT9UsOopoq\" outputId=\"e68ee04d-ad31-41a3-ddeb-22cd45ee26e7\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#derivatives","title":"Derivatives:","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#1-dldy_pred-loss-with-respect-to-the-prediction-y_pred","title":"1. dL/d(y_pred): Loss with respect to the prediction (y_pred)","text":"<p>dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#2-dy_preddz-prediction-y_pred-with-respect-to-z-sigmoid-derivative","title":"2. d(y_pred)/dz: Prediction (y_pred) with respect to z (sigmoid derivative)","text":"<p>dy_pred_dz = y_pred * (1 - y_pred)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#3-dzdw-and-dzdb-z-with-respect-to-w-and-b","title":"3. dz/dw and dz/db: z with respect to w and b","text":"<p>dz_dw = x # dz/dw = x dz_db = 1 # dz/db = 1 (bias contributes directly to z)</p> <p>dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw dL_db = dloss_dy_pred * dy_pred_dz * dz_db print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw:.4f}\") print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db:.4f}\") <pre><code>&lt;!-- #region id=\"a2fQoQ2Booqy\" --&gt;\n## **Automatic Gradient of Loss Calculation w.r.t Weight and Bias using Autograd**\n&lt;!-- #endregion --&gt;\n\n```python executionInfo={\"elapsed\": 2, \"status\": \"ok\", \"timestamp\": 1737241624557, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"kqNYJc2PqrSw\"\n# Inputs\nx = torch.tensor(6.7) # Input feature\ny = torch.tensor(0.0) # True Label (Binary)\n\nw = torch.tensor(1.0, requires_grad=True) # Weight\nb = torch.tensor(0.0, requires_grad=True) # Bias\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 765, \"status\": \"ok\", \"timestamp\": 1737241655224, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"wbeE6290q6hS\" outputId=\"ba92e823-d93f-40d4-fdb6-77430c2d0577\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#forward-pass","title":"Forward pass","text":"<p>z = w * x + b # Weighted sum (Linear Transformation) y_pred = torch.sigmoid(z) # Predicted Probability (Activation)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#compute-binary-cross-entropy-loss","title":"Compute binary cross-entropy loss","text":"<p>loss = binary_cross_entropy_loss(y_pred, y) print(loss) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 3, \"status\": \"ok\", \"timestamp\": 1737241720252, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"2h5JdvgYrJNM\" outputId=\"38c41093-495b-4ad2-f3e5-84be8666412f\"\nloss.backward()\n\nprint(w.grad)\nprint(b.grad)\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#calculate-gradients-for-multiple-inputs","title":"Calculate Gradients for Multiple Inputs","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 257, \"status\": \"ok\", \"timestamp\": 1737242652322, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"kjvJMULTs4Tn\" outputId=\"16b29523-b469-4361-ca9f-268d686ad11c\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#create-a-pytorch-tensor-with-multiple-inputs","title":"Create a PyTorch tensor with multiple inputs","text":"<p>x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) print(x) y = (x ** 2).mean() print(y) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 250, \"status\": \"ok\", \"timestamp\": 1737242653175, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"PR2RB3AZtPAa\" outputId=\"17851f38-c10b-4eaa-833a-fd8b9b0c9721\"\ny.backward()\nx.grad\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#clearing-gradients","title":"Clearing Gradients","text":"<p>Gradients can be cleared using the <code>optimizer.zero_grad()</code> function when using optimizers. For manually tracking gradients, you can reset the gradients by assigning None to the .grad attribute of the tensor.</p> <p>```python executionInfo={\"elapsed\": 234, \"status\": \"ok\", \"timestamp\": 1737242656088, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"a-0-Z1mUt1c2\" x = torch.tensor(6.0, requires_grad=True) y = (x ** 2) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 211, \"status\": \"ok\", \"timestamp\": 1737242657604, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"6DQp4tX7uRU5\" outputId=\"48edb965-965c-44e4-fec0-f00a8bc92f63\"\ny.backward()\nprint(x.grad)\nx.grad.zero_()\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#disable-gradient-tracking","title":"Disable Gradient Tracking","text":"<p>n PyTorch, you can disable gradient tracking when gradients are not needed, such as during inference or evaluations, to improve computational efficiency. This is done using the <code>torch.no_grad()</code>context manager or by setting <code>requires_grad=False</code> for specific tensors.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} executionInfo={\"elapsed\": 252, \"status\": \"ok\", \"timestamp\": 1737242857201, \"user\": {\"displayName\": \"Krishnagopal Halder\", \"userId\": \"16954898871344510854\"}, \"user_tz\": -60} id=\"xEsyVP6rvHXn\" outputId=\"76743d51-0de3-4b3f-9160-6c264c6a8a61\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#create-tensors-with-gradient-tracking-enabled","title":"Create tensors with gradient tracking enabled","text":"<p>x = torch.tensor(6.7, requires_grad=True) w = torch.tensor(1.0, requires_grad=True) b = torch.tensor(0.0, requires_grad=True)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#perform-operations-without-gradient-tracking","title":"Perform operations without gradient tracking","text":"<p>with torch.no_grad():     z = w * x + b  # No gradients will be tracked for this operation     y_pred = torch.sigmoid(z)</p> <p>print(f\"z: {z}\") print(f\"y_pred: {y_pred}\")</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/03_pytorch_autograd/#verify-that-gradients-are-not-tracked","title":"Verify that gradients are not tracked","text":"<p>print(f\"Requires Grad (z): {z.requires_grad}\")  # Output: False ```</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/","title":"Pytorch Training Pipeline","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#pytorch-training-pipeline","title":"PyTorch Training Pipeline","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom torchmetrics import Accuracy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#read-the-dataset","title":"Read the Dataset","text":"<pre><code># Load the breast cancer dataset using Pandas\ndata = pd.read_csv(r\"D:\\GITHUB\\PyTorch-for-Deep-Learning-and-Machine-Learning\\datasets\\breast_cancer_data.csv\")\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#data-pre-processing","title":"Data Pre-processing","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#data-cleaning","title":"Data Cleaning","text":"<pre><code># Drop the irrelevant columns\ndata.drop(columns=['id', 'Unnamed: 32'], inplace=True)\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#train-test-split","title":"Train-Test Split","text":"<pre><code># Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(columns='diagnosis'),\n    data['diagnosis'],\n    test_size=0.3,\n    random_state=42\n)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#feature-scaling","title":"Feature Scaling","text":"<pre><code># Print the column information\nX_train.info()\n</code></pre> <pre><code># Scale the input variables using standarad scaler\nscaler = StandardScaler()\nX_train_scaled =scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(X_train_scaled.shape, X_test_scaled.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#label-encoding","title":"Label Encoding","text":"<pre><code># Encode the target variable using label encoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\nprint(y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#convert-numpy-arrays-to-pytoch-tensors","title":"Convert NumPy Arrays to PyToch Tensors","text":"<pre><code>X_train_tensor = torch.from_numpy(X_train_scaled)\nX_test_tensor = torch.from_numpy(X_test_scaled)\ny_train_tensor = torch.from_numpy(y_train_encoded)\ny_test_tensor = torch.from_numpy(y_test_encoded)\n\nprint(X_train_tensor.shape, X_test_tensor.shape, y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#building-the-model","title":"Building the Model","text":"<pre><code>class MySimpleNN:\n\n    def __init__(self, X: torch.Tensor):\n\n        self.weights = torch.rand(X.shape[1], 1, dtype=torch.float64, requires_grad=True)\n        self.bias = torch.rand(1, dtype=torch.float64, requires_grad=True)\n\n    def forward(self, X: torch.Tensor):\n\n        z = torch.matmul(X, self.weights) + self.bias\n        y_pred = torch.sigmoid(z)\n\n        return y_pred\n\n    def loss_func(self, y_pred, y):\n        # Clamp predictions to avoid log(0)\n        epsilon = 1e-7\n        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n\n        # Calculate loss\n        loss = -(y_train_tensor * torch.log(y_pred) + (1 - y_train_tensor) * torch.log(1 - y_pred)).mean()\n\n        return loss\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#model-parameters","title":"Model Parameters","text":"<pre><code># Set the learning rate and number of epoch\nlr = 0.1 # learning rate\nepochs = 25\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#training-pipeline","title":"Training Pipeline","text":"<pre><code># Create an object of the model\nmodel = MySimpleNN(X_train_tensor)\n\n# Define a loop\nfor epoch in range(epochs):\n\n    # Forward pass\n    y_pred = model.forward(X_train_tensor)\n\n    # Loss calculation\n    loss = model.loss_func(y_pred, y_train_tensor)\n\n    # Backward pass\n    loss.backward()\n\n    # Parameters update\n    with torch.no_grad():\n        model.weights -= lr * model.weights.grad\n        model.bias -= lr * model.bias.grad\n\n    # Zero gradients\n    model.weights.grad.zero_()\n    model.bias.grad.zero_()\n\n    # Print loss in each epoch\n    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/04_pytorch_training_pipeline/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Model evaluation\nwith torch.no_grad():\n    y_pred = model.forward(X_test_tensor)\n    y_pred = (y_pred &gt; 0.5).float()\n\n# Calculate the accuracy using torch metrics\naccuracy = Accuracy(task='binary')\nprint('Accuracy on testing data:', accuracy(y_pred.squeeze(), y_test_tensor).item())\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/","title":"Pytorch Nn Module","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#pytorch-nn-module","title":"PyTorch NN Module","text":"<p>The torch.nn module in PyTorch is a core library that provides a wide array of classes and functions designed to help developers build neural networks efficiently and effectively. It abstracts the complexity of creating and training neural networks by offering pre-built layers, loss functions, activation functions, and other utilities, enabling you to focus on designing and experimenting with model architectures.</p> <p>Key Components of torch.nn: 1. Modules (Layers):     - <code>nn.Module</code>: The base class for all neural network modules. Your custom models and     layers should subclass this class.     - Common Layers: Includes layers like <code>nn.Linear</code> (fully connected layer), <code>nn.Conv2d</code>     (convolutional layer), <code>nn.LSTM</code> (recurrent layer), and many others.</p> <ol> <li> <p>Activation Functions:</p> <ul> <li>Functions like <code>nn.ReLU</code>, <code>nn.Sigmoid</code>, and <code>nn.Tanh</code> introduce non-linearities to the model, allowing it to learn complex patterns.</li> </ul> </li> <li> <p>Loss Functions:</p> <ul> <li>Provides loss functions such as <code>nn.CrossEntropyLoss</code>, <code>nn.MSELoss</code>, and <code>nn.NLLLoss</code> to quantify the difference between the model's predictions and the actual targets.</li> </ul> </li> <li> <p>Container Modules:</p> <ul> <li><code>nn.Sequential</code>: A sequential container to stack layers in order.</li> </ul> </li> <li> <p>Regularization and Dropout:</p> <ul> <li>Layers like <code>nn.Dropout</code> and <code>nn.BatchNorm2d</code> help prevent overfitting and improve the model's ability to generalize to new data.</li> </ul> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport torch\nfrom torch import nn\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#read-the-dataset","title":"Read the Dataset","text":"<pre><code># Load the breast cancer dataset using Pandas\ndata = pd.read_csv(r\"D:\\GITHUB\\pytorch-for-deep-Learning-and-machine-learning\\datasets\\breast_cancer_data.csv\")\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#data-pre-processing","title":"Data Pre-processing","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#data-cleaning","title":"Data Cleaning","text":"<pre><code># Drop the irrelevant columns\ndata.drop(columns=['id', 'Unnamed: 32'], inplace=True)\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#train-test-split","title":"Train-Test Split","text":"<pre><code># Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(columns=['diagnosis']),\n    data['diagnosis'],\n    test_size=0.3,\n    random_state=42\n)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#feature-scaling","title":"Feature Scaling","text":"<pre><code># Print the column information\nX_train.info()\n</code></pre> <pre><code># Scale the input variables using standarad scaler\nscaler = StandardScaler()\nX_train_scaled =scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(X_train_scaled.shape, X_test_scaled.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#label-encoding","title":"Label Encoding","text":"<pre><code># Encode the target variable using label encoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\nprint(y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#convert-numpy-arrays-to-pytorch-tensor","title":"Convert NumPy Arrays to PyTorch Tensor","text":"<pre><code>X_train_tensor = torch.from_numpy(X_train_scaled).type(torch.float32)\nX_test_tensor = torch.from_numpy(X_test_scaled).type(torch.float32)\ny_train_tensor = torch.from_numpy(y_train_encoded).type(torch.float32)\ny_test_tensor = torch.from_numpy(y_test_encoded).type(torch.float32)\n\nprint(X_train_tensor.shape, X_test_tensor.shape, y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#build-a-simple-nn-model","title":"Build a Simple NN Model","text":"<pre><code># Create a simple neural network model with a single node\nclass MySimpleNN(nn.Module):\n\n    def __init__(self, num_features):\n        super().__init__()\n\n        self.linear = nn.Linear(num_features, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, X: torch.Tensor):\n        out = self.linear(X)\n        out = self.sigmoid(out)\n\n        return out\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#training-pipeline","title":"Training Pipeline","text":"<pre><code># Create an object of the model\nmodel = MySimpleNN(X_train_tensor.shape[1])\n# Show the model summary\nsummary(model)\n</code></pre> <pre><code># Set the learning rate and number of epoch\nlr = 0.1 # learning rate\nepochs = 25\n\n# Define a loss function and an optimizer\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</code></pre> <pre><code># Define a loop\nfor epoch in range(epochs):\n\n    # Forward pass\n    y_pred = model(X_train_tensor)\n\n    # Loss calculation\n    loss = loss_fn(y_pred.squeeze(), y_train_tensor)\n\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Parameters update\n    optimizer.step()\n\n    # Print loss in epoch\n    print(f'Epoch: {epoch + 1}, Loss: {loss}')\n</code></pre> <pre><code># Print the model weights and bias\nprint('Model weights:')\nprint(model.linear.weight)\n\nprint('Model bias:')\nprint(model.linear.bias)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Make predictions on testing data\nwith torch.no_grad():\n    y_pred = model(X_test_tensor)\n\n# Calculate the accuracy using torchmetrics\naccuracy = Accuracy(task='binary')\nprint('Accuracy on testing data:', accuracy(y_pred.squeeze(), y_test_tensor).item())\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#build-a-nn-model-with-a-hidden-layer","title":"Build a NN Model with a Hidden Layer","text":"<pre><code># Create a neural network with a hidden layers\nclass MyComplexNN(nn.Module):\n\n    def __init__(self, num_feature):\n        super().__init__()\n        self.linear1 = nn.Linear(in_features=num_feature, out_features=3)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(in_features=3, out_features=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, X: torch.Tensor):\n        out = self.linear1(X)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n</code></pre> <pre><code># Build the same model with sequential container\nclass MyComplexNN(nn.Module):\n\n    def __init__(self, num_features):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(num_features, 3), # input layer\n            nn.ReLU(), # activation\n            nn.Linear(3, 1), # hidden layer\n            nn.Sigmoid()\n        )\n\n    def forward(self, X: torch.Tensor):\n        out = self.network(X)\n\n        return out\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#training-pipeline_1","title":"Training Pipeline","text":"<pre><code># Create an object of the model class\nmodel = MyComplexNN(X_train_tensor.shape[1])\n# Print the model summary\nsummary(model)\n</code></pre> <pre><code># # Set the learning rate and number of epoch\nlr = 0.01\nepochs = 25\n\n# Define a loss function and an optimizer\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</code></pre> <pre><code># Define a loop\nfor epoch in range(epochs):\n\n    # Forward pass\n    y_pred = model(X_train_tensor)\n\n    # Loss calculation\n    loss = loss_fn(y_pred.squeeze(), y_train_tensor)\n\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Parameters update\n    optimizer.step()\n\n    # Print epoch loss\n    print(f'Epoch: {epoch + 1}, Loss: {loss}')\n</code></pre> <pre><code># Print the model weights and biases\nprint('Model weights:')\nprint(model.network[0].weight)\nprint('Model biases:')\nprint(model.network[0].bias)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/05_pytorch_nn_module/#model-evaluation_1","title":"Model Evaluation","text":"<pre><code># Make predictions on testing data\nwith torch.no_grad():\n    y_pred = model(X_test_tensor)\n\n# Calculate the accuracy using torchmetrics\naccuracy = Accuracy(task='binary')\nprint('Accuracy on testing data:', accuracy(y_pred.squeeze(), y_test_tensor).item())\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/","title":"Dataset And Dataloader","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#datasets-and-dataloaders","title":"Datasets and DataLoaders","text":"<p>In PyTorch, <code>Dataset</code> and <code>DataLoader</code> are fundamental for handling data. Here's a breakdown of how they work and how to use them:</p> <p>1. Dataset The Dataset class is essentially a blueprint. When you create a custom Dataset, you decide how data is loaded and returned. It defines: - <code>__init__()</code>: which tells how data should be loaded. - <code>__len__()</code>: which returns the total number of samples. - <code>__getitem__(index)</code>: which returns the data (and label) at the given index.</p> <p>2. DataLoader The DataLoader wraps a Dataset and handles batching, shuffling, and parallel loading for you.</p> <p>DataLoader Control Flow: - At the start of each epoch, the DataLoader (if shuffle=True) shuffles indices(using a sampler). - It divides the indices into chunks of batch_size. - for each index in the chunk, data samples are fetched from the Dataset object - The samples are then collected and combined into a batch (using collate_fn) - The batch is returned to the main training loop</p> <p>Tips: 1. Custom Collate Function: For datasets that have variable-length inputs, you can use a custom collate function. 2. Lazy Loading: If your dataset is too large, implement lazy loading in the <code>__getitem__</code> method by loading data directly from files. 3. Data Augmentation: Use <code>torchvision.transforms</code> for on-the-fly data augmentation.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#a-note-about-samplers","title":"A Note about Samplers","text":"<p>In PyTorch, the sampler in the DataLoader determines the strategy for selecting samples from the dataset during data loading. It controls how indices of the dataset are drawn for each batch.</p> <p>Types of Samplers: PyTorch provides several predefined samplers, and you can create custom ones: 1. <code>SequentialSampler</code>:    - Samples elements sequentially, in the order they appear in the dataset.    - Default when shuffle=False. 2. <code>RandomSampler</code>:    - Samples elements randomly without</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#a-note-about-collate_fn","title":"A Note about <code>collate_fn</code>","text":"<p>The collate_fn in PyTorch's DataLoader is a function that specifies how to combine a list of samples from a dataset into a single batch. By default, the DataLoader uses a simple batch collation mechanism, but collate_fn allows you to customize how the data should be processed and batched.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#important-parameters-of-dataloader","title":"Important Parameters of DataLoader","text":"<p>The DataLoader class in PyTorch comes with several parameters that allow you to customize how data is loaded, batched, and preprocessed. Some of the most commonly used and important parameters include:</p> <ol> <li> <p>dataset (mandatory):</p> <ul> <li>The Dataset from which the DataLoader will pull data.</li> <li>Must be a subclass of torch.utils.data.Dataset that implements <code>__getitem__</code> and <code>__len__</code>.</li> </ul> </li> <li> <p>batch_size:</p> <ul> <li>How many samples per batch to load.</li> <li>Default is 1.</li> <li>Larger batch sizes can speed up training on GPUs but require more memory.</li> </ul> </li> <li> <p>shuffle:</p> <ul> <li>If True, the DataLoader will shuffle the dataset indices each epoch.</li> <li>Helpful to avoid the model becoming too dependent on the order of samples.</li> </ul> </li> <li> <p>num_workers:</p> <ul> <li>The number of worker processes used to load data in parallel.</li> <li>Setting num_workers &gt; 0 can speed up data loading by leveraging multiple CPU cores, especially if I/O or preprocessing is a bottleneck.</li> </ul> </li> <li> <p>pin_memory:</p> <ul> <li>If True, the DataLoader will copy tensors into pinned (page-locked) memory before returning them.</li> <li>This can improve GPU transfer speed and thus overall training throughput, particularly on CUDA systems.</li> </ul> </li> <li> <p>drop_last:</p> <ul> <li>If True, the DataLoader will drop the last incomplete batch if the total number of samples is not divisible by the batch size.</li> <li>Useful when exact batch sizes are required (for example, in some batch normalization scenarios).</li> </ul> </li> <li> <p>collate_fn:</p> <ul> <li>A callable that processes a list of samples into a batch (the default simply stacks tensors).</li> <li>Custom collate_fn can handle variable-length sequences, perform custom batching logic, or handle complex data structures.</li> </ul> </li> <li> <p>sampler:</p> <ul> <li>sampler defines the strategy for drawing samples (e.g., for handling imbalanced classes, or custom sampling strategies).</li> <li>batch_sampler works at the batch level, controlling how batches are formed.</li> <li>Typically, you don\u2019t need to specify these if you are using batch_size and shuffle. However, they provide lower-level control if you have advanced requirements.</li> </ul> </li> </ol>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#read-the-dataset","title":"Read the Dataset","text":"<pre><code># Load the breast cancer dataset using Pandas\ndata = pd.read_csv(r\"D:\\GITHUB\\pytorch-for-deep-Learning-and-machine-learning\\datasets\\breast_cancer_data.csv\")\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#data-pre-processing","title":"Data Pre-processing","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#data-cleaning","title":"Data Cleaning","text":"<pre><code># Drop the irrelevant columns\ndata.drop(columns=['id', 'Unnamed: 32'], inplace=True)\nprint(data.shape)\ndata.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#train-test-split","title":"Train-Test Split","text":"<pre><code># Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(columns=['diagnosis']),\n    data['diagnosis'],\n    test_size=0.3,\n    random_state=42\n)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#feature-scaling","title":"Feature Scaling","text":"<pre><code># Print the column information\nX_train.info()\n</code></pre> <pre><code># Scale the input variables using standarad scaler\nscaler = StandardScaler()\nX_train_scaled =scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(X_train_scaled.shape, X_test_scaled.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#label-encoding","title":"Label Encoding","text":"<pre><code># Encode the target variable using label encoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\nprint(y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#convert-numpy-arrays-to-pytorch-tensors","title":"Convert NumPy Arrays to PyTorch Tensors","text":"<pre><code>X_train_tensor = torch.from_numpy(X_train_scaled).type(torch.float32)\nX_test_tensor = torch.from_numpy(X_test_scaled).type(torch.float32)\ny_train_tensor = torch.from_numpy(y_train_encoded).type(torch.float32)\ny_test_tensor = torch.from_numpy(y_test_encoded).type(torch.float32)\n\nprint(X_train_tensor.shape, X_test_tensor.shape, y_train_encoded.shape, y_test_encoded.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#create-dataset-and-dataloader","title":"Create Dataset and DataLoader","text":"<pre><code># Create a custon dataset class\nclass CustomDataset(Dataset):\n\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, index):\n        return self.features[index], self.labels[index]\n</code></pre> <pre><code># Create an object of the custom dataset class\ntrain_dataset = CustomDataset(X_train_tensor, y_train_tensor)\ntest_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n\nprint('Length of the training dataset:', train_dataset.__len__())\n# Print a row from train dataset\ntrain_dataset.__getitem__(10)\n</code></pre> <pre><code># Create dataloader object\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\n# Print first two batches from the training dataaset\nfor idx , (batch_features, batch_labels) in enumerate(train_dataloader):\n    print(batch_features)\n    print(batch_labels)\n    print('-'*50)\n\n    if idx == 1:\n        break\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#build-a-neural-network-model","title":"Build a Neural Network Model","text":"<pre><code>class MyModel(nn.Module):\n\n    def __init__(self, n_features):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(n_features, 3),\n            nn.ReLU(),\n            nn.Linear(3, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, X: torch.Tensor):\n        out = self.network(X)\n        return out\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#training-pipeline","title":"Training Pipeline","text":"<pre><code># Create an object of the model\nmodel = MyModel(X_train_tensor.shape[1])\nsummary(model)\n</code></pre> <pre><code># Set the learning rate and number of epochs\nlr = 0.01\nepochs = 50\n\n# Define a loss function and an optimizer\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n</code></pre> <pre><code># Define a loop\nfor epoch in range(epochs):\n\n    avg_loss = 0\n\n    # Iterate training batches\n    for batch_X, batch_y in train_dataloader:\n\n        # Forward pass\n        y_pred = model(batch_X)\n\n        # Loss calculation\n        loss = loss_fn(y_pred.squeeze(), batch_y)\n        avg_loss += loss\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Backward pass\n        loss.backward()\n\n        # Parameters update\n        optimizer.step()\n\n    # Validate on testing batches\n    avg_test_loss = 0\n\n    for batch_X, batch_y in test_dataloader:\n        with torch.no_grad():\n            test_preds = model(batch_X)\n            test_loss = loss_fn(test_preds.squeeze(), batch_y)\n            avg_test_loss += test_loss\n\n    # Print the epoch loss\n    print(f'Epoch: {epoch}, Loss: {(avg_loss / len(train_dataloader)):.2f}, Val Loss: {(avg_test_loss / len(test_dataloader)):.2f}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/06_dataset_and_dataloader/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Model evaluation using test dataloader\nmodel.eval() # Set the model to evaluation mode\naccuracy_list = []\naccuracy = Accuracy(task='binary')\n\n# Make predictions on testing data\nwith torch.no_grad():\n    for batch_X, batch_y in test_dataloader:\n        y_pred = model(batch_X).squeeze()\n        y_pred = (y_pred &gt; 0.5).float()\n        batch_acc = accuracy(y_pred, batch_y)\n\n        accuracy_list.append(batch_acc)\n\n# Calculate overall accuracy\noverall_acc = np.mean(accuracy_list)\nprint(f'Accuracy: {overall_acc:.2f}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/","title":"Buidling Ann Using Pytorch","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#building-ann-using-pytorch","title":"Building ANN using PyTorch","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchinfo import summary\nfrom torchmetrics.classification import MulticlassAccuracy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\n\n# Setup device agnostic code to run the model on GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Current device: {device}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#read-the-dataset","title":"Read the Dataset","text":"<pre><code># Load the Fashion MNIST dataset using Pandas\ntrain_data = pd.read_csv(r'D:\\GITHUB\\pytorch-for-deep-learning-and-machine-learning\\datasets\\fashion-mnist_train.csv')\ntest_data = pd.read_csv(r'D:\\GITHUB\\pytorch-for-deep-learning-and-machine-learning\\datasets\\fashion-mnist_test.csv')\n\nprint(train_data.shape, test_data.shape)\ntrain_data.head()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#data-pre-processing","title":"Data Pre-processing","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#split-the-data-into-input-and-target","title":"Split the Data into Input and Target","text":"<pre><code># Split the data into target and labels\nX_train, y_train = train_data.drop(columns=['label']), train_data['label']\nX_test, y_test = test_data.drop(columns=['label']), test_data['label']\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nX_train.head()\n</code></pre> <pre><code># Create a 4x4 grid of random images from training data\nrandom_ids = [random.randint(0, X_train.shape[0]) for i in range(16)]\n\nfig, axes = plt.subplots(ncols=4, nrows=4, figsize=(8, 8))\n\nfor i, ax in enumerate(axes.flat):\n    img = X_train.iloc[random_ids[i]].values.reshape(28, 28)\n    ax.imshow(img)\n    ax.axis('off')\n    ax.set_title(f'Label: {y_train[random_ids[i]]}')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#scale-the-data","title":"Scale the Data","text":"<pre><code>X_train = X_train / 255.0\nX_test = X_test / 255.0\n\nprint(X_train.shape, X_test.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#convert-numpy-arrays-to-pytorch-tensors","title":"Convert NumPy Arrays to PyTorch Tensors","text":"<pre><code>X_train = torch.from_numpy(X_train.values).type(torch.float32)\nX_test = torch.from_numpy(X_test.values).type(torch.float32)\ny_train =  torch.from_numpy(y_train.values).type(torch.long)\ny_test =  torch.from_numpy(y_test.values).type(torch.long)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#create-dataset-and-dataloader","title":"Create Dataset and DataLoader","text":"<pre><code># Create a Fashion MNIST dataset class\nclass FashionMNISTDataset(Dataset):\n\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, index):\n        return self.features[index], self.labels[index]\n\n# Create the training and testing dataset\ntrain_dataset = FashionMNISTDataset(X_train, y_train)\ntest_dataset = FashionMNISTDataset(X_test, y_test)\n</code></pre> <pre><code># Create dataloader object\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True)\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#build-an-artificial-neural-network-ann-model","title":"Build an Artificial Neural Network (ANN) Model","text":"<pre><code># Create an ANN class with two hidden layers\nclass MyANNModel(nn.Module):\n\n    def __init__(self, n_features):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(in_features=n_features, out_features=128),\n            nn.ReLU(),\n            nn.Linear(in_features=128, out_features=64),\n            nn.ReLU(),\n            nn.Linear(in_features=64, out_features=10),\n        )\n\n    def forward(self, X: torch.Tensor):\n        out = self.network(X)\n        return out\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#training-pipeline","title":"Training Pipeline","text":"<pre><code># Create an object of the model\nmodel = MyANNModel(X_train.shape[1]).to(device)\nsummary(model)\n</code></pre> <pre><code># Define the learning rate and the number of epochs\nlr = 0.1\nepochs = 50\n\n# Define the optimizer and the loss function\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n</code></pre> <pre><code># Define a loop \nfor epoch in range(epochs):\n\n    ## Training\n    model.train()\n\n    total_train_loss = 0\n\n    for batch_X, batch_y in train_dataloader:\n\n        # Move data to gpu\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\n        # Forward pass\n        y_pred = model(batch_X)\n\n        # Loss calculation\n        loss = loss_fn(y_pred, batch_y)\n        total_train_loss += loss\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Backward pass\n        loss.backward()\n\n        # Parameters update\n        optimizer.step()\n\n    ## Testing\n    model.eval()\n\n    total_test_loss = 0\n\n    for batch_X, batch_y in test_dataloader:\n\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\n        with torch.no_grad():\n            test_pred = model(batch_X)\n            loss = loss_fn(test_pred, batch_y)\n            total_test_loss += loss\n\n    # Print the epoch training and testing loss\n    print(f'Epoch: {epoch+1}, Train Loss: {(total_train_loss / len(train_dataloader)):.4f}, Test Loss: {(total_test_loss / len(test_dataloader)):.2f}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/PyTorch_CampusX/07_buidling_ann_using_pytorch/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Model evaluation using test dataloader\nmodel.eval()\n\n# Initialize the accuracy metric\naccuracy = MulticlassAccuracy(num_classes=10).to(device)  # Move the metric to the same device as the model\n\n# Evaluate on the test dataset\nfor batch_X, batch_y in test_dataloader:\n    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\n    # Forward pass\n    y_pred = model(batch_X)\n    y_pred = torch.argmax(y_pred, axis=1)  # Get the predicted class indices\n\n    # Update the accuracy metric\n    accuracy.update(y_pred, batch_y)\n\n# Compute the final accuracy\nfinal_accuracy = accuracy.compute()\nprint(f'Accuracy: {final_accuracy:.2f}')\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/","title":"Simple Linear Regression Using Pytorch","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#simple-linear-regression-using-pytorch","title":"Simple Linear Regression using PyTorch","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"AEiwvpPWtWDT\" outputId=\"77848429-147b-4b82-caba-4faf860d042e\" from google.colab import drive, userdata drive.mount(\"/content/drive\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xe_ZdxCYuRYH\" outputId=\"64fa4da6-cf29-4b58-c5e3-50649d24021b\"\n# Download the data from kaggle\n!kaggle datasets download -d saquib7hussain/experience-salary-dataset\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"XLHH7b5EvePQ\" outputId=\"9efc0c5b-1852-42d0-825e-69605de94b45\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#unzip-the-dataset","title":"Unzip the dataset","text":"<p>!unzip /content/experience-salary-dataset.zip <pre><code>```python id=\"uO38bMnzrGsY\"\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams[\"font.serif\"] = \"Times New Roman\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#read-the-data","title":"Read the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"WsXoQ1WIvngB\" outputId=\"9f27fc4e-ddd1-457d-87a4-6597bfffd6e3\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#read-the-data_1","title":"Read the data","text":"<p>data = pd.read_csv(\"/content/Experience-Salary.csv\") print(data.shape) data.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 147} id=\"97RDQODkvzu7\" outputId=\"98a05d2f-d8b0-4f40-8c22-decee3979fa0\"\n# Check the NaN values\ndata.isnull().sum()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 450} id=\"Gy7mCfgdxTHV\" outputId=\"df29976d-0c52-4124-e666-ff930e989875\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#plot-the-data","title":"Plot the data","text":"<p>sns.scatterplot(data=data, x=\"exp(in months)\", y=\"salary(in thousands)\"); <pre><code>&lt;!-- #region id=\"DuDJO76Lv8Kc\" --&gt;\n## **Train-Test-Split**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"o0-ZrL_yv_jM\" outputId=\"1c91eb4e-02f9-43df-fbd0-8c65de410f38\"\n# Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    data[\"exp(in months)\"],\n    data[\"salary(in thousands)\"],\n    test_size=0.3,\n    random_state=42\n)\n\nX_train.shape, X_test.shape\n</code></pre></p> <p>```python id=\"9lsm9CsJ57Yj\" X_train, X_test = torch.tensor(X_train.to_numpy()), torch.tensor(X_test.to_numpy()) y_train, y_test = torch.tensor(y_train.to_numpy()), torch.tensor(y_test.to_numpy()) <pre><code>&lt;!-- #region id=\"zYUw_3Oyxj5J\" --&gt;\n## **Build the Model using PyTorch**\n&lt;!-- #endregion --&gt;\n\n```python id=\"AzBAdweJxreQ\"\n# Create a LinearRegression class\nclass LinearRegression(nn.Module):\n    # Constructor\n    def __init__(self):\n        super().__init__()\n        # paremeters (weight and bias)\n        self.weight = nn.Parameter(torch.randn(1,\n                                               requires_grad=True,\n                                               dtype=torch.float))\n        self.bias = nn.Parameter(torch.randn(1,\n                                             requires_grad=True,\n                                             dtype=torch.float))\n\n    # Forward method to define the computation in the model\n    def forward(self, X):\n        return self.weight * X + self.bias\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"k7rvz4ip2BSO\" outputId=\"e10befce-4332-4d94-cc1c-e9607fcdd9c7\" torch.manual_seed(42)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#instantiate-a-model-object","title":"Instantiate a model object","text":"<p>model = LinearRegression()</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#print-the-initial-parameters","title":"Print the initial parameters","text":"<p>list(model.parameters()) <pre><code>```python id=\"LKjA7JRd1p_t\"\n# Define the loss function and the optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(params=model.parameters(),\n                            lr=0.001)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"8c69dGWl2dEL\" outputId=\"cd023ace-a04e-4832-b31c-2ef4ece1efcc\"</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#train-the-model","title":"Train the model","text":"<p>epochs = 100</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#store-the-epoch-results","title":"Store the epoch results","text":"<p>epoch_count = [] train_loss_values = [] test_loss_values = []</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#training-loop","title":"Training loop","text":"<p>for epoch in range(epochs):</p> <pre><code># Set the model to training mode\nmodel.train()\n\n# 1. Forward pass\ny_pred = model(X_train)\n\n# 2. Calculate the training loss\ntrain_loss = loss_fn(y_train, y_pred)\n\n# 3. Zero the gradients\noptimizer.zero_grad()\n\n# 4. Backward pass\ntrain_loss.backward()\n\n# 5. Perform gradient descent (update parameters)\noptimizer.step()\n\n## Testing\nmodel.eval()  # Set the model to evaluation mode\n\nwith torch.no_grad():  # Disable gradient calculation\n    # 1. Forward pass on the test set\n    test_pred = model(X_test)\n\n    # 2. Calculate the test loss\n    test_loss = loss_fn(y_test, test_pred)\n\n# Append epoch and loss values (detach to avoid computation graph)\nepoch_count.append(epoch)\ntrain_loss_values.append(train_loss.item())  # Use .item() to get the scalar value\ntest_loss_values.append(test_loss.item())    # Use .item() to get the scalar value\n\n# Print out epoch number and loss values\nprint(f\"Epoch: {epoch+1} | Train Loss: {train_loss.item():.4f} | Test Loss: {test_loss.item():.4f}\")\n</code></pre> <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 449} id=\"kzooYiNn7dRb\" outputId=\"dfddfb38-b9f0-4ffd-efea-59db11acf69b\"\nplt.plot(epoch_count, train_loss_values, label=\"Train Loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show();\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#plot-the-regression-line","title":"Plot the Regression Line","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 450} id=\"q8WtTzYmRbZ3\" outputId=\"a6f62ab5-731b-4561-a75c-669f73ec7e8e\" with torch.no_grad():     preds = model(data.iloc[:, 0].values)</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/00_simple_linear_regression_using_pytorch/#plot-the-data_1","title":"Plot the data","text":"<p>sns.scatterplot(data=data, x=\"exp(in months)\", y=\"salary(in thousands)\") plt.plot(data.iloc[:, 0].values, preds, c=\"r\", label=\"Regression line\") plt.legend() plt.show(); <pre><code>&lt;!-- #region id=\"yeVQ730BS4ur\" --&gt;\n## **Save the Model**\n&lt;!-- #endregion --&gt;\n\n```python id=\"0M1NGEKoS7ii\"\ntorch.save(model.state_dict(), \"lr_model.pth\")\n</code></pre></p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/","title":"Intel Image Classification Using Tinyvgg","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/#intel-image-classification-using-tinyvgg","title":"Intel Image Classification using TinyVGG","text":""},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nimport os\nfrom tqdm import tqdm\nimport cv2\nimport random\nimport torch\nfrom torch import nn\nfrom torchmetrics import Accuracy\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams['font.serif'] = \"Times New Roman\"\n\ntorch.__version__\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/#explore-the-data-folders","title":"Explore the Data Folders","text":"<pre><code># Define the folder paths\ntrain_path = \"/kaggle/input/intel-image-classification/seg_train/seg_train\"\ntest_path = \"/kaggle/input/intel-image-classification/seg_test/seg_test\"\npred_path = \"/kaggle/input/intel-image-classification/seg_pred/seg_pred\"\n\n# Get the number of images per class for each paths\nfor path in [train_path, test_path]:\n    print(f\"{path}\\n---------------\")\n    categorical_paths = glob(pathname=path+\"/*\")\n    for cat_path in categorical_paths:\n        folder_name = cat_path.split(\"/\")[-1]\n        no_of_images = len(os.listdir(cat_path))\n        print(f\"Found {no_of_images} images in folder {folder_name}.\") \n\n    print(\"\\n\")\n\n# Print the number of images in the 'seg_pred' folder\nprint(f\"Found {len(os.listdir(pred_path))} images in folder seg_pred.\")\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/#prepare-the-data","title":"Prepare the Data","text":"<pre><code># Check the size of the images\n# Get the number of images per class for each paths\nfor path in [train_path, test_path]:\n    print(f\"{path}\\n---------------\")\n    categorical_paths = glob(pathname=path+\"/*\")\n    image_sizes = []\n    for cat_path in tqdm(categorical_paths):\n        file_paths = glob(cat_path+\"/*\")\n        for f_path in file_paths:\n            img = plt.imread(f_path)\n            shape = img.shape\n            image_sizes.append(shape)\n\n    print(pd.Series(image_sizes).value_counts())\n</code></pre> <pre><code># Resize the images and store in seperate variables\nX_train, X_test = [], []\ny_train, y_test = [], []\nX_pred = []\n\ncode = {'buildings':0 ,'forest':1,'glacier':2,'mountain':3,'sea':4,'street':5}\n\n# Set the resize parameter\nSIZE = 128\n\n# Get the number of images per class for each paths\nfor path in [train_path, test_path]:\n    print(f\"{path}\\n---------------\")\n    categorical_paths = glob(pathname=path+\"/*\")\n    for cat_path in tqdm(categorical_paths):\n        cat_name = cat_path.split(\"/\")[-1]\n        file_paths = glob(cat_path+\"/*\")\n        for f_path in file_paths:\n            img = cv2.imread(f_path)\n            img_resized = cv2.resize(img, (SIZE, SIZE))\n\n            if \"train\" in path:\n                X_train.append(img_resized)\n                y_train.append(code[cat_name])\n            else:\n                X_test.append(img_resized)\n                y_test.append(code[cat_name])\n\nfor f_path in tqdm(glob(pathname=pred_path+\"/*\")):\n    img = cv2.imread(f_path)\n    img_resized = cv2.resize(img, (SIZE, SIZE))\n    X_pred.append(img_resized)\n</code></pre> <pre><code># Change the list to numpy array\nX_train, X_test, X_pred = np.array(X_train), np.array(X_test), np.array(X_pred)\ny_train, y_test = np.array(y_train), np.array(y_test)\n\nX_train.shape, X_test.shape, X_pred.shape, y_train.shape, y_test.shape\n</code></pre> <pre><code># Convert the channel last to channel first\nX_train, X_test, X_pred = np.transpose(X_train, (0, 3, 1, 2)), np.transpose(X_test, (0, 3, 1, 2)), np.transpose(X_pred, (0, 3, 1, 2))\nX_train.shape, X_test.shape, X_pred.shape, y_train.shape, y_test.shape\n</code></pre> <pre><code># Plot some random images from the training data\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows*cols+1):\n    random_id = random.randint(0, X_train.shape[0])\n    X, y = X_train[random_id], y_train[random_id]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.transpose(X, [1, 2, 0]))\n    plt.title([key for key, value in code.items() if value == y][0])\n    plt.axis(False)\n</code></pre> <pre><code># Plot some random images from the testing data\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows*cols+1):\n    random_id = random.randint(0, X_test.shape[0])\n    X, y = X_test[random_id], y_test[random_id]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.transpose(X, [1, 2, 0]))\n    plt.title([key for key, value in code.items() if value == y][0])\n    plt.axis(False)\n</code></pre> <pre><code># Convert the data into tensors\nX_train, y_train = torch.from_numpy(X_train).type(torch.float), torch.from_numpy(y_train).type(torch.LongTensor)\nX_test, y_test = torch.from_numpy(X_test).type(torch.float), torch.from_numpy(y_test).type(torch.LongTensor)\nX_pred = torch.from_numpy(X_pred).type(torch.float)\n</code></pre> <pre><code># Apply normalization\nX_train, X_test, X_pred = X_train/255, X_test/255, X_pred/255\nX_train.shape, X_test.shape, X_pred.shape, y_train.shape, y_test.shape\n</code></pre>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/projects/01_intel-image-classification-using-tinyvgg/#build-the-model-tinyvgg-using-pytorch","title":"Build the Model (TinyVGG) using PyTorch","text":"<pre><code># Setup the device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</code></pre> <pre><code># Build the TinyVGG architecture\nclass TinyVGG(nn.Module):\n    def __init__(self, in_channels=3, hidden_units=10, output_shape=10):\n        super().__init__()\n        # First conv layer\n        self.conv_layer_1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)  # Halves the spatial dimensions (128 -&gt; 64)\n        )\n\n        # Second conv layer\n        self.conv_layer_2 = nn.Sequential(\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)  # Halves the spatial dimensions again (64 -&gt; 32)\n        )\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=hidden_units * 32 * 32, out_features=hidden_units)\n        )\n\n    def forward(self, X: torch.Tensor):\n        X = self.conv_layer_1(X)\n        X = self.conv_layer_2(X)\n        X = self.classifier(X)\n        return X\n</code></pre> <pre><code># Create an instance of TinyVGG\ntorch.manual_seed(42)\n\nmodel = TinyVGG(in_channels=3, hidden_units=10, output_shape=len(code)).to(device)\nmodel\n</code></pre> <pre><code># Setup the loss function and the optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n\n# Setup the accuracy function using torchmetrics\naccuracy_fn = Accuracy(task=\"multiclass\", num_classes=len(code)).to(device)\n</code></pre> <pre><code># Prepare dataloaders\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\npred_dataloader = DataLoader(TensorDataset(X_pred), batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test_dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of pred_dataloader: {len(pred_dataloader)} batches of {BATCH_SIZE}\")\n</code></pre> <pre><code># Start the training loop\ntorch.manual_seed(42)\n\n# Define the epochs\nepochs = 50\n\n# Store all the history in a dictionary\nhistory = {\n    \"epoch\": [],\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"test_loss\": [],\n    \"test_acc\": []\n}\n\nfor epoch in range(epochs):\n    print(f\"Epoch: {epoch} | \", end=\"\")\n\n    ## Training\n    train_avg_loss, train_avg_acc = 0.0, 0.0\n\n    for X, y in train_dataloader:\n        X, y = X.to(device), y.to(device)\n\n        # Set the model in training mode\n        model.train()\n\n        # Perform the steps\n        y_pred = model(X)                # Forward pass\n        loss = loss_fn(y_pred, y)        # Calculate the loss\n        train_avg_loss += loss.item()    # Accumulate loss as a scalar\n\n        # Calculate accuracy\n        acc = accuracy_fn(y_pred.argmax(dim=1), y).item()\n        train_avg_acc += acc\n\n        optimizer.zero_grad()            # Zero the gradients\n        loss.backward()                  # Backpropagation\n        optimizer.step()                 # Gradient descent\n\n    # Divide total train_avg_loss and train_avg_acc by length of train dataloader\n    train_avg_loss /= len(train_dataloader)\n    train_avg_acc /= len(train_dataloader)\n\n    ## Testing\n    test_avg_loss, test_avg_acc = 0.0, 0.0\n    model.eval()  # Set the model to evaluation mode\n\n    with torch.inference_mode():  # Disable gradient computation for testing\n        for X, y in test_dataloader:\n            X, y = X.to(device), y.to(device)\n            test_pred = model(X)\n            test_loss = loss_fn(test_pred, y)\n            test_avg_loss += test_loss.item()  # Accumulate test loss as a scalar\n\n            # Calculate test accuracy\n            test_acc = accuracy_fn(test_pred.argmax(dim=1), y).item()\n            test_avg_acc += test_acc\n\n    # Divide total test_avg_loss and test_avg_acc by length of test dataloader\n    test_avg_loss /= len(test_dataloader)\n    test_avg_acc /= len(test_dataloader)\n\n    # Print out training and testing results\n    history[\"epoch\"].append(epoch)\n    history[\"train_loss\"].append(train_avg_loss)\n    history[\"train_acc\"].append(train_avg_acc)\n    history[\"test_loss\"].append(test_avg_loss)\n    history[\"test_acc\"].append(test_avg_acc)\n\n    print(f\"Train Loss: {train_avg_loss:.4f}, Train Accuracy: {train_avg_acc:.4f} | \", end=\"\")\n    print(f\"Test Loss: {test_avg_loss:.4f}, Test Accuracy: {test_avg_acc:.4f}\")\n</code></pre> <pre><code>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes = axes.flatten()\n\nsns.lineplot(x=history[\"epoch\"], y=history[\"train_loss\"], label=\"Train\", ax=axes[0])\nsns.lineplot(x=history[\"epoch\"], y=history[\"test_loss\"], label=\"Test\", ax=axes[0])\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].grid();\n\nsns.lineplot(x=history[\"epoch\"], y=history[\"train_acc\"], c=\"C2\", label=\"Train\", ax=axes[1])\nsns.lineplot(x=history[\"epoch\"], y=history[\"test_acc\"], c=\"C3\", label=\"Test\", ax=axes[1])\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].grid()\n\nplt.tight_layout();\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/","title":"Python Basics: Learn to Code from Scratch","text":"<p>Welcome to \"Python Basics: Learn to Code from Scratch\"! This is a comprehensive course designed for beginners who want to learn Python from scratch. In this repository, you will find all the course materials, including notebooks, assignments, and solutions.</p> <p>Throughout the course, you will learn the basics of Python programming, including data types, operators, conditionals, functions, object-oriented programming, and more. You will also learn how to use Python to solve real-world problems and build simple applications.</p> <p>All the code examples and projects in the course are provided in this repository, along with detailed explanations and instructions. You will have the opportunity to practice what you learn by completing assignments and projects, which will be reviewed by the course instructors.</p> <p>Whether you are a complete beginner or have some programming experience, this course will provide you with a solid foundation in Python programming. So, let's get started!</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/01_Introduction_to_Python/","title":"Introduction To Python","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/01_Introduction_to_Python/#introduction-to-python","title":"Introduction to Python","text":"<p>Python is a popular, high-level programming language that is used in many different fields such as web development, scientific computing, data analysis, artificial intelligence, and more. Python is easy to learn and has a simple syntax, making it a great language for beginners.</p> <p>A high-level programming language is a programming language that is designed to be easy to read and write for humans, with less focus on the details of the computer hardware and operating system. High-level languages often use English-like syntax and provide built-in functions and libraries for common tasks, allowing programmers to write complex programs with fewer lines of code.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/01_Introduction_to_Python/#why-python","title":"Why Python?","text":"<p>Python is a versatile, high-level programming language that has become increasingly popular in recent years. Here are some of the reasons why you should consider using Python for your next project:</p> <ul> <li> <p>Easy to learn and use: Python has a simple and intuitive syntax that is easy to learn, even for beginners. This makes it a great language for learning programming fundamentals.</p> </li> <li> <p>Large standard library: Python comes with a large standard library that provides built-in support for many common programming tasks, such as web development, data analysis, and scientific computing. This can save you time and effort by providing pre-built modules that you can use in your projects.</p> </li> <li> <p>Cross-platform compatibility: Python code can be run on a wide range of platforms, including Windows, macOS, Linux, and even mobile devices. This makes it a great language for building applications that need to run on multiple platforms.</p> </li> <li> <p>Community and support: Python has a large and active community of developers who contribute to the language, libraries, and tools. This means that there is a wealth of resources and support available online, including documentation, forums, and tutorials.</p> </li> <li> <p>Data science and machine learning: Python has become the de facto language for data science and machine learning due to its extensive libraries such as NumPy, Pandas, Matplotlib, Scikit-learn, and Tensorflow. It's an ideal language for building data-centric applications, data analysis, and machine learning models.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/01_Introduction_to_Python/#what-it-is-used-for","title":"What It Is Used For?","text":"<p>Python has a wide range of applications. Here are some of the common uses of Python:</p> <ul> <li> <p>Web Development: Python is used to build web applications, websites, and web services. It has several frameworks, including Django and Flask, that provide a robust and scalable web development environment.</p> </li> <li> <p>Data Science: Python is the language of choice for data science and machine learning due to its extensive libraries, including NumPy, Pandas, Matplotlib, Scikit-learn, and TensorFlow. It's an ideal language for building data-centric applications, data analysis, and machine learning models.</p> </li> <li> <p>Scientific Computing: Python is used extensively in scientific computing, thanks to its support for numerical calculations and data visualization. It has libraries such as NumPy, SciPy, and Matplotlib that provide a powerful set of tools for scientific computing.</p> </li> <li> <p>Scripting: Python is often used for scripting tasks, such as automating repetitive tasks, system administration, and network programming. It has a simple syntax and powerful scripting capabilities that make it an ideal language for scripting.</p> </li> <li> <p>Education: Python is a popular language for teaching programming fundamentals. It has a simple and intuitive syntax, making it easy for beginners to learn, and it's also a powerful language that can be used for a wide range of applications.</p> </li> <li> <p>Desktop GUI Applications: Python can be used to build desktop GUI applications using frameworks such as PyQt, PyGTK, and wxPython. These frameworks provide an easy-to-use interface for building desktop applications.</p> </li> <li> <p>Game Development: Python is used in game development, thanks to its simplicity and ease of use. It has several game development libraries, such as Pygame, that provide a powerful set of tools for building games.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/02_Python_Comments/","title":"Python Comments","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/02_Python_Comments/#python-comments","title":"Python Comments","text":"<p>Comments are used in programming to explain code, make notes, and provide context for other developers who may be reading the code. In Python, comments start with the \"#\" symbol and continue until the end of the line. Comments are ignored by the Python interpreter, so they do not affect the execution of the code.</p> <p>Significance: * Comments can be used to explain Python code. * Comments can be used to make the code more readable. * Comments can be used to prevent execution when testing code.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/02_Python_Comments/#creating-a-comment","title":"Creating a Comment","text":"<p>Comments start with the \"#\" symbol and Python interpreter will ignore them.</p> <pre><code># This is an example of how to write comment in Python.\nprint(\"Hello World\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/01%20Introduction%20to%20Python/02_Python_Comments/#multi-line-comments","title":"Multi-Line Comments","text":"<p>In Python, you can create multiline comments by enclosing them in triple quotes (''' ''') or triple double quotes (\"\"\" \"\"\"). This allows you to write comments that span multiple lines, without needing to add \"#\" to each line</p> <pre><code>\"\"\"\nThis is an\nexample of\nMulti-Line Comments.\n\"\"\"\nprint(\"Hello World\")\n</code></pre> <pre><code>'''\nThis is an\nexample of\nMulti-Line Comments too.\n'''\nprint(\"Hello World\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/01_Introduction_to_Variables/","title":"Introduction To Variables","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/01_Introduction_to_Variables/#introduction-to-variables","title":"Introduction to Variables","text":"<p>Variables in Python are containers that store data values. They are used to hold and manipulate data in a program. Python uses a dynamic typing system, which means that you don't need to declare the data type of a variable before using it.</p> <p>Variables are fundamental to programming for two reasons:  * Variables keep values accessible: For example,The result of a time-consuming operation can be assigned to a variable so that the operation need not be performed each time we need the result.</p> <ul> <li>Variables give values context: For example, The number 56 could mean lots of different things, such as the number of students in a class, or the average weight of all students in the class. Assigning the number 56 to a variable with a name like num_students would make more sense, to distinguish it from another variable average_weight, which would refer to the average weight of the students. This way we can have different variables pointing to different values.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/01_Introduction_to_Variables/#how-are-values-assigned-to-a-variable","title":"How are Values Assigned to A Variable?","text":"<p>In Python, you can assign values to variables using the assignment operator (=). The assignment operator assigns the value on the right-hand side of the operator to the variable on the left-hand side of the operator.</p> <p>For Example:Now let us create a variable namely myNum to hold a specific number.</p> <pre><code># Creating a new variable\nmyNum = 4\nmyNum\n</code></pre> <pre><code># Creating multiple variables\nmyInt = 4\nmyReal = 2.5\nmyChar = \"a\"\nmyString = \"hello\"\nprint(myInt)\nprint(myReal)\nprint(myChar)\nprint(myString)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/01_Introduction_to_Variables/#naming-a-variable","title":"Naming a Variable","text":"<p>When naming a variable in Python, there are some rules and conventions you should follow to make your code more readable and understandable. Here are some guidelines for naming variables:</p> <ul> <li>Variable names must start with a letter or underscore (_), but cannot start with a number.</li> <li>Variable names can only contain letters, numbers, and underscores (_). They cannot contain spaces or special characters.</li> <li>Variable names should be descriptive and meaningful, so that other programmers can understand what the variable represents.</li> <li>Variable names are case-sensitive. For example:- The variable names Temp and temp are different</li> <li>Variable names should be written in lowercase, with words separated by underscores (_). This convention is called \"snake_case\" and is widely used in Python.</li> <li>Avoid using reserved keywords as variable names, such as \"if\", \"while\", \"for\", \"and\", \"or\", etc.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/01_Introduction_to_Variables/#examples","title":"Examples","text":"<pre><code># Correct Variables\na1 = 5\n_b2 = 10\nb = 10\n</code></pre> <pre><code># Incorrect Variables\n# 1a = 5\n# 23b = 10\n# 1@ = 5\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/","title":"Introduction To Datatypes","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#introduction-to-datatypes","title":"Introduction to Datatypes","text":"<p>In Python, data types are used to categorize and represent different types of data that can be used in a program. They are used to classify different types of data that can be stored and manipulated in a program. The most common data types in Python include integers, floats, strings, booleans, and lists.</p> <p>The most common built-in data types in Python are: * Integer: Represents whole numbers, such as 1, 2, 3, -4, -5, etc.</p> <ul> <li> <p>Float: Represents decimal numbers, such as 3.14, 2.5, -1.0, etc.</p> </li> <li> <p>String: Represents a sequence of characters, such as \"hello\", \"world\", \"123\", etc.</p> </li> <li> <p>Boolean: Represents either True or False.</p> </li> <li> <p>List: Represents an ordered collection of values, which can be of any data type. For example, [1, 2, 3], [\"apple\", \"banana\", \"orange\"], etc.</p> </li> <li> <p>Tuple: Similar to a list, but immutable, meaning it cannot be modified once created. For example, (1, 2, 3), (\"apple\", \"banana\", \"orange\"), etc.</p> </li> <li> <p>Set: Represents an unordered collection of unique values. For example, {1, 2, 3}, {\"apple\", \"banana\", \"orange\"}, etc.</p> </li> <li> <p>Dictionary: Represents a collection of key-value pairs, where each key is associated with a value. For example, {\"name\": \"John\", \"age\": 30}, {\"fruit\": \"apple\", \"color\": \"red\"}, etc</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#examples","title":"Examples","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#integer-datatype","title":"Integer Datatype","text":"<p>Integers are whole numbers, such as 1, 2, 3, etc. They can be positive or negative.</p> <pre><code># Integer Datatype\nmyInt = 8\nprint(myInt, \"is a type of\", type(myInt))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#float-datatype","title":"Float Datatype","text":"<p>Floats are numbers with a decimal point, such as 3.14, 2.5, etc. They can also be positive or negative.</p> <pre><code># Float Datatype\nmyFloat = 8.5\nprint(myFloat, \"is a type of\", type(myFloat))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#string-datatype","title":"String Datatype","text":"<p>Strings are sequences of characters, such as \"hello\", \"world\", etc. They are enclosed in quotation marks (single or double).</p> <pre><code># String Datatype\nmyString1 = \"GeoNext\"\nmyString2 = \"8.5\"\nprint(myString1, \"is a type of\", type(myString1))\nprint(myString2, \"is a type of\", type(myString2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#boolean-datatype","title":"Boolean Datatype","text":"<p>Booleans are values that can be either True or False. They are often used in conditional statements and loops.</p> <pre><code># Boolean Datatype\nmyBool1 = True\nmyBool2 = False\nprint(myBool1, \"is a type of\", type(myBool1))\nprint(myBool2, \"is a type of\", type(myBool2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#list-datatype","title":"List Datatype","text":"<p>Lists are ordered collections of values, which can be of any data type. They are enclosed in square brackets and separated by commas, like [1, 2, 3] or [\"apple\", \"banana\", \"orange\"].</p> <pre><code># List Datatype\nmyList1 = [\"Apple\", \"Banana\", \"Orange\"]\nmyList2 = [1, 2, 3] \nmyList3 = [\"Apple\", 2, 3.5]  \nprint(myList1, \"is a type of\", type(myList1))\nprint(myList2, \"is a type of\", type(myList2))\nprint(myList3, \"is a type of\", type(myList3))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#tuple-datatype","title":"Tuple Datatype","text":"<p>Tuples are similar to a list, but immutable, meaning it cannot be modified once created. They are enclosed in round brackets and separated by commas, such as (1, 2, 3) or (\"apple\", \"banana\", \"orange\").</p> <pre><code># Tuple Datatype\nmyTuple1 = (\"Apple\", \"Banana\", \"Orange\")\nmyTuple2 = (1, 2, 3)\nmyTuple3 = (\"Apple\", 2, 3.5)\nprint(myTuple1, \"is a type of\", type(myTuple1))\nprint(myTuple2, \"is a type of\", type(myTuple2))\nprint(myTuple3, \"is a type of\", type(myTuple3))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#set-datatype","title":"Set Datatype","text":"<p>Set represents an unordered collection of unique values. They are enclosed in curly brackets and separated by commas. For example, {1, 2, 3}, {\"apple\", \"banana\", \"orange\"}, etc.</p> <pre><code># Tuple Datatype\nmySet1 = {\"Apple\", \"Banana\", \"Orange\"}\nmySet2 = {1, 2, 3, 2}\nmySet3 = {\"Apple\", 2, 3.5, \"Apple\"}\nprint(mySet1, \"is a type of\", type(mySet1))\nprint(mySet2, \"is a type of\", type(mySet2))\nprint(mySet3, \"is a type of\", type(mySet3))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/02%20Variables%20and%20Datatypes/02_Introduction_to_Datatypes/#dictionary-datatype","title":"Dictionary Datatype","text":"<p>It represents a collection of key-value pairs, where each key is associated with a value. Dictionaries are enclosed with curly brackets and the items of the dictionary are separated by commas. For example, {\"name\": \"John\", \"age\": 30}, {\"fruit\": \"apple\", \"color\": \"red\"}, etc.</p> <pre><code># Dictionary Datatype\nmyDict1 = {\"fruit\": \"Apple\", \"color\": \"Red\", \"is_edible\": True}\nprint(myDict1, \"is a type of\", type(myDict1))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/","title":"Print Function","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/#python-print-function","title":"Python print() Function","text":"<p>The print() function in Python is used to output text or other data to the console. It takes one or more arguments, which can be of any data type, and prints them to the console.</p> <p>The basic syntax of print() is: print(object(s), sep=separator, end=end, file=file, flush=flush)</p> <ul> <li>object(s): One or more objects to print. This can be a string, a variable, or any otherobject.</li> <li>sep: Separator between the objects to be printed. By default, it is a space character.</li> <li>end: The character to be printed at the end of the statement. By default, it is a newline character.</li> <li>file: Output stream to which the text will be printed. By default, it is the console.</li> <li>flush: A Boolean value that specifies whether to flush the output buffer. By default, it is False.</li> </ul> <pre><code>print(\"Hello World\")\n</code></pre> <pre><code>print(\"Hello, GIS lovers!\")\n</code></pre> <pre><code>print(\"GeoNext is best for GIS content\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/#printing-more-than-one-object","title":"Printing more than one object","text":"<pre><code>print(\"Hello\", \"How are you?\")\n</code></pre> <pre><code>a = 5\nb = 10\nprint(a, b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/#python-end-parameter-in-print","title":"Python end parameter in print()","text":"<pre><code>print(\"Welcome to\", end=\" \")\nprint(\"GeoNext\")\n</code></pre> <pre><code>print(\"Follow \", end=\"@\")\nprint(\"GeoNext\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/#python-sep-parameter-in-print","title":"Python sep parameter in print()","text":"<pre><code>print(\"09\", \"03\", \"2023\", sep=\"-\")\n</code></pre> <pre><code>print(\"krish\", \"GeoNext\", sep=\"@\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/01_Print_Function/#string-concatenation","title":"String Concatenation","text":"<pre><code>print(\"a\"+\"bc\")\n</code></pre> <pre><code>print(\"Geo\" + \"Next\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/02_Taking_Input/","title":"Taking Input","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/02_Taking_Input/#python-input-function","title":"Python input() Function","text":"<p>The input() function in Python is used to accept user input from the keyboard. It prompts the user to enter a value, reads the input from the user, and returns it as a string.</p> <p>The basic syntax of input() is:  input(prompt)</p> <ul> <li>prompt: A string that is displayed to the user as a prompt for input. It is an optional argument, and if it is not provided, the function will display an empty prompt.</li> </ul> <pre><code>name = input(\"Enter your name: \")\nprint(name)\n</code></pre> <pre><code>print(type(name))\n</code></pre> <p>When we use the input() function in Python to get user input, the input is always returned as a string. If we want to perform any numerical operations on the input, we will need to convert it to an integer using the int() function.</p> <pre><code>num = input(\"Enter a number: \")\nprint(num, type(num))\n</code></pre> <pre><code>num = int(input(\"Enter a numbert: \"))\nprint(num, type(num))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/02_Taking_Input/#taking-a-space-separated-input-in-one-line","title":"Taking a space-separated input in one line","text":"<pre><code>x, y = input(\"Enter the longitude and latitude: \").split()\nprint(x)\nprint(y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/03%20Basic%20Input%20and%20Output/02_Taking_Input/#exercise-add-two-numbers","title":"Exercise: Add two numbers","text":"<pre><code>num1 = int(input(\"Enter the first number: \"))\nnum2 = int(input(\"Enter the second number: \"))\ns = num1 + num2\nprint(\"The sum of two numbers is\", s)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/01_Introduction_to_Operators/","title":"Introduction To Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/01_Introduction_to_Operators/#operators-in-python","title":"Operators in Python","text":"<p>In Python, operators are special symbols or keywords that are used to perform operations on values or variables. They can be classified into several categories based on the type of operation they perform:</p> <p>Python supports a wide range of operators, which can be categorized as follows:</p> <ul> <li> <p>Arithmetic Operators: These are used to perform arithmetic operations such as addition, subtraction, multiplication, division, modulus, and exponentiation.</p> </li> <li> <p>Comparison Operators: These are used to compare values and return a Boolean value (True or False) based on the result of the comparison.</p> </li> <li> <p>Assignment Operators: These are used to assign values to variables.</p> </li> <li> <p>Logical Operators: These are used to perform logical operations such as AND, OR, and NOT.</p> </li> <li> <p>Bitwise Operators: These are used to perform operations on binary numbers.</p> </li> <li> <p>Membership Operators: These are used to test whether a value or variable is a member of a sequence.</p> </li> <li> <p>Identity Operators: These are used to test whether two objects are the same or not.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/01_Introduction_to_Operators/#example","title":"Example","text":"<pre><code>print(10 + 15)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/","title":"Arithemetic Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#arithmetic-operators","title":"Arithmetic Operators","text":"<p>Arithmetic operators are used to perform basic mathematical operations on numeric data types in Python. There are several arithmetic operators available in Python, which are listed below:</p> <ul> <li> <p>Addition (+): The addition operator is used to add two or more numbers or concatenate two or more strings.</p> </li> <li> <p>Subtraction (-): The subtraction operator is used to subtract one number from another.</p> </li> <li> <p>Multiplication (*): The multiplication operator is used to multiply two or more numbers.</p> </li> <li> <p>Division (/): The division operator is used to divide one number by another. It always returns a float value, even if the division result is an integer.</p> </li> <li> <p>Floor Division (//): The floor division operator is used to divide one number by another and return the integer quotient. It rounds down to the nearest whole number.</p> </li> <li> <p>Modulus (%): The modulus operator is used to find the remainder of the division operation.</p> </li> <li> <p>Exponentiation (**): The exponentiation operator is used to raise a number to a power.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#addition","title":"Addition (+)","text":"<pre><code># Addition of two numbers\na = 10\nb = 15 \nadd = a + b\nprint(add)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#subtraction-","title":"Subtraction (-)","text":"<pre><code># Subtraction of two numbers\na = 20\nb = 10\nsub = a - b\nprint(sub)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#multiplication","title":"Multiplication (*)","text":"<pre><code># Multiplication of two numbers\na = 3\nb = 5\nmul = a * b\nprint(mul)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#division","title":"Division (/)","text":"<pre><code># Division of two numbers\na = 15\nb = 4\ndiv = a / b\nprint(div)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#floor-division","title":"Floor Division (//)","text":"<pre><code># Floor division of two numbers\na = 15\nb = 4\nfdiv = a // b\nprint(fdiv)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#modulus","title":"Modulus (%)","text":"<pre><code># Remainder of division operation\na = 15\nb = 4\nmod = a % b\nprint(mod)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/02_Arithemetic_Operators/#exponentiation","title":"Exponentiation (**)","text":"<pre><code># Power of a number\na = 4\nb = 2\nexp = a ** b\nprint(exp)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/","title":"Assignment Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#assignment-operators","title":"Assignment Operators","text":"<p>Assignment operators are used to assign values to variables in Python. They are a shorthand way of performing arithmetic or bitwise operations and then assigning the result to a variable.</p> <p>There are several types of assignment operators available in Python, which are listed below:</p> <ul> <li> <p>Simple assignment (=): The simple assignment operator is used to assign a value to a variable.</p> </li> <li> <p>Addition assignment (+=): The addition assignment operator adds a value to a variable and then assigns the result to the same variable.</p> </li> <li> <p>Subtraction assignment (-=): The subtraction assignment operator subtracts a value from a variable and then assigns the result to the same variable.</p> </li> <li> <p>Multiplication assignment (*=): The multiplication assignment operator multiplies a variable by a value and then assigns the result to the same variable.</p> </li> <li> <p>Division assignment (/=): The division assignment operator divides a variable by a value and then assigns the result to the same variable.</p> </li> <li> <p>Modulus assignment (%=): The modulus assignment operator finds the remainder of dividing a variable by a value and then assigns the result to the same variable.</p> </li> <li> <p>Exponentiation assignment (**=): The exponentiation assignment operator raises a variable to a power and then assigns the result to the same variable.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#simple-assignment","title":"Simple Assignment (=)","text":"<pre><code>a = 5\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#addition-assignment","title":"Addition Assignment (+=)","text":"<pre><code>a += 5\nprint(a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#subtraction-assignment-","title":"Subtraction Assignment (-=)","text":"<pre><code>a -= 5\nprint(a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#multiplication-assignment","title":"Multiplication Assignment (*=)","text":"<pre><code>a *= 5\nprint(a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#division-assignment","title":"Division Assignment (/=)","text":"<pre><code>a /= 5\nprint(a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#modulus-assignment","title":"Modulus Assignment (%=)","text":"<pre><code>a %= 2\nprint(a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/03_Assignment_Operators/#exponentiation-assignment","title":"Exponentiation Assignment (**=)","text":"<pre><code>b = 4\nb **= 2\nprint(b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/","title":"Comparison Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#comparison-operators","title":"Comparison Operators","text":"<p>Comparison operators are used to compare values or expressions in Python, and they always return a Boolean value (True or False) depending on the result of the comparison.</p> <p>Here are the comparison operators in Python:</p> <ul> <li> <p>Equal to (==): This operator checks if the left-hand side is equal to the right-hand side.</p> </li> <li> <p>Not equal to (!=): This operator checks if the left-hand side is not equal to the right-hand side.</p> </li> <li> <p>Greater than (&gt;): This operator checks if the left-hand side is greater than the right-hand side.</p> </li> <li> <p>Less than (&lt;): This operator checks if the left-hand side is less than the right-hand side.</p> </li> <li> <p>Greater than or equal to (&gt;=): This operator checks if the left-hand side is greater than or equal to the right-hand side.</p> </li> <li> <p>Less than or equal to (&lt;=): This operator checks if the left-hand side is less than or equal to the right-hand side.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#equal-to","title":"Equal to (==)","text":"<pre><code>a = 15\nb = 10\nprint(a==b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#not-equal-to","title":"Not equal to (!=)","text":"<pre><code>print(a!=b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#greater-than","title":"Greater than (&gt;)","text":"<pre><code>print(a&gt;b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#less-than","title":"Less than (&lt;)","text":"<pre><code>print(a&lt;b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#greater-than-or-equal-to","title":"Greater than or equal to (&gt;=)","text":"<pre><code>print(a&gt;=b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/04_Comparison_Operators/#less-than-or-equal-to","title":"Less than or equal to (&gt;=)","text":"<pre><code>print(a&lt;=b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/05_Logical_Operators/","title":"Logical Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/05_Logical_Operators/#logical-operators","title":"Logical Operators","text":"<p>Logical operators are used to combine two or more conditions in Python. They always return a Boolean value (True or False) depending on the result of the logical operation.</p> <p>Here are the logical operators in Python:</p> <ul> <li> <p>and: This operator returns True if both conditions are True.</p> </li> <li> <p>or: This operator returns True if at least one of the conditions is True.</p> </li> <li> <p>not: This operator returns the opposite Boolean value of the condition.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/05_Logical_Operators/#and","title":"and","text":"<pre><code>x = True\ny = False\nprint(x and y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/05_Logical_Operators/#or","title":"or","text":"<pre><code>print(x or y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/05_Logical_Operators/#not","title":"not","text":"<pre><code>print(not x)\nprint(not y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/06_Identity_Operators/","title":"Identity Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/06_Identity_Operators/#identity-operators","title":"Identity Operators","text":"<p>Identity operators are used to compare the memory addresses of two objects in Python. They always return a Boolean value (True or False) depending on the result of the comparison.</p> <p>Here are the identity operators in Python:</p> <ul> <li> <p>is: This operator returns True if both variables point to the same object in memory.</p> </li> <li> <p>is not: This operator returns True if both variables do not point to the same object in memory.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/06_Identity_Operators/#is","title":"is","text":"<pre><code>a = 10\nb = 10\nprint(id(a))\nprint(id(b))\nprint(a is b)\n</code></pre> <pre><code>name1 = \"GeoNext\"\nname2  =\"GeoNext\"\nprint(id(name1))\nprint(id(name2))\nprint(name1 is name2)\n</code></pre> <pre><code>lst1 = [1, 2, 3]\nlst2 = [1, 2, 3]\nprint(id(lst1), id(lst2))\nprint(lst1 is lst2) # Output will be false since lists are mutable\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/06_Identity_Operators/#is-not","title":"is not","text":"<pre><code>print(a is not b)\n</code></pre> <pre><code>print(name1 is not name2)\n</code></pre> <pre><code>print(lst1 is not lst2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/07_Membership_Operators/","title":"Membership Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/07_Membership_Operators/#membership-operators","title":"Membership Operators","text":"<p>Membership operators are used to test if a value is a member of a sequence or not. The two membership operators in Python are:</p> <ul> <li> <p>in: This operator returns True if a value is found in the sequence, and False otherwise.</p> </li> <li> <p>not in: This operator returns True if a value is not found in the sequence, and False otherwise.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/07_Membership_Operators/#in","title":"in","text":"<pre><code>name = \"GeoNext\"\nlst = [1, 2, 3]\nprint(\"G\" in name)\nprint(3 in lst)\nprint(\"S\" in name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/07_Membership_Operators/#not-in","title":"not in","text":"<pre><code>print(\"S\" not in name)\n</code></pre> <pre><code>print(1 not in lst)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/","title":"Bitwise Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-operators","title":"Bitwise Operators","text":"<p>Bitwise operators are used to perform operations on the binary representation of integer values. In Python, the following bitwise operators are available:</p> <ul> <li> <p>&amp; (Bitwise AND): This operator performs a bitwise AND operation on the binary representation of two integers.</p> </li> <li> <p>| (Bitwise OR): This operator performs a bitwise OR operation on the binary representation of two integers.</p> </li> <li> <p>^ (Bitwise XOR): This operator performs a bitwise XOR (exclusive OR) operation on the binary representation of two integers.</p> </li> <li> <p>~ (Bitwise NOT): This operator performs a bitwise NOT operation on the binary representation of an integer, which flips all the bits.</p> </li> <li> <p>&lt;&lt; (Bitwise left shift): This operator shifts the bits of an integer to the left by a specified number of positions.</p> </li> <li> <p>(Bitwise right shift) &gt;&gt;: This operator shifts the bits of an integer to the right by a specified number of positions.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-and","title":"&amp; (Bitwise AND)","text":"<pre><code>a = 20\nb = 8\nprint(a &amp; b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-or","title":"| (Bitwise OR)","text":"<pre><code>print(a | b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-xor","title":"^ (Bitwise XOR)","text":"<pre><code>print(a ^ b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-not","title":"~ (Bitwise NOT)","text":"<pre><code>print(~a)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-left-shift","title":"&lt;&lt; (Bitwise left shift)","text":"<pre><code>print(a &lt;&lt; 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/04%20Operators%20in%20Python/08_Bitwise_Operators/#bitwise-right-shift","title":"&gt;&gt; (Bitwise right shift)","text":"<pre><code>print(a &gt;&gt; 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/01_Conditional_Statement/","title":"Conditional Statement","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/01_Conditional_Statement/#conditional-statement","title":"Conditional Statement","text":"<p>In Python, conditional statements are used to execute certain code if a particular condition is true. The most commonly used conditional statements in Python are if, elif, and else. This course material will cover the syntax and usage of these statements.</p> <ul> <li> <p>If Statement: The if statement is used to execute code if a specific condition is true.</p> </li> <li> <p>If-Else Statement: The if-else statement is used to execute one block of code if a condition is true and another block of code if it is false. </p> </li> <li> <p>If-Elif-Else Statement: The if-elif-else statement is used to execute one block of code if the first condition is true, another block of code if the second condition is true, and so on. If none of the conditions are true, then the code inside the else block will be executed. </p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/01_Conditional_Statement/#if-statement","title":"If Statement","text":"<pre><code># Check whether a given number is even or odd.\nnum = int(input(\"Enter a number: \"))\n\nif num % 2 == 0:\n    print(num, \"is even.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/01_Conditional_Statement/#if-else-statement","title":"If-Else Statement","text":"<pre><code># Check whether a given number is even or odd.\nnum = int(input(\"Enter a number: \"))\n\nif num % 2 == 0:\n    print(num, \"is even.\")\nelse:\n    print(num, \"is odd.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/01_Conditional_Statement/#if-elif-else-statement","title":"If-Elif-Else Statement","text":"<pre><code># Find the largest among three numbers\na = 10\nb = 8\nc = 15\n\nif a &gt;= b and a &gt;= c:\n    print(\"a is greater.\")\nelif b &gt;= a and b &gt;= c:\n    print(\"b is greater.\")\nelse:\n    print(\"c is greater.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/02_Loops/","title":"Loops","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/02_Loops/#loops","title":"Loops","text":"<p>In Python, loops are used to execute a block of code repeatedly. There are two types of loops in Python: for loops and while loops.</p> <ul> <li> <p>For Loops: For loops are used to iterate over a sequence of values. The for loop in Python is used to iterate over a sequence (list, tuple, string, dictionary) or other iterable objects. Iterating over a sequence is called traversal.</p> </li> <li> <p>While Loops: While loops are used to execute a block of code as long as a condition is true. We generally use this loop when we don't know the number of times to iterate beforehand.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/02_Loops/#for-loop","title":"for loop","text":"<pre><code># Print all numbers from 0 to n\nn = int(input(\"Enter the number yo want to print: \"))\n\nfor i in range (0, n+1):\n    print(i)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/02_Loops/#python-range-function","title":"Python range() Function","text":"<p>The range function is often used in for loops to generate a sequence of numbers.</p> <pre><code>for i in range(5):\n    print(i)\n</code></pre> <pre><code>for i in range(10, 15):\n    print(i)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/02_Loops/#while-loop","title":"while loop","text":"<pre><code>i = 0\nwhile i &lt; 5:\n    print(i)\n    i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/","title":"Break Continue Pass Return","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/#break-continue-pass-and-return-statement","title":"Break, Continue, Pass and Return Statement","text":"<p>Introduction In Python programming, there are several types of control flow statements that allow you to alter the normal flow of program execution. These include the break, continue, pass, and return statements. In this course material, we will discuss each of these statements and how they can be used in Python programming.</p> <ul> <li> <p>The break Statement: The break statement is used to prematurely exit out of a loop. It works with the for loop and the while loop. When a break statement is executed within a loop, the loop is immediately terminated, and the program continues to run from the first statement after the loop. The break statement is typically used to stop a loop when a certain condition has been met.</p> </li> <li> <p>The continue Statement: The continue statement is used to skip to the next iteration of a loop without executing any statements within the loop for the current iteration. It works with the for loop and the while loop. The continue statement is typically used to skip over certain values in a loop.</p> </li> <li> <p>Pass Statement: The pass statement is a null operation statement, meaning that it does nothing. It is used as a placeholder when there is no code to be executed in a certain part of a program.</p> </li> <li> <p>Return Statement: The return statement is used to end the execution of a function and return a value to the caller. When the interpreter reaches the return statement, it immediately exits the function and returns control back to the caller with the value provided.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/#break-statement","title":"Break Statement","text":"<pre><code>i = 0\nprint(\"Program started.\")\n\nwhile i &lt; 5:\n    if i == 3:\n        print(\"Executing break statement in the next statement.\")\n        break\n    print(i)\n    i += 1\n\nprint(\"Program ended.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/#continue-statement","title":"Continue Statement","text":"<pre><code>i = 0\nprint(\"Program started.\")\n\nwhile i &lt; 5:\n    if i == 3:\n        i += 1\n        continue\n    print(i)\n    i += 1\n\nprint(\"Program ended.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/#pass-statement","title":"Pass Statement","text":"<pre><code>i = 0\nprint(\"Program started.\")\n\nwhile i &lt; 5:\n    if i == 3:\n        i += 1\n        pass\n    else:\n        print(i)\n        i += 1\n\nprint(\"Program ended.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/05%20Conditional%20Statements%20and%20Loop/03_Break_Continue_Pass_Return/#return-statement","title":"Return Statement","text":"<pre><code># Factorial of nth number\ndef factorial(n):\n    fac = 1\n    if n == 0 or n == 1:\n        return 1\n    else:\n        for i in range(1, n+1):\n            fac = fac * i\n        return fac\n</code></pre> <pre><code>factorial(5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/01_Function_in_Python/","title":"Function In Python","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/01_Function_in_Python/#function-in-python","title":"Function in Python","text":"<p>In Python, a function is a block of reusable code that performs a specific task. Functions are used to break down large programs into smaller, more manageable pieces, and to improve code readability and reusability. A function is like a black box that can take certain input(s) as its parameters and can output a value after performing a few operations on the parameters. Functions are defined using the \"def\" keyword, followed by the function name and parameter list, and the code block that defines what the function does.</p> <p>Significance of functions in python: * Reusability: Functions can be reused in different parts of the program or in different programs altogether, which saves time and effort.</p> <ul> <li> <p>Readability: Functions help to make code more readable by encapsulating complex logic into a single block of code with a descriptive name.</p> </li> <li> <p>Modularity: Functions allow us to break down a large program into smaller, more manageable parts. This makes it easier to understand and maintain the codebase.</p> </li> <li> <p>Debugging: Functions can be used to isolate and debug specific parts of a program, which can be helpful when troubleshooting issues.</p> </li> </ul> <p>Basic syntax of a function: <pre><code>def &lt;function-name&gt;(&lt;parameters&gt;):\n    \"\"\"Function's docstring\"\"\"\n    &lt;Expressions/Statements/Instructions&gt;\n</code></pre></p> <pre><code>## Define a function to add two numbers\ndef add(x, y):\n    \"\"\"This function returns the sum of two numbers\"\"\"\n    return x + y\n</code></pre> <pre><code>add(2, 3)\n</code></pre> <pre><code>add(x=9, y=10) \n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/01_Function_in_Python/#arguments-and-parameters","title":"Arguments and Parameters","text":"<p>In Python, both parameters and arguments are used in functions to pass values between the function and the calling code. However, they have different meanings and are used in different contexts.</p> <ul> <li> <p>Parameter: A parameter is a variable in the function definition that represents a value that must be passed to the function. It's like a placeholder that tells the function what kind of value to expect when it's called. <pre><code>def add(x, y):\n    return x + y\n</code></pre></p> </li> <li> <p>Argument: An argument, on the other hand, is a value that is passed to a function when it's called. It's the actual value that is used in the function when it's executed. <pre><code>s = add(2, 3)\n</code></pre></p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/","title":"Types Of Functions And Function Overloading","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/#types-of-functions","title":"Types of Functions","text":"<p>In Python, there are several types of functions that serve different purposes. Here are some of the most commonly used types: * Built-in Functions: These are functions that are pre-defined in Python and can be used directly in our code without the need for any additional definitions. Examples include print(), len(), and input().</p> <ul> <li> <p>User-Defined Functions: These are functions that are defined by the user for a specific purpose. They can be called from anywhere in the code and can take one or more arguments. User-defined functions can be very simple or very complex, depending on the task they are designed to perform.</p> </li> <li> <p>Anonymous Functions (Lambda Functions): These are small, one-line functions that can be defined without a name using the \"lambda\" keyword. They are commonly used when you need to pass a function as an argument to another function.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/#built-in-functions","title":"Built-in Functions","text":"<pre><code>name = \"GeoNext\"\n# Built-in functions\nprint(name)\nprint(len(name))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/#user-defined-functions","title":"User-Defined Functions","text":"<pre><code>def multiply(a, b):\n    \"\"\"This function returns the multiplication of two numbers\"\"\"\n    return a * b\n\nmultiply(2, 3)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/#lambda-function","title":"Lambda Function","text":"<pre><code>square = lambda x: x**2\nsquare(2)\n</code></pre> <pre><code>divide = lambda x, y: x / y\ndivide(6, 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/02_Types_of_Functions_and_Function_Overloading/#function-overloading","title":"Function Overloading","text":"<p>Function overloading is a programming concept that allows a function to have multiple implementations with the same name but different parameters. In some programming languages such as C++ and Java, function overloading is supported natively. However, in Python, function overloading is not supported in the same way, but there are ways to achieve similar functionality.</p> <p>In python, we can define multiple functions with the same name but only the last function will be considered. all the rest gets hidden.</p> <pre><code>def add(x, y):\n    return x + y\n\ndef add(x, y, z):\n    return x + y + z\n</code></pre> <pre><code># add(1, 2) # Throws an error\nadd(1, 2, 3)\n</code></pre> <p>In the above code, there are two add() methods, where only last methed of them can be used. Calling any of other methods will produce an error. Like here calling add(1, 2) will throw an error.</p> <p>This issue can be overcomed by the following method:</p> <pre><code># !pip install multipledispatch\n</code></pre> <pre><code>from multipledispatch import dispatch\n</code></pre> <pre><code>@dispatch(int, int)\ndef add(x, y):\n    return x + y\n\n@dispatch(int, int, int)\ndef add(x, y, z):\n    return x + y + z\n</code></pre> <pre><code>add(1, 2)\n</code></pre> <pre><code>add(1, 2, 3)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/03_Scope_of_Variables/","title":"Scope Of Variables","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/03_Scope_of_Variables/#scope-of-variables","title":"Scope of Variables","text":"<p>In a Python program, all variables may not be accessible at all location in that program. A variable's scope refers to the part(s) of the program in which the variable is accessible. A variable will only be visible to and accessible by the code blocks in its scope.</p> <p>There are broadly two kinds of scope in Python: * Global Scope: A variable declared outside of any function has global scope. This means that the variable is accessible throughout the entire program. A variable declared outside a function is known as a global variable.</p> <ul> <li>Local Scope: A variable declared inside a function is said to have local scope. This means that the variable is accessible only within the function in which it was declared. A variable declared inside a function is known as local variable.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/03_Scope_of_Variables/#creating-a-global-variable","title":"Creating a Global Variable","text":"<pre><code>x = \"Global Variable\"\ndef checkScope():\n    print(\"x is a\", x)\ncheckScope()\n</code></pre> <pre><code>print(x)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/03_Scope_of_Variables/#creating-a-local-variable","title":"Creating a Local Variable","text":"<pre><code>def checkLocal():\n    y = \"Local Variable\"\n    print(\"y is a\",  y)\ncheckLocal()\n</code></pre> <pre><code># If we call a local variable outside of its scope, we will get an error.\n# print(y) \n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/04_Concept_of_args_and_kwargs/","title":"Concept Of Args And Kwargs","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/04_Concept_of_args_and_kwargs/#concept-of-args-and-kwargs","title":"Concept of args and *kwargs","text":"<p>In Python, it's possible to define functions that accept a variable number of arguments. This can be useful when we don't know in advance how many arguments a function will need to handle, or when we want to make our code more flexible. Python uses these special symbols for passing arguments:</p> <ul> <li>*args (For Non-Keyword Arguments)</li> <li>**kwargs (For Keyword Arguments)</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/04_Concept_of_args_and_kwargs/#args-for-non-keyword-arguments","title":"*args (For Non-Keyword Arguments)","text":"<p>In Python, args is a special syntax used to pass a variable number of positional arguments to a function. The args notation allows us to pass any number of arguments to a function, and those arguments will be collected into a tuple.</p> <p>For example, if we want to make an addition function that supports taking any number of arguments and able to add them all together. In this case we can use *args.</p> <p>One important thing to note is that args must come at the end of a function's parameter list, after any named arguments. This is because args collects all positional arguments that are not matched to named parameters, so any named parameters that come after *args will be ignored</p> <pre><code>def sum(*args):\n    s = 0\n    for i in args:\n        s += i\n    print(s)\n</code></pre> <pre><code>sum(1, 2, 3, 4, 5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/04_Concept_of_args_and_kwargs/#kwargs-for-keyword-arguments","title":"**kwargs (For Keyword Arguments)","text":"<p>In Python, kwargs is a special syntax used to pass a variable number of keyword arguments to a function. The kwargs notation allows us to pass any number of keyword arguments to a function, and those arguments will be collected into a dictionary.</p> <p>A keyword argument is where we provide a name to the variable as we pass it into the function.</p> <p>One important thing to note is that kwargs must come after args in a function's parameter list, if both are used. This is because args collects all positional arguments that are not matched to named parameters, while kwargs collects all keyword arguments that are not matched to named parameters.</p> <pre><code>def myFunction(**kwargs):\n    for key, value in kwargs.items():\n        print(key, \":\", value)\n</code></pre> <pre><code>myFunction(name=\"Krishnagopal Halder\", age=22)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/04_Concept_of_args_and_kwargs/#example-with-args-and-kwargs","title":"Example with args and *kwargs","text":"<pre><code>def personalInfo(name, age, *args, **kwargs):\n    print(\"The name of the peron is\", name)\n    print(\"The age of the person is\", age)\n    print(\"Other informations:\")\n    for key, value in kwargs.items():\n        print(key, \":\", value)\n    print(\"Unknown informations:\")\n    for i in args:\n        print(i)    \n</code></pre> <pre><code>personalInfo(\"Krishnagopal Halder\", 22, \"First arg\", \"Second arg\", state=\"WB\", country=\"India\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/05_Python_Default_Parameters/","title":"Python Default Parameters","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/06%20Function%20in%20Python/05_Python_Default_Parameters/#python-default-parameters","title":"Python Default Parameters","text":"<p>In Python, function parameters can have default values. We can provide a default value to a parameter by using the assignment(=) operator.</p> <pre><code>def programmer(name, age, coreLang=\"Python\"):\n    print(\"The name of the programmer is\", name)\n    print(\"The age of the programmer is\", age)\n    print(\"The core programming language is\", coreLang)\n</code></pre> <pre><code># Calling function with the default value of coreLang parameter\nprogrammer(\"Krishnagopal Halder\", 22)\n</code></pre> <pre><code># Calling function with the changed value of coreLang parameter\nprogrammer(\"Arav Mahanti\", 25, \"C++\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/","title":"Two Dimensional List","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#two-dimensional2-d-list","title":"Two-Dimensional(2-D) List","text":"<p>A two-dimensional list is a list of lists where each list within the main list represents a row of data. Two-dimensional lists are commonly used in data analysis and scientific computing to represent matrices and tables.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#creating-a-two-dimensional-list","title":"Creating a Two-Dimensional List","text":"<p>Consider an example of recording test scores of 4 students, in 4 subjects. Such data can be represented as a two-dimensional list</p> <pre><code>student1 = [9, 8, 6, 5]\nstudent2 = [8, 10, 4, 7]\nstudent3 = [7, 3, 8, 9]\nstudent4 = [10, 8, 7, 9]\n</code></pre> <pre><code>marks_of_students = [student1, student2, student3, student4]\nprint(marks_of_students)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#accessing-values-in-a-two-dimensional-list","title":"Accessing Values in a Two-Dimensional List","text":"<p>To access an element in a two-dimensional list, we use the row and column indices of the element. The row index specifies the inner list, and the column index specifies the position within the inner list.</p> <pre><code># List at index 0 in marks_of_students\nprint(marks_of_students[0])\n\n# Element at index 2 in list at index 0\nprint(marks_of_students[0][2])\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#input-of-two-dimensional-list","title":"Input of Two-Dimensional List","text":"<p>We can also input data into a two-dimensional list from user input using loops and the input function. We will discuss two common ways of taking user input: * Line Separated Input: Different rows in different lines. * Space Separated Input: Taking input in a single line.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#line-separated-input","title":"Line Separated Input","text":"<pre><code># Number of rows\nn = int(input())\ninputList = [[int(col) for col in input().split()] for row in range(n)]\nprint(inputList)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#space-separated-input","title":"Space Separated Input","text":"<pre><code># Number of rows\nrow = int(input())\n\n# Number of columns\ncol = int(input())\n\n# Converting input string to list\ninputList = input().split(\" \") \n\nfinalList = [[int(inputList[i*col+j])for j in range(col)]for i in range(row)]\n\nprint(finalList)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#printing-in-multiple-lines-like-a-matrix","title":"Printing in Multiple Lines Like a Matrix","text":"<p>We can use a nested for loops to print a 2-D List. The outer loop iterates over the rows and the inner loop will itearte over columns.</p> <pre><code>myList = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\n\n# Iterate in row of 2-D List\nfor row in myList:\n    for col in row:\n        print(col, end=\" \")\n    print()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/01_Two_Dimensional_List/#concept-of-jagged-lists","title":"Concept of Jagged Lists","text":"<p>In Python, a jagged list is a list of lists where the sublists may have different lengths. Jagged lists are useful when we need to store data that is not necessarily uniform or when we want to conserve memory by not allocating space for unused elements. Jagged lists are commonly used in data science, natural language processing, and machine learning.</p> <pre><code>jgList1 = [[1, 2, 3], 4, 5, [6, 7], [8, 9, 10]]\njgList2 = [[10, 11, 12], [18, 19], [20, 22, 23, 24], [10, 12]]\n</code></pre> <pre><code>print(jgList1[3])\nprint(jgList2[2])\n</code></pre> <pre><code>a = [1, 2, 3, 4]\nprint(a.index(2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/","title":"List Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#list-methods","title":"List Methods","text":"<p>In Python, a list is a versatile and commonly used data structure that allows you to store a collection of items. Lists can contain elements of different types, such as integers, strings, or even other lists. Python provides several built-in methods that allow you to perform various operations on lists efficiently.</p> <pre><code>myLst1 = [1, 2, 3, 4]\nmyLst2 = [5, 6, 7, 8]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#accessing-and-modifying-list-elements","title":"Accessing and Modifying List Elements","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#append","title":"append()","text":"<p>The append() method allows you to add an element to the end of a list.</p> <pre><code>myLst2.append(9)\nmyLst2\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#insert","title":"insert()","text":"<p>The insert() method allows you to insert an element at a specific position in the list.</p> <pre><code>myLst1.insert(0, 0)\nmyLst1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#extend","title":"extend()","text":"<p>The extend() method is used to append elements from another list to the end of the current list.</p> <pre><code>f1 = [\"Mango\", \"Orange\"]\nf2 = [\"Watermelon\", \"Grapes\"]\nf1.extend(f2)\nprint(f1)\nprint(f2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#remove","title":"remove()","text":"<p>The remove() method removes the first occurrence of a specified element from the list.</p> <pre><code>f1\n</code></pre> <pre><code>f1.remove(\"Grapes\")\nf1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#pop","title":"pop()","text":"<p>The pop() method removes and returns an element at a specific index in the list. If no index is provided, it removes the last element.</p> <pre><code>f1\n</code></pre> <pre><code>f1.pop(1)\nf1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#index","title":"index()","text":"<p>The index() method returns the index of the first occurrence of a specified element in the list.</p> <pre><code>lst1 = [1, 2, 5, 7, 1, 5]\n</code></pre> <pre><code>lst1.index(5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#count","title":"count()","text":"<p>The count() method returns the number of occurrences of a specified element in the list.</p> <pre><code>lst1\n</code></pre> <pre><code>lst1.count(5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#sorting-and-reversing-lists","title":"Sorting and Reversing Lists","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#sort","title":"sort()","text":"<p>The sort() method sorts the list in ascending order. It modifies the original list and does not return a new sorted list.</p> <pre><code>lst1\n</code></pre> <pre><code>lst1.sort()\nlst1\n</code></pre> <pre><code>lst2 = [\"a\", \"c\", \"b\"]\nlst2.sort()\nlst2\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#reverse","title":"reverse()","text":"<p>The reverse() method reverses the order of elements in the list. It modifies the original list and does not return a new reversed list.</p> <pre><code>print(lst1)\nlst1.reverse()\nprint(lst1)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#other-list-operations","title":"Other List Operations","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#len","title":"len()","text":"<p>The len() function returns the number of elements in a list.</p> <pre><code>len(lst1)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#clear","title":"clear()","text":"<p>The clear() method removes all elements from a list, making it empty.</p> <pre><code>c = [\"a\", \"b\", 1]\nprint(c)\nc.clear()\nprint(c)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/02_List_Methods/#copy","title":"copy()","text":"<p>The copy() method creates a shallow copy of a list. Any modifications made to the original list will not affect the copied list, and vice versa.</p> <pre><code>l1 = [1, 2, 3]\nid(l1)\n</code></pre> <pre><code>l2 = l1.copy()\nprint(l2)\nprint(id(l2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/","title":"List Slicing","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/#list-slicing","title":"List Slicing","text":"<p>List slicing is a powerful feature in Python that allows you to extract a portion of a list or create a new list containing a subset of elements from an existing list. Slicing provides a concise and flexible way to work with lists and access specific elements based on their indices.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/#basic-list-slicing","title":"Basic List Slicing","text":"<p>List slicing is performed using the colon (:) operator, which allows you to specify the start and end indices of the slice. The start index is inclusive, while the end index is exclusive, meaning that the element at the end index is not included in the slice.</p> <pre><code>my_nums = [1, 2, 3, 4, 5, 6]\nsliced_nums = my_nums[0:4]\nsliced_nums\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/#slicing-with-step","title":"Slicing with Step","text":"<p>You can also specify a step size while slicing a list. The step size determines the interval between elements in the resulting slice.</p> <pre><code>my_nums\n</code></pre> <pre><code>sliced_nums_with_step = my_nums[0:5:2]\nsliced_nums_with_step\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/#negative-indexing","title":"Negative Indexing","text":"<p>Python allows negative indexing, which means you can access elements from the end of the list by using negative indices. The last element has an index of -1, the second-last element has an index of -2, and so on.</p> <pre><code>last_num = my_nums[-1]\nlast_num\n</code></pre> <pre><code>sliced_num_neg_index = my_nums[-4: -2]\nsliced_num_neg_index\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/03_List_Slicing/#slicing-and-assignment","title":"Slicing and Assignment","text":"<p>List slicing can also be used to modify a slice of a list by assigning new values to the slice.</p> <pre><code>my_nums\n</code></pre> <pre><code>my_nums[0:3] = [7, 8, 9]\nmy_nums\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/04_List_Comprehension/","title":"List Comprehension","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/04_List_Comprehension/#list-comprehension","title":"List Comprehension","text":"<p>List comprehension is a concise and elegant way of creating new lists based on existing lists. A list comprehension consists of an expression followed by for statement inside square brackets.</p> <p>Syntax newList = [\"expression\" for \"item\" in iterable]</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/07%20List%20in%20Python/04_List_Comprehension/#example","title":"Example","text":"<pre><code>pow2 = [2 ** x for x in range(10)]\nprint(pow2)\n</code></pre> <pre><code># The above code can be written in this way as well\npow2 = []\nfor i in range(10):\n    pow2.append(2 ** i)\nprint(pow2)\n</code></pre> <pre><code># We can use other expression to modify and create a new list\nnewList = [x for x in range(10) if x % 2 == 1]\nprint(newList)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/","title":"Introduction To Strings","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/#string","title":"String","text":"<p>In Python, a string is a sequence of characters enclosed within single, double or triple quotes. It is one of the most commonly used data types in Python. In this module, we will explore various operations that can be performed on strings in Python.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/#string-creation","title":"String Creation","text":"<p>To create a string in Python, simply enclose a sequence of characters within single, double or triple quotes. For example:</p> <pre><code># Defining strings in Python\n\n# Using single quote\nname1 = 'GeoNext'\nprint(name1)\n\n# using double quote\nname2 = \"Krishnagopal Halder\"\nprint(name2)\n\n# We can use triple quotes to create docstrings and/or multiline strings.\nmessage = '''\nHello, my name is Krishnagopal.\nI am modern GIS geek.\nThanks for joining today's class.\n'''\nprint(message)\n\nprint(type(name1))\nprint(type(name2))\nprint(type(message))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/#accessing-characters-in-a-string","title":"Accessing Characters in a String","text":"<p>To access individual characters in a string, we use indexing. In Python, indexing starts at 0. </p> <pre><code>print(name1[0]) # Output: G\nprint(name1[1]) # Output: e\nprint(name1[2]) # Output: o\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/#negative-indexing","title":"Negative Indexing","text":"<p>Python allows negative indexing for strings. The index of -1 refers to the last character, -2 refers to the second last character, and so on. The negative indexing starts from the last character in the string.</p> <pre><code>print(name1[-1]) # Output: t\nprint(name1[-2]) # Output: x \nprint(name1[-3]) # Output: e \nprint(name1[-4]) # Output: N \n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/01_Introduction_to_Strings/#string-slicing","title":"String Slicing","text":"<p>Slicing a string means extracting a substring from the original string. We use the slice operator : for this purpose.</p> <pre><code>print(name1[0:3]) # Output: Geo\nprint(name1[:]) # Output: GeoNext\nprint(name1[3:]) # Output: Next\n</code></pre> <pre><code>print(name1[-1]) # Output: t\nprint(name1[-4:]) # Output: Next\nprint(name1[-4:-1]) # Output: Nex\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/","title":"String Concatenation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/#string-concatenation","title":"String Concatenation","text":"<p>String concatenation is the process of combining two or more strings into a single string. String concatenation can be done using many ways. We can perform string concatenation using the following ways: * Using + operator  * Using join() method  * Using % operator  * Using format() function</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/#using-operator","title":"Using + Operator","text":"<p>The simplest way to concatenate two strings in Python is to use the + operator. The + operator functions is similar to the arithmetic + operator, but here both the operands must be string.</p> <pre><code>first_name = \"Krishnagopal\"\nlast_name = \"Halder\"\nprint(first_name + \" \" + last_name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/#using-join-method","title":"Using join() Method","text":"<p>The join() method is a powerful and efficient way to concatenate strings in Python. It takes an iterable object (such as a list or tuple) and joins the elements of the iterable with the string. If the iterable contains any non-string values, it raises a TypeError exception.</p> <pre><code>words = [\"Krishnagopal\", \"Halder\", \"is\", \"a\", \"student\", \"of\", \"Geoinformatics\"]\nprint(\" \".join(words))\nname = \" \".join([first_name, last_name])\nprint(name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/#using-operator_1","title":"Using % Operator","text":"<p>We have used % operator for string formatting, but it can also be used for string concatenation. % operator helps both in string concatenation and string formatting.</p> <pre><code>n1 = \"Geo\"\nn2 = \"Next\"\nprint(\"%s%s\"%(n1, n2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/02_String_Concatenation/#using-format-function","title":"Using format() function","text":"<p>The format() function is a powerful way to format strings in Python. It provides a flexible and efficient way to combine strings and variables into a single string.</p> <pre><code>name = \"Krishnagopal Halder\"\ncourse = \"Python\"\nmessage = \"Hello, {}. Thank you for joining this {} online course.\".format(name, course)\nprint(message)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/","title":"String Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#string-methods","title":"String Methods","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#repeatingreplicating-strings","title":"Repeating/Replicating Strings","text":"<p>In Python, we can repeat or replicate strings using the multiplication operator *. The multiplication operator takes two operands: a string and an integer. When we multiply a string by an integer, Python returns a new string that contains the original string repeated the specified number of times.</p> <pre><code>name = \"GeoNext\"\nprint(name * 3)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#string-slicing","title":"String Slicing","text":"<p>String slicing is the process of extracting a part of a string. In Python, we can slice a string using two methods. * Extending Indexing * slice() Constructor</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#extending-indexing","title":"Extending Indexing","text":"<p>In Python, we can slice a string using the indexing operator []. The indexing operator takes one or two arguments: the index of the character we want to extract, or the start and end indices of the slice we want to extract.</p> <pre><code>name = \"GeoNext\"\nprint(name[3])\nprint(name[0:3])\nprint(name[3:])\nprint(name[::2])\n</code></pre> <pre><code># Print string in reverse\nprint(name[::-1])\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#slice-constructor","title":"slice() Constructor","text":"<p>Python provides a built-in function called slice() that we can use to create slice objects. Slice objects can then be passed as arguments to the indexing operator to slice a string. Syntax: slice(start, stop, step) * start: Optional. An integer that specifies the starting index of the slice. If omitted, it defaults to None, which means the slice starts at the beginning of the string. * stop: Required. An integer that specifies the ending index of the slice. This index is not included in the slice. * step: Optional. An integer that specifies the step size of the slice. If omitted, it defaults to None, which means the slice uses a step size of 1.</p> <pre><code>name = \"GeoNext\"\ns1 = slice(0, 3)\ns2 = slice(3, 8)\ns3 = slice(0, 8, 2)\nprint(name[s1])\nprint(name[s2])\nprint(name[s3])\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/03_String_Methods/#string-comparison","title":"String Comparison","text":"<p>Comparing strings is the process of comparing two or more strings to determine whether they are equal or not. String comparison in Python takes place character by character i.e. each character in the same index, are compared with each other.</p> <p>There are several ways to compare strings in Python. One way is to use the comparison operators such as ==, !=, &lt;, &lt;=, &gt;, and &gt;=. These operators compare the strings lexicographically and return a Boolean value of True or False depending on the comparison result.</p> <pre><code>str1 = \"Krishnagopal\"\nstr2 = \"Krishnagopal\"\nstr3 = \"Halder\"\nprint(str1 == str2) # Checks if two strings are equal or not\nprint (str1 != str3) # Checks if two strings are not equal\nprint(str1 &lt; str3) # Checks if the first string is less than the second string\nprint(str1 &lt;= str3) # Checks if the first string is less than or equal to the second string\nprint(str1 &gt; str3) # Checks if the first string is greater than the second string\nprint(str1 &gt;= str3) # Checks if the first string is greater than or equal to the second string\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/04_Iterating_on_Strings/","title":"Iterating On Strings","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/04_Iterating_on_Strings/#iterating-on-strings","title":"Iterating on Strings","text":"<p>Iterating over strings means to access each character in the string one at a time and perform some operation on it. In Python, there are several ways to iterate over strings.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/04_Iterating_on_Strings/#using-for-loop","title":"Using for loop","text":"<pre><code>str1 = \"Krishnagopal Halder\"\n\n# Using for loop\nfor i in str1:\n    print(i)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/04_Iterating_on_Strings/#using-for-loop-and-range","title":"Using for loop and range()","text":"<pre><code>for i in range(len(str1)):\n    print(str1[i], end=\" \")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/08%20String/04_Iterating_on_Strings/#using-while-loop","title":"Using while loop","text":"<pre><code>i = 0\nwhile i &lt; len(str1):\n    print(str1[i])\n    i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/","title":"Introduction To Oop","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/#introduction-to-object-oriented-programming-oop","title":"Introduction to Object Oriented Programming (OOP)","text":"<p>Object-Oriented Programming (OOP) is a powerful programming paradigm that allows developers to organize and structure their code in a more intuitive and modular way. It focuses on the creation of objects, which are instances of classes, and the interaction between these objects. Python, being a versatile and dynamic language, fully supports OOP principles and provides robust tools and features to implement them.</p> <p>Why do we use object-oriented programming? 1. Modularity and code reusability. 2. Abstraction and encapsulation. 3. Organizing and maintaining code. 4. Inheritance for code reuse. 5. Flexibility through polymorphism. 6. Collaboration in team development. 7. Scalability and extensibility.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/#what-is-an-object","title":"What is an Object?","text":"<p>The object is an entity that has a state and behavior associated with it. It may be any real-world object like the mouse, keyboard, chair, table, pen, etc.</p> <p>Integers, strings, floating-point numbers, even arrays, and dictionaries, are all objects. More specifically, any single integer or any single string is an object. The number 12 is an object, the string\"  Hello, world\" is an object, a list is an object that can hold other objects, and so on. You've been using objects all along and may not even realize it.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/#what-is-a-class","title":"What is a Class?","text":"<p>A class is a blueprint (a plan basically) that isused to define (bind together) a set of variables and methods (Characteristics) that are common to all objects of a particular kind. </p> <p>Example: If Car is a class, then Maruti 800 is an object of the Car class. All cars share similar features like wheels, 1 steeringwheel, windows, breaks, etc. Maruti 800 (the Car object) has all these features.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/#classes-vs-objects-or-instances","title":"Classes vs Objects (Or Instances)","text":"<ul> <li> <p>Classes are used to create user-defined data structures. Classes define functions called methods , which identify the behaviors and actions that an object created from the class can perform with its data. </p> </li> <li> <p>In this module, you\u2019ll create a Car class that stores some information about the characteristics and behaviors that an individual Car can have. </p> </li> <li> <p>A class is a blueprint for how something should be defined. It doesn\u2019t contain any data. The Car class specifies that a name and a top-speed are necessary for defining a Car , but it doesn\u2019t contain the name or top-speed of any specific Car.</p> </li> <li> <p>While the class is the blueprint, an instance is an object that is built from a class and contains real data. An instance of the Car class is not a blueprint anymore. It\u2019s an actual car with a name , like Creta, and with a top speed  of 200 Km/Hr.</p> </li> <li> <p>Put another way, a class is like a form or a questionnaire. An instance is like a form that has been filled out with information. Just like many people can fill out the same form with their unique information, many instances can be created from a single class.</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/01_Introduction_to_OOP/#defining-a-class-in-python","title":"Defining a Class in Python","text":"<pre><code># Creating a car class\nclass Car:\n    pass\n</code></pre> <pre><code># Creating an instance (object) from that car class\ncar1 = Car\n# Giving some attribute to the car object\ncar1.name = \"Maruti 800\"\ncar1.topspeed = 120\n</code></pre> <pre><code># Checking the attribute of the car object\nprint(\"The name of the car is\", car1.name)\nprint(\"The topspeed of the car is\", car1.topspeed, \"Km/h\")\n</code></pre> <pre><code># Creating another car object\ncar2 = Car\n# Giving the attribute to the new car object\ncar2.name = \"Creta\"\ncar2.topspeed = 400\ncar2.color = \"Yellow\"\n</code></pre> <pre><code># Checking the attribute of the new car object\nprint(\"The name of the new car is\", car2.name)\nprint(\"The topseed of the new car is\", car2.topspeed, \"Km/h\")\nprint(\"The color of the new car is\", car2.color)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/","title":"Constructor","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/#constructor","title":"Constructor","text":"<p>Constructors are special methods in Python classes that are used to initialize objects. The constructor method is automatically called when an object is created from a class. In Python, the constructor method is defined using the init() method.</p> <p>Syntax of a constructor: <pre><code>def __init__(self):\n    # body of the constructor\n</code></pre></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/#types-of-constructors","title":"Types of constructors","text":"<ul> <li> <p>Default Constructor: A default constructor is provided by Python if no constructor is explicitly defined in a class. It takes no parameters and doesn't perform any initialization. Its definition has only one argument which is a reference to the instance being constructed known as self.  Example: def init(self):</p> </li> <li> <p>Parameterized Constructor: A parameterized constructor is defined with one or more parameters to initialize the object's attributes during creation. It allows you to pass values at the time of object creation to set initial attribute values. Example: def init(self, parameter1, parameter2, ...):</p> </li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/#the-self-parameter","title":"The self Parameter","text":"<p>The self parameter is a convention used in method definitions within a class. It represents the instance of the object on which the method is being called. It acts as a reference to the current object and allows access to its attributes and methods.</p> <ul> <li>The self parameter is a reference to the current instance of the class and is used to access variables that belong to the class.</li> <li>It does not have to be named 'self', you can call it whatever you like, but it has to be the first parameter of any function in the class.</li> <li>You can give .init() any number of parameters, but the first parameter will always be a variable called self.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/#defining-a-class-with-constructor","title":"Defining a Class with Constructor","text":"<pre><code># Creating a car class with parameterized constructor\nclass Car:\n    def __init__(self, name, topspeed, color):\n        self.name = name\n        self.topspeed = topspeed\n        self.color = color\n</code></pre> <pre><code># Creating an instance (object) from that car class\ncar1 = Car(\"Ferrari\", 400, \"Red\")\n</code></pre> <pre><code># Checking the attribute of the car object\nprint(\"The name of the car is\", car1.name)\nprint(\"The topspeed of the car is\", car1.topspeed, \"Km/h\")\nprint(\"The color of the car is\", car1.color)\n</code></pre> <pre><code># Add a function to the car class that will print the car attributes\nclass Car:\n    def __init__(self, name, topspeed, color):\n        self.name = name\n        self.topspeed = topspeed\n        self.color = color\n    def car_info(self):\n        print(\"The name of the car is\", self.name)\n        print(\"The topspeed of the car is\", self.topspeed, \"Km/h\")\n        print(\"The color of the car is\", self.color)\n</code></pre> <pre><code># Creating an object of the car class\ncar2 = Car(\"Maruti 800\", 120, \"White\")\n# Calling the car_info function to the object\ncar2.car_info()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/02_Constructor/#example","title":"Example:","text":"<p>An example of a class for an application form in Python:</p> <pre><code>class  ApplicationForm:\n    def __init__(self, name, age, dob, phone):\n        self.name = name\n        self.age = age\n        self.dob = dob\n        self.phone = phone\n\n    def set_email(self, email):\n        self.email = email\n\n    def set_address(self, address):\n        self.address = address\n\n    def print_form(self):\n        print(\"Application Form:\")\n        print(f\"Name: {self.name}\")\n        print(f\"Age: {self.age}\")\n        print(f\"DOB: {self.dob}\")\n        print(f\"Phone: {self.phone}\")\n        print(f\"Email: {self.email}\")\n        print(f\"Address: {self.address}\")\n</code></pre> <pre><code>form1 = ApplicationForm(\"Kunal Roy\", 20, \"29-08-2002\", 6245684125)\nform1.set_email(\"roy_kunal2000@gmail.com\")\nform1.set_address(\"Newtown, Kolkata, West Bengal, India\")\n</code></pre> <pre><code>form1.print_form()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/03_Types_of_Attributes/","title":"Types Of Attributes","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/03_Types_of_Attributes/#types-of-attributes","title":"Types of Attributes","text":"<p>In Python, attributes are variables that belong to an object or class. They store data that defines the characteristics or properties of the object. There are two types of attributes commonly used in Python:</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/03_Types_of_Attributes/#instance-attributes","title":"Instance Attributes:","text":"<p>Attributes created in .init() are called instance attributes. An instance attribute's value is specific to a particular instance of the class. All car objects have a name and a topspeed, but the values for the name and topspeed will vary depending on the car instance. Different objects of the Car class have different names and top speeds.</p> <p>Characteristics: * Instance attributes are specific to each instance of a class. * They are defined and assigned within the constructor (init() method) using the self keyword. * Each object of the class can have different values for these attributes. * Example: self.name, self.topseed, etc.</p> <pre><code># Example of Instance Attribute with Car class\nclass Car:\n    # Attributed defined within the constructor are instance attributes\n    def __init__(self, name, topspeed):\n        self.name = name\n        self.topspeed = topspeed\n\n    def print_details(self):\n        print(f\"Car Name: {self.name}\")\n        print(f\"Top Speed: {self.topspeed} Km/h\")\n</code></pre> <pre><code># Creating instances from Car class\ncar1 = Car(\"Maruti 800\", 120)\ncar2 = Car(\"Ferrari\", 400)\n</code></pre> <pre><code># Printing the details of the car objects\ncar1.print_details()\n</code></pre> <pre><code>car2.print_details()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/03_Types_of_Attributes/#class-attribute","title":"Class Attribute","text":"<p>Class attributes are attributes that have the same value for all class instances. You can define a class attribute by assigning a value to a variable name outside of .init().</p> <p>Characteristics: * Class attributes are shared among all instances of a class. * They are defined outside any method within the class scope. * Class attributes are the same for every object of the class. * They are accessed using the class name or instance object. * Example: className.attribute, self.attribute, etc.</p> <pre><code># Example of Class Attribute with Car class\nclass Car:\n    # Creating a class attribute\n    no_of_wheels = 4\n\n    def __init__(self, name, topspeed):\n        self.name = name\n        self.topspeed = topspeed\n\n    def print_details(self):\n        print(f\"Car Name: {self.name}\")\n        print(f\"Top Speed: {self.topspeed} Km/h\")\n        print(f\"No of Wheels: {self.no_of_wheels}\")\n</code></pre> <pre><code># Creating instances from Car class\ncar1 = Car(\"Creta\", 300)\ncar2 = Car(\"Toyato\", 240)\n</code></pre> <pre><code># Printing the car details\ncar1.print_details()\n</code></pre> <pre><code>car2.print_details()\n</code></pre> <pre><code># Accessing the class attributes of the Car Class\nprint(\"No of Wheels:\", Car.no_of_wheels)\nprint(\"No of Wheels of car1:\", car1.no_of_wheels)\nprint(\"No of Wheels of car2:\", car2.no_of_wheels)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/03_Types_of_Attributes/#example","title":"Example:","text":"<p>Creating a Student class with Class and Instance Attributes</p> <pre><code>class Student:\n    school = \"MBD DAV Public School\"\n    def __init__(self, name, rollno):\n        self.name = name\n        self.rollno = rollno\n\n    def printDetails(self):\n        print(\"Stuent Name:\", self.name)\n        print(\"Roll No:\", self.rollno)\n        print(\"School:\", self.school)\n</code></pre> <pre><code># Creating instances from Student class\nstudent1 = Student(\"Ayan Ghosh\", 1)\nstudent2 = Student(\"Sayan Pal\", 2)\n</code></pre> <pre><code># Print the details of the student object\nstudent1.printDetails()\n</code></pre> <pre><code>student2.printDetails()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/04_Inheritance/","title":"Inheritance","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/04_Inheritance/#inheritance","title":"Inheritance","text":"<p>Inheritance is a fundamental concept in object-oriented programming (OOP). It allows a class to inherit attributes and methods from another class, known as the parent or base class. Inheritance enables code reuse and promotes the concept of hierarchy and specialization.</p> <p>The class which inherits the properties of the other is known as subclass (derived class or child class) and the class whose properties are inherited is known as superclass (base class, parent class).</p> <p>Example: Let us take a real-life example to understand inheritance. Let's assume that Human is a class that has properties such as height, weight, age, etc and functionalities (or methods) such as eating(), sleeping), dreaming(), working (), etc. Now we want to create Male and Female classes. Both males and females are humans and they share some common properties (like height, weight, age, etc) and behaviours (or functionalities like eating(), sleeping(), etc), so they can inherit these properties and functionalities from the Human class. Both males and females have some characteristics specific to them (like men have short hair and females have long hair). Such properties can be added to the Male and Female classes separately. This approach makes us write less code as both the classes inherit several properties and functions from the superclass, thus we didn't have to re-write them. Also, this makes it easier to read the code.</p> <p>Python Inheritance Syntax: <pre><code>class SuperClass:\n    # Body of base class\n\nclass SubClass(SuperClass):\n    # Body of the derived class\n</code></pre></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/04_Inheritance/#example","title":"Example:","text":"<p>Here's an example of inheritance in Python using a superclass 'Vehicle' and subclasses 'Car' and 'Motorcycle':</p> <pre><code># Creating a superclass 'Vehicle'\nclass Vehicle:\n    def __init__(self, brand, model, color):\n        self.brand = brand\n        self.model = model\n        self.color = color\n\n    def drive(self):\n        print(\"Driving the vehicle.\")\n\n    def stop(self):\n        print(\"Stopping the vehicle.\")\n</code></pre> <p>Constructor in Subclass: The constructor in subclass must call the constructor of the superclass by accessing the init() method of the superclass in the following format: <pre><code>&lt;SuperClassName&gt;.__init__(self, &lt;Parameter1&gt;, &lt;Parameter2&gt;, ...)\n</code></pre> Note: The parameters being passed in this call must be same as the parameters being passed in the superclass init() function, otherwise it will throw an error. </p> <pre><code># Creating a subclass 'Car'\nclass Car(Vehicle):\n    def __init__(self, brand, model, color, topSpeed):\n        super().__init__(brand, model, color)\n        self.topSpeed = topSpeed\n\n    def drive(self):\n        print(\"Driving the car.\")\n\n    def open_trunk(self):\n        print(\"Opening the trunk.\")\n</code></pre> <pre><code># Creating another subclass 'Motocycle'\nclass Motorcycle(Vehicle):\n    def __init__(self, brand, model, color, topSpeed):\n        super().__init__(brand, model, color)\n        self.topSpeed = topSpeed\n\n    def drive(self):\n        print(\"Driving the motorcycle\")\n\n    def indicator(self):\n        print(\"Turning on the indicator.\")\n</code></pre> <pre><code># Creating instances of subclasses\ncar1 = Car(\"Maruti\", \"Maruti 800\", \"White\", 150)\nmotorcycle1 = Motorcycle(\"Kawasaki\", \"Kawasaki Ninja H2\", \"Black\", 293)\n</code></pre> <pre><code># Calling overridden methods\ncar1.drive()\nmotorcycle1.drive()\n</code></pre> <pre><code># Calling subclass-specific methods\ncar1.open_trunk()\nmotorcycle1.indicator()\n</code></pre> <pre><code># Calling superclass methods\ncar1.stop()\nmotorcycle1.stop()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/04_Inheritance/#example_1","title":"Example:","text":"<p>Here's an example of inheritance in Python using a superclass 'Polygon' and subclasses 'Triangle':</p> <pre><code># Creating a superclass 'Polygon'\nclass Polygon:\n\n    # Constructor\n    def __init__(self, no_of_sides):\n        self.no_of_sides = no_of_sides\n        self.sideLengths = [0 for i in range(no_of_sides)]\n\n    # Take user input for side lengths\n    def inputSideLengths(self):\n        self.sideLengths = [int(input(\"Enter Side Length: \")) for i in range(self.no_of_sides)]\n\n    # Print the side lengths of the polygon\n    def displaySideLengths(self):\n        for i in range(len(self.sideLengths)):\n            print(f\"Side {i+1}: {self.sideLengths[i]}\")\n</code></pre> <pre><code># Creating a subclass 'Triangle'\nclass Triangle(Polygon):\n\n    def __init__(self):\n        # Calling constructor of superclas\n        Polygon.__init__(self, 3)\n\n    def calculateArea(self):\n        a, b, c = self.sideLengths\n        # Calculate the semi-perimeter\n        s = (a + b + c) / 2\n        area = (s * (s-a) * (s-b) * (s-c)) ** 0.5\n        print(\"The area of the triangle is %0.2f\" %area)\n</code></pre> <pre><code># Instantiating a Triangle object\ntriangle1 = Triangle()\n\n# Input the side legths of the triangle\ntriangle1.inputSideLengths()\n</code></pre> <pre><code># Display the side lengths\ntriangle1.displaySideLengths()\n</code></pre> <pre><code># Calculate the area of the triangle\ntriangle1.calculateArea()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/05_Access_Modifiers/","title":"Access Modifiers","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/05_Access_Modifiers/#access-modifiers","title":"Access Modifiers","text":"<p>Access Modifiers are the specifications that can be defined to fix boundaries for classes when accessing member functions (methods) or member variables are considered. Various object-oriented languages like C++, Java, Python control access modifications which are used to restrict access to the variables and methods of the class. Most programming languages majorly have the following three forms of access modifiers, which are Public, Private, and Protected in a class.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/05_Access_Modifiers/#public-modifier","title":"Public Modifier","text":"<p>The members of a class that are declared public are easily accessible from any part of the program. By default, all members (attributes and methods) in a Python class are considered public and can be accessed from anywhere. Consider the given example:</p> <pre><code># Creating a student class with public member\nclass Pub_Student:\n    name = None # public member by default\n    age = None # public member\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def public_method(self):\n        print(\"This is a public method.\")\n</code></pre> <pre><code># Creating objets/instances from Student class\npub_obj = Pub_Student(\"Sohon Roy\", 16)\n</code></pre> <pre><code># Calling the public members of the class\nprint(pub_obj.name)\nprint(pub_obj.age)\npub_obj.public_method()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/05_Access_Modifiers/#protected-modifier","title":"Protected Modifier","text":"<p>The members (attributes and methods) of a class that are declared protected are only accessible to a class derived from it. Data members of a class are declared protected by adding a single underscore '_' symbol before the data member of that class. However, this naming convention does not enforce actual access restrictions. It serves more as a hint to other developers that a member should be treated as protected. Here's an example:</p> <pre><code># Creating a student class with protected member\nclass Pro_Student:\n    _name = None # protected data member\n    age = None # public data member\n\n    # constructor\n    def __init__(self, name, age):\n        self._name = name\n        self.age = age\n\n    def _protected_method(self):\n        print(\"This is a protected method.\")\n</code></pre> <pre><code># Creating objets/instances from Student class\npro_obj = Pro_Student(\"Ayan Ghosh\", 16)\n</code></pre> <pre><code># Calling the protected members of the class\nprint(pro_obj._name)\npro_obj._protected_method()\n</code></pre> <pre><code># Calling the public members of the class\nprint(pro_obj.age)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/05_Access_Modifiers/#private-modifier","title":"Private Modifier","text":"<p>The member of a class that are declared private are accessible within the class only. A private access modifier is the most secure access modifier. Data members of a class are declared private by adding a double underscore '__' symbol before the data member of that class. Here's an example: </p> <pre><code># Creating a student class with private member\nclass Pri_Student:\n    name = None # public data member\n    __age = None # private data member\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self.__age = age\n\n    def __private_method(self):\n        print(\"This is a private method.\")\n</code></pre> <pre><code># Creating objets/instances from Student class\npri_obj = Pri_Student(\"Bikash Dey\", 21)\n</code></pre> <pre><code># Calling the public members of the class\nprint(pri_obj.name)\n</code></pre> <pre><code># Calling the private members of the class\n#print(pri_obj.__age)\n</code></pre> <p>We will get an AttributeError when we try to access the '__age' attribute. This is because the __age is a private attribute and hence it cannot be accessed from outside the class.</p> <p>Name Mangling: Private members in Python are name-mangled, which means their names are modified to make them harder to access. The name-mangling scheme is as follows: a double underscore __ at the beginning of an attribute or method name is replaced with _ClassName, where ClassName is the name of the class.</p> <pre><code># Accessing the private members of the class\nprint(pri_obj._Pri_Student__age)\npri_obj._Pri_Student__private_method()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/06_Access_Modifiers_with_Inheritance/","title":"Access Modifiers With Inheritance","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/06_Access_Modifiers_with_Inheritance/#access-modifiers-with-inheritance","title":"Access Modifiers with Inheritance","text":"<p>Inheritance in object-oriented programming allows a subclass (derived class) to inherit the attributes and methods of a superclass (base class). When it comes to access modifiers, the behavior is slightly different depending on the specific programming language. Let's discuss how access modifiers work with inheritance in Python:</p> <ol> <li> <p>Public Members: Public members of the superclass are accessible by the subclass. They retain their public visibility in the subclass.</p> </li> <li> <p>Protected Members: Protected members of the superclass can be accessed by the subclass, similar to public members. However, it is still a convention and not enforced by the language. Even though the protected members are intended to be protected, it can still be accessed and invoked by the Subclass. However, it is important to respect the convention and consider it as a hint to other developers.</p> </li> <li> <p>Private Members: Private members of the superclass are not directly accessible by the subclass. However, it is possible to access them indirectly by defining public or protected methods in the superclass that provide access to the private members.</p> </li> </ol> <pre><code># Creating a super/parent class\nclass Vehicle:\n    def __init__(self, brand, name, color, product_no):\n        self.brand = brand # public member\n        self.name = name # public member\n        self.color = color # public member\n        self.__product_no = product_no # private member\n\n    def _set_owner(self, owner, phone): # protected member\n        self._owner = owner\n        self._phone = phone\n\n    def _print_owner_details(self): # protected member\n        print(f\"Owner Name: {self._owner}\")\n        print(f\"Phone No: {self._phone}\")\n\n    def print_vehicle_details(self): # public member\n        print(f\"Vehicle Brand: {self.brand}\")\n        print(f\"Vehicle Name: {self.name}\")\n        print(f\"Color: {self.color}\")\n\n    def __show_product_no(self): # private method\n        print(f\"Vehicle Product No: {self.__product_no}\")\n</code></pre> <pre><code># Creating a sub/child class\nclass Car(Vehicle):\n    def __init__(self, brand, name, color):\n        super().__init__(brand, name, color, None)\n</code></pre> <pre><code># Creating instances from the child class\ncar1 = Car(\"Maruti\", \"Maruti 800\", \"White\")\n</code></pre> <pre><code># Calling the public members\ncar1.print_vehicle_details()\n</code></pre> <pre><code># Calling the protected members\ncar1._set_owner(\"Subrata Dey\", 6245614786)\ncar1._print_owner_details()\n</code></pre> <pre><code># Calling the private members\n# It will throw an error\n# car1.__product_no \n# car1._Car__product_no\n# car1.__show_product_no()\n</code></pre> <pre><code># Creating an instance of the super/parent class\nvehicle1 = Vehicle(\"Ferrari\", \"Ferrari XZ\", \"Black\", 1001)\n</code></pre> <pre><code># Calling the private members\nvehicle1._Vehicle__show_product_no()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/07_Types_of_Methods/","title":"Types Of Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/07_Types_of_Methods/#types-of-methods","title":"Types of Methods","text":"<p>Inside a Python class, you can define various types of methods, each serving a specific purpose. Here are some commonly used types of methods in Python classes:</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/07_Types_of_Methods/#instance-method","title":"Instance Method","text":"<p>Instance methods are the most common type of methods in Python classes. These are so-called instance methods because they can access the unique data of their instance. If you have two objects each created from a car class, then they each may have different properties. They may have different colors, engine sizes, seats, and so on.</p> <p>Instance methods must have self as a parameter, but you don't need to pass this in every time. Self is another Python special term. Inside any instance method, you can use self to access any data or methods that may reside in your class. You won't be able to access them without going through self.</p> <p>Finally, as instance methods are the most common, there's no decorator needed. Any method you create will automatically be created as an instance method unless you tell Python otherwise.</p> <p>Usage: * Performing calculations or operations on instance attributes. * Modifying the state of an individual instance. * Implementing instance-specific functionality.</p> <pre><code># Creating a Circle class with instance methods \nclass Circle:\n    _pi = 3.14159 # protected data member\n\n    # constructor\n    def __init__(self, radius):\n        self.radius = radius\n\n    # instance method  \n    def calculate_area(self):\n        area = self._pi * (self.radius)**2\n        return area\n\n    # instance method\n    def calculate_perimeter(self):\n        perimeter = 2 * self._pi * self.radius\n        return perimeter\n</code></pre> <pre><code># Creating an object from Circle class\ncircle1 = Circle(5)\n# Calling the instance methods using the object\nprint(\"The area of the circle:\", circle1.calculate_area())\nprint(\"The perimeter of the circle:\", circle1.calculate_perimeter())\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/07_Types_of_Methods/#class-methods","title":"Class Methods","text":"<p>Class methods are bound to the class itself rather than the instance. They have access to the class-level attributes and can be called using the class name or an instance. Class methods are defined using the @classmethod decorator, and the first parameter is conventionally named cls to represent the class.</p> <p>Usage: * Modifying or accessing class-level variables. * Performing operations that are relevant to the class as a whole. * Providing alternative ways to create instances of a class.</p> <pre><code># Creating a rectangle class\nclass Rectangle:\n    width = 0 # class-level variable\n    height = 0 # class-level variable\n\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def calculate_area(self):\n        area = self.width * self.height\n        return area\n\n    @classmethod\n    def change_size(cls, new_width, new_height):\n        cls.width = new_width\n        cls.height = new_height\n</code></pre> <pre><code># Creating an instance from the rectangle class\nrectangle1 = Rectangle(10, 5)\n# Calling the instance method\nprint(\"The area of the rectangle:\", rectangle1.calculate_area())\n</code></pre> <pre><code># Checking the class-level variable\nprint(Rectangle.width)\nprint(Rectangle.height)\n</code></pre> <pre><code># Modifying the class-level variable using class method\nRectangle.change_size(6, 4)\n# Checking the new class-level variable\nprint(Rectangle.width)\nprint(Rectangle.height)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/07_Types_of_Methods/#static-methods","title":"Static Methods","text":"<p>Static methods, much like class methods, are methods that are bound to a class rather than its object. They do not require a class instance creation. So, they are not dependent on the state of the object.</p> <p>The difference between a static method and a class method is: * The static method knows nothing about the class and just deals with the parameters. * The class method works with the class since its parameter is always the class itself.</p> <p>Usage: * Implementing utility functions that are logically related to the class. * Performing calculations or operations that don't require access to instance or class attributes. * Grouping related functions together within a class for organizational purposes.</p> <pre><code># Creating a MathUtils class\nclass MathUtils:\n\n    @staticmethod\n    def add(a, b, *args):\n        sum_of_numbers = a + b\n        for i in args:\n            sum_of_numbers += i\n        return sum_of_numbers\n\n    @staticmethod\n    def multiply(a, b, *args):\n        product_of_numbers = a * b\n        for i in args:\n            product_of_numbers *= i\n        return product_of_numbers\n</code></pre> <pre><code># Using the static methods\nMathUtils.add(2, 3)\n</code></pre> <pre><code>MathUtils.add(2, 3, 4, 5)\n</code></pre> <pre><code>MathUtils.multiply(2, 3)\n</code></pre> <pre><code>MathUtils.multiply(2, 3, 4, 5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/","title":"Polymorphism","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/#polymorphism","title":"Polymorphism","text":"<p>The literal meaning of polymorphism is the condition of occurrence in different forms. Polymorphism is a very important concept in programming. It refers to the use of a single type entity (method, operator, or object) to represent different types in different scenarios. Let's take a few examples:</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/#example-1-polymorphism-in-addition-operator","title":"Example 1: Polymorphism in addition(+) operator","text":"<p>We know that the + operator is used extensively in Python programs. But, it does not have a single usage. For integer data types, the + operator is used to perform an arithmetic addition operation.</p> <pre><code>a = 5\nb = 10\nprint(a+b)\n</code></pre> <p>Hence, the above program outputs 15.  Similarly, for string data types, the + operator is used to perform concatenation.</p> <pre><code>str1 = \"Geo\"\nstr2 = \"Next\"\nprint(str1 + str2)\n</code></pre> <p>As a result, the above program outputs \"GeoNext\".  Here, we can see that a single operator + has been used to carry out different operations for distinct data tvpes. This is one of the most simple occurrences of polymorphism in Python.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/#example-2-functional-polymorphism-in-python","title":"Example 2: Functional Polymorphism in Python","text":"<p>There are some functions in Python which are compatible to run with multiple data types. One such function is the len() function. It can run with many data types in Python. Let's look at some example use cases of the function:</p> <pre><code>print(len(\"GeoNext\"))\nprint(len([1, 2, 4, 2, 7]))\nprint(len({\"a\": 1, \"b\": 2}))\n</code></pre> <p>Here, we can see that many data types such as string, list, tuple, set, and dictionary can work with the len() function. However, we can see that it returns specific information(the length) about the specific data types.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/#class-polymorphism-in-python","title":"Class Polymorphism in Python","text":"<p>Polymorphism is a very important concept in Object-Oriented Programming. We can use the concept of polymorphism while creating class methods as Python allows different classes to have methods with the same name.</p> <p>We can then later generalize calling these methods by disregarding the object we are working with. Let's look at an example:</p> <pre><code># Creating a Male class\nclass Male:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def info(self):\n        print(\"Hi, I am a Male.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n\n# Creating a Female class\nclass Female:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def info(self):\n        print(\"Hi, I am a Female.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n</code></pre> <pre><code># Creating instances\nmale1 = Male(\"Bijay Bose\", 48)\nfemale1 = Female(\"Puja Bose\", 45)\n</code></pre> <pre><code># Running a loop over the set of objects\n# Calling the info() function common to both\nfor human in (male1, female1):\n    human.info()\n    print(\"\\n\")\n</code></pre> <p>Here, we have created two classes Male and Female. They share a similar structure and have the same method info(). However, notice that we have not created a common superclass or linked the classes together in any way. Even then, we can pack these two different objects into a tuple and iterate through them using a common human variable. It is possible due to polymorphism. We can call both the info() methods by just using human.info () call, where human is first male1 (Instance of Male) and then female1 (Instance of Female).</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/09%20Object%20Oriented%20Programming/08_Polymorphism/#polymorphism-and-inheritance","title":"Polymorphism and Inheritance","text":"<p>Like in other programming languages, the child classes in Python also inherit methods and attributes from the parent class. We can redefine certain methods and attributes specifically to fit the child class, which is known as Method Overriding.</p> <p>Polymorphism allows us to access these overridden methods and attributes that have the same name as the parent class. Let's look at an example:</p> <pre><code># Creating a Human Class (Parent / Super class)\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def info(self):\n        print(f\"Hi, I am {self.name}.\")\n\n# Creating a Male Class (Child / Sub class)\nclass Male(Human):\n    def __init__(self, name, age):\n        super().__init__(name)\n        self.age = age\n\n    def info(self):\n        print(f\"Hi, I am  {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n        print(\"I am a Male.\")\n\n# Creating a Female Class (Child / Sub class)\nclass Female(Human):\n    def __init__(self, name, age):\n        super().__init__(name)\n        self.age = age\n\n    def info(self):\n        print(f\"Hi, I am  {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n        print(\"I am a Female.\")\n</code></pre> <pre><code># Creating instances\nhuman1 = Human(\"Aakash Dutta\")\nmale1 = Male(\"Dipesh Sigha\", 28)\nfemale1 = Female(\"Sayani Sengupta\", 25)\n</code></pre> <pre><code># Running a loop over the set of objects\n# Calling the info() function common to all the classes\nfor human in (human1, male1, female1):\n    human.info()\n    print(\"\\n\")\n</code></pre> <p>Due to polymorphism, the Python interpreter automatically recognizes that the info() method for object male1 (Male class) is overridden. So, it uses the one defined in the subclass Male. Same with the object female1 (Female Class).</p> <p>Note: Method Overloading, a way to create multiple methods with the same name but different arguments, is not possible in Python.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/01_Introduction_to_Exception_Handling/","title":"Introduction To Exception Handling","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/01_Introduction_to_Exception_Handling/#exception-handling","title":"Exception Handling","text":"<p>An Exception (error) is an event due to which the normal flow of the program's instructions gets disrupted. Errors in Python can be of the following two types i.e. Syntax errors and Exceptions.</p> <ul> <li>While exceptions are raised when some internal events occur which changes the normal flow of the program.</li> <li>On the other hand, Errors are those type of problems in a program due to which the program will stop the execution.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/01_Introduction_to_Exception_Handling/#difference-between-syntax-errors-and-exceptions","title":"Difference between Syntax Errors and Exceptions","text":"<p>Syntax Error: As the name suggests that this error is caused by the wrong syntax in the code. It leads to the termination of the program.</p> <p>Example:</p> <pre><code># a = 10\n# if (a &gt; 10)\n#     print(\"Example\")\n</code></pre> <p>We will get the output as: SyntaxError: expected ':'. The syntax error is because there should be a \":\" (colon) at the end of an if statement. Since that is not presenet in the program, it gives a syntax error.</p> <p>Exceptions: Exceptions are raised when the program is syntactically correct but the code resulted in an error. This error does not stop the execution of the program, however, it changes the normal flow of the program.</p> <p>Example:</p> <pre><code># balance = 10000\n# remaining = balance / 0\n# print(remaining)\n</code></pre> <p>The above example raised the ZeroDivisionError exception, as we are trying to divide a number by 0 which is not defined and arithmetically not possible.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/01_Introduction_to_Exception_Handling/#exceptions-in-python","title":"Exceptions in Python","text":"<ul> <li>Python has many built-in exceptions that are raised when your program encounters an error (something in the program goes wrong).</li> <li>When these exceptions occur, the Python interpreter stops the current process and passes it to the calling process until it is handled.</li> <li>If not handled, the program will crash.</li> <li>For example, let us consider a program where we have a function A that calls function B, which in turn calls function C. If an exception occurs in function C but is not handled in C, the exception passes to B and then to A.</li> <li>If never handled, an error message is displayed and the program comes to a sudden unexpected halt.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/01_Introduction_to_Exception_Handling/#some-common-exceptions","title":"Some Common Exceptions","text":"<p>A list of common exceptions that can be thrown from a standard Python program is given below. * ZeroDivisionError: This occurs when a number is divided by zero. * NameError: It occurs when a name is not found. It may be local or global. * IndentationError: It occurs when incorrect indentation is given. * IOError: It occurs when an Input Output operation fails. * EOFError: It occurs when the end of the file is reached, and yet operations are being performed.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/02_try_except_Statement/","title":"Try Except Statement","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/02_try_except_Statement/#try-except-statement","title":"try-except Statement","text":"<p>In Python, exceptions can be handled using try-except blocks.  * If the Python program contains suspicious code that may throw the exception, we must place that code in the try block.  * The try block must be followed by the except statement, which contains a block of code that will be executed in case there is some exception in the try block.  * We can thus choose what operations to perform once we have caught the exception</p> <p>Syntax: <pre><code>try:\n    # Some Code...\n\nexcept SomeException as e:\n    # Optional block\n    # Handling of exception (if required)\n</code></pre></p> <p>Example:</p> <pre><code>myList = [\"a\", 0, 2]\n\nfor i in myList:\n    try:\n        print(\"The list item is:\", i)\n        reciprocal = 1/int(i)\n        print(\"The reciprocal of\", i, \"is\", reciprocal)\n\n    except Exception as e: #Using Exception class\n        print(\"Oops!\", e.__class__, \"occurred.\")\n        print()\n</code></pre> <ul> <li>In this program, we loop through the values of a list 'myList'. </li> <li>As previously mentioned, the portion that can cause an exception is placed inside the try block. </li> <li>If no exception occurs, the except block is skipped and normal flow continues (for last value). </li> <li>But if any exception occurs, it is caught by the except block (first and second values).</li> <li>Here, we print the name of the exception using the exc_info() function inside sys module. </li> <li>We can see that element \u201ca\u201d causes ValueError and 0 causes ZeroDivisionError.</li> </ul>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/02_try_except_Statement/#catching-specific-exceptions-in-python","title":"Catching Specific Exceptions in Python","text":"<ul> <li>In the above example, we did not mention any specific exception in the except clause. </li> <li>This is not a good programming practice as it will catch all exceptions and handle every case in the same way. </li> <li>We can specify which exceptions an except clause should catch. </li> <li>A try clause can have any number of except clauses to handle different exceptions, however, only one will be executed in case an exception occurs. </li> <li>You can use multiple except blocks for different types of exceptions. </li> <li>We can even use a tuple of values to specify multiple exceptions in an except clause. Here is an example to understand this better:</li> </ul> <p>Example:</p> <pre><code>try:\n    a = 10/0\n    print(a)\n\nexcept (ArithmeticError, IOError):\n    print(\"Arithmetic Exception\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/03_try_except_else_Statement/","title":"Try Except Else Statement","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/03_try_except_else_Statement/#try-except-else-statement","title":"try-except-else Statement","text":"<p>We can also use the else statement with the try-except statement in which, we can place the code which will be executed in the scenario if no exception occurs in the else block. The syntax is given below:</p> <p>Syntax: <pre><code>try:\n    # Some code...\n\nexcept SomeException as e:\n    # Optional block\n    # Handling of exception (if required)\n\nelse:\n    # execute if no exception\n</code></pre></p> <p>Example:</p> <pre><code>def divide_numbers(x, y):\n    try:\n        result = x / y\n\n    except ZeroDivisionError as e:\n        print(\"Error: Divison by zero is not allowed.\")\n\n    else:\n        print(\"Result:\", result)\n</code></pre> <pre><code>divide_numbers(10, 0)\n</code></pre> <pre><code>divide_numbers(10, 2)\n</code></pre> <p>We get this output because there is no exception in the try block when x is 10 and y is 2, and hence the else block is executed. If there was an exception in the try block, in the case of x=10, y=0, the else block will be skipped and except block will be executed.</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/04_finally_Statement/","title":"Finally Statement","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/04_finally_Statement/#finally-statement","title":"finally Statement","text":"<p>The try statement in Python can have an optional finally clause. This clause is executed no matter what and is generally used to release external resources. Here is an example of file operations to illustrate this: </p> <p>Let\u2019s first understand how the try and except works -  * First, the try clause is executed i.e. the code between try and except clause.  * If there is no exception, then only try clause will run, except clause will not get executed.  * If any exception occurs, the try clause will be skipped and except clause will run.  * If any exception occurs, but the except clause within the code doesn\u2019t handle it, it is passed on to the outer try statements. If the exception is left unhandled, then the execution stops.  * A try statement can have more than one except clause.</p> <p>Syntax: <pre><code>try:\n    # Some code...\n\nexcept:\n    # Optional block\n    # Handling of exception (if required)\n\nelse:\n    # Execute if no exception\n\nfinally:\n    # Some code... (always executed)\n</code></pre></p> <p>Example:</p> <pre><code>def divide(x, y):\n    try:\n        result = x / y\n\n    except ZeroDivisionError:\n        print(\"Error! Division by zero is not allowed.\")\n\n    else:\n        print(\"Result:\", result)\n\n    finally:\n        # This block is always executed\n        # regardless of exception generation\n        print(\"This is always executed.\")\n</code></pre> <pre><code># Look at parameters and note the working of program\ndivide(5, 2)\n</code></pre> <pre><code>divide(5, 0)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/10%20Exception%20Handling/04_finally_Statement/#raising-exception-in-python","title":"Raising Exception in Python","text":"<p>In Python programming, exceptions are raised when errors occur at runtime. We can also manually raise exceptions using the raise keyword. We can optionally pass values to the exception to clarify why that exception was raised. Given below are some examples to help you understand this better.</p> <pre><code>&gt;&gt;&gt; raise KeyboardInterrupt\nTraceback (most recent call last):\n...\nKeyboardInterrupt\n</code></pre> <pre><code>&gt;&gt;&gt; raise MemoryError(\"This is an argument\")\nTraceback (most recent call last):\n...\nMemoryError: This is an argument\n</code></pre> <p>Example:</p> <pre><code>try:\n    a = int(input(\"Enter a number: \"))\n\n    if a &lt;= 0:\n        raise ValueError(\"Please enter a positive number.\")\n\nexcept ValueError as ve:\n    print(ve)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/","title":"Access Modifiers","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/#access-modifiers","title":"Access Modifiers","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/#public-modifier","title":"Public Modifier","text":"<pre><code># Creating a student class with public member\n# member = method + variables inside of class\nclass Pub_Student:\n    name = None # public member by default\n    age = None # public member\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def public_method(self): # public member\n        print(\"This is a public method.\")\n</code></pre> <pre><code># Creating objects/instances from the Pub_Student class\npub_obj = Pub_Student(\"Sovon Roy\", 22)\n</code></pre> <pre><code># Calling the public members of the class\nprint(pub_obj.name)\nprint(pub_obj.age)\npub_obj.public_method()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/#protected-modifier","title":"Protected Modifier","text":"<pre><code># Creating a student class with protected member\nclass Pro_Student:\n    name = None # public member\n    _age = None # protected member\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self._age = age\n\n    def print_details(self): # publc member (method)\n        print(f\"Student Name: {self.name}\")\n        print(f\"Student age: {self._age}\")\n\n    def _protected_method(self): # protected member (method)\n        print(\"This is a protected method.\")\n</code></pre> <pre><code># Creating objects/instances from the Pro_Student class\npro_obj = Pro_Student(\"Anil De\", 14)\n</code></pre> <pre><code># Calling the protected members of the class\nprint(pro_obj._age)\npro_obj._protected_method()\n</code></pre> <pre><code># Calling the public members of the class\nprint(pro_obj.name)\npro_obj.print_details()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/#private-modifier","title":"Private Modifier","text":"<pre><code># Creating a student class with private member\nclass Pri_Student:\n    name = None # public member\n    __age = None # private member\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self.__age = age\n\n    def print_details(self): #public member\n        print(f\"Student Name: {self.name}\")\n        print(f\"Student Age: {self.__age}\")\n\n    def __private_method(self): # private member\n        print(\"This is a private method\")\n</code></pre> <pre><code># Creating objects/instances from the Pri_Student class\npri_obj = Pri_Student(\"Bikram Roy\", 16)\n</code></pre> <pre><code># Calling the public members of the class\nprint(pri_obj.name)\npri_obj.print_details()\n</code></pre> <pre><code># Calling the private members of the class\n# pri_obj.__age\n# pri_obj.__private_method()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers/#name-mangling","title":"Name Mangling","text":"<pre><code>pri_obj._Pri_Student__age\n</code></pre> <pre><code>pri_obj._Pri_Student__private_method()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers_with_Inheritence/","title":"Access Modifiers With Inheritence","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Access_Modifiers_with_Inheritence/#access-modifiers-with-inheritance","title":"Access Modifiers with Inheritance","text":"<pre><code># Creating a super/parent class\nclass Vehicle:\n    def __init__(self, brand, name, color, product_no):\n        self.brand = brand # public member\n        self.name = name # public member\n        self.color = color # public member\n        self.__product_no = product_no # private member\n\n    def _set_owner(self, owner, phone): # protected member\n        self._owner = owner\n        self._phone = phone\n\n    def _print_owner_details(self): # protected member\n        print(f\"Owner Name: {self._owner}\")\n        print(f\"Phone No: {self._phone}\")\n\n    def print_vehicle_details(self): #public member\n        print(f\"Vehicle Brand: {self.brand}\")\n        print(f\"Vehicle Name: {self.name}\")\n        print(f\"Vehicle Color: {self.color}\")\n\n    def __show_product_no(self): # private member\n        print(f\"Vehicle Product No: {self.__product_no}\")\n</code></pre> <pre><code>#  Create an instance from the Vehicle class\nvehicle1 = Vehicle(\"Maruti\", \"Maruti 800\", \"White\", 1001)\n</code></pre> <pre><code>vehicle1.print_vehicle_details()\n</code></pre> <pre><code>vehicle1._set_owner(\"Ajay Ghosh\", 6254864958)\n</code></pre> <pre><code>vehicle1._print_owner_details()\n</code></pre> <pre><code>vehicle1._Vehicle__product_no\n</code></pre> <pre><code>vehicle1._Vehicle__show_product_no() # Name Mangling\n</code></pre> <pre><code># Creating a sub/child class\nclass Car(Vehicle):\n    def __init__(self, brand, name, color):\n        super().__init__(brand, name, color, None)      \n</code></pre> <pre><code># Create an instance from Car class\ncar1 = Car(\"Ferrari\", \"Ferrai XZ\", \"Black\")\n</code></pre> <pre><code>car1.print_vehicle_details()\n</code></pre> <pre><code>car1._set_owner(\"Dip Ghosh\", 7182648975)\n</code></pre> <pre><code>car1._print_owner_details()\n</code></pre> <pre><code># car1._Car__product_no \n# This will throw an error. Private members cannot be accessed from outside the class.\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/","title":"Arithmetic Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#arithmetic-operators","title":"Arithmetic Operators","text":"<pre><code>a = 2\nb = 3\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#addition","title":"Addition (+)","text":"<pre><code># Addition of two numbers\nadd = a + b\n</code></pre> <pre><code>add\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#subtraction-","title":"Subtraction (-)","text":"<pre><code># Subtraction of two numbers\nsubtract = a - b\n</code></pre> <pre><code>subtract\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#multiplication","title":"Multiplication (*)","text":"<pre><code># Multiplication of two numbers\nmultiply = a * b\n</code></pre> <pre><code>multiply\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#division","title":"Division (/)","text":"<pre><code># Division of two numbers\ndivision = a / b\n</code></pre> <pre><code>division\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#floor-division","title":"Floor Division (//)","text":"<pre><code># Floor division of two numbers\nx = 14\ny = 5\n</code></pre> <pre><code>fdiv = x // y\nfdiv\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#modulus","title":"Modulus (%)","text":"<pre><code># Remainder of division operation\nmodulus = x % y\nmodulus\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Arithmetic_Operators/#exponentiation","title":"Exponentiation (**)","text":"<pre><code># Power of a number\npower = a ** b\n</code></pre> <pre><code>power\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/","title":"Assignment Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#assignment-operators","title":"Assignment Operators","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#simple-assignment","title":"Simple Assignment (=)","text":"<pre><code>a = 5\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#addition-assignment","title":"Addition Assignment (+=)","text":"<pre><code>a += 2\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#subtraction-assignment-","title":"Subtraction Assignment (-=)","text":"<pre><code>a -= 2\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#multiplication-assignment","title":"Multiplication assignment (*=)","text":"<pre><code>a *= 2\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#division-assignment","title":"Division assignment (/=)","text":"<pre><code>a /= 2\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#modulus-assignment","title":"Modulus assignment (%=)","text":"<pre><code>a %= 2\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Assignment_Operators/#exponentiation-assignment","title":"Exponentiation assignment (**=)","text":"<pre><code>a **= 2\n</code></pre> <pre><code>a\n</code></pre> <pre><code>a = 3\n</code></pre> <pre><code>a **= 3\n</code></pre> <pre><code>a\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/","title":"Bitwise Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-operators","title":"Bitwise Operators","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-and","title":"&amp; (Bitwise AND)","text":"<pre><code>a = 20\nb = 8\n</code></pre> <pre><code>print(a &amp; b)\n</code></pre> <pre><code>x = 7\ny = 5\n</code></pre> <pre><code>print(x &amp; y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-or","title":"| (Bitwise OR)","text":"<pre><code>print(a | b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-xor","title":"^ (Bitwise XOR)","text":"<pre><code>print(a ^ b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-not","title":"~ (Bitwise NOT)","text":"<pre><code>print(~a)\n</code></pre> <pre><code>c = 28\nprint(~c)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-left-shift","title":"&lt;&lt; (Bitwise left shift)","text":"<pre><code>print(a &lt;&lt; 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Bitwise_Operators/#bitwise-right-shift","title":"&gt;&gt; (Bitwise right shift)","text":"<pre><code>print(a &gt;&gt; 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Break_Continue_Pass/","title":"Break Continue Pass","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Break_Continue_Pass/#break-continue-and-pass-statement","title":"Break, Continue and Pass Statement","text":"<pre><code>cities = [\"Mumbai\", \"Chennai\", \"Delhi\", \"Kolkata\"]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Break_Continue_Pass/#break-statement","title":"Break Statement","text":"<pre><code>print(\"Program Started\")\n\nfor city in cities:\n    if city == \"Delhi\":\n        break\n    else:\n        print(city)\n\nprint(\"Program ended.\")\n</code></pre> <pre><code># First iteration, city = \"Mumbai\"\n# Second iteration, city = \"Chennai\"\n# Third iteration, city = \"Delhi\"\n</code></pre> <pre><code>i = 0\nprint(\"Program Started\")\n\nwhile i &lt; 5:\n    if i == 3:\n        print(\"Executing break statement in the next statement.\")\n        break\n    else:\n        print(i)\n        i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Break_Continue_Pass/#continue-statement","title":"Continue Statement","text":"<pre><code>cities = [\"Mumbai\", \"Chennai\", \"Delhi\", \"Kolkata\"]\n</code></pre> <pre><code>for city in cities: # Level 1\n    if city == \"Delhi\": # Level2 \n        continue  # Level 3\n    else:  # Level 2\n        print(city) # Level 3\n</code></pre> <pre><code>i = 0\nwhile i &lt; 5:\n    if i == 3:\n        i += 1\n        continue\n    else:\n        print(i)\n        i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Break_Continue_Pass/#pass-statement","title":"Pass Statement","text":"<pre><code>cities = [\"Mumbai\", \"Chennai\", \"Delhi\", \"Kolkata\"]\n</code></pre> <pre><code>for city in cities:\n    pass\n</code></pre> <pre><code>i = 0\nwhile i &lt; 5:\n    if i == 3:\n        i += 1\n        pass\n    else:\n        print(i)\n        i += 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/","title":"Comparison Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#comparison-operators","title":"Comparison Operators","text":"<pre><code>a = 10 # a &lt; b\nb = 15\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#equal-to","title":"Equal to (==)","text":"<pre><code>a == b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#not-equal-to","title":"Not equal to (!=)","text":"<pre><code>a != b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#greater-than","title":"Greater than (&gt;)","text":"<pre><code>a &gt; b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#less-than","title":"Less than (&lt;)","text":"<pre><code>a &lt; b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#greater-than-or-equal-to","title":"Greater than or equal to (&gt;=)","text":"<pre><code>a &gt;= b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Comparison_Operators/#less-than-or-equal-to","title":"Less than or equal to (&gt;=)","text":"<pre><code>a &lt;= b\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Concept%20of%20args%20and%20kwargs/","title":"Concept Of Args And Kwargs","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Concept%20of%20args%20and%20kwargs/#concept-of-args-and-kwargs","title":"Concept of args and *kwargs","text":"<pre><code>def add(x, y):\n    return x + y \n</code></pre> <pre><code>add(15, 10)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Concept%20of%20args%20and%20kwargs/#args-for-non-keyword-arguments","title":"*args (For Non-Keyword Arguments)","text":"<pre><code>def sum_of_numbers(*args):\n    sum_of_num = 0\n    for num in args:\n        sum_of_num += num\n    return sum_of_num\n</code></pre> <pre><code>sum_of_numbers(15, 51, 455, 17, 68, 156, 147)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Concept%20of%20args%20and%20kwargs/#kwargs-for-keyword-arguments","title":"**kwargs (For Keyword Arguments)","text":"<pre><code>def myFunction(**kwargs):\n    for key, value in kwargs.items():\n        print(key, \":\", value)\n</code></pre> <pre><code>myFunction(name=\"Krishnagopal Halder\", city=\"Bankura\", season=\"Summer\")\n</code></pre> <pre><code>def sum_of_3num(x, y, z, *args):\n    sum_of_num = x + y + z\n    for num in args:\n        sum_of_num += num\n    return sum_of_num\n</code></pre> <pre><code>sum_of_3num(5, 6, 10, 15, 17, 65, 78, 10)\n</code></pre> <pre><code>def personalInfo(name, age, *args, **kwargs):\n    print(\"The name of the person is\", name)\n    print(\"The age of the person is\", age)\n    if len(kwargs) &gt; 0:\n        print(\"Other information\")\n    for key, value in kwargs.items():\n        print(key, \":\", value)\n    if len(args) &gt; 0:\n        print(\"Unknown information\")\n    for i in args:\n        print(i)\n</code></pre> <pre><code>personalInfo(\"Sujoy Ghosh\", 40)\n</code></pre> <pre><code>personalInfo(\"Ajay Sen\", 28, \"First arg\", \"Second arg\", state=\"WB\", country=\"India\")\n</code></pre> <pre><code>personalInfo(\"Bijay Roy\", 48, mail=\"abc@gmail.com\", mob=6314566428, state=\"WB\", country=\"India\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/","title":"Conditional Statement","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/#conditional-statement","title":"Conditional Statement","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/#if-statement","title":"if Statement","text":"<pre><code># Check whether a given number is even or odd.\nnum = int(input(\"Enter a number: \"))\n</code></pre> <pre><code>if (num % 2) == 0:\n    print(num, \"is even\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/#if-else-statement","title":"if-else Statement","text":"<pre><code># Check whether a given number is even or odd\nnum2 = int(input(\"Enter a number: \"))\n</code></pre> <pre><code>if (num2 % 2) == 0:\n    print(num2, \"is even\")\nelse:\n    print(num2, \"is odd\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/#if-elif-else-statement","title":"if-elif-else Statement","text":"<pre><code># Find the largest among three numbers\n\n# Taking a space-separated input from the user\nx, y, z = input(\"Enter three numbers separated by space: \").split()\n</code></pre> <pre><code>x = int(x)\ny = int(y)\nz = int(z)\n</code></pre> <pre><code>if x &gt;= y and x &gt;= z:\n    print(\"x is greater\")\nelif y &gt;= x and y &gt;= z:\n    print(\"y is greater\")\nelse:\n    print(\"z is greater\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Conditional_Statement/#examples","title":"Examples","text":"<pre><code>city_pop = int(input(\"Enter the city population: \"))\n</code></pre> <pre><code>if city_pop &gt;= 10000000:\n    print(\"The city is a Super City.\")\nelif city_pop &gt;= 5000000:\n    print(\"The city is a Mega City.\")\nelif city_pop &gt;= 1000000:\n    print(\"The city is a Large City.\")\nelif city_pop &gt;= 500000:\n    print(\"The city is a Medium City.\")\nelse:\n    print(\"The city is a Small City.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Constructor/","title":"Constructor","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Constructor/#constructor","title":"Constructor","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Constructor/#defining-a-class-with-constructor","title":"Defining a Class with Constructor","text":"<pre><code>class Car:\n    # Default constructor\n    def __init__(self, name, topspeed, color):\n        pass\n</code></pre> <pre><code># Creating a class with parameterized constructor\nclass Car:\n    def __init__(self, name, topspeed, color):\n        self.name = name\n        self.topspeed = topspeed\n        self.color = color\n</code></pre> <pre><code># Creating an object from the car class\ncar1 = Car(\"Ferrari\", 400, \"Red\")\ncar2 = Car(\"Maruti 800\", 120, \"White\")\ncar3 = Car(\"Creta\", 300, \"Yellow\")\n</code></pre> <pre><code># Print the attributes of the car1 object\nprint(\"The name of the first car is\", car1.name)\nprint(\"The topspeed of the first car is\", car1.topspeed, \"Km/h\")\nprint(\"The color of the first car is\", car1.color)\n</code></pre> <pre><code># Print the attributes of the car2 object\nprint(\"The name of the second car is\", car2.name)\nprint(\"The topspeed of the second car is\", car2.topspeed, \"Km/h\")\nprint(\"The color of the second car is\", car2.color)\n</code></pre> <pre><code># Print the attributes of the car2 object\nprint(\"The name of the third car is\", car3.name)\nprint(\"The topspeed of the third car is\", car3.topspeed, \"Km/h\")\nprint(\"The color of the third car is\", car3.color)\n</code></pre> <pre><code># Creating the car class from scratch\nclass Car:\n    def __init__(self, name, topspeed, color):\n        self.name = name\n        self.topspeed = topspeed\n        self.color = color\n\n    def car_info(self):\n        print(\"Car's Information:\")\n        print(\"The name of the car is\", self.name)\n        print(\"The topspeed of the car is\", self.topspeed)\n        print(\"The color of the car is\", self.color)\n</code></pre> <pre><code># Creating 3 instances / objects from the car class\ncar1 = Car(\"Ferrari\", 400, \"Red\")\ncar2 = Car(\"Maruti 800\", 120, \"White\")\ncar3 = Car(\"Creta\", 300, \"Yellow\")\n</code></pre> <pre><code># Print the information of the car1\ncar1.car_info()\n</code></pre> <pre><code># Print the information of the car2\ncar2.car_info()\n</code></pre> <pre><code># Print the information of the car3\ncar3.car_info()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Constructor/#creating-application-form-using-class-in-python","title":"Creating Application Form using Class in Python","text":"<pre><code>class ApplicationForm:\n    def __init__(self, name, age, dob, phone):\n        self.name = name\n        self.age = age\n        self.dob = dob\n        self.phone = phone\n\n    def set_email(self, email):\n        self.email = email\n\n    def set_address(self, address):\n        self.address = address\n\n    def print_form(self):\n        print(\"Application Form:\")\n        print(f\"Name: {self.name}\")\n        print(f\"Age: {self.age}\")\n        print(f\"DOB: {self.dob}\")\n        print(f\"Phone: {self.phone}\")\n        print(f\"Emai: {self.email}\")\n        print(f\"Address: {self.address}\")\n</code></pre> <pre><code># Create some object of the Application Form\nform1 = ApplicationForm(\"Bijay Roy\", 26, \"2000-01-01\", 6245648967)\nform1.set_email(\"roy_bijay123@gmail.com\")\nform1.set_address(\"Newtown, Kolkata\")\n</code></pre> <pre><code># Printing the applicant form\nform1.print_form()\n</code></pre> <pre><code># Creating another object\nform2 = ApplicationForm(\"Sovon Dey\", 28, \"1994-01-12\", 7894256127)\nform2.set_email(\"sovon123@gmail.com\")\nform2.set_address(\"Arambag, Hooghly\")\n</code></pre> <pre><code>form2.print_form()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Exercise%20Solution/","title":"Exercise Solution","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Exercise%20Solution/#exercise-solution","title":"Exercise Solution","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Exercise%20Solution/#temperature-converter","title":"Temperature Converter","text":"<pre><code>temp = int(input(\"Enter the temperature in C: \"))\nscale = input(\"Enter the Scale (F, K)\")\n</code></pre> <pre><code>if scale == \"F\":\n    result = temp * (9/5) + 32\n    print(\"The temperatur in Fahrenheit is\", result)\nif  scale == \"K\":\n    result = temp + 273.15\n    print(\"The temperatur in Kelvin is\", result)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Exercise%20Solution/#sum-of-n-natural-numbers","title":"Sum of N natural numbers","text":"<pre><code>num = int(input(\"Enter the number upto where you want the sum of all natural numbers: \"))\n</code></pre> <pre><code>mySum = 0\nfor i in range(1, num+1):\n    mySum += i\n\nprint(\"The sum of all natural numbers upto\", num, \"is\")\nprint(mySum)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Exercise%20Solution/#number-comparison","title":"Number Comparison","text":"<pre><code>num1 = int(input(\"Enter the first number: \"))\nnum2 = int(input(\"Enter the second number: \"))\n</code></pre> <pre><code>if num1 == num2:\n    print(\"Both numbers are same.\")\nelif num1 &gt; num2:\n    print(\"First number is greater than the Second number.\")\nelse:\n    print(\"Second number is greater than the First number.\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Function_in_Python/","title":"Function In Python","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Function_in_Python/#function-in-python","title":"Function in Python","text":"<pre><code>def addTwoNumbers(x, y):\n    \"This function returns the sum of two numbers\"\n    sum_of_two_numbers = x + y\n    return sum_of_two_numbers\n</code></pre> <pre><code>addTwoNumbers(15, 45)\n</code></pre> <pre><code>def multiplyTwoNumbers(x, y):\n    \"This function returns the product of two numbers\"\n    product = x * y\n    return product\n</code></pre> <pre><code>multiplyTwoNumbers(1526, 152)\n</code></pre> <pre><code>def divideTwoNumbers(x, y):\n    \"This function returns the division of two numbers\"\n    divide = x / y\n    return divide\n</code></pre> <pre><code>divideTwoNumbers(10, 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Function_in_Python/#create-a-function-that-will-calculate-the-standard-deviation-of-a-dataset","title":"Create a function that will calculate the standard deviation of a dataset","text":"<pre><code>myLst = [10, 14, 36, 12, 14, 75, 28, 45, 31, 98, 18, 24, 45, 30]\n</code></pre> <pre><code>sum(myLst) # Sum of all the numbers\n</code></pre> <pre><code>len(myLst) # Number of items\n</code></pre> <pre><code># Mean = (sum of all the numbers) / (no of items)\nmean = sum(myLst) / len(myLst)\nmean\n</code></pre> <pre><code>from math import sqrt\ndef standard_deviation(lst):\n    \"This function return the standard deviation value of a dataset\"\n    mean = sum(lst) / len(lst)\n    sum_of_squares = 0\n\n    for num in lst:\n        deviation = num - mean\n        sqrd_deviation = deviation ** 2\n        sum_of_squares += sqrd_deviation\n\n    std_deviation =  sqrt((sum_of_squares / (len(lst)-1)))\n    return std_deviation\n</code></pre> <pre><code>standard_deviation(myLst)\n</code></pre> <pre><code>standard_deviation([1, 2, 3, 4, 5])\n</code></pre> <pre><code>def average(lst):\n    \"This function return the average of a dataset\"\n    sum_of_dataset = sum(lst)\n    no_of_items = len(lst)\n    avg = sum_of_dataset / no_of_items\n    return avg\n</code></pre> <pre><code>myData = [1, 2, 3, 4, 5]\n</code></pre> <pre><code>average(myData)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Identity_Operators/","title":"Identity Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Identity_Operators/#identity-operators","title":"Identity Operators","text":"<pre><code>x = 10\ny = 10\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Identity_Operators/#is","title":"is","text":"<pre><code>print(id(x))\nprint(id(y))\n</code></pre> <pre><code>print(x is y)\n</code></pre> <pre><code>name1 = \"KrishnagopalHalder\"\nname2 = \"KrishnagopalHalder\"\n</code></pre> <pre><code>print(id(name1))\nprint(id(name2))\n</code></pre> <pre><code>print(name1 is name2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Identity_Operators/#is-not","title":"is not","text":"<pre><code>a = 10\nb = 15\n</code></pre> <pre><code>print(id(a))\nprint(id(b))\n</code></pre> <pre><code>print(a is not b)\n</code></pre> <pre><code>print(name1 is not name2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Identity_Operators/#example","title":"Example","text":"<pre><code>lst1 = [1, 2, 3]\nlst2 = [1, 2, 3]\n</code></pre> <pre><code>print(id(lst1), id(lst2))\n</code></pre> <pre><code>print(lst1 is lst2)\n</code></pre> <pre><code>print(lst1 is not lst2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/","title":"Inheritance","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#inheritance","title":"Inheritance","text":"<p>```python id=\"yRE8JWpBWciJ\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#defining-the-parentsuper-class","title":"Defining the parent/super class","text":"<p>class Vehicle:   def init(self, brand, name, color):     self.brand = brand     self.name = name     self.color = color</p> <p>def drive(self):     print(\"Driving the vehicle.\")</p> <p>def stop(self):     print(\"Stopping the vehicle\") <pre><code>```python id=\"shvM7Y87Xaso\"\n# Defining the child/sub class\nclass Car(Vehicle):\n  def __init__(self, brand, name, color, topspeed, no_of_windows):\n    super().__init__(brand, name, color)\n    self.topspeed = topspeed\n    self.no_of_windows = no_of_windows\n\n  def drive(self):\n    print(\"Driving the car.\")\n\n  def open_trunk(self):\n    print(\"Opening the trunk.\")\n</code></pre></p> <p>```python id=\"yt1km9V5YsFo\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#defining-another-child-class","title":"Defining another child class","text":"<p>class Motorcycle(Vehicle):   def init(self, brand, name, color, topspeed):     super().init(brand, name, color)     self.topspeed = topspeed</p> <p>def drive(self):     print(\"Driving the motorcycle.\")</p> <p>def indicator(self):     print(\"Turning on the indicator.\") <pre><code>```python id=\"qtvJ3NScZkTh\"\n# Creating instances from subclass\ncar1 = Car(\"Maruti\", \"Maruti 800\", \"White\", 120, 4)\nmotorcycle1 = Motorcycle(\"Kawasaki\", \"Kawasaki Ninja\", \"Black\", 290)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wVIZ6HBcaQM3\" outputId=\"704fd665-e8f9-412e-f611-d382ea7fdff2\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#calling-the-overriden-methods","title":"Calling the overriden methods","text":"<p>car1.drive() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"_AOOmlZVa8Wh\" outputId=\"b7e33d5a-87fb-444b-fcad-ef8ee2eb6778\"\nmotorcycle1.drive()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"MeOFx3-VbGDq\" outputId=\"1377fa9d-ac36-43f7-b414-a929a9bb8db7\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#calling-the-subclass-specific-methods","title":"Calling the subclass-specific methods","text":"<p>car1.open_trunk() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"qCW_eL7PbopP\" outputId=\"2bbcd1dc-5720-4048-967d-2d03901ef3b6\"\nmotorcycle1.indicator()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"6At2c08scjP0\" outputId=\"a10fa5b0-213a-4042-b2c7-eebe3a4f501b\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Inheritance/#calling-superclass-methods","title":"Calling superclass methods","text":"<p>car1.stop() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"jMCPrDj7cuye\" outputId=\"66ebda16-384a-4ee4-ea6c-9fdbd755d6d6\"\nmotorcycle1.stop()\n</code></pre></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Jupyter%20Interface/","title":"Introduction To Jupyter Interface","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p> <pre><code>print(\"Hello World\")\nprint(\"Hello Python!\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/","title":"Introduction To Markdown","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p> <p></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#heading-1","title":"Heading 1","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#heading-2","title":"Heading 2","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#heading-3","title":"Heading 3","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#heading-4","title":"Heading 4","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#heading-5","title":"Heading 5","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction%20to%20Markdown/#introduction-to-markdown","title":"Introduction to Markdown","text":"<p>Python is a widely used general-purpose, high level programming language. It was created by Guido van Rossum in 1991 and further developed by the Python Software Foundation. It was designed with an emphasis on code readability, and its syntax allows programmers to express their concepts in fewer lines of code.</p> <p>Python is a programming language that lets you work quickly and integrate systems more efficiently.</p> <p>There are two major Python versions: Python 2 and Python 3. Both are quite different.</p> <p>This is an ordered list: 1. Numpy 2. Pandas 3. Geopandas</p> <p>This is an unordered list: * Numpy * Pandas * Geopandas</p> <p>This is an unordered list: - Numpy - Pandas - Geopandas</p> <p>To bold a specific word, enclosed the word with ** Python</p> <pre><code>print(\"Hello World!\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/","title":"Introduction To Datatypes","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#introduction-to-datatypes","title":"Introduction to Datatypes","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#integer-datatype","title":"Integer Datatype","text":"<pre><code>myInt = 10\nmyInt\n</code></pre> <pre><code># To check the datatype of any variable we can use python built-in type() function\ntype(myInt)\n</code></pre> <pre><code>myInt2 = -10\nmyInt2\n</code></pre> <pre><code>type(myInt2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#float-datatype","title":"Float Datatype","text":"<pre><code>myFloat1 = 3.5\nmyFloat1\n</code></pre> <pre><code>type(myFloat1)\n</code></pre> <pre><code>myFloat2 = -3.5\nmyFloat2\n</code></pre> <pre><code>type(myFloat2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#string-datatype","title":"String Datatype","text":"<pre><code>myStr1 = \"Hello World!\"\nmyStr1\ntype(myStr1)\n</code></pre> <pre><code>myStr2 = \"123\"\ntype(myStr2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#boolean-datatype","title":"Boolean Datatype","text":"<pre><code>myBool1 = True\ntype(myBool1)\n</code></pre> <pre><code>myBool2 = False\ntype(myBool2)\n</code></pre> <pre><code>is_passed = True \n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#list-datatype","title":"List Datatype","text":"<pre><code>myList1 = [1, 2, 3, 4, 5]\ntype(myList1)\n</code></pre> <pre><code>shopping_list = [\"apple\", \"orange\", \"jam\", \"cold-drinks\", \"sweets\"]\nshopping_list\n</code></pre> <pre><code>type(shopping_list)\n</code></pre> <pre><code># In a list datatype, you can store different datattypes\nmyList2 = [1, 2.5, \"apple\", False]\ntype(myList2)\n</code></pre> <pre><code>myList3 = [1]\ntype(myList3)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#tuple-datatype","title":"Tuple Datatype","text":"<pre><code>myTuple1 = (1, 2, 3, 4, 5)\ntype(myTuple1)\n</code></pre> <pre><code>shopping_list2 = (\"orange\", \"mango\", \"milk\", \"brush\", \"ice-cream\")\nshopping_list2\n</code></pre> <pre><code>myTuple2 = (1, 2.5, \"orange\", True)\nmyTuple2\ntype(myTuple2)\n</code></pre> <pre><code># Lists are mutable, means the value of a list item can be changed\nmyList1[0] = 5\nmyList1\n</code></pre> <pre><code># Tuples are immutable, means the value of any tuple can not be changed.\n# myTuple1[0] = 5\n# myTuple1\n</code></pre> <pre><code>coords_kolkata = (22.5726, 88.3639)\n</code></pre> <pre><code>pop_kolkata  = [14900000]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#set-datatype","title":"Set Datatype","text":"<pre><code>mySet1 = {1, 2, 3, 3, 2, 1}\nmySet1\n</code></pre> <pre><code>mySet2 = {\"Suman\", \"Sourin\", \"Bikash\", \"Suman\"}\nmySet2\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Datatypes/#dictionary-datatype","title":"Dictionary Datatype","text":"<pre><code>myDict1 = {\"fruit\": \"mango\", \"color\": \"yellow\", \"is_edible\": True}\n</code></pre> <pre><code>type(myDict1)\n</code></pre> <pre><code>cityData = {\n    \"name\": \"Midnapore\",\n    \"coords\": (22.4257, 87.3199),\n    \"area\": 18.65,\n    \"pin\": 721101,\n    \"places\": [\"Rabindranagar\", \"Mohanpur\"]\n}\n</code></pre> <pre><code>type(cityData[\"area\"])\n</code></pre> <pre><code># Creating a complex dictionary\ncitiesData = {\n    \"name\": \"Mumbai\", \n    \"coords\": (19.0760, 72.8777), \n    \"pop\": 21297000,\n    \"area\":  603.4\n}\n</code></pre> <pre><code>popDen = round(citiesData[\"pop\"] / citiesData[\"area\"])\nprint(\"Population Density of Mumbai City:\", popDen)\n</code></pre> <pre><code>cityData2 = {\n    \"Kolkata\": {\"coords\": (22.5726, 88.3639), \"pop\": 14900000},\n    \"Mumbai\": {\"coords\": (19.0010, 72.8397), \"pop\": 21297000}\n}\n</code></pre> <pre><code>cityData2[\"Kolkata\"][\"pop\"]\n</code></pre> <pre><code>cityData2[\"Mumbai\"][\"pop\"]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/","title":"Introduction To Library And Module","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#introduction-to-library-and-modules","title":"Introduction to Library and Modules","text":"<pre><code># !pip install matplotlib\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#import-any-python-library","title":"Import Any Python Library","text":"<pre><code>import numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#numpy","title":"NumPy","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#creation-of-one-dimensional-arrays","title":"Creation of One-Dimensional Arrays","text":"<pre><code>myList = [1, 2, 3, 4]\ntype(myList)\n</code></pre> <pre><code>myArray = np.array([1, 2, 3, 4], dtype=\"int8\")\ntype(myArray)\n</code></pre> <pre><code>myArray.ndim\n</code></pre> <pre><code>myArray.dtype\n</code></pre> <pre><code># Accesing elements of an one-dimensional array\nmyArray[2]\n</code></pre> <p>### Creation of Two-Dimensional Arrays</p> <pre><code>myList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n</code></pre> <pre><code>myArray2D = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"int8\")\n</code></pre> <pre><code>myArray2D\n</code></pre> <pre><code>myArray2D.dtype\n</code></pre> <pre><code>myArray2D.ndim\n</code></pre> <pre><code>myArray2D.shape\n</code></pre> <pre><code>myArray2D[1, 1]\n</code></pre> <pre><code>myArray2D[2, 2]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#pandas","title":"Pandas","text":"<pre><code># Read CSV File using pandas\nfile_path = r\"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\DailyDelhiClimateTest.csv\"\n</code></pre> <pre><code>df = pd.read_csv(file_path)\ntype(df)\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>df.tail()\n</code></pre> <pre><code>df.head(10)\n</code></pre> <pre><code>df.shape\n</code></pre> <pre><code>df.ndim\n</code></pre> <pre><code>df.dtypes\n</code></pre> <pre><code>new_df = df[[\"date\", \"meantemp\", \"humidity\"]][df[\"meantemp\"] &gt; 30]\n</code></pre> <pre><code>new_df.shape\n</code></pre> <pre><code>new_df.head()\n</code></pre> <pre><code>meantemp = new_df[\"meantemp\"]\n</code></pre> <pre><code>type(meantemp)\n</code></pre> <pre><code>humidity = new_df[\"humidity\"]\n</code></pre> <pre><code>type(humidity)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Library_and_Module/#plotting-the-data","title":"Plotting the Data","text":"<pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>plt.scatter(x=meantemp, y=humidity, c=\"red\")\nplt.xlabel(\"Mean Temperature (in C)\")\nplt.ylabel(\"Humidity\")\nplt.title(\"Scatterplot between Mean Temp and Humidity\")\nplt.grid()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_OOP/","title":"Introduction To Oop","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_OOP/#object-oriented-programming-oop","title":"Object Oriented Programming (OOP)","text":"<pre><code># Creating a car class\nclass Car:\n    pass\n</code></pre> <pre><code># Creating an instance (Object) from car class\ncar1 = Car\n</code></pre> <pre><code># Giving some attribute to the car object\ncar1.name = \"Maruti 800\"\ncar1.topspeed = 120\n</code></pre> <pre><code># Print the characteristics of the car object\nprint(\"The name of the car\", car1.name)\nprint(\"The topspeed of the car is\", car1.topspeed, \"Km/h\")\n</code></pre> <pre><code># Creating another car object\ncar2 = Car\n</code></pre> <pre><code># Giving some attribute to the new car object\ncar2.name = \"Ferrari\"\ncar2.topspeed = 400\ncar2.color = \"Red\"\n</code></pre> <pre><code>print(\"The name of the car is\", car2.name)\nprint(\"The topspeed of the car is\", car2.topspeed, \"Km/h\")\nprint(\"The color of the car is\", car2.color)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/","title":"Introduction To Strings","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/#introduction-to-string","title":"Introduction to String","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/#string-creation","title":"String Creation","text":"<pre><code>name = \"Krishnagopal Halder\"\n</code></pre> <pre><code>fruit = 'Mango'\ntype(fruit)\n</code></pre> <pre><code>type(name)\n</code></pre> <pre><code>message = '''\nThis\nis a \nmultiline string\n'''\n</code></pre> <pre><code>print(message)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/#accessing-characters-in-a-string","title":"Accessing Characters in a String","text":"<pre><code>name = \"GeoNext\"\n</code></pre> <pre><code>name[3]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/#negative-indexing","title":"Negative Indexing","text":"<pre><code>name[-4] # I have used Negative index\n</code></pre> <pre><code>name[3] # I have used Positive index\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Strings/#string-slicing","title":"String Slicing","text":"<pre><code>name\n</code></pre> <pre><code>name[0:3]\n</code></pre> <pre><code>name[3:]\n</code></pre> <pre><code>name[-4:-1]\n</code></pre> <pre><code>name[::3]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Variables/","title":"Introduction To Variables","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Introduction_to_Variables/#introduction-to-variables","title":"Introduction to Variables","text":"<pre><code>name = \"Krishnagopal Halder\"\nname\n</code></pre> <pre><code>id(name)\n</code></pre> <pre><code>age = 22\nage\n</code></pre> <pre><code>id(age)\n</code></pre> <pre><code>message = \"Hello, Good evening.\"\nmessage\n</code></pre> <pre><code>id(message)\n</code></pre> <pre><code>name = \"GeoNext\"\nname\n</code></pre> <pre><code>id(name)\n</code></pre> <pre><code>name\n</code></pre> <pre><code>print(name)\nprint(age)\nprint(message)\n</code></pre> <pre><code>student_age = 16\nstudent_age\n</code></pre> <pre><code>avg_marks = 85\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Iterating%20on%20Strings/","title":"Iterating On Strings","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Iterating%20on%20Strings/#iterating-on-strings","title":"Iterating on Strings","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Iterating%20on%20Strings/#using-for-loop","title":"Using for loop","text":"<pre><code>name = \"Krishnagopal Halder\"\n</code></pre> <pre><code>for character in name:\n    print(character)\n</code></pre> <pre><code>len(name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Iterating%20on%20Strings/#using-for-loop-and-range","title":"Using for loop and range()","text":"<pre><code>for i in range(len(name)):# 0 - 19\n    print(f\"The Character {i+1}:\", name[i])\n    i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Iterating%20on%20Strings/#using-while-loop","title":"Using while loop","text":"<pre><code>i = 0 \nwhile i &lt; len(name):\n    print(name[i])\n    i += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Comprehension/","title":"List Comprehension","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Comprehension/#list-comprehension","title":"List Comprehension","text":"<pre><code>myList = [1, 2, 3, 4]\n</code></pre> <pre><code>type(myList)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Comprehension/#example","title":"Example","text":"<pre><code>pow2 = [2 ** x for x in range(10)] # range(10) 0-9\n</code></pre> <pre><code>pow2\n</code></pre> <pre><code># The above code can be written in this way also\npow2 = []\nfor i in range(10):\n    pow2.append(2 ** i)\nprint(pow2)\n</code></pre> <pre><code># We can use other expression to modify and create a new list\noddNumbers = [x for x in range(1, 11) if x % 2 == 1]\n</code></pre> <pre><code>print(oddNumbers)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/","title":"List Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#list-methods","title":"List Methods","text":"<pre><code>myList = [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#accessing-and-modifying-list-elements","title":"Accessing and Modifying List Elements","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#append","title":"append()","text":"<pre><code>myList.append(6)\nmyList\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#insert","title":"insert()","text":"<pre><code>myList.insert(0, 0.5)\nmyList\n</code></pre> <pre><code>myList.insert(4, 3.5)\nmyList\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#extend","title":"extend()","text":"<pre><code>lst1 = [1, 2, 3]\nlst2 = [4, 5, 6]\nlst1.extend(lst2)\nlst1\n</code></pre> <pre><code>lst2\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#remove","title":"remove()","text":"<pre><code>fruitList = [\"Mango\", \"Orange\", \"Watermelon\", \"Grapes\"]\n</code></pre> <pre><code>fruitList.remove(\"Orange\")\nfruitList\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#pop","title":"pop()","text":"<pre><code>fruitList = [\"Mango\", \"Orange\", \"Watermelon\", \"Grapes\"]\n</code></pre> <pre><code>fruitList.pop()\nfruitList\n</code></pre> <pre><code>fruitList.pop(1)\nfruitList\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#index","title":"index()","text":"<pre><code>my_nums = [1, 2, 1, 7, 5, 5, 3, 2]\n</code></pre> <pre><code>my_nums.index(7)\n</code></pre> <pre><code>my_nums.index(5)\n</code></pre> <pre><code>my_nums.index(2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#count","title":"count()","text":"<pre><code>my_nums.count(5)\n</code></pre> <pre><code>my_nums.count(7)\n</code></pre> <pre><code>my_nums.count(2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#sorting-and-reversing-lists","title":"Sorting and Reversing Lists","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#sort","title":"sort()","text":"<pre><code>my_list = [1, 5, 6, 4, 2, 1, 0, 9, 8]\n</code></pre> <pre><code>my_list.sort(reverse=True)\n</code></pre> <pre><code>my_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#reverse","title":"reverse()","text":"<pre><code>my_list\n</code></pre> <pre><code>my_list.reverse()\nmy_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#other-list-operations","title":"Other List Operations","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#len","title":"len()","text":"<pre><code>len(my_list) \n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#clear","title":"clear()","text":"<pre><code>my_list\n</code></pre> <pre><code>my_list.clear()\nmy_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Methods/#copy","title":"copy()","text":"<pre><code>my_list1 = [1, 2, 3]\ncopied_list = my_list1.copy()\ncopied_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/","title":"List Slicing","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/#list-slicing","title":"List Slicing","text":"<pre><code>my_list = [1, 2, 3, 4, 5, 6] \n# I want a list of first three elements\n</code></pre> <pre><code>subset_list = my_list[0:3] #3-1\nsubset_list\n</code></pre> <pre><code>subset_list2 = my_list[2:5]\nsubset_list2\n</code></pre> <pre><code>subset_list3 = my_list[:4]\nsubset_list3\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/#basic-list-slicing","title":"Basic List Slicing","text":"<pre><code>my_nums = [1, 2, 3, 4, 5, 6]\nsliced_list = my_nums[0:4]\nsliced_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/#slicing-with-step","title":"Slicing with Step","text":"<pre><code>sliced_with_step = my_nums[0:5:2]\nsliced_with_step\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/#negative-indexing","title":"Negative Indexing","text":"<pre><code>my_nums = [1, 2, 3, 4, 5, 6]\n</code></pre> <pre><code>my_nums[-3]\n</code></pre> <pre><code>sliced_with_neg_index = my_nums[-3:]\nsliced_with_neg_index\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/List_Slicing/#slicing-and-assignment","title":"Slicing and Assignment","text":"<pre><code>new_list = [1, 2, 3, 4, 5, 6]\n</code></pre> <pre><code>new_list[0:3] = [7, 8, 9]\n</code></pre> <pre><code>new_list\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Logical_Operators/","title":"Logical Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Logical_Operators/#logical-operators","title":"Logical Operators","text":"<pre><code>x = True\ny = False\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Logical_Operators/#and","title":"and","text":"<pre><code>print(x and y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Logical_Operators/#or","title":"or","text":"<pre><code>print(x or y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Logical_Operators/#not","title":"not","text":"<pre><code>print(not x)\n</code></pre> <pre><code>print(not y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Loops/","title":"Loops","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Loops/#loops","title":"Loops","text":"<pre><code>myList = [1, 2, 3, 4]\n</code></pre> <pre><code>for i in myList:\n    print(i ** 2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Loops/#for-loop","title":"for loop","text":"<pre><code># Print all number from 0 to n\nn = int(input(\"Enter the number you want to print upto: \"))\n</code></pre> <pre><code># Range function will give a sequence of number from 0 to n \nfor i in range(0, 5):\n    print(i)\n</code></pre> <pre><code>for i in range(0, n+1):\n    print(i)\n</code></pre> <pre><code>for i in range(10, 16): # 10, 11, 12, 13, 14\n    print(i)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Loops/#while-loop","title":"while loop","text":"<pre><code>i = 0 \nwhile i &lt; 5:\n    print(i)\n    i += 1\n</code></pre> <pre><code># First iteration, i = 0\n# Second iteration, i = 1\n# Third iteration, i = 2\n# Fourth iteration, i = 3\n# Fifth iteration, i = 4\n# Sixth iteration, i = 5\n</code></pre> <pre><code>num = 10\n\nwhile num &lt; 15:\n    print(num, \"is less than 15\")\n    num += 1\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Membership_Operators/","title":"Membership Operators","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Membership_Operators/#membership-operators","title":"Membership Operators","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Membership_Operators/#in","title":"in","text":"<pre><code>name = \"GeoNext\"\nprint(\"G\" in name)\n</code></pre> <pre><code>lst1 = [1, 2, 3]\n</code></pre> <pre><code>print(5 in lst1)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Membership_Operators/#not-in","title":"not in","text":"<pre><code>name2 = \"Apple\"\n</code></pre> <pre><code>print(\"N\" not in name2)\n</code></pre> <pre><code>lst2 = [5, 6, 7, 10]\n</code></pre> <pre><code>print(10 not in lst2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/","title":"Polymorphism","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/#polymorphism","title":"Polymorphism","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/#example-1-polymorphism-in-addition-operator","title":"Example 1: Polymorphism in addition(+) operator","text":"<pre><code>a = 5 # int datatype\nb = 10\n</code></pre> <pre><code>a + b\n</code></pre> <pre><code>type(a)\n</code></pre> <pre><code>str1 = \"Geo\"\nstr2 = \"Next\"\n</code></pre> <pre><code>type(str1)\n</code></pre> <pre><code>str1 + str2\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/#example-2-functional-polymorphism-in-python","title":"Example 2: Functional Polymorphism in Python","text":"<pre><code>list1 = [1, 2, 3, 4, 5]\nstr1 = \"GeoNext\"\ndict1 = {\"a\": 10, \"b\": 20}\n</code></pre> <pre><code>print(len(list1))\nprint(len(str1))\nprint(len(dict1))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/#class-polymorphism-in-python","title":"Class Polymorphism in Python","text":"<pre><code>class Male:\n\n    # constructor\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def info(self):\n        print(\"Hello, I am Male.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n</code></pre> <pre><code>class Female:\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def info(self):\n        print(\"Hello, I am Female.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n</code></pre> <pre><code># Creating instances\nmale1 = Male(\"Ayan Pal\", 48)\nfemale1 = Female(\"Sayani Pal\", 40)\n</code></pre> <pre><code># Running a loop over the set of objects\n# Calling the info() function common to both\nfor human in (male1, female1):\n    human.info()\n    print(\"\\n\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Polymorphism/#polymorphism-and-inheritance","title":"Polymorphism and Inheritance","text":"<pre><code># Create a parent / super class\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def info(self):\n        print(\"I am a Human.\")\n        print(f\"My name is {self.name}\")\n</code></pre> <pre><code># Create two child class\nclass Male(Human):\n    def __init__(self, name, age):\n        super().__init__(name)\n        self.age = age\n\n    def info(self):\n        print(\"Hello, I am a Male.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n</code></pre> <pre><code>class Female(Human):\n    def __init__(self, name, age):\n        super().__init__(name)\n        self.age = age\n\n    def info(self):\n        print(\"Hello, I am a Female.\")\n        print(f\"My name is {self.name}.\")\n        print(f\"I am {self.age} years old.\")\n</code></pre> <pre><code># Creating instances\nhuman1 = Human(\"Ayan Roy\")\nmale1 = Male(\"Bijay Pal\", 28)\nfemale1 = Female(\"Jayita Sen\", 25)\n</code></pre> <pre><code>for i in (human1, male1, female1):\n    i.info()\n    print(\"\\n\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python%20Default%20Parameters/","title":"Python Default Parameters","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python%20Default%20Parameters/#python-default-parameter","title":"Python Default Parameter","text":"<pre><code>def programmer(name, age, coreLang=\"Python\"):\n    print(\"The name of the programmer is\", name)\n    print(\"The age of the programmer is\", age)\n    print(\"The core programming language is\", coreLang)\n</code></pre> <pre><code># Calling the funtion with the deafult value of coreLang parameter\nprogrammer(\"Abir Sen\", 26)\n</code></pre> <pre><code># Calling function with the changed value of coreLang parameter\nprogrammer(\"Abir Sen\", 26, \"C++\")\n</code></pre> <pre><code>programmer(name=\"Abir Sen\", age=26, coreLang=\"C++\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_Comments/","title":"Python Comments","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_Comments/#python-comments","title":"Python Comments","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_Comments/#single-line-comments","title":"Single Line Comments","text":"<pre><code># This is a single line comment\nprint(\"Hello world!\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_Comments/#multi-line-comments","title":"Multi Line Comments","text":"<pre><code>\"\"\"\nThis is\na Multi Line\nComment\n\"\"\"\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_input_Function/","title":"Python Input Function","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_input_Function/#python-input-function","title":"Python input() Function","text":"<pre><code>name = input(\"Write your name: \")\n</code></pre> <pre><code>print(name)\n</code></pre> <pre><code>weight = int(input(\"What is your weight: \"))\n</code></pre> <pre><code>print(weight)\n</code></pre> <pre><code>type(weight)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_input_Function/#taking-a-space-separated-input-in-one-line","title":"Taking a space-separated input in one line","text":"<pre><code>x, y = input(\"Enter the longitude and latitude: \").split()\n</code></pre> <pre><code>x = float(x)\ny = float(y)\n</code></pre> <pre><code>print(\"The Longitude is\", x)\nprint(\"The Latitude is\", y)\n</code></pre> <pre><code>type(x)\n</code></pre> <pre><code>type(y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_input_Function/#exercise-add-two-numbers","title":"Exercise: Add two numbers","text":"<pre><code>num1 = int(input(\"Enter the first number: \"))\nnum2 = int(input(\"Enter the second number: \"))\n</code></pre> <pre><code>user_sum = num1 + num2\nprint(\"The sum of two numbers is\", user_sum)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_print_Function/","title":"Python Print Function","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_print_Function/#python-print-function","title":"Python print() Function","text":"<pre><code>print(\"Hello World!\")\n</code></pre> <pre><code>print(100)\n</code></pre> <pre><code># Printing more than one object\nprint(\"Kolkata\",14900000,255)\n</code></pre> <pre><code>a = 15\nb = 20\n</code></pre> <pre><code>print(a, b)\nprint(a + b)\n</code></pre> <pre><code>print(\"Sum of a and b is:\", a+b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_print_Function/#end-parameter","title":"end Parameter","text":"<pre><code>print(a, end=\" \")\nprint(b)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_print_Function/#sep-parameter","title":"sep Parameter","text":"<pre><code>print(a, b, sep=\"-\")\n</code></pre> <pre><code>day = 30\nmonth = 4\nyear = 2023\nprint(day, month, year, sep=\"/\")\n</code></pre> <pre><code>companyName = \"Apple\"\ntext = \"Follow\"\n</code></pre> <pre><code># Follow@Apple\n</code></pre> <pre><code>print(text, companyName, sep=\"@\")\n</code></pre> <pre><code> # Follow@Apple 30/4/2023\n</code></pre> <pre><code>print(\"Follow@Apple 30/4/2023\")\n</code></pre> <pre><code>print(text, companyName, day, month, year, sep=\"@\")\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Python_print_Function/#string-concatenation","title":"String Concatenation","text":"<pre><code>first_name = \"Krishnagopal\"\nlast_name = \"Halder\"\nfull_name = first_name + \" \" + last_name\n</code></pre> <pre><code>print(full_name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Scope%20of%20Variables/","title":"Scope Of Variables","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Scope%20of%20Variables/#scope-of-variables","title":"Scope of Variables","text":"<pre><code>def myName():\n    name = \"Local Variable\"\n    city = \"Bankura\"\n    print(name)\n    print(city)\n</code></pre> <pre><code>myName()\n</code></pre> <pre><code>name = \"Global Variable\"\n</code></pre> <pre><code>print(name)\n</code></pre> <pre><code>city = \"Bankura\"\n</code></pre> <pre><code>print(city)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Scope%20of%20Variables/#creating-a-global-variable","title":"Creating a Global Variable","text":"<pre><code>x = \"Global Variable\"\ndef checkScope():\n    print(\"x is a\", x)\n</code></pre> <pre><code>checkScope()\n</code></pre> <pre><code>print(x)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Scope%20of%20Variables/#creating-a-local-variable","title":"Creating a Local Variable","text":"<pre><code>def checkLocal():\n    y = \"Local Variable\"\n    print(\"y is a\", y)\n</code></pre> <pre><code>checkLocal()\n</code></pre> <pre><code># If we call a local variable outside of its scope, we will get an error.\n# print(y)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/","title":"String Concatenation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#string-concatenation","title":"String Concatenation","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#using-operator","title":"Using + Operator","text":"<pre><code>name1 = \"Geo\"\nname2 = \"Next\"\n</code></pre> <pre><code>full_name = name1 + name2\nfull_name\n</code></pre> <pre><code>first_name = \"Krishnagopal\"\nlast_name = \"Halder\"\n</code></pre> <pre><code>full_name = first_name + \" \" + last_name\nfull_name\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#using-join-method","title":"Using join() Method","text":"<pre><code>words = [\"Here\", \"I\", \"am\", \"concatinating\", \"multiple\", \"strings\", \"with\", \"join\", \"method.\"]\nmessage = \" \".join(words)\nprint(message)\nprint(type(words))\nprint(type(message))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#using-operator_1","title":"Using % Operator","text":"<pre><code>name1 = \"Geo\"\nname2 = \"Next\"\nprint(\"%s%s\"%(name1, name2))\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#using-format-function","title":"Using format() function","text":"<pre><code>name = \"Krishnagopal\"\ncourse_name = \"Python\"\nmessage = \"Hello {}, Thank you for joining today's {} class.\".format(name, course_name)\nprint(message)\n</code></pre> <pre><code># Lets use format function with user inputs.\nname = input(\"What is your name?\")\ncourse_name = input(\"In which course you want to enroll?\")\nmessage = \"Hello {}, Thank you for showing interest in {} online course.\".format(name, course_name)\nprint(message)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Concatenation/#f-strings","title":"f Strings","text":"<pre><code>message = f\"Hello {name}, Thank you for showing interest in {course_name} online course.\"\nprint(message)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Methods/","title":"String Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Methods/#string-methods","title":"String Methods","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Methods/#repeatingreplicating-strings","title":"Repeating/Replicating Strings","text":"<pre><code>name = \"GeoNext\"\nrepeat_name = name*3\nprint(repeat_name)\n</code></pre> <pre><code>name[::-1]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Methods/#slice-constructor","title":"slice() Constructor","text":"<pre><code>s1 = slice(0, 3)\ns2 = slice(3, 7)\nprint(name[s1])\nprint(name[s2])\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/String_Methods/#string-comparison","title":"String Comparison","text":"<pre><code>name1 = \"Geo\"\nname2 = \"Geo\"\nname3 = \"Next\"\n</code></pre> <pre><code>print(name1 == name2)\n</code></pre> <pre><code>print(name1 == name3)\n</code></pre> <pre><code>len(name1)\n</code></pre> <pre><code>len(name3)\n</code></pre> <pre><code>print(name1 &lt; name3)\n</code></pre> <pre><code>print(name1 &gt;= name3)\n</code></pre> <pre><code>print(name1 &lt;= name3)\n</code></pre> <pre><code>print(name1 != name2)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/","title":"Two Dimensional List","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#two-dimensional2-d-list","title":"Two-Dimensional(2-D) List","text":"<pre><code>myList = [1, 2, 3, 4]\n</code></pre> <pre><code>student1 = [9, 8, 6, 5]\nstudent2 = [8, 10, 4, 7]\nstudent3 = [7, 3, 8, 9]\nstudent4 = [10, 8, 7, 9]\n</code></pre> <pre><code>marks_of_students = [student1, student2, student3, student4]\n</code></pre> <pre><code>marks_of_students\n</code></pre> <pre><code>import numpy as np\n</code></pre> <pre><code>myArr = np.array([1, 2, 3, 4])\n</code></pre> <pre><code>myArr.ndim\n</code></pre> <pre><code>myArr2D = np.array([[1, 2, 3, 4], [5, 6, 7, 9], [10, 11, 12, 13]])\n</code></pre> <pre><code>myArr2D\n</code></pre> <pre><code>myArr2D.shape\n</code></pre> <pre><code>myArr2D.ndim\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#accessing-values-in-a-two-dimensional-list","title":"Accessing Values in a Two-Dimensional List","text":"<pre><code>marks_of_students\n</code></pre> <pre><code># List at index 0 in marks_of_students\nmarks_of_students[2]\n</code></pre> <pre><code># Element at index 1 in list at index 1\nmarks_of_students[1][1]\n</code></pre> <pre><code># Element at index 3 in list at index 1\nmarks_of_students[3][1]\n</code></pre> <pre><code>myList = [1, 2, 3]\n</code></pre> <pre><code>myList[2]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#input-of-two-dimensional-list","title":"Input of Two-Dimensional List","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#line-separated-input","title":"Line Separated Input","text":"<pre><code># Number of rows\nprint(\"How many number of rows do you want?\")\nn = int(input())\ninputList = [[int(col) for col in input().split()] for row in range(n)]\n</code></pre> <pre><code>inputList\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#space-separated-input","title":"Space Separated Input","text":"<pre><code># Number of rows\nprint(\"How many number of rows do you want: \")\nrow = int(input())\n\n# Number of columns\nprint(\"How many number of columns do you want: \")\ncol = int(input())\n\n# Converting input string to list\ninputList = input().split()\n\nfinalList = [[int(inputList[i*col+j]) for j in range(col)] for i in range(row)]\nprint(finalList)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#printing-in-multiple-lines-like-a-matrix","title":"Printing in Multiple Lines Like a Matrix","text":"<pre><code>finalList\n</code></pre> <pre><code># Iterate in row of 2-D list\nfor row in finalList:\n    for col in row:\n        print(col, end=\" \")\n    print()\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Two%20Dimensional%20List/#jagged-list","title":"Jagged List","text":"<pre><code>jg_list = [[1, 2, 3], 4, 5, [5, 6, 7,9], [2, 0]]\n</code></pre> <pre><code>jg_list[3]\n</code></pre> <pre><code>jg_list[3][3]\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/","title":"Types Of Attributes","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#types-of-attributes","title":"Types of Attributes","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#instance-attributes","title":"Instance Attributes","text":"<p>```python id=\"UyAklNp5OsFY\" class Car:   # Creating the constructor   # Attributes defined within the constructor are instance attributes   def init(self, name, topspeed):     self.name = name     self.topspeed = topspeed</p> <p>def print_details(self):     print(\"Car Name:\", self.name)     print(\"Top Speed:\", self.topspeed, \"Km/h\") <pre><code>```python id=\"SefcmFMdQBJK\"\n# Creating instances from Car class\ncar1 = Car(\"Maruti 800\", 120)\ncar2 = Car(\"Ferrari\", 400)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xqfWFYCzP-ZK\" outputId=\"dc19a5fd-0947-4b0c-9d66-946f4eb01568\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#printing-the-details-of-the-car-objects","title":"Printing the details of the car objects","text":"<p>car1.print_details() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"0uxlKwh3Qp4v\" outputId=\"69480eb8-c9a9-41d8-f580-9a54c8c5add2\"\ncar2.print_details()\n</code></pre></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#class-attributes","title":"Class Attributes","text":"<p>```python id=\"uoTZ3rqQRTba\" class Car:   # Creating a class attribute   no_of_wheels = 4</p> <p>def init(self, name, topspeed):     self.name = name     self.topspeed = topspeed</p> <p>def print_details(self):     print(\"Car Name:\", self.name)     print(\"Top Speed:\", self.topspeed, \"Km/h\")     print(\"No of wheels:\", self.no_of_wheels) <pre><code>```python id=\"R8tw1Ay2SIZC\"\n# Creating some instances from the new car class\ncar1 = Car(\"Creta\", 300)\ncar2 = Car(\"Toyato\", 240)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"xGKvJT44SU46\" outputId=\"7df8b5ca-48ce-4fa3-9e5f-53d80bfee4b5\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#print-the-car-details","title":"Print the car details","text":"<p>car1.print_details() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"njgfCOnTSiWs\" outputId=\"bf2f223c-6ee2-44e4-8f84-d9a108f18f85\"\ncar2.print_details()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"z4bALEkQSqyV\" outputId=\"20e83144-258b-40a9-c112-58971f65f5de\"</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Attributes/#calling-the-class-attribute","title":"Calling the class attribute","text":"<p>car1.no_of_wheels car2.no_of_wheels <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Qqf1iDcrTTWp\" outputId=\"7b59e218-49f7-40bd-fccc-f3f80b782f70\"\nCar.no_of_wheels\n</code></pre></p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Function/","title":"Types Of Function","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Function/#types-of-function","title":"Types of Function","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Function/#built-in-function","title":"Built-in Function","text":"<pre><code>name = [1, 2, 3, 4, 5]\n</code></pre> <pre><code>len(name)\n</code></pre> <pre><code>print(name)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Function/#lambda-function","title":"Lambda Function","text":"<pre><code>square = lambda x: x ** 2 # One line function\n</code></pre> <pre><code>square(10)\n</code></pre> <pre><code>square(36)\n</code></pre> <pre><code>average = lambda x, y: (x + y) / 2\n</code></pre> <pre><code>average(10, 20)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Function/#function-overloading","title":"Function Overloading","text":"<pre><code>from multipledispatch import dispatch\n</code></pre> <pre><code>@dispatch(int, int)\ndef add(x, y):\n    return x + y\n</code></pre> <pre><code>add(2, 3)\n</code></pre> <pre><code>@dispatch(int, int, int)\ndef add(x, y, z):\n    return x + y + z\n</code></pre> <pre><code>add(2, 3)\n</code></pre> <pre><code>add(2, 3, 5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Methods/","title":"Types Of Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Methods/#types-of-methods","title":"Types of Methods","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Methods/#instance-method","title":"Instance Method","text":"<pre><code>class Circle:\n    _pi = 3.14159 # protected data member\n\n    # constructor\n    def __init__(self, radius):\n        self.radius = radius\n\n    # instance method\n    def calculate_area(self):\n        area = self._pi * (self.radius)**2\n        return area\n\n    # instance method\n    def calulate_perimeter(self):\n        perimeter = 2 * self._pi * (self.radius)\n        return perimeter\n</code></pre> <pre><code># Creating an object / instance from circle class\ncircle1 = Circle(5)\n</code></pre> <pre><code>circle1.radius\n</code></pre> <pre><code>print(\"The area of the circle:\", circle1.calculate_area())\n</code></pre> <pre><code>print(\"The perimeter of the circle:\", circle1.calulate_perimeter())\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Methods/#class-methods","title":"Class Methods","text":"<pre><code>class Rectangle:\n    width = 0 # class-level variable\n    height = 0 # class-level variable\n\n    #constructor\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    # instance method\n    def calculate_area(self):\n        area = self.width * self.height\n        return area\n\n    # class method\n    @classmethod\n    def change_size(cls, new_width, new_height):\n        cls.width = new_width\n        cls.height = new_height   \n</code></pre> <pre><code># Creating an instance from the rectangle class\nrectangle1 = Rectangle(10, 5)\n</code></pre> <pre><code># Calling the instance method\nprint(\"The area of the rectangle is:\", rectangle1.calculate_area())\n</code></pre> <pre><code>print(Rectangle.width)\nprint(Rectangle.height)\n</code></pre> <pre><code>print(rectangle1.width)\nprint(rectangle1.height)\n</code></pre> <pre><code># Modifying the class-level variable using the class method\nRectangle.change_size(5, 5)\n</code></pre> <pre><code># Checking the new class-level variable\nprint(Rectangle.width)\nprint(Rectangle.height)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Types_of_Methods/#static-methods","title":"Static Methods","text":"<pre><code>class MathUtils:\n\n    @staticmethod\n    def add(x, y, *args):\n        sum_of_num = x + y\n        for i in args: # args is a tuple ()\n            sum_of_num += i\n        return sum_of_num\n\n    @staticmethod\n    def multiply(x, y, *args):\n        product_of_num = x * y\n        for i in args:\n            product_of_num *= i\n        return product_of_num\n</code></pre> <pre><code>MathUtils.add(10, 5)\n</code></pre> <pre><code>MathUtils.multiply(10, 5)\n</code></pre> <pre><code>MathUtils.add(10, 20, 30, 40, 17, 56) # *args = (30, 40, 17, 56)\n</code></pre> <pre><code>MathUtils.multiply(10, 20, 3, 4, 5)\n</code></pre>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Untitled/","title":"Untitled","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/Class%20Code/Untitled/#create-a-program-that-calculates-the-area-of-a-rectangle-given-its-length-and-width-the-program-should-take-the-length-and-width-as-user-input-perform-the-calculation-and-then-print-the-result-to-the-console","title":"Create a program that calculates the area of a rectangle, given its length and width. The program should take the length and width as user input, perform the calculation, and then print the result to the console.","text":"<pre><code>length = float(input(\"Enter the length of the rectangle:\"))\nwidth = float(input(\"Enter the width of the rectangle:\"))\n</code></pre> <pre><code>area = length * width\n</code></pre> <pre><code>print(\"The area of the rectangle is\", area)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/","title":"Python for Beginners to Pros","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#1-basics-of-python-programming","title":"1. Basics of Python Programming","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-basics","title":"Python Basics","text":"<ul> <li>Short info about DSMP</li> <li>About Python</li> <li>Python Output/print function</li> <li>Python Data Types</li> <li>Python Variables</li> <li>Python comments</li> <li>Python Keywords and Identifiers</li> <li>Python User Input</li> <li>Python Type conversion</li> <li>Python Literals</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-operators-if-else-loops","title":"Python Operators + if-else + Loops","text":"<ul> <li>Start of the session</li> <li>Python Operators</li> <li>Python if-else</li> <li>Python Modules</li> <li>Python While Loop</li> <li>Python for loop</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-strings","title":"Python Strings","text":"<ul> <li>Introduction</li> <li>Solving Loop problems</li> <li>Break, continue, pass statement in loops</li> <li>Strings</li> <li>String indexing</li> <li>String slicing</li> <li>Edit and delete a string</li> <li>Operations on String</li> <li>Common String functions</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#time-complexity","title":"Time complexity","text":"<ul> <li>Start of the Session</li> <li>PPT presentation on Time Complexity (Efficiency in Programming and Orders of Growth)</li> <li>Examples</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#interview-questions","title":"Interview Questions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#2-python-data-types","title":"2. Python Data Types","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-lists","title":"Python Lists","text":"<ul> <li>Introduction</li> <li>Array vs List</li> <li>How lists are stored in a memory</li> <li>Characteristics of Python List</li> <li>Code Example of Lists</li> <li>Create and access a list</li> <li>append(), extend(), insert()</li> <li>Edit items in a list</li> <li>Deleting items from a list</li> <li>Arithmetic, membership and loop operations on a List</li> <li>Various List functions</li> <li>List comprehension</li> <li>2 Ways to traverse a list</li> <li>Zip() function</li> <li>Python List can store any kind of objects</li> <li>Disadvantages of Python list</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#tuples-set-dictionary","title":"Tuples + Set + Dictionary","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#tuple","title":"Tuple","text":"<ul> <li>Create and access a tuple</li> <li>Can we edit and add items to a tuple?</li> <li>Deletion</li> <li>Operations on tuple</li> <li>Tuple functions</li> <li>List vs tuple</li> <li>Tuple unpacking</li> <li>Zip() on tuple</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#set","title":"Set","text":"<ul> <li>Create and access a set</li> <li>Can we edit and add items to a set?</li> <li>Deletion</li> <li>Operations on set</li> <li>set functions</li> <li>Frozen set (immutable set)</li> <li>Set comprehension</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#dictionary","title":"Dictionary","text":"<ul> <li>Create dictionary</li> <li>Accessing items</li> <li>Add, remove, edit key-value pairs</li> <li>Operations on dictionary</li> <li>Dictionary functions</li> <li>Dictionary comprehension</li> <li>Zip() on dictionary</li> <li>Nested comprehension</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-functions","title":"Python Functions","text":"<ul> <li>Create function</li> <li>Arguments and parameters</li> <li>args and kwargs</li> <li>How to access documentation of a function</li> <li>How functions are executed in a memory</li> <li>Variable scope</li> <li>Nested functions with examples</li> <li>Functions are first class citizens</li> <li>Deletion of function</li> <li>Returning of function</li> <li>Advantages of functions</li> <li>Lambda functions</li> <li>Higher order functions</li> <li>map(), filter(), reduce()</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#array-interview-questions","title":"Array Interview Questions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#week-2-interview-questions","title":"Week 2 Interview Questions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/","title":"Python Exercise (11 20)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#python-exercise-11-20","title":"Python Exercise (11-20)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-11","title":"Question 11","text":"<p>Write a program which accepts a sequence of comma separated 4 digit binary numbers as its input and then check whether they are divisible by 5 or not. The numbers that are divisible by 5 are to be printed in a comma separated sequence.</p> <p>Example: <pre><code>0100,0011,1010,1001\n</code></pre> Then the output should be: <pre><code>1010\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-12","title":"Question 12","text":"<p>Write a program, which will find all such numbers between 1000 and 3000 (both included) such that each digit of the number is an even number. The numbers obtained should be printed in a comma-separated sequence on a single line.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-13","title":"Question 13","text":"<p>Write a program that accepts a sentence and calculate the number of letters and digits. Suppose the following input is supplied to the program: <pre><code>hello world! 123\n</code></pre> Then, the output should be: <pre><code>LETTERS 10\nDIGITS 3\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-14","title":"Question 14","text":"<p>Write a program that accepts a sentence and calculate the number of upper case letters and lower case letters. Suppose the following input is supplied to the program: <pre><code>Hello world!\n</code></pre> Then, the output should be: <pre><code>UPPER CASE 1\nLOWER CASE 9\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-15","title":"Question 15","text":"<p>Write a program that computes the value of a+aa+aaa+aaaa with a given digit as the value of a. Suppose the following input is supplied to the program: <pre><code>9\n</code></pre> Then, the output should be: <pre><code>11106\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-16","title":"Question 16","text":"<p>Use a list comprehension to select each odd number in a list. The list is input by a sequence of comma-separated numbers. Suppose the following input is supplied to the program: <pre><code>1,2,3,4,5,6,7,8,9\n</code></pre> Then, the output should be: <pre><code>1,3,5,7,9\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-17","title":"Question 17","text":"<p>Question: Write a program that computes the net amount of a bank account based a transaction log from console input. The transaction log format is shown as following: <pre><code>D 100\nW 200\n</code></pre> D means deposit while W means withdrawal. Suppose the following input is supplied to the program: <pre><code>D 300\nD 300\nW 200\nD 100\n</code></pre> Then, the output should be: <pre><code>500\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-18","title":"Question 18","text":"<p>A website requires the users to input username and password to register. Write a program to check the validity of password input by users. Following are the criteria for checking the password: 1. At least 1 letter between [a-z] 2. At least 1 number between [0-9] 1. At least 1 letter between [A-Z] 3. At least 1 character from [$#@] 4. Minimum length of transaction password: 6 5. Maximum length of transaction password: 12</p> <p>Your program should accept a sequence of comma separated passwords and will check them according to the above criteria. Passwords that match the criteria are to be printed, each separated by a comma.</p> <p>If the following passwords are given as input to the program:</p> <p><pre><code>ABd1234@1,a F1#,2w3E*,2We3345\n</code></pre> Then, the output of the program should be: <pre><code>ABd1234@1\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-19","title":"Question 19","text":"<p>You are required to write a program to sort the (name, age, height) tuples by ascending order where name is string, age and height are numbers. The tuples are input by console. The sort criteria is: 1: Sort based on name; 2: Then sort based on age; 3: Then sort by score. The priority is that name &gt; age &gt; score. If the following tuples are given as input to the program:</p> <p><pre><code>Tom,19,80\nJohn,20,90\nJony,17,91\nJony,17,93\nJson,21,85\n</code></pre> Then, the output should be: <pre><code>[('John', '20', '90'), ('Jony', '17', '91'), ('Jony', '17', '93'), ('Json', '21', '85'), ('Tom', '19', '80')]\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2811-20%29/#question-20","title":"Question 20","text":"<p>Define a class with a generator which can iterate the numbers, which are divisible by 7, between a given range 0 and n.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/","title":"Python Exercise (21 30)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#python-exercise-21-30","title":"Python Exercise (21-30)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-21","title":"Question 21","text":"<p>A robot moves in a plane starting from the original point (0,0). The robot can move toward UP, DOWN, LEFT and RIGHT with a given steps. The trace of robot movement is shown as the following: <pre><code>UP 5\nDOWN 3\nLEFT 3\nRIGHT 2\n</code></pre> The numbers after the direction are steps. Please write a program to compute the distance from current position after a sequence of movement and original point. If the distance is a float, then just print the nearest integer. Example: If the following tuples are given as input to the program: <pre><code>UP 5\nDOWN 3\nLEFT 3\nRIGHT 2\n</code></pre> Then, the output of the program should be: <pre><code>2\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-22","title":"Question 22","text":"<p>Write a program to compute the frequency of the words from the input. The output should output after sorting the key alphanumerically.</p> <p>Suppose the following input is supplied to the program: <pre><code>New to Python or choosing between Python 2 and Python 3? Read Python 2 or Python 3.\n</code></pre> Then, the output should be: <pre><code>2:2\n3.:1\n3?:1\nNew:1\nPython:5\nRead:1\nand:1\nbetween:1\nchoosing:1\nor:2\nto:1\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-23","title":"Question 23","text":"<p>Write a method which can calculate square value of number</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-24","title":"Question 24","text":"<p>Python has many built-in functions, and if you do not know how to use it, you can read document online or find some books. But Python has a built-in document function for every built-in functions.</p> <p>Please write a program to print some Python built-in functions documents, such as abs(), int(), raw_input()</p> <p>And add document for your own function</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-25","title":"Question 25","text":"<p>Define a class, which have a class parameter and have a same instance parameter.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-26","title":"Question 26","text":"<p>Define a function which can compute the sum of two numbers.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-27","title":"Question 27","text":"<p>Define a function that can convert a integer into a string and print it in console.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-28","title":"Question 28","text":"<p>Define a function that can receive two integer numbers in string form and compute their sum and then print it in console.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-29","title":"Question 29","text":"<p>Define a function that can accept two strings as input and concatenate them and then print it in console.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2821-30%29/#question-30","title":"Question 30","text":"<p>Define a function that can accept two strings as input and print the string with maximum length in console. If two strings have the same length, then the function should print all strings line by line.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/","title":"Python Exercise (31 40)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#python-exercise-31-40","title":"Python Exercise (31-40)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-31","title":"Question 31","text":"<p>Define a function which can print a dictionary where the keys are numbers between 1 and 20 (both included) and the values are square of keys.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-32","title":"Question 32","text":"<p>Define a function which can generate a dictionary where the keys are numbers between 1 and 20 (both included) and the values are square of keys. The function should just print the keys only.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-33","title":"Question 33","text":"<p>Define a function which can generate and print a list where the values are square of numbers between 1 and 20 (both included).</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-34","title":"Question 34","text":"<p>Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print the first 5 elements in the list.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-35","title":"Question 35","text":"<p>Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print the last 5 elements in the list.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-36","title":"Question 36","text":"<p>Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print all values except the first 5 elements in the list.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-37","title":"Question 37","text":"<p>Define a function which can generate and print a tuple where the value are square of numbers between 1 and 20 (both included).</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-38","title":"Question 38","text":"<p>With a given tuple (1,2,3,4,5,6,7,8,9,10), write a program to print the first half values in one line and the last half values in one line.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-39","title":"Question 39","text":"<p>Write a program to generate and print another tuple whose values are even numbers in the given tuple (1,2,3,4,5,6,7,8,9,10).</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2831-40%29/#question-40","title":"Question 40","text":"<p>Write a program which accepts a string as input to print \"Yes\" if the string is \"yes\" or \"YES\" or \"Yes\", otherwise print \"No\".</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/","title":"Python Exercise (41 50)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#python-exercise-version-notebook","title":"Python Exercise -Version Notebook","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-41","title":"Question 41","text":"<p>Write a program which can map() to make a list whose elements are square of elements in [1,2,3,4,5,6,7,8,9,10].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-42","title":"Question 42","text":"<p>Write a program which can map() and filter() to make a list whose elements are square of even number in [1,2,3,4,5,6,7,8,9,10].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-43","title":"Question 43","text":"<p>Write a program which can filter() to make a list whose elements are even number between 1 and 20 (both included).</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-44","title":"Question 44","text":"<p>Write a program which can map() to make a list whose elements are square of numbers between 1 and 20 (both included).</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-45","title":"Question 45","text":"<p>Define a class named American which has a static method called printNationality.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-46","title":"Question 46","text":"<p>Define a class named American and its subclass NewYorker.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-47","title":"Question 47","text":"<p>Define a class named Circle which can be constructed by a radius. The Circle class has a method which can compute the area.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-48","title":"Question 48","text":"<p>Define a class named Rectangle which can be constructed by a length and width. The Rectangle class has a method which can compute the area.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-49","title":"Question 49","text":"<p>Define a class named Shape and its subclass Square. The Square class has an init function which takes a length as argument. Both classes have a area function which can print the area of the shape where Shape's area is 0 by default.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2841-50%29/#question-50","title":"Question 50","text":"<p>Please raise a RuntimeError exception.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/","title":"Python Exercise (51 60)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#python-exercise-51-60","title":"Python Exercise (51-60)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-51","title":"Question 51","text":"<p>Write a function to compute 5/0 and use try/except to catch the exceptions.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-52","title":"Question 52","text":"<p>Define a custom exception class which takes a string message as attribute.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-53","title":"Question 53","text":"<p>Assuming that we have some email addresses in the \"username@companyname.com\" format, please write program to print the user name of a given email address. Both user names and company names are composed of letters only.</p> <p>Example: If the following email address is given as input to the  program: <pre><code>john@google.com\n</code></pre> Then, the output of the program should be: <pre><code>john\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-54","title":"Question 54","text":"<p>Assuming that we have some email addresses in the \"username@companyname.com\" format, please write program to print the company name of a given email address. Both user names and company names are composed of letters only.</p> <p>Example: If the following email address is given as input to the program: <pre><code>john@google.com\n</code></pre> Then, the output of the program should be: <pre><code>google\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#hints","title":"Hints","text":"<p>Use \\w to match letters.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-55","title":"Question 55","text":"<p>Write a program which accepts a sequence of words separated by whitespace as input to print the words composed of digits only.</p> <p>Example: If the following words is given as input to the program: <pre><code>2 cats and 3 dogs.\n</code></pre> Then, the output of the program should be: <pre><code>['2', '3']\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-56","title":"Question 56","text":"<p>Print a unicode string \"hello world\".</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-57","title":"Question 57","text":"<p>Write a program to read an ASCII string and to convert it to a unicode string encoded by utf-8.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#hints_1","title":"Hints","text":"<p>Use unicode()/encode() function to convert.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-58","title":"Question 58","text":"<p>Write a special comment to indicate a Python source code file is in unicode.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-59","title":"Question 59","text":"<p>Write a program to compute 1/2+2/3+3/4+...+n/n+1 with a given n input by console (n&gt;0).</p> <p>Example: If the following n is given as input to the program: <pre><code>5\n</code></pre> Then, the output of the program should be: <pre><code>3.55\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2851-60%29/#question-60","title":"Question 60","text":"<p>Write a program to compute: <pre><code>f(n)=f(n-1)+100 when n&gt;0\nand f(0)=1\n</code></pre> with a given n input by console (n&gt;0).</p> <p>Example: If the following n is given as input to the program: <pre><code>5\n</code></pre> Then, the output of the program should be: <pre><code>500\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <p>Solution 1</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/","title":"Python Exercise (61 70)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py312     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#python-exercise-61-70","title":"Python Exercise (61-70)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-61","title":"Question 61","text":"<p>The Fibonacci Sequence is computed based on the following formula: <pre><code>f(n)=0 if n=0\nf(n)=1 if n=1\nf(n)=f(n-1)+f(n-2) if n&gt;1\n</code></pre> Please write a program to compute the value of f(n) with a given n input by console.</p> <p>Example: If the following n is given as input to the program: <pre><code>7\n</code></pre> Then, the output of the program should be: <pre><code>13\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#hints","title":"Hints","text":"<p>We can define recursive function in Python.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-62","title":"Question 62","text":"<p>The Fibonacci Sequence is computed based on the following formula: <pre><code>f(n)=0 if n=0\nf(n)=1 if n=1\nf(n)=f(n-1)+f(n-2) if n&gt;1\n</code></pre> Please write a program to compute the value of f(n) with a given n input by console.</p> <p>Example: If the following n is given as input to the program: <pre><code>7\n</code></pre> Then, the output of the program should be: <pre><code>0,1,1,2,3,5,8,13\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-63","title":"Question 63","text":"<p>Please write a program using generator to print the even numbers between 0 and n in comma separated form while n is input by console.</p> <p>Example: If the following n is given as input to the program: <pre><code>10\n</code></pre> Then, the output of the program should be: <pre><code>0,2,4,6,8,10\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#hints_1","title":"Hints","text":"<p>Use yield to produce the next value in generator.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-64","title":"Question 64","text":"<p>Please write a program using generator to print the numbers which can be divisible by 5 and 7 between 0 and n in comma separated form while n is input by console.</p> <p>Example: If the following n is given as input to the program: <pre><code>100\n</code></pre> Then, the output of the program should be: <pre><code>0,35,70\n</code></pre> In case of input data being supplied to the question, it should be assumed to be a console input.</p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-65","title":"Question 65","text":"<p>Please write assert statements to verify that every number in the list [2,4,6,8] is even.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-66","title":"Question 66","text":"<p>Please write a program which accepts basic mathematic expression from console and print the evaluation result.</p> <p>Example: If the following n is given as input to the program: <pre><code>35 + 3\n</code></pre> Then, the output of the program should be: <pre><code>38\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-67","title":"Question 67","text":"<p>Please write a binary search function which searches an item in a sorted list. The function should return the index of element to be searched in the list.</p> <p>Solution1</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-68","title":"Question 68","text":"<p>Please generate a random float where the value is between 10 and 100 using Python module.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-69","title":"Question 69","text":"<p>Please generate a random float where the value is between 5 and 95 using Python module.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2861-70%29/#question-70","title":"Question 70","text":"<p>Please write a program to output a random even number between 0 and 10 inclusive using random module and list comprehension.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/","title":"Python Exercise (71 80)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#python-exercise-71-80","title":"Python Exercise (71-80)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-71","title":"Question 71","text":"<p>Please write a program to output a random number, which is divisible by 5 and 7, between 10 and 150 inclusive using random module and list comprehension.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-72","title":"Question 72","text":"<p>Please write a program to generate a list with 5 random numbers between 100 and 200 inclusive.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-73","title":"Question 73","text":"<p>Please write a program to randomly generate a list with 5 even numbers between 100 and 200 inclusive.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-74","title":"Question 74","text":"<p>Please write a program to randomly generate a list with 5 numbers, which are divisible by 5 and 7 , between 1 and 1000 inclusive.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-75","title":"Question 75","text":"<p>Please write a program to randomly print a integer number between 7 and 15 inclusive.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-76","title":"Question 76","text":"<p>Please write a program to compress and decompress the string \"hello world!hello world!hello world!hello world!\".</p> <p>Use zlib.compress() and zlib.decompress() to compress and decompress a string.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-77","title":"Question 77","text":"<p>Please write a program to print the running time of execution of \"1+1\" for 100 times.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-78","title":"Question 78","text":"<p>Please write a program to shuffle and print the list [3,6,7,8].</p> <p>Use shuffle() function to shuffle a list.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-79","title":"Question 79","text":"<p>Please write a program to generate all sentences where subject is in [\"I\", \"You\"] and verb is in [\"Play\", \"Love\"] and the object is in [\"Hockey\",\"Football\"].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2871-80%29/#question-80","title":"Question 80","text":"<p>Please write a program to print the list after removing even numbers in [5,6,77,45,22,12,24].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/","title":"Python Exercise (81 90)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#python-exercise-81-90","title":"Python Exercise (81-90)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-81","title":"Question 81","text":"<p>By using list comprehension, please write a program to print the list after removing numbers which are divisible by 5 and 7 in [12,24,35,70,88,120,155].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-82","title":"Question 82","text":"<p>By using list comprehension, please write a program to print the list after removing the 0th, 2nd, 4th,6th numbers in [12,24,35,70,88,120,155].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-83","title":"Question 83","text":"<p>By using list comprehension, please write a program to print the list after removing the 2nd - 4th numbers in [12,24,35,70,88,120,155].</p> <p>Use list comprehension to delete a bunch of element from a list. Use enumerate() to get (index, value) tuple.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-84","title":"Question 84","text":"<p>By using list comprehension, please write a program generate a 3*5*8 3D array whose each element is 0.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-85","title":"Question 85","text":"<p>By using list comprehension, please write a program to print the list after removing the 0th,4th,5th numbers in [12,24,35,70,88,120,155].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-86","title":"Question 86","text":"<p>By using list comprehension, please write a program to print the list after removing the value 24 in [12,24,35,24,88,120,155].</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#hints","title":"Hints","text":"<p>Use list's remove method to delete a value.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-87","title":"Question 87","text":"<p>With two given lists [1,3,6,78,35,55] and [12,24,35,24,88,120,155], write a program to make a list whose elements are intersection of the above given lists.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#hints_1","title":"Hints","text":"<p>Use set() and \"&amp;=\" to do set intersection operation.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-88","title":"Question 88","text":"<p>With a given list [12,24,35,24,88,120,155,88,120,155], write a program to print this list after removing all duplicate values with original order reserved.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#hints_2","title":"Hints","text":"<p>Use set() to store a number of values without duplicate.|</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-89","title":"Question 89","text":"<p>Define a class Person and its two child classes: Male and Female. All classes have a method \"getGender\" which can print \"Male\" for Male class and \"Female\" for Female class.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#question-90","title":"Question 90","text":"<p>Please write a program which count and print the numbers of each character in a string input by console.</p> <p>Example: If the following string is given as input to the program: <pre><code>abcdefgabc\n</code></pre> Then, the output of the program should be: <pre><code>a,2\nc,2\nb,2\ne,1\nd,1\ng,1\nf,1\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2881-90%29/#hints_3","title":"Hints","text":"<p>Use dict to store key/value pairs. Use dict.get() method to lookup a key with default value.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/","title":"Python Exercise (91 100)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#python-exercise-91-100","title":"Python Exercise (91-100)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-91","title":"Question 91","text":"<p>Please write a program which accepts a string from console and print it in reverse order.</p> <p>Example: If the following string is given as input to the program: <pre><code>rise to vote sir\n</code></pre> Then, the output of the program should be:* <pre><code>ris etov ot esir\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints","title":"Hints","text":"<p>Use list[::-1] to iterate a list in a reverse order.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-92","title":"Question 92","text":"<p>Please write a program which accepts a string from console and print the characters that have even indexes.</p> <p>Example: If the following string is given as input to the program: <pre><code>H1e2l3l4o5w6o7r8l9d\n</code></pre> Then, the output of the program should be: <pre><code>Helloworld\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_1","title":"Hints","text":"<p>Use list[::2] to iterate a list by step 2.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-93","title":"Question 93","text":"<p>Please write a program which prints all permutations of [1,2,3]</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_2","title":"Hints","text":"<p>Use itertools.permutations() to get permutations of list.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-94","title":"Question 94","text":"<p>Write a program to solve a classic ancient Chinese puzzle:  We count 35 heads and 94 legs among the chickens and rabbits in a farm. How many rabbits and how many chickens do we have?</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-95","title":"Question 95","text":"<p>Given the participants' score sheet for your University Sports Day, you are required to find the runner-up score. You are given  scores. Store them in a list and find the score of the runner-up.</p> <p>If the following string is given as input to the program: <pre><code>5\n2 3 6 6 5\n</code></pre> Then, the output of the program should be: <pre><code>5\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_3","title":"Hints","text":"<p>Make the scores unique and then find 2nd best number</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-96","title":"Question 96","text":"<p>You are given a string S and width W. Your task is to wrap the string into a paragraph of width.</p> <p>If the following string is given as input to the program: <pre><code>ABCDEFGHIJKLIMNOQRSTUVWXYZ\n4\n</code></pre> Then, the output of the program should be: <pre><code>ABCD\nEFGH\nIJKL\nIMNO\nQRST\nUVWX\nYZ\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_4","title":"Hints","text":"<p>Use wrap function of textwrap module|</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-97","title":"Question 97","text":"<p>You are given an integer, N. Your task is to print an alphabet rangoli of size N. (Rangoli is a form of Indian folk art based on creation of patterns.)</p> <p>Different sizes of alphabet rangoli are shown below: <pre><code>#size 3\n\n----c----\n--c-b-c--\nc-b-a-b-c\n--c-b-c--\n----c----\n\n#size 5\n\n--------e--------\n------e-d-e------\n----e-d-c-d-e----\n--e-d-c-b-c-d-e--\ne-d-c-b-a-b-c-d-e\n--e-d-c-b-c-d-e--\n----e-d-c-d-e----\n------e-d-e------\n--------e--------\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_5","title":"Hints","text":"<p>First print the half of the Rangoli in the given way and save each line in a list. Then print the list in reverse order to get the rest.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-98","title":"Question 98","text":"<p>You are given a date. Your task is to find what the day is on that date.</p> <p>Input</p> <p>A single line of input containing the space separated month, day and year, respectively, in MM DD YYYY format. <pre><code>08 05 2015\n</code></pre></p> <p>Output</p> <p>Output the correct day in capital letters. <pre><code>WEDNESDAY\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_6","title":"Hints","text":"<p>Use weekday function of calender module</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-99","title":"Question 99","text":"<p>Given 2 sets of integers, M and N, print their symmetric difference in ascending order. The term symmetric difference indicates those values that exist in either M or N but do not exist in both.</p> <p>Input</p> <p>The first line of input contains an integer, M.The second line contains M space-separated integers.The third line contains an integer, N.The fourth line contains N space-separated integers. <pre><code>4\n2 4 5 9\n4\n2 4 11 12\n</code></pre></p> <p>Output</p> <p>Output the symmetric difference integers in ascending order, one per line. <pre><code>5\n9\n11\n12\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_7","title":"Hints","text":"<p>Use \\'^\\' to make symmetric difference operation.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#question-100","title":"Question 100","text":"<p>You are given  words. Some words may repeat. For each word, output its number of occurrences. The output order should correspond with the input order of appearance of the word. See the sample input/output for clarification.</p> <p>If the following string is given as input to the program: <pre><code>4\nbcdef\nabcdefg\nbcde\nbcdef\n</code></pre> Then, the output of the program should be: <pre><code>3\n2 1 1\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python%20Exercise%20%2891-100%29/#hints_8","title":"Hints","text":"<p>Make a list to get the input order and a dictionary to count the word frequency</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/","title":"Python Exercise (1 10)","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#python-exercise-1-10","title":"Python Exercise (1-10)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#question-1","title":"Question 1","text":"<p>Write a program which will find all such numbers which are divisible by 7 but are not a multiple of 5, between 2000 and 3200 (both included). The numbers obtained should be printed in a comma-separated sequence on a single line.</p> <p>```python vscode={\"languageId\": \"plaintext\"}</p> <pre><code>&lt;!-- #region id=\"Yig61r6o_XEn\" --&gt;\n## Question 2\n\nWrite a program which can compute the factorial of a given numbers.\nThe results should be printed in a comma-separated sequence on a single line.\nSuppose the following input is supplied to the program:\n8\nThen, the output should be:\n40320\n\nHints:\nIn case of input data being supplied to the question, it should be assumed to be a console input.\n&lt;!-- #endregion --&gt;\n\n```python vscode={\"languageId\": \"plaintext\"}\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#question-3","title":"Question 3","text":"<p>With a given integral number n, write a program to generate a dictionary that contains (i, i*i) such that is an integral number between 1 and n (both included). and then the program should print the dictionary. Suppose the following input is supplied to the program: 8 Then, the output should be:</p> <p>Hints: In case of input data being supplied to the question, it should be assumed to be a console input. Consider use dict()</p> <p>```python vscode={\"languageId\": \"plaintext\"}</p> <pre><code>&lt;!-- #region id=\"QzqHCtDp_XEp\" --&gt;\n## Question 4\n\nWrite a program which accepts a sequence of comma-separated numbers from console and generate a list and a tuple which contains every number.\nSuppose the following input is supplied to the program:\n34,67,55,33,12,98\nThen, the output should be:\n['34', '67', '55', '33', '12', '98']\n('34', '67', '55', '33', '12', '98')\n\nHints:\nIn case of input data being supplied to the question, it should be assumed to be a console input.\ntuple() method can convert list to tuple\n&lt;!-- #endregion --&gt;\n\n```python vscode={\"languageId\": \"plaintext\"}\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#question-5","title":"Question 5","text":"<p>Define a class which has at least two methods: getString: to get a string from console input printString: to print the string in upper case. Also please include simple test function to test the class methods.</p> <p>Hints: Use init method to construct some parameters</p> <p>```python vscode={\"languageId\": \"plaintext\"}</p> <pre><code>&lt;!-- #region id=\"KsweINJ__XEq\" --&gt;\n## Question 6\n\n\nWrite a program that calculates and prints the value according to the given formula:\nQ = Square root of [(2 * C * D)/H]\nFollowing are the fixed values of C and H:\nC is 50. H is 30.\nD is the variable whose values should be input to your program in a comma-separated sequence.\nExample\nLet us assume the following comma separated input sequence is given to the program:\n100,150,180\nThe output of the program should be:\n18,22,24\n\nHints:\nIf the output received is in decimal form, it should be rounded off to its nearest value (for example, if the output received is 26.0, it should be printed as 26)\nIn case of input data being supplied to the question, it should be assumed to be a console input.\n&lt;!-- #endregion --&gt;\n\n```python vscode={\"languageId\": \"plaintext\"}\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#question-7","title":"Question 7","text":"<p>Write a program which takes 2 digits, X,Y as input and generates a 2-dimensional array. The element value in the i-th row and j-th column of the array should be i*j. Note: i=0,1.., X-1; j=0,1,\u00a1\u00adY-1. Example Suppose the following inputs are given to the program: 3,5 Then, the output of the program should be: [[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8]]</p> <p>Hints: Note: In case of input data being supplied to the question, it should be assumed to be a console input in a comma-separated form.</p> <p>```python vscode={\"languageId\": \"plaintext\"}</p> <pre><code>&lt;!-- #region id=\"wZjooJZf_XEr\" --&gt;\n## Question 8\n\nWrite a program that accepts a comma separated sequence of words as input and prints the words in a comma-separated sequence after sorting them alphabetically.Suppose the following input is supplied to the program:\nwithout,hello,bag,world\n\nThen, the output should be:\nbag,hello,without,world\n&lt;!-- #endregion --&gt;\n\n```python vscode={\"languageId\": \"plaintext\"}\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Exercises/Python_Exercise_%281_10%29/#question-9","title":"Question 9","text":"<p>Write a program that accepts sequence of lines as input and prints the lines after making all characters in the sentence capitalized. Suppose the following input is supplied to the program:</p> <p><pre><code>Hello world\nPractice makes perfect\n</code></pre> Then, the output should be: <pre><code>HELLO WORLD\nPRACTICE MAKES PERFECT\n</code></pre> Hints: In case of input data being supplied to the question, it should be assumed to be a console input.</p> <p>```python vscode={\"languageId\": \"plaintext\"}</p> <p><pre><code>&lt;!-- #region id=\"jwuSEoYo_XEr\" --&gt;\n## Question 10\n\nWrite a program that accepts a sequence of whitespace separated words as input and prints the words after removing all duplicate words and sorting them alphanumerically.\nSuppose the following input is supplied to the program:\n</code></pre> hello world and practice makes perfect and hello world again <pre><code>Then, the output should be:\n</code></pre> again and hello makes perfect practice world <pre><code>Hints:\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nWe use set container to remove duplicated data automatically and then use sorted() to sort the data.\n&lt;!-- #endregion --&gt;\n\n```python vscode={\"languageId\": \"plaintext\"}\n</code></pre></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/","title":"Python Strings","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#python-strings","title":"Python Strings","text":"<p>In Python, a string is a sequence of characters used to represent text. Strings are enclosed in quotes, either single (<code>'</code>), double (<code>\"</code>), triple single (<code>'''</code>), or triple double (<code>\"\"\"</code>). They are immutable, meaning they cannot be changed after creation. Strings can be concatenated, sliced, and have various methods for manipulation like <code>.lower()</code>, <code>.upper()</code>, and <code>.split()</code>. They also support formatting through f-strings and the <code>format()</code> method.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#creating-string","title":"Creating String","text":"<pre><code># Single-line strings\nmyStr = 'Hello'\nmyStr = \"Hello\"\n\n# Multi-line strings\nmyStr = '''Hello'''\nmyStr = \"\"\"Hello\"\"\"\n\n# Typecasting\nmyStr = str(myStr)\n\nprint(myStr)\n</code></pre> <p>The format <code>\"It's raining outside.\"</code> demonstrates the use of single quotes inside a string enclosed in double quotes. This is useful for including an apostrophe in the text without needing an escape character.</p> <pre><code>\"It's raining outside.\"\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#accessing-substrings-from-a-string","title":"Accessing Substrings from a String","text":"<p>In Python, both positive and negative indexing are used to access elements in sequences such as strings, lists, and tuples.</p> <ul> <li> <p>Positive Indexing: Positive indexing starts from 0, with the first element of the sequence having an index of 0, the second element an index of 1, and so on.</p> </li> <li> <p>Negative Indexing: Negative indexing starts from -1, with the last element of the sequence having an index of -1, the second-to-last element an index of -2, and so on.</p> </li> </ul> <pre><code># Positive Indexing\nmyStr = \"hello world\"\nprint(myStr[0])\nprint(myStr[6])\n# print(myStr[42]) # outputs error\n</code></pre> <pre><code># Negative Indexing\nprint(myStr[-1])\nprint(myStr[-5])\n# print(myStr[-67]) # outputs error\n</code></pre> <pre><code># Slicing\nstrSlice = myStr[:5]\nprint(strSlice)\nprint(myStr[1:5])\nprint(myStr[:])\nprint(myStr[0:6:2]) # With steps\nprint(myStr[::-1]) # Reverse string\n</code></pre> <p>When slicing with negative indexing in Python, the start index should be greater than the stop index for the slice to include elements from left to right in the sequence. This is because negative indexing counts from the end of the sequence.</p> <pre><code>print(myStr[-5:])\nprint(myStr[-1:-6:-1])\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#editing-and-deleting-strings","title":"Editing and Deleting Strings","text":"<p>Strings in Python are immutable, which means once they are created, they cannot be changed. However, you can create new strings based on modifications to existing ones. Additionally, while you cannot delete a part of a string, you can delete the entire string.</p> <pre><code>myStr = \"hello world\"\n# myStr[0] = \"H\" # outputs error\n</code></pre> <pre><code># Delete a string\ndel myStr\n</code></pre> <pre><code># # Quick question\n# # Guess the output of this code cell\n# myStr = \"hello world\"\n# del myStr[-1:-5:-2]\n# print(myStr)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#operations-on-strings","title":"Operations on Strings","text":"<p>Python allows various types of operations on strings, including arithmetic, relational, logical operations, loops, and membership operations.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#arithmetic-operations","title":"Arithmetic Operations","text":"<ol> <li>Concatenation: Combining two strings using the <code>+</code> operator.</li> <li>Repetition: Repeating a string multiple times using the <code>*</code> operator.</li> </ol> <pre><code># Concatenation\nprint(\"Hello\" + \" \" + \"World\")\n\n# Repetition\nprint(\"*\" * 50)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#relational-operations","title":"Relational Operations","text":"<p>Relational operations compare strings lexicographically (based on the Unicode value of each character).</p> <ol> <li> <p>Equality (<code>==</code>)</p> </li> <li> <p>Inequality (<code>!=</code>)</p> </li> <li> <p>Greater than (<code>&gt;</code>)</p> </li> <li> <p>Less than (<code>&lt;</code>)</p> </li> <li> <p>Greater than or equal to (<code>&gt;=</code>)</p> </li> <li> <p>Less than or equal to (<code>&lt;=</code>)</p> </li> </ol> <pre><code>\"delhi\" == \"mumbai\"\n</code></pre> <pre><code>\"mumbai\" &gt; \"pune\"\n</code></pre> <p>\ud83e\udd14 Explanation:  The output of <code>\"mumbai\" &gt; \"pune\"</code> is <code>False</code> because string comparison in Python is done lexicographically, meaning it compares strings based on the Unicode values of their characters. It follows the same rules as alphabetical ordering in dictionaries.</p> <p>Lexicographical Comparison:</p> <p>When comparing <code>\"mumbai\"</code> and <code>\"pune\"</code>, Python compares each character from the start until it finds a difference:</p> <ol> <li>Compare the first characters: <code>'m'</code> and <code>'p'</code>.</li> <li>The Unicode value of <code>'m'</code> (109) is less than the Unicode value of <code>'p'</code> (112).</li> </ol> <p>Since <code>'m'</code> is less than <code>'p'</code>, <code>\"mumbai\"</code> is considered less than <code>\"pune\"</code> lexicographically, and thus <code>\"mumbai\" &gt; \"pune\"</code> evaluates to <code>False</code>.</p> <pre><code>\"Pune\" &gt; \"pune\"\n</code></pre> <p>\ud83e\udd14 Explanation: </p> <p>The output of <code>\"Pune\" &gt; \"pune\"</code> is <code>False</code> because of the way Python handles lexicographical comparisons, which are based on the Unicode values of the characters. In Unicode, uppercase letters have lower values than lowercase letters. When comparing <code>\"Pune\"</code> and <code>\"pune\"</code>, Python compares each character from the start until it finds a difference:</p> <ol> <li>Compare the first characters: <code>'P'</code> and <code>'p'</code>.</li> <li>The Unicode value of <code>'P'</code> (80) is less than the Unicode value of <code>'p'</code> (112).</li> </ol> <p>Since <code>'P'</code> is less than <code>'p'</code>, <code>\"Pune\"</code> is considered less than <code>\"pune\"</code> lexicographically, and thus <code>\"Pune\" &gt; \"pune\"</code> evaluates to <code>False</code>.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#logical-operations","title":"Logical Operations","text":"<pre><code>\"hello\" and \"world\"\n</code></pre> <p>\ud83e\udd14 Explanation: </p> <p>The expression <code>\"hello\" and \"world\"</code> in Python evaluates to <code>\"world\"</code> due to the way the <code>and</code> logical operator functions. In Python, the <code>and</code> operator returns the first falsy value it encounters or the last value if all values are truthy. Both <code>\"hello\"</code> and <code>\"world\"</code> are non-empty strings, and non-empty strings are considered truthy in Boolean contexts. Since both operands are truthy, the <code>and</code> operator returns the last value, which is <code>\"world\"</code>. This behavior ensures that if any operand in the chain is falsy, it stops evaluating further and returns that falsy value; otherwise, it returns the last operand, which in this case is <code>\"world\"</code>.</p> <pre><code>\"hello\" or \"world\"\n</code></pre> <p>\ud83e\udd14 Explanation:</p> <p>The expression <code>\"hello\" or \"world\"</code> in Python evaluates to <code>\"hello\"</code> due to the behavior of the <code>or</code> logical operator. In Python, the <code>or</code> operator returns the first truthy value it encounters or the last value if all are falsy. In this expression, <code>\"hello\"</code> is a non-empty string, which is considered truthy in a Boolean context. Because <code>\"hello\"</code> is truthy, the <code>or</code> operator does not need to evaluate the second operand, <code>\"world\"</code>, and immediately returns <code>\"hello\"</code>. This mechanism ensures that the <code>or</code> operator returns the first truthy value found, which in this case is <code>\"hello\"</code>, making the entire expression evaluate to <code>\"hello\"</code>.</p> <pre><code>\"\" and \"world\"\n</code></pre> <p>\ud83e\udd14 Explanation:</p> <p>The expression <code>\"\" and \"world\"</code> in Python evaluates to <code>\"\"</code> because of how the <code>and</code> logical operator works. In Python, the <code>and</code> operator returns the first falsy value it encounters or the last value if all operands are truthy. In this case, <code>\"\"</code> is an empty string, which is considered falsy in a Boolean context. As a result, when evaluating the expression <code>\"\" and \"world\"</code>, Python immediately encounters the falsy value <code>\"\"</code> and returns it without evaluating the second operand, <code>\"world\"</code>. This behavior ensures that the <code>and</code> operator stops at the first falsy value and returns it, making the entire expression evaluate to <code>\"\"</code>.</p> <pre><code>\"\" or \"world\"\n</code></pre> <p>\ud83e\udd14 Explanation: </p> <p>The expression <code>\"\" or \"world\"</code> in Python evaluates to <code>\"world\"</code> due to the behavior of the <code>or</code> logical operator. In Python, the <code>or</code> operator returns the first truthy value it encounters or the last value if all operands are falsy. In this case, <code>\"\"</code> is an empty string, which is considered falsy in a Boolean context. When evaluating the expression <code>\"\" or \"world\"</code>, Python first encounters the falsy value <code>\"\"</code> and then moves on to evaluate the next operand, <code>\"world\"</code>, which is a non-empty string and therefore truthy. Since <code>\"world\"</code> is the first truthy value in the expression, the <code>or</code> operator returns <code>\"world\"</code>. This demonstrates how the <code>or</code> operator ensures that the first truthy value is returned, making the entire expression evaluate to <code>\"world\"</code>.</p> <pre><code>print(not \"\")\nprint(not \"hello\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#loops-on-strings","title":"Loops on Strings","text":"<p>Strings in Python are iterable, meaning you can loop through each character in a string using different types of loops. This allows you to perform operations or process each character individually.</p> <pre><code>for i in \"hello\":\n    print(i)\n</code></pre> <pre><code># # Quick question\n# # Guess the output of this code cell\n# for i in \"delhi\":\n#     print(\"pune\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#membership-operations","title":"Membership Operations","text":"<p>Membership operations in Python allow you to check whether a substring exists within another string. These operations are done using the <code>in</code> and <code>not in</code> operators.</p> <pre><code>\"D\" in \"Delhi\"\n</code></pre> <pre><code>\"d\" not in \"Delhi\"\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#string-functions","title":"String Functions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#common-functions","title":"Common Functions","text":"<p>Python provides several built-in functions to perform operations on strings. Four commonly used functions are <code>len()</code>, <code>max()</code>, <code>min()</code>, and <code>sorted()</code>.</p> <ol> <li> <p><code>len()</code>: The <code>len()</code> function returns the number of characters in a string.</p> </li> <li> <p><code>max()</code>: The <code>max()</code> function returns the character with the highest Unicode value from the string. If the string is empty, it raises a <code>ValueError</code>.</p> </li> <li> <p><code>min()</code>: The <code>min()</code> function returns the character with the lowest Unicode value from the string. If the string is empty, it raises a <code>ValueError</code>.</p> </li> <li> <p><code>sorted()</code>: The <code>sorted()</code> function returns a list of characters from the string, sorted in ascending order based on their Unicode values.</p> </li> </ol> <pre><code>myStr = \"Hello\"\n\nprint(len(myStr))\nprint(max(myStr))\nprint(min(myStr))\nprint(sorted(myStr)) # Sorted in ascending order\nprint(sorted(myStr, reverse=True)) # Sorted in descending order\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#capitalizetitleupperlowerswapcase","title":"Capitalize/Title/Upper/Lower/Swapcase","text":"<ol> <li> <p><code>capitalize()</code>: Capitalizes the first character of the string and converts all other characters to lowercase.</p> </li> <li> <p><code>title()</code>: Capitalizes the first character of each word in the string and converts all other characters to lowercase.</p> </li> <li> <p><code>upper()</code>: Converts all characters in the string to uppercase.</p> </li> <li> <p><code>lower()</code>: Converts all characters in the string to lowercase.</p> </li> <li> <p><code>swapcase()</code>: Swaps the case of all characters in the string; converts uppercase characters to lowercase and vice versa.</p> </li> </ol> <pre><code>myStr = \"HeLLo WoRLd\"\n\nprint(myStr.capitalize())\nprint(myStr.title())\nprint(myStr.upper())\nprint(myStr.lower())\nprint(myStr.swapcase())\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#countfindindex","title":"Count/Find/Index","text":"<ol> <li> <p><code>count()</code>: Returns the number of occurrences of a specified substring in the string.</p> </li> <li> <p><code>find()</code>: Returns the lowest index of the specified substring if it is found in the string; otherwise, it returns <code>-1</code>.</p> </li> <li> <p><code>index()</code>: Returns the lowest index of the specified substring if it is found in the string; otherwise, it raises a <code>ValueError</code>.</p> </li> </ol> <pre><code>myStr = \"My name is Krishnagopal\"\n\nprint(myStr.count(\"i\"))\nprint(myStr.find(\"is\"))\nprint(myStr.index(\"is\"))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#endswithstartswith","title":"Endswith/Startswith","text":"<ol> <li> <p><code>endswith()</code>: Checks if the string ends with a specified suffix. It returns <code>True</code> if the string ends with the suffix, and <code>False</code> otherwise.</p> </li> <li> <p><code>startswith()</code>: Checks if the string starts with a specified prefix. It returns <code>True</code> if the string starts with the prefix, and <code>False</code> otherwise.</p> </li> </ol> <pre><code>print(myStr.startswith(\"My\"))\nprint(myStr.startswith(\"name\"))\n\nprint(myStr.endswith(\"l\"))\nprint(myStr.endswith(\"is\"))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#format","title":"Format","text":"<p>The <code>format()</code> method in Python is used to format strings by embedding values within them. It allows you to insert and format values into a string using curly braces <code>{}</code> as placeholders.</p> <pre><code>first_name = \"Krishnagopal\"\nlast_name = \"Halder\"\n\n\"My name is {} {}\".format(first_name, last_name)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#isalnumisalphaisdigitisidentifier","title":"isalnum/isalpha/isdigit/isidentifier","text":"<ol> <li> <p><code>isalnum()</code>: Returns <code>True</code> if all characters in the string are alphanumeric (i.e., letters and digits) and there is at least one character; otherwise, it returns <code>False</code>.</p> </li> <li> <p><code>isalpha()</code>: Returns <code>True</code> if all characters in the string are alphabetic (i.e., letters) and there is at least one character; otherwise, it returns <code>False</code>.</p> </li> <li> <p><code>isdigit()</code>: Returns <code>True</code> if all characters in the string are digits and there is at least one character; otherwise, it returns <code>False</code>.</p> </li> <li> <p><code>isidentifier()</code>: Returns <code>True</code> if the string is a valid identifier (i.e., it starts with a letter or an underscore and consists of letters, digits, or underscores), and <code>False</code> otherwise.</p> </li> </ol> <pre><code>print(\"Alpha123\".isalnum())\nprint(\"Krishnagopal\".isalpha())\nprint(\"3214\".isdigit())\nprint(\"fist_name\".isidentifier())\nprint(\"1first_name\".isidentifier())\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#splitjoin","title":"Split/Join","text":"<p>Here\u2019s a brief overview of the <code>split()</code> and <code>join()</code> string methods in Python:</p> <ol> <li><code>split()</code></li> <li>Description: Splits a string into a list of substrings based on a specified delimiter (separator). By default, it splits on any whitespace and removes extra whitespace.</li> <li>Syntax: <code>string.split(separator, maxsplit)</code></li> <li><code>separator</code> (optional): The delimiter on which to split the string. If not specified, whitespace is used.</li> <li> <p><code>maxsplit</code> (optional): The maximum number of splits to perform. The default value <code>-1</code> means \"all occurrences.\"</p> </li> <li> <p><code>join()</code></p> </li> <li>Description: Joins elements of an iterable (such as a list or tuple) into a single string, with a specified separator between each element.</li> <li>Syntax: <code>separator.join(iterable)</code></li> <li><code>separator</code>: The string used as a separator between elements of the iterable.</li> <li><code>iterable</code>: The iterable whose elements will be joined into a single string.</li> </ol> <pre><code>myStr = \"My name is Krishnagopal Halder\"\n\nprint(myStr.split())\nprint(myStr.split(\"is\"))\n</code></pre> <pre><code>print(\" \".join(['My', 'name', 'is', 'Krishnagopal', 'Halder']))\nprint(\"-\".join(['My', 'name', 'is', 'Krishnagopal', 'Halder']))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#replace","title":"Replace","text":"<p>The <code>replace()</code> method is used to replace occurrences of a specified substring with another substring within a string. </p> <ul> <li>Usage: This method is useful for replacing parts of a string with another string.</li> <li>Syntax: <code>string.replace(old, new, count)</code></li> <li><code>old</code>: The substring to be replaced.</li> <li><code>new</code>: The substring to replace the old substring with.</li> <li><code>count</code> (optional): The maximum number of occurrences to replace. If not specified, all occurrences are replaced.</li> </ul> <pre><code>myStr.replace(\"Krishnagopal Halder\", \"Krishna\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#strip","title":"Strip","text":"<p>The <code>strip()</code> method is used to remove leading and trailing whitespace (or specified characters) from a string.</p> <ul> <li>Usage: This method is useful for cleaning up strings by removing unwanted whitespace or specific characters from both ends of the string.</li> <li>Syntax: <code>string.strip([chars])</code></li> <li> <p><code>chars</code> (optional): A string specifying the set of characters to be removed. If not specified, the method removes whitespace by default.</p> </li> <li> <p>Variants</p> </li> <li> <p><code>strip()</code>: Removes characters from both ends of the string.</p> </li> <li><code>lstrip()</code>: Removes characters from the left end of the string.</li> <li><code>rstrip()</code>: Removes characters from the right end of the string.</li> </ul> <pre><code>\"Krishnagopal Halder         \".strip()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings/#exercises","title":"Exercises","text":"<pre><code># Find the length of a given string without using the len() function\n</code></pre> <pre><code># Extract username from a given email. \n# Eg if the email is halder24krishnagopal@gmail.com \n# then the username should be halder24krishnagopal\n</code></pre> <pre><code># Count the frequency of a particular character in a provided string. \n# Eg 'hello how are you' is the string, the frequency of h in this string is 2.\n</code></pre> <pre><code># Write a program which can remove a particular character from a string.\n</code></pre> <pre><code># Write a program that can check whether a given string is palindrome or not.\n# abba\n# malayalam\n</code></pre> <pre><code># Write a program to count the number of words in a string without split()\n</code></pre> <pre><code># Write a python program to convert a string to title case without using the title()\n</code></pre> <pre><code># Write a program that can convert an integer to string.\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings_solutions/","title":"Python Strings Solutions","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/03_python_strings_solutions/#python-strings-exercises","title":"Python Strings - Exercises","text":"<pre><code># Find the length of a given string without using the len() function\n\nlen_of_string = 0\n\nstr_input = input(\"Enter your string: \")\n\nfor i in str_input:\n    len_of_string += 1\n\nprint(\"Input string:\", str_input)\nprint(\"The length of the string is:\", len_of_string)\n</code></pre> <pre><code># Extract username from a given email. \n# Eg if the email is halder23krishnagopal@gmail.com \n# then the username should be halder23krishnagopal\n\nemail = input(\"Enter your email: \")\n\nusername = email.split(\"@\")[0]\n\nprint(\"Email: \", email)\nprint(\"Username:\", username)\n</code></pre> <pre><code># Count the frequency of a particular character in a provided string. \n# Eg 'hello how are you' is the string, the frequency of h in this string is 2.\n\ninput_string = input(\"Enter your string: \")\ninput_character = input(\"Enter the character: \")\n\nchar_frequency = input_string.count(input_character)\n\nprint(\"Input string: \", input_string)\nprint(\"The frequency of\", input_character, \"in the string is\", char_frequency)\n</code></pre> <pre><code># Write a program which can remove a particular character from a string.\n\ninput_string = input(\"Enter your string: \")\nchar_to_be_removed = input(\"Enter the character to be removed: \")\nnew_string = \"\"\n\nfor i in input_string:\n    if i == char_to_be_removed:\n        continue\n    new_string = new_string + i\n\nprint(\"Input string: \", input_string)\nprint(\"Character to be removed: \", char_to_be_removed)\nprint(\"New string:\", new_string)\n</code></pre> <pre><code># Write a program that can check whether a given string is palindrome or not.\n# abba\n# malayalam\n\ninput_string = input(\"Enter your string: \")\n\nreversed_string = input_string[::-1]\n\nprint(\"Input string: \", input_string)\nprint(\"Reversed string: \", reversed_string)\n\nif input_string == reversed_string:\n    print(\"The string is palindrome.\")\n\nelse:\n    print(\"The string is not a palindrome.\")\n</code></pre> <pre><code># Write a program to count the number of words in a string without split()\n\ninput_string = input(\"Enter your string: \")\n\nwords = list()\nnumber_of_words = 0\nword = \"\"\n\nfor i in input_string:\n    if i != \" \":\n        word = word + i\n\n    else:\n        words.append(word)\n        word = \"\"\nwords.append(word)\n\nprint(\"Input string: \", input_string)\nprint(\"Words:\", words)\nprint(\"The number of words in the string:\", len(words))\n</code></pre> <pre><code># Write a python program to convert a string to title case without using the title()\n\ninput_string = input(\"Enter your string: \")\ntitle_words = []\n\nfor word in input_string.split(\" \"):\n    new_word = word[0].upper() + word[1:].lower()\n    title_words.append(new_word)\n\nprint(\"Input string: \", input_string)\nprint(\"String in title case: \", \" \".join(title_words))\n</code></pre> <pre><code># Write a program that can convert an integer to string.\n\ninput_int = input(\"Enter your integer: \")\n\nprint(\"Input integer:\", int(input_int))\nprint(\"Datatype:\", type(input_int))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/","title":"Time Complexity","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#time-complexity","title":"Time Complexity","text":"<p>Time complexity is a computational concept that describes the amount of time an algorithm takes to complete as a function of the length of the input. It provides an upper bound on the time an algorithm will take to run and is typically expressed using Big O notation, which describes the worst-case scenario.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#what-is-efficiency-in-programming","title":"What is Efficiency in Programming?","text":"<p>Efficiency in programming refers to how effectively an algorithm or code performs in terms of resource usage. This includes both time efficiency (how fast the code runs) and space efficiency (how much memory the code uses). Efficient code performs its intended task using the least amount of computational resources possible, balancing both time and space constraints.</p> <p>Key aspects of efficiency in programming include:</p> <ol> <li>Time Efficiency:</li> <li>Execution Time: How quickly an algorithm completes its task. This is often evaluated using time complexity analysis (e.g., Big O notation).</li> <li> <p>Response Time: How long it takes for a system or application to respond to a user action or request.</p> </li> <li> <p>Space Efficiency:</p> </li> <li>Memory Usage: The amount of memory an algorithm or program uses during execution. This is often evaluated using space complexity analysis.</li> <li>Data Structures: Choosing the most appropriate data structures can significantly impact memory usage and access times.</li> </ol> <p>Efficient programming is crucial for creating high-performance applications, especially in environments with limited resources or where performance is critical, such as real-time systems, large-scale data processing, and high-frequency trading systems.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#techniques-to-measure-time-complexity","title":"Techniques to Measure Time Complexity","text":"<p>Measuring time efficiency involves various techniques to evaluate how quickly an algorithm or program executes. Here are some common techniques:</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#1-measuring-time-to-execute","title":"1. Measuring Time to Execute","text":"<p>Manually measure the execution time of a code block using built-in functions. Measuring time to execute is a common approach to evaluating the performance of an algorithm or piece of code. However, this method has several limitations and potential problems:</p> <ol> <li>System Dependence</li> <li>Hardware Variability: Execution time can vary significantly across different hardware configurations (e.g., different CPUs, memory speeds).</li> <li> <p>Operating System Variability: Different operating systems or even different states of the same OS can affect performance due to background processes and system load.</p> </li> <li> <p>Environment Variability</p> </li> <li>Background Processes: Other running applications or processes can consume CPU time and memory, leading to inconsistent measurements.</li> <li> <p>Network Latency: For programs that depend on network resources, network latency and bandwidth fluctuations can affect execution time.</p> </li> <li> <p>Measurement Overhead</p> </li> <li>Timing Overhead: The act of measuring execution time itself can introduce overhead, especially for very short code segments, making the measurement less accurate.</li> <li>Profiling Overhead: Using profiling tools can add significant overhead, distorting the actual performance characteristics of the code.</li> </ol> <pre><code>import time\n\nstart = time.time()\n\nfor i in range(100):\n    print(i)\n\nend = time.time()\n\nprint(\"Time taken:\", end-start, \"seconds.\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#2-counting-operations-involved","title":"2. Counting Operations Involved","text":"<p>Manually count the number of key operations (e.g., comparisons, assignments) in the code to estimate its time complexity.</p> <ul> <li> <p>Advantages:</p> </li> <li> <p>Hardware Independence: Provides a measure of algorithm efficiency that is not affected by the underlying hardware or system   environment.</p> </li> <li> <p>Theoretical Insight: Offers a clear understanding of the algorithm's behavior and complexity, helping to predict performance for different input sizes.</p> </li> <li> <p>Granularity: Allows for detailed analysis of specific parts of the algorithm, identifying potential bottlenecks and areas for optimization.</p> </li> <li> <p>Predictability: Enables consistent and repeatable results, as the operation count is deterministic for a given input.</p> </li> <li> <p>Disadvantages:</p> </li> <li> <p>Complexity in Implementation: Manually counting operations can be tedious and error-prone, especially for complex algorithms.</p> </li> <li> <p>Simplification Assumptions: Often focuses on a single type of operation (e.g., comparisons, swaps), which may oversimplify the actual performance characteristics.</p> </li> <li> <p>Ignoring Constant Factors: Does not account for constant time operations or overheads, which can be significant in practical scenarios.</p> </li> <li> <p>No Direct Execution Time: Provides an abstract measure of complexity but does not translate directly to actual execution time, which is also influenced by factors like memory access patterns and cache performance.</p> </li> </ul> <pre><code># Function to convert Celsius (\u00b0C) value to its Fahrenheit (\u00b0F)\ndef cel_to_farh(c):\n    return c * 9/5 + 32 # total 3 operations\n\n\n# Function to compute sum of digits in all numbers from 1 to n\ndef mysum(n):\n    sum = 0 # 1st operation\n    for i in range(n+1): # 3n operations\n        sum += i\n    return sum # total 1 + 3n operations\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#3-orders-of-growth","title":"3. Orders of Growth","text":"<p>Orders of growth are used to describe the asymptotic behavior of an algorithm's time or space complexity as the input size increases. This concept helps in comparing the efficiency of different algorithms, particularly for large input sizes. </p> <p>Goals:  The goals of understanding and analyzing orders of growth in algorithm design and analysis are fundamental to creating efficient and scalable software. Here are the key goals: 1. Want to evaluate program's efficiency when the input is very big. 2. Want to express the growth of program's run time as input size grows. 3. Want to put an upper bound on growth - as tight as possible. 4. Do not need to be precise: \"order of\" not \"exact\" growth. 5. We will look at the largest factors in run time (which section of the program will take the longest to run?) 6. Thus, generally we want tight upper bound on growth, as function of the size input, in worst case.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/04_time_complexity/#exact-steps-vs-o","title":"Exact Steps vs O()","text":"<ul> <li>Computes factorial</li> <li>Number of steps</li> <li>Worst case asymptotic complexity:</li> <li>ignore additive constants</li> <li>ignore multiplicative constants</li> </ul> <pre><code># Write a function to calculate the factorial of any given number\ndef factorial(n):\n    \"\"\"assuming n is an integer &gt;= 0\"\"\"\n    result = 1 # 1 operation\n    while n &gt; 1: # 1 operation\n        result *= n # 2 operations\n        n -= 1 # 2 operations\n    return result # 1 operation\n\n# Total operations: 5n + 2\n</code></pre> <pre><code>\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/","title":"Functions In Python","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#functions-in-python","title":"Functions in Python","text":"<p>In Python, functions are blocks of reusable code that perform a specific task. Functions help to organize code, avoid repetition, and improve modularity. They allow you to define a piece of logic once and use it multiple times throughout your program.</p> <p></p> <p>Abstraction and decomposition are two key concepts in programming that help manage complexity, especially when using functions.</p> <ol> <li> <p>Abstraction: Abstraction is the process of hiding the complex details of a task and exposing only the essential features or functionalities. In the context of functions, abstraction means creating a function that performs a specific task without requiring the user of the function to know how it works internally.</p> </li> <li> <p>Decomposition: Decomposition is the process of breaking down a large problem into smaller, more manageable parts. In the context of functions, it involves splitting a complex task into simpler sub-tasks, where each sub-task can be handled by a separate function. This approach makes code easier to understand, test, and maintain.</p> </li> </ol>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#syntax-of-a-function","title":"Syntax of a Function","text":"<p>The syntax of a function in Python includes the function header, the function body, and optionally a return statement. Here's a breakdown of the structure:</p> <p></p> <pre><code># Write a simple function to check whether a number is even or not\ndef is_even(i):\n    \"\"\"\n    Optional docstrings which tells us about the function, things like what inputs are required, what will the function return.\n\n    Args:\n        i (int): The input number to check.\n\n    Returns:\n        bool: True if the number is even, False otherwise.\n    \"\"\"\n\n    if i % 2 == 0:\n        return True\n    else:\n        return False\n\n# Check the function (Function calling)\nis_even(3)\n</code></pre> <pre><code># Print the ddcumentation of a function\nprint(is_even.__doc__)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#two-point-of-views","title":"Two Point of Views","text":"<ul> <li>Function Creator's View:</li> <li>Clarity and modularity: Design the function to be reusable, well-organized, and easy to understand.</li> <li> <p>Robustness: Handle different input cases and ensure reliable performance with proper documentation.</p> </li> <li> <p>Function User's View:</p> </li> <li>Simplicity: The function should be easy to use with clear inputs and outputs.</li> <li>Reliability: The user expects the function to work correctly without needing to know its internal workings.</li> </ul> <pre><code># Modify the function to handle invalid data types gracefully without throwing an error\ndef is_even(i):\n    \"\"\"Optional docstrings which tells us about the function, things like what inputs are required, what will the function return\n\n    Args:\n        i (int): The input number to check.\n\n    Returns:\n        bool: True if the number is even, False otherwise.\n    \"\"\"\n\n    if type(i) == int:\n        if i % 2 == 0:\n            return True\n        else:\n            return False\n    else:\n        print(\"Are you Mad?\")\n\n# Check the function (Function calling)\nis_even(\"y\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#parameters-vs-arguments","title":"Parameters vs Arguments","text":"<ul> <li>Parameters are the variables listed inside the function's definition. They act as placeholders for the values that the function expects to receive.</li> </ul> <p>Example:   <pre><code>def greet(name):  # 'name' is a parameter\n    print(f\"Hello, {name}!\")\n</code></pre></p> <ul> <li>Arguments are the actual values passed to the function when it is called. These values are assigned to the corresponding parameters.</li> </ul> <p>Example:   <pre><code>greet(\"Alice\")  # \"Alice\" is an argument\n</code></pre></p> <p>Key Differences: - Parameters are used when defining a function. - Arguments are the actual values supplied to the function during execution.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#types-of-arguments","title":"Types of Arguments","text":"<p>In Python, there are several types of arguments that can be passed to a function. These include:</p> <ol> <li>Default Arguments: A default argument in Python is an argument that takes a default value if no value is provided for it when the function is called. If the caller does not provide a corresponding argument during the function call, the default value is used.</li> </ol> <pre><code># Example\ndef greet(name, age=18): # Default argument for 'age'\n    print(f\"Hello, {name}! You are {age} years old.\")\n\ngreet(\"Aditi\")  # Only 'name' is provided; 'age' uses its default value of 18\n</code></pre> <ol> <li>Positional Arguments: Positional arguments are arguments that are passed to a function in the correct positional order, meaning the first argument is assigned to the first parameter, the second argument to the second parameter, and so on. The order in which arguments are passed matters, and they must match the function parameters' positions.</li> </ol> <pre><code># Example\ndef greet(name, age):\n    print(f\"Hello, {name}! You are {age} years old.\")\n\n# Calling the function with positional arguments\ngreet(\"Aditi\", 18)  # Output: Hello, Aditi! You are 18 years old.\n</code></pre> <ol> <li>Keyword Arguments: Keyword arguments are arguments that are passed to a function using the name of the parameter explicitly, allowing you to assign values to specific parameters regardless of their order in the function definition. This makes the function call more readable and flexible.</li> </ol> <pre><code># Example\ndef greet(name, age):\n    print(f\"Hello, {name}! You are {age} years old.\")\n\n# Calling the function with keyword arguments\ngreet(age=18, name=\"Aditi\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#args-vs-kwargs","title":"<code>*args</code> vs <code>**kwargs</code>","text":"<p>In Python, <code>*args</code> and <code>**kwargs</code> are used in function definitions to allow the function to accept an arbitrary number of arguments. They are particularly useful when you don't know how many arguments will be passed to the function.</p> <p>1. <code>*args</code> (Non-keyword variable-length arguments): - <code>*args</code> allows a function to accept any number of positional arguments. These arguments are passed as a tuple inside the function.</p> <ul> <li>Key Points about <code>*args</code>:</li> <li>You can pass any number of positional arguments.</li> <li>Inside the function, <code>args</code> is treated as a tuple.</li> <li>The <code>*</code> is required before the parameter name (<code>args</code> can be replaced with any name, e.g., <code>*numbers</code>).</li> </ul> <pre><code># Write a function to calculate average of n numbers\ndef average(*args):\n\n    sum_of_nums = 0\n    for i in args:\n        sum_of_nums += i\n\n    return sum_of_nums / len(args)\n\n# Call the function\naverage(2, 4, 6)\n</code></pre> <p>2. <code>**kwargs</code> (Keyword variable-length arguments): - <code>**kwargs</code> allows a function to accept any number of keyword arguments. These arguments are passed as a dictionary inside the function.</p> <ul> <li>Key Points about <code>**kwargs</code>:</li> <li>You can pass any number of keyword arguments.</li> <li>Inside the function, <code>kwargs</code> is treated as a dictionary where the keys are parameter names, and the values are the corresponding arguments.</li> <li>The <code>**</code> is required before the parameter name (<code>kwargs</code> can be replaced with any name, e.g., <code>**info</code>).</li> </ul> <pre><code># Write a function to display the information of a person\ndef display_info(**kwargs):\n\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\ndisplay_info(name=\"Aditi\", age=18, city=\"Bonn\", country=\"Germany\")\n</code></pre> <p>3. Combining <code>*args</code> and <code>**kwargs</code>:    - We can also write a function that combines normal parameters, <code>*args</code>, and <code>**kwargs</code>. But in this case, the parameters must follow a specific order:</p> <pre><code> 1. **Regular Parameters**: Required positional parameters come first.\n 2. **`*args`**: Allows for additional positional arguments (as a tuple) and must be placed after regular parameters.\n 3. **Default Parameters**: Parameters with default values follow `*args`.\n 4. **`**kwargs`**: Allows for additional keyword arguments (as a dictionary) and must be placed last.\n</code></pre> <pre><code># Example\ndef employee_info(name, age, *args, company=\"Google\", **kwargs):\n    \"\"\"\n    Display the information about an employee.\n\n    Args:\n        name (str): The employee's name\n        age (int): The employee's age\n        *args: Additional positional arguments (e.g., hobbies).\n        company (str, optional): Company name. Defaults to \"Google\".\n        **kwargs: Additional keyword arguments (e.g., address, skills).\n\n    Returns:\n        None\n    \"\"\"\n\n    # Print the normal parameters\n    print(f\"Name: {name}\")\n    print(f\"Age: {age}\")\n\n    # Print the default parameter\n    print(f\"Company: {company}\")\n\n    # Print additional positional arguments\n    if args:\n        print(\"Hobbies:\")\n        for i in args:\n            print(f\"\\t{i}\")\n\n    # Print additional keyword arguments\n    if kwargs:\n        for key, value in kwargs.items():\n            print(f\"{key}: {value}\")\n\n# Call the function\nemployee_info(\"Sundar Pichai\", 48, \"Watching Cricket\", \"Listening Music\", \n              email=\"pichaiS@gmail.com\", phone=123456789)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#how-functions-are-executed-in-memory","title":"How Functions are Executed in Memory?","text":"<p>When a function runs, here\u2019s what happens in simple terms:</p> <ol> <li> <p>Function Call: When you call a function, a special \"box\" (called a stack frame) is created to hold everything the function needs, like its inputs (parameters) and temporary variables.</p> </li> <li> <p>Memory Use: The function uses two main areas in memory:</p> </li> <li>Stack: For storing its inputs and local variables (small, short-term things).</li> <li> <p>Heap: For bigger, longer-lasting things like objects or large data (if needed).</p> </li> <li> <p>Running the Function: The computer follows the instructions in the function, using the data in the stack to do its job.</p> </li> <li> <p>Finishing Up: When the function is done, it gives back a result (if needed) and removes the \"box\" (stack frame) it used, so the memory is freed up for the next function.</p> </li> <li> <p>Memory Cleanup: If the function made any big objects or data, the computer may clean them up later to free up space (in some languages, this happens automatically). </p> </li> </ol> <p>So, the function gets its own space to work, does its job, and then cleans up when it\u2019s done. To visualize the concept, please try this website: https://pythontutor.com/</p> <pre><code># Try the tool with this function\n# Write a simple function to check whether a number is even or not\ndef is_even(i):\n\n    if i % 2 == 0:\n        return True\n    else:\n        return False\n\n# Check the function (Function calling)\nis_even(3)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#functions-with-no-return-statement","title":"Functions with No Return Statement","text":"<p>When a function doesn't have a <code>return</code> statement, it still runs and does its tasks, but it doesn\u2019t send any value back to where it was called. In most programming languages, such functions automatically return a special value, like <code>None</code> in Python, or they just complete without returning anything in other languages like C or Java.</p> <p>What Happens: 1. Execution: The function runs and performs its operations just like any other function. 2. No Return: If there\u2019s no <code>return</code> statement, the function simply finishes after its last instruction. 3. Default Return: In languages like Python, the function automatically returns <code>None</code>, meaning \"nothing.\" In other languages, it may not return any value at all.</p> <pre><code>L = [1, 2, 3]\nprint(L.append(4))\nprint(L)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#variable-scope","title":"Variable Scope","text":"<p>Variable scope refers to the part of a program where a variable can be accessed or used. There are different scopes that determine where a variable is visible and how long it exists in memory.</p> <p>Types of Variable Scope:</p> <ol> <li>Local Scope:</li> <li>Variables defined inside a function or block are local variables.</li> <li>They can only be accessed within that function or block.</li> <li> <p>Once the function finishes, the local variables are deleted from memory.</p> </li> <li> <p>Global Scope:</p> </li> <li>Variables defined outside all functions or blocks are global variables.</li> <li>They can be accessed from anywhere in the program, including inside functions.</li> <li>Global variables exist throughout the program\u2019s execution.</li> </ol> <pre><code># Example-1\ndef g(y): # 'y' is a local variable\n    print(x)\n    print(x+1)\n\nx = 5 # 'x' is a global variable\ng(x)\nprint(x)\n</code></pre> <ul> <li><code>x</code> is a global variable, accessible both inside and outside the function <code>g()</code>. It remains unchanged after the function call.</li> <li><code>y</code> is a local variable to the function <code>g()</code>, only available inside the function, but it is not used in this code.</li> <li>Inside the function, <code>x</code> refers to the global <code>x</code>, since there's no local <code>x</code> defined in the function.</li> </ul> <pre><code># Exmaple-2\ndef f(y):\n    x = 1 # here, 'x' is a local variable\n    x += 1\n    print(x)\n\nx = 5 # here, 'x' is a global varialble. There is no relation with the variable 'x' in the local scope\nf(x)\nprint(x)\n</code></pre> <ul> <li><code>x</code> (global): Defined outside the function (<code>x = 5</code>), accessible globally but not affected by the function <code>f()</code>.</li> <li><code>x</code> (local): Defined inside the function (<code>x = 1</code>), exists only within the function <code>f()</code> and is independent of the global <code>x</code>.</li> </ul> <pre><code># # Example-3\n# def h(y):\n#     x += 1\n\n# x = 5\n# h(x)\n# print(x)\n</code></pre> <ul> <li><code>x</code> (global): Defined outside the function (<code>x = 5</code>), intended to be accessible globally.</li> <li><code>x</code> (local): Inside the function <code>h(y)</code>, there is an attempt to modify <code>x</code> using <code>x += 1</code>, but no local <code>x</code> is defined. This leads to an error because the function tries to modify the global <code>x</code> without declaring it as global inside the function. </li> </ul> <pre><code># Example-3\ndef h(y):\n    global x\n    x += 1\n\nx = 5\nh(x)\nprint(x)\n</code></pre> <ul> <li><code>x</code> (global): The variable <code>x</code> is defined globally with <code>x = 5</code> and is accessed and modified inside the function <code>h(y)</code> using the <code>global</code> keyword. </li> <li><code>x</code> (local): Inside the function <code>h(y)</code>, <code>x</code> is treated as a global variable because of the <code>global</code> keyword. This allows the function to modify the global <code>x</code>.</li> </ul> <pre><code># Example-4\ndef f(x):\n    x = x + 1\n    print(\"in f(x): x =\", x)\n    return x\n\nx = 3\nz = f(x)\nprint(\"in main program scope: z =\", z)\nprint(\"in main program scope: x =\", x)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#nested-functions","title":"Nested Functions","text":"<p>A nested function is a function that is defined inside another function. In Python (and many other programming languages), you can define functions inside other functions to create more modular and maintainable code. The inner function is encapsulated within the outer function, meaning it is only accessible within the outer function and not from the outside scope.</p> <p>Key points about nested functions: 1. Scope: The inner function can access variables from the outer function, but the outer function cannot access the inner function's variables unless returned explicitly. 2. Encapsulation: Nested functions are useful for hiding helper functions that are not meant to be accessed from outside. 3. Closures: If the inner function captures the variables of the outer function and keeps them in memory after the outer function has finished executing, this is called a closure.</p> <pre><code># Example-1\ndef outer_function():\n    def inner_function():\n        print(\"inside inner function\")\n    inner_function()\n    print(\"inside outer function\")\n\nouter_function()\n</code></pre> <pre><code># Example-2\ndef g(x):\n    def h():\n        x = \"abc\"\n    x = x + 1\n    print(\"in g(x): x =\", x)\n    h()\n    return x\n\nx = 3\nz = g(x)\n</code></pre> <pre><code># Example-3\ndef g(x):\n    def h(x):\n        x = x+1\n        print(\"in h(x): x =\", x)\n    x = x + 1\n    print(\"in g(x): x =\", x)\n    h(x)\n    return x\n\nx = 3\nz = g(x)\nprint(\"in main program scope: x =\", x)\nprint(\"in main program scope: z =\", z)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#functions-are-1st-class-citizens","title":"Functions are 1st Class Citizens","text":"<p>In Python, functions are first-class citizens, meaning they are treated like any other object/datatype, such as integers, strings, or lists. This concept allows functions to be used flexibly and passed around within the code just like other objects.</p> <pre><code># type and id\ndef square(num):\n    return num**2\n\nprint(type(square))\nprint(id(square))\n</code></pre> <pre><code># reassign\nx = square\nprint(id(x))\nprint(x(3))\n</code></pre> <pre><code># deleting a function\ndel square\n</code></pre> <pre><code># storing\nL = [1, 2, 3, 4, square]\nL[-1](3)\n</code></pre> <pre><code>S = {square}\nS # Function is immutable as it can be stored in a set\n</code></pre> <pre><code># returning a function\ndef f():\n    def x(a, b):\n        return a+b\n    return x\n\nval = f()(3, 4)\nprint(val)\n</code></pre> <pre><code># function as argument\ndef func_a():\n    print(\"inside func_a\")\n\ndef func_b(z):\n    print(\"inside func_b\")\n    return z()\n\nprint(func_b(func_a))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#benefits-of-using-a-function","title":"Benefits of using a Function","text":"<ol> <li>Code Reusability: Define once, use multiple times, reducing redundancy.</li> <li>Modularity: Break complex tasks into smaller, manageable parts.</li> <li>Abstraction: Hide internal details; users only need to know what the function does.</li> <li>Code Organization: Group related tasks, making code easier to read and maintain.</li> <li>Simplified Debugging/Testing: Test individual functions independently.</li> <li>Reduces Code Duplication: Avoid repeated code by reusing functions.</li> <li>Improved Maintainability: Easier to update logic in one place.</li> <li>Supports Recursion: Solve problems by calling a function within itself.</li> <li>Facilitates Functional Programming: Enables higher-order functions and functional programming patterns.</li> </ol>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#lambda-function","title":"Lambda Function","text":"<p>A lambda function in Python is a small, anonymous function defined using the <code>lambda</code> keyword. Unlike regular functions defined with <code>def</code>, lambda functions are typically used for short, simple operations and are defined in a single line.</p> <p>Key characteristics: - Anonymous: Lambda functions don't require a name. - Single Expression: They can contain only a single expression (no statements or multiple lines). - Syntax:    <pre><code>lambda arguments: expression\n</code></pre></p> <p> </p> <pre><code># x -&gt; x^2\nsquare = lambda x: x**2\nsquare(2)\n</code></pre> <pre><code># x, y -&gt; x+y\nsum_ot_two_numbers = lambda x, y: x + y\nsum_ot_two_numbers(5, 2)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#difference-between-lambda-vs-normal-function","title":"Difference between Lambda vs Normal Function","text":"<p>Here\u2019s a concise comparison between lambda and normal (def) functions:</p> <ol> <li>Syntax:</li> <li>Lambda: <code>lambda args: expression</code> (one-liner)</li> <li> <p>Normal: <code>def function_name(args):</code> (multi-line)</p> </li> <li> <p>Name:</p> </li> <li>Lambda: Anonymous, unless assigned to a variable.</li> <li> <p>Normal: Always named.</p> </li> <li> <p>Use Case:</p> </li> <li>Lambda: For short, simple tasks.</li> <li> <p>Normal: For complex, multi-step logic.</p> </li> <li> <p>Expressions:</p> </li> <li>Lambda: Single expression only.</li> <li> <p>Normal: Multiple statements allowed.</p> </li> <li> <p>Readability:</p> </li> <li>Lambda: Compact, less readable for complex tasks.</li> <li> <p>Normal: Clearer, especially for longer code.</p> </li> <li> <p>Return:</p> </li> <li>Lambda: Implicit return.</li> <li>Normal: Explicit <code>return</code>.</li> </ol> <p>Lambda functions are best for short, throwaway tasks; normal functions are better for more complex or reusable code. Lambda functions are generally used with Higher Order Function (HOF).</p> <pre><code># Check if a string has 'a'\ncheck_a = lambda s: 'a' in s\ncheck_a(\"Hello\")\n</code></pre> <pre><code># Check a number whether it is odd or even\nodd_or_even = lambda num: \"even\" if num % 2 == 0 else \"odd\"\nodd_or_even(3)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#higher-order-functions","title":"Higher Order Functions","text":"<p>A Higher-Order Function is a function that either:</p> <ol> <li>Takes one or more functions as arguments, or</li> <li>Returns a function as its result.</li> </ol> <p>In other words, higher-order functions operate on other functions, treating them as \"first-class citizens\" (objects that can be passed, returned, or assigned).</p> <pre><code># Example\ndef square(num):\n    return num**2\n\ndef cube(num):\n    return x**3\n\n# Higher Order Function (HOF)\ndef transform(f, L):\n    output = []\n    for i in L:\n        output.append(f(i))\n\n    print(output)\n\nL = [1, 2, 3, 4, 5]\ntransform(square, L)\n</code></pre> <pre><code>transform(lambda x: x**3, L)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#map","title":"Map","text":"<p>The <code>map()</code> function in Python applies a given function to each item of an iterable (like a list or tuple) and returns an iterator (or map object) with the results.</p> <p>Syntax: <pre><code>map(function, iterable)\n</code></pre> - function: The function to apply to each element. - iterable: The iterable (list, tuple, etc.) whose items will be passed into the function.</p> <pre><code># Square of the items of a list\nlist(map(lambda x: x**2, [1, 2, 3, 4, 5, 6]))\n</code></pre> <pre><code># Odd/Even labelling of list items\nL = [1, 2, 3, 4, 5]\nlist(map(lambda num: \"even\" if num%2 == 0 else \"odd\", L))\n</code></pre> <pre><code># Fetch names from a list of dict\nusers = [\n    {\n        \"name\": \"Rahul\",\n        \"age\": 15,\n        \"gender\": \"male\"\n\n    },\n    {\n        \"name\": \"Aditi\",\n        \"age\": 14,\n        \"gender\": \"female\"\n    },\n    {\n        \"name\": \"Sneha\",\n        \"age\": 13,\n        \"gender\": \"female\"\n    }\n]\n\nlist(map(lambda user: user[\"name\"], users))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#filter","title":"Filter","text":"<p>The <code>filter()</code> function in Python is used to filter elements from an iterable (like a list or tuple) based on a condition defined in a function. It returns an iterator containing only the elements for which the function returns <code>True</code>.</p> <p>Syntax: <pre><code>filter(function, iterable)\n</code></pre> - function: A function that tests each element and returns <code>True</code> or <code>False</code>. - iterable: The iterable (list, tuple, etc.) to be filtered.</p> <pre><code># Number greater than 5\nL = [3, 4, 5, 6, 7]\n\nlist(filter(lambda x: x &gt; 5, L))\n</code></pre> <pre><code># Fetch fruits starting with 'a'\nfruits = [\"apple\", \"guava\", \"cherry\"]\n\nlist(filter(lambda x: x.startswith(\"a\"), fruits))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/07_functions_in_python/#reduce","title":"Reduce","text":"<p>The <code>reduce()</code> function in Python is used to apply a binary function (a function that takes two arguments) cumulatively to the items of an iterable, reducing the iterable to a single accumulated value.</p> <p>It is part of the <code>functools</code> module, so you need to import it first.</p> <p>Syntax: <pre><code>from functools import reduce\nreduce(function, iterable, [initializer])\n</code></pre> - function: A function that takes two arguments and performs an operation (like addition, multiplication, etc.). - iterable: The iterable (list, tuple, etc.) whose elements are processed. - initializer (optional): A value that is used to start the accumulation. If provided, it's used as the initial value; otherwise, the first element of the iterable is used.</p> <p>How it works: - First: It applies the function to the first two elements of the iterable. - Next: It applies the function to the result of the previous operation and the next element of the iterable. - Repeat: It continues this process until only one result remains.</p> <pre><code># Sum of all numbers\nfrom functools import reduce\n\nreduce(lambda x, y: x+y, [1, 2, 3, 4, 5])\n</code></pre> <pre><code># Find min\nreduce(lambda x, y: min(x, y), [23, 11, 45, 10, 5])\n</code></pre> <pre><code>reduce(lambda x, y: x if x&lt;y else y, [23, 11, 45, 10, 5])\n</code></pre> <pre><code># Find max\nreduce(lambda x, y: x if x&gt;y else y, [23, 11, 45, 10, 5])\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/","title":"Object Oriented Programming 1","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#object-oriented-programming-part-1","title":"Object Oriented Programming (Part 1)","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects,\" which can contain both data (in the form of fields, also known as attributes or properties) and code (in the form of methods, which are functions). OOP allows for the structuring of software in a way that models real-world entities, making the code more modular, flexible, and easier to maintain.</p> <p>In Object-Oriented Programming (OOP), we can build our own data types using classes. These user-defined data types (also known as custom classes) allow us to create objects that represent real-world entities or abstract concepts, with their own attributes (data) and methods (functions).</p> <p>For example, instead of relying solely on primitive data types like integers or strings, we can design a class to model more complex data, such as a <code>Person</code>, <code>Car</code>, or <code>BankAccount</code>. Each of these classes can encapsulate specific properties (e.g., name, balance, speed) and behaviors (e.g., withdraw money, accelerate, display information), enabling us to work with these objects in a structured and intuitive way.</p> <p>By defining our own data types, we extend the language\u2019s capabilities and create reusable, modular components that can fit the needs of our particular application. This flexibility is one of the core strengths of OOP, promoting better code organization, readability, and maintainability.</p> <p></p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#class-and-object","title":"Class and Object","text":"<p>In Object-Oriented Programming (OOP), Class and Object are fundamental concepts.</p> <p>Class: A class is a blueprint or template for creating objects. It defines a set of attributes (data) and methods (functions) that the objects created from the class will have.</p> <p>Object: An object is an instance of a class. When a class is defined, no memory is allocated until we create objects. Each object has its own values for the attributes defined in the class and can use the methods of the class.</p> <p>For example, when you work with Python\u2019s built-in data types like lists, strings, or integers, you\u2019re actually working with objects of classes predefined in Python. These objects have their own methods and properties.</p> <p>Let\u2019s analyze the following code:</p> <pre><code>L = [1, 2, 3]\nprint(type(L))  # Output: &lt;class 'list'&gt;\n# L.upper()       # This will raise an AttributeError\n</code></pre> <ul> <li><code>L = [1, 2, 3]</code>: Here, <code>L</code> is an object of the class <code>list</code>. The class <code>list</code> is the blueprint for Python\u2019s list data structure, and <code>L</code> is an instance (or object) of this class.</li> <li><code>print(type(L))</code>: This statement prints the type of <code>L</code>, which is <code>&lt;class 'list'&gt;</code>. This tells us that <code>L</code> is an object of the class <code>list</code>.</li> <li><code>L.upper()</code>: This will raise an <code>AttributeError</code> because the <code>list</code> class does not have a method called <code>upper</code>. The <code>upper()</code> method exists for <code>str</code> (string) objects, not for lists.</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#creating-class-and-object","title":"Creating Class and Object","text":"<pre><code># We should write class name in Pascal Case\n# Write a class to represent an ATM system\nclass ATM:\n    # constructor (special function)\n    def __init__(self):\n        self.pin = \"\"\n        self.balance = 0\n        self.menu()\n\n    def menu(self):\n        user_input = input(\n            \"\"\"\n            Hi how can I help you?\n            1. Press 1 to create pin\n            2. Press 2 to change pin\n            3. Press 3 to check balance\n            4. Press 4 to withdraw\n            5. Anything else to exit\n            \"\"\")\n\n        if user_input == \"1\":\n            self.create_pin()\n\n        elif user_input == \"2\":\n            self.change_pin()\n\n        elif user_input == \"3\":\n            self.check_balance()\n\n        elif user_input == \"4\":\n            self.withdraw()\n\n        else:\n            exit()\n\n    def create_pin(self):\n        user_pin = input(\"Enter your PIN:\")\n        self.pin = user_pin\n\n        user_balance = int(input(\"Enter balance:\"))\n        self.balance = user_balance\n\n        print(\"PIN created successfully!\")\n        self.menu()\n\n    def change_pin(self):\n        old_pin = input(\"Enter old PIN:\")\n\n        if old_pin == self.pin:\n            # Let user change the PIN\n            new_pin = input(\"Enter new PIN:\")\n            self.pin = new_pin\n            print(\"PIN change successfully.\")\n            self.menu()\n\n        else:\n            print(\"You entered incorrect PIN.\")\n            self.menu()\n\n    def check_balance(self):\n        user_pin = input(\"Enter your PIN:\")\n        if user_pin == self.pin:\n            print(\"Your balance is \", self.balance)\n\n        else:\n            print(\"You entered incorrect PIN.\")\n\n    def withdraw(self):\n        user_pin = input(\"Enter your PIN:\")\n        if user_pin == self.pin:\n            amount = int(input(\"Enter the amount\"))\n            if amount &lt;= self.balance:\n                self.balance = self.balance - amount\n                print(\"Withdrawl successfull. New balance is \", self.balance)\n\n            else:\n                print(\"Your account has insufficient balance.\")\n\n        else:\n            print(\"You entered incorrect PIN.\")\n\n        self.menu()\n</code></pre> <pre><code># Create an object of the ATM class\n# obj = ATM()\n# print(type(obj))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#class-diagram","title":"Class Diagram","text":"<p>Here\u2019s the class diagram for the given <code>ATM</code> system, following the conventions where class names are written in Pascal Case:</p> <p>ATM Class Diagram</p> <pre><code>+---------------------+\n|        ATM          |\n+---------------------+\n| - pin: str          |\n| - balance: int      |\n+---------------------+\n| + __init__()        |\n| + menu()            |\n| + create_pin()      |\n| + change_pin()      |\n| + check_balance()   |\n| + withdraw()        |\n+---------------------+\n</code></pre> <p>Explanation:</p> <ul> <li>ATM is the class name.</li> <li>Attributes (fields):</li> <li><code>pin</code>: A private attribute that stores the user's PIN. It's initialized as an empty string.</li> <li> <p><code>balance</code>: A private attribute that stores the user's account balance. It's initialized as <code>0</code>.</p> </li> <li> <p>Methods (operations):</p> </li> <li><code>__init__()</code>: The constructor, which initializes the <code>pin</code> and <code>balance</code> attributes and calls the <code>menu()</code> method.</li> <li><code>menu()</code>: Displays a menu with options for creating a PIN, changing a PIN, checking balance, withdrawing money, or exiting the program.</li> <li><code>create_pin()</code>: Allows the user to create a PIN and set an initial balance.</li> <li><code>change_pin()</code>: Allows the user to change the current PIN if the old PIN is correctly provided.</li> <li><code>check_balance()</code>: Prompts the user to enter their PIN and displays the account balance if the PIN is correct.</li> <li><code>withdraw()</code>: Prompts the user for their PIN and withdraws the specified amount from the account if the PIN is correct and there are sufficient funds.</li> </ul> <p>In class diagrams, \"+\" and \"-\" symbols represent the visibility (or access control) of the class members (attributes and methods). Here's what they mean:</p> <ul> <li>\"+\" (Public):</li> <li> <p>This indicates that the attribute or method is public, meaning it can be accessed from outside the class. In Python, all attributes and methods are public by default unless otherwise specified.</p> </li> <li> <p>\"-\" (Private):</p> </li> <li>This indicates that the attribute or method is private, meaning it is only accessible from within the class itself and not from outside the class. In Python, you can make an attribute or method private by using a leading double underscore (<code>__</code>), though it's more of a convention in Python as the language doesn't enforce strict private access.</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#methods-vs-functions","title":"Methods vs Functions","text":"<p>In Python, the terms methods and functions are often used interchangeably, but they have distinct meanings based on their context.</p> <ol> <li>Functions:</li> <li>A function is a block of reusable code that performs a specific task. It can exist independently and is not associated with any particular object.</li> <li> <p>Functions are defined using the <code>def</code> keyword and can take inputs (parameters) and return outputs (values).</p> </li> <li> <p>Methods:</p> </li> <li>A method is similar to a function but is associated with an object (an instance of a class). In other words, a method is a function that \"belongs to\" an object.</li> <li>Methods are called on objects and can access and modify the data (attributes) within the object they belong to.</li> <li>Methods are defined inside a class and must take at least one parameter (<code>self</code>), which refers to the instance of the class.</li> </ol> <pre><code>L = [1, 2, 3]\nlen(L) # function -&gt; because it is outside the 'list' class\nL.append(4) # method -&gt; because it is inside the 'list' class\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#magic-methods-aka-dunder-methods","title":"Magic Methods (a.k.a Dunder Methods)","text":"<p>Magic methods in Python, also known as dunder (double underscore) methods, are special methods that allow you to define how objects of a class should behave in certain operations. They have names that begin and end with double underscores (<code>__</code>), such as <code>__init__</code>, <code>__str__</code>, <code>__add__</code>, etc.</p> <p>Magic methods are used to implement the behavior of operators, object construction, type conversion, and other operations. Python automatically invokes these methods in specific situations, making them very powerful for customizing the behavior of objects.</p> <p>Common Magic Methods:</p> <ol> <li><code>__init__(self, ...)</code> (Constructor)</li> <li> <p>Called when a new instance of a class is created. It is used to initialize the object's state.</p> </li> <li> <p><code>__str__(self)</code> (String Representation)</p> </li> <li> <p>Called when <code>str()</code> or <code>print()</code> is used on an object. It defines the human-readable string representation of the object.</p> </li> <li> <p><code>__len__(self)</code> (Length of Object)</p> </li> <li>Called when <code>len()</code> is used on an object. It defines how the object should respond when its length is queried.</li> </ol>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#concept-of-self","title":"Concept of <code>self</code>","text":"<p>In Python, the <code>self</code> keyword is used in the context of class and instance methods. It refers to the current instance of the class and allows access to the instance's attributes and methods.</p> <p>Characteristics: - <code>self</code> is a reference to the current instance of the class. - It is used to access attributes and methods associated with that instance. - <code>self</code> is passed automatically to instance methods in Python. - It\u2019s a convention to name the first parameter of instance methods <code>self</code>, but any name can technically be used (though not recommended).</p> <pre><code># Write a class to represent an ATM system\nclass ATM:\n    def __init__(self):\n        self.pin = \"\"\n        self.balance = 0\n        print(\"Address of self:\", id(self))\n        # self.menu()\n\n    def menu(self):\n        user_input = input(\n            \"\"\"\n            Hi how can I help you?\n            1. Press 1 to create pin\n            2. Press 2 to change pin\n            3. Press 3 to check balance\n            4. Press 4 to withdraw\n            5. Anything else to exit\n            \"\"\")\n\n        if user_input == \"1\":\n            self.create_pin()\n        else:\n            exit()\n\n    def create_pin(self):\n        user_pin = input(\"Enter your PIN:\")\n        self.pin = user_pin\n\n        user_balance = int(input(\"Enter balance:\"))\n        self.balance = user_balance\n\n        print(\"PIN created successfully!\")\n        self.menu()\n</code></pre> <pre><code># Create an object of the ATM class\nobj = ATM()\nprint(\"Address of the object:\", id(obj))\n</code></pre> <pre><code># Create another object of the ATM class\nobj2 = ATM()\nprint(\"Address of the object 2:\", id(obj2))\n</code></pre> <pre><code># class Fraction\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/08_object_oriented_programming_1/#build-custom-class","title":"Build Custom Class","text":"<pre><code># Create a 'Fraction' class to represent fractions and perform arithmetic operations\nclass Fraction:\n\n    # Parameterized Constructor: Initializes the numerator and denominator\n    def __init__(self, x, y):\n        self.num = x  # Numerator of the fraction\n        self.den = y  # Denominator of the fraction\n\n    # String representation of the fraction (e.g., '3/4')\n    def __str__(self):\n        return \"{}/{}\".format(self.num, self.den)\n\n    # Addition of two fractions\n    def __add__(self, other):\n        new_num = self.num * other.den + other.num * self.den\n        new_den = self.den * other.den\n\n        return \"{}/{}\".format(new_num, new_den)\n\n    # Subtraction of two fractions\n    def __sub__(self, other):\n        new_num = self.num * other.den - other.num * self.den\n        new_den = self.den * other.den\n\n        return \"{}/{}\".format(new_num, new_den)\n\n    # Multiplication of two fractions\n    def __mul__(self, other):\n        new_num = self.num * other.num\n        new_den = self.den * other.den\n\n        return \"{}/{}\".format(new_num, new_den)\n\n    # Division of two fractions\n    def __truediv__(self, other):\n        new_num = self.num * other.den\n        new_den = self.den * other.num\n\n        return \"{}/{}\".format(new_num, new_den)\n\n    # Convert the fraction to decimal\n    def convert_to_decimals(self):\n        return self.num / self.den\n</code></pre> <pre><code># Create two fraction objects\nfraction1 = Fraction(2, 4)\nfraction2 = Fraction(4, 6)\n\nprint(\"Fraction 1 object:\", fraction1)\nprint(\"Fraction 2 object:\", fraction2)\nprint(\"Addition of two fractions:\", fraction1 + fraction2)\nprint(\"Subtraction of two fractions:\", fraction1 - fraction2)\nprint(\"Multiplication of two fractions:\", fraction1 * fraction2)\nprint(\"Division of two fractions:\", fraction1 / fraction2)\nprint(\"Decimal representation of Fraction 1:\", fraction1.convert_to_decimals())\nprint(\"Decimal representation of Fraction 2:\", fraction2.convert_to_decimals())\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/","title":"Object Oriented Programming 2","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#object-oriented-programming-part-2","title":"Object Oriented Programming (Part 2)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#problem-statement-write-oop-classes-to-handle-the-following-scenarios","title":"Problem Statement: Write OOP classes to handle the following scenarios:","text":"<ul> <li>A user can create and view 2D coordinates</li> <li>A user can find out the distance between 2 coordinates</li> <li>A user can find the distance of a coordinate from origin</li> <li>A user can check if a point lies on a given line</li> <li>A user can find the distance between a given 2D point and a given line</li> </ul> <pre><code># Write classes for coordinate geometry\nclass Point:\n\n    # parameterized constructor\n    def __init__(self, x, y):\n        self.x_coord = x\n        self.y_coord = y\n\n    # print representation using magic methods\n    def __str__(self):\n        return \"&lt;{}, {}&gt;\".format(self.x_coord, self.y_coord)\n\n    # method to calculate euclidean distance\n    def euclidean_distance(self, other):\n        return ((self.x_coord - other.x_coord)**2 + (self.y_coord - other.y_coord)**2)**0.5\n\n    # method to distance from origin\n    def distance_from_origin(self):\n        # return (self.x_coord**2 + self.y_coord*82)**5 # alternative\n        return self.euclidean_distance(Point(0, 0))\n\nclass Line:\n\n    # parameterized constructor\n    def __init__(self, A, B, C):\n        self.A = A\n        self.B = B\n        self.C = C\n\n    # print representation using magic methods\n    def __str__(self):\n        return f\"{self.A}x + {self.B}y + {self.C} = 0\"\n\n    # method to check a point fall on a line\n    def point_on_line(line, point):\n        if line.A*point.x_coord + line.B*point.y_coord + line.C == 0:\n            return True # point falls on the line\n        else:\n            return False # point doesn't fall on the line\n\n    # method to calculate shortest distance between a line and a point\n    def shortest_distance(line, point):\n        return abs(line.A*point.x_coord + line.B*point.y_coord + line.C) / (line.A**2 + line.B**2)**0.5\n\n    # method to check whether two line segments intersect each other\n    def is_intersected(line1, line2):\n        if ((line1.A / line2.A) != (line1.B / line2.B)) or ((line1.A / line2.A) == (line1.B / line2.B) == (line1.C / line2.C)):\n            return True # lines are intersecting each other\n        else:\n            return False # lines are not intersecting each other\n</code></pre> <pre><code># Create two Point objects\npoint1 = Point(0, 0)\npoint2 = Point(1, 1)\n\n# Create three Line objects\nline1 = Line(1, 1, -2)\nline2 = Line(2, 1, 1)\nline3 = Line(1, 1, -4)\n\nprint(\"First point object:\", point1)\nprint(\"Second point object:\", point2)\nprint(\"First line object:\", line1)\n\nprint(\"Euclidean distance between point1 and point2\", point1.euclidean_distance(point2))\nprint(\"Distance of the point2 from the origin\", point2.distance_from_origin())\nprint(\"point2 falls on the line1?:\", line1.point_on_line(point2))\nprint(\"Shortest distance between line1 and point1 is:\", line1.shortest_distance(point1))\nprint(\"Line2 is intersecting Line3:\", line2.is_intersected(line3))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#how-object-access-attributes","title":"How Object access Attributes","text":"<p>In Python, objects access attributes using the dot notation (<code>object.attribute</code>).</p> <pre><code># Create a Person class\nclass Person:\n\n    def __init__(self, name, country):\n        self.name = name\n        self.country = country\n\n    def greet(self):\n        if self.country == \"india\":\n            print(\"Namaste\", self.name)\n        else:\n            print(\"Hello\", self.name)\n</code></pre> <pre><code># Create a Person object\nperson1 = Person(\"Krishnagopal\", \"India\")\n\n# accessing the attributes\nprint(\"Name:\", person1.name)\nprint(\"Country:\", person1.country)\n\n# accessing the methods\nperson1.greet()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#attribute-creation-from-outside-of-the-class","title":"Attribute Creation from Outside of the Class","text":"<pre><code># Create a new attribute to the object outside of the class\nperson1.gender = \"male\"\nperson1.gender\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#reference-variables","title":"Reference Variables","text":"<p>A reference variable in Python is a variable that refers to (or \"points to\") an object in memory. Python variables do not hold the actual data directly but instead hold a reference (memory address) to the object where the data is stored.</p> <ul> <li>Reference variables hold the objects</li> <li>We can create objects without reference variable as well</li> <li>An object can have multiple reference variables</li> <li>Assigning a new reference variable to an existing object does not create a new object</li> </ul> <pre><code># Object without a reference\nclass Person:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\nPerson(\"krishnagopal\", \"male\")\n</code></pre> <pre><code>p = Person(\"krishnagopal\", \"male\")\nq = p\n\n# multiple reference\nprint(id(p))\nprint(id(q))\n</code></pre> <pre><code>print(p.name)\nprint(q.name)\nq.name = \"ankit\"\nprint(q.name)\nprint(p.name)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#pass-by-reference","title":"Pass by Reference","text":"<p>In programming, pass by reference refers to passing the memory address of a variable (a reference) to a function. This means that if the function modifies the parameter, it directly affects the original variable since both refer to the same memory location.</p> <pre><code>class Person:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\n# outside the class -&gt; function\ndef greet(person):\n    print(f\"Hi! my name is {person.name} and I am a {person.gender}.\")\n    p1 = Person(\"ankit\", \"male\")\n    return p1\n\np = Person(\"krishnagopal\", \"male\")\nx = greet(p)\nprint(x.name)\nprint(x.gender)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#mutability-of-an-object","title":"Mutability of an Object","text":"<p>In Python, mutability refers to whether an object's value can be changed after it has been created. Objects in Python fall into two categories based on their mutability:</p> <ol> <li>Mutable Objects: Can be changed after creation.</li> <li>Immutable Objects: Cannot be changed after creation.</li> </ol> <p>To determine whether an object is mutable or immutable, you can check the memory address of the object using <code>id()</code>. If the address changes after modification, the object is immutable.</p> <pre><code>class Person:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\n# outside of the class -&gt; function\ndef greet(person):\n    person.name = \"ankit\"\n    return person\n\np = Person(\"krishnagopal\", \"male\")\nprint(id(p))\np1 = greet(p)\nprint(id(p1))\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#instance-variable","title":"Instance Variable","text":"<p>An instance variable is a variable that is tied to a specific instance of a class. Each object (or instance) of a class can have its own unique values for these variables. These variables store the state or attributes of the object.</p> <pre><code># instance variable -&gt; name, country\nclass Person:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\np1 = Person(\"ankit\", \"india\")\np2 = Person(\"krishnagopal\", \"germany\")\n</code></pre> <pre><code>print(p1.name)\nprint(p2.name)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#encapsulation","title":"Encapsulation","text":"<p>Encapsulation in Python is a fundamental concept in object-oriented programming (OOP) that refers to bundling data (attributes) and the methods (functions) that operate on the data into a single unit, typically a class. Encapsulation helps in restricting direct access to certain components of an object, thereby maintaining the integrity of the data and promoting modular, secure code design.</p> <p>Key Aspects of Encapsulation: 1. Data Hiding:    - Encapsulation provides a way to hide the internal state of an object and protect it from unintended interference and misuse.    - Access to the object's internal state is controlled through public methods (getters and setters).</p> <ol> <li>Access Modifiers:</li> <li>Public: Members (attributes or methods) with no underscores are accessible from anywhere.</li> <li>Protected: Members prefixed with a single underscore (<code>_attribute</code>) indicate that they are intended to be used within the class or subclasses (not enforced by Python but a convention).</li> <li>Private: Members prefixed with a double underscore (<code>__attribute</code>) are name-mangled to prevent direct access outside the class.</li> </ol> <p>Advantages of Encapsulation: - Improves Security: Protects sensitive data from being accessed or modified accidentally. - Enhances Code Modularity: Changes to the internal implementation of a class can be made without affecting the external code. - Promotes Maintainability: Encapsulation simplifies debugging and updating of code. - Encourages Abstraction: Focuses on what an object does rather than how it does it.</p> <p>Encapsulation is an essential part of designing robust and scalable systems in Python.</p> <pre><code># Create an ATM class\nclass ATM:\n\n    def __init__(self):\n        self.pin = \"\"\n        self.__balance = 0\n        self.menu()\n\n    def get_balance(self):\n        return self.__balance\n\n    def set_balance(self, new_balance):\n        if type(new_balance) == int:\n            self.__balance = new_balance\n\n        else:\n            print(\"Only integer is supported\")\n\n    def menu(self):\n        user_input = input(\n            \"\"\"\n            Hi how can I help you?\n            1. Press 1 to create pin\n            2. Press 2 to change pin\n            3. Press 3 to check balance\n            4. Press 4 to withdraw\n            5. Anything else to exit\n            \"\"\")\n\n        if user_input == \"1\":\n            self.create_pin()\n\n        elif user_input == \"2\":\n            self.change_pin()\n\n        elif user_input == \"3\":\n            self.check_balance()\n\n        elif user_input == \"4\":\n            self.withdraw()\n\n        else:\n            pass\n\n    def create_pin(self):\n        user_pin = input(\"Enter your PIN: \")\n        self.pin = user_pin\n\n        user_balance = input(\"Enter your balance: \")\n        self.__balance = user_balance\n\n        print(\"PIN created successfully\")\n\n    def change_pin(self):\n        old_pin = input(\"Enter old PIN:\")\n\n        if old_pin == self.pin:\n            # Let user change the PIN\n            new_pin = input(\"Enter new PIN:\")\n            self.pin = new_pin\n            print(\"PIN change successfully.\")\n            self.menu()\n\n        else:\n            print(\"You entered incorrect PIN.\")\n            self.menu()\n\n    def check_balance(self):\n        user_pin = input(\"Enter your PIN:\")\n        if user_pin == self.pin:\n            print(\"Your balance is \", self.__balance)\n\n        else:\n            print(\"You entered incorrect PIN.\")\n\n    def withdraw(self):\n        user_pin = input(\"Enter your PIN:\")\n        if user_pin == self.pin:\n            amount = int(input(\"Enter the amount\"))\n            if amount &lt;= self.__balance:\n                self.__balance = self.__balance - amount\n                print(\"Withdrawl successfull. New balance is \", self.__balance)\n\n            else:\n                print(\"Your account has insufficient balance.\")\n\n        else:\n            print(\"You entered incorrect PIN.\")\n\n        self.menu()\n</code></pre> <pre><code># atm_obj = ATM()\n# atm_obj.get_balance()\n</code></pre> <pre><code># atm_obj.set_balance(1000)\n# atm_obj.get_balance()\n</code></pre> <pre><code># atm_obj.withdraw()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#collection-of-objects","title":"Collection of Objects","text":"<p>In Python, objects can be organized into collections like lists, sets, or dictionaries, enabling efficient storage and manipulation of related data. For example, by creating a list of objects from a class, we can store multiple instances and process them collectively. This allows operations like iteration, filtering, or accessing specific attributes for all objects in the collection. This approach is particularly useful for managing and performing batch operations on similar data entities.</p> <pre><code># List of objects\nclass Person:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\np1 = Person(\"krishnagopal\", \"male\")\np2 = Person(\"abir\", \"male\")\np3 = Person(\"dipak\", \"male\")\n\nL = [p1, p2, p3]\n\nfor i in L:\n    print(i.name)\n</code></pre> <pre><code>D = {\"person1\": p1, \"person2\": p2, \"person3\": p3}\n\nfor key, value in D.items():\n    print(key, \":\", value.name, \",\", value.gender)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/09_object_oriented_programming_2/#static-variables","title":"Static Variables","text":"<p>A static variable in Python refers to a variable that is shared among all instances of a class. It belongs to the class rather than any specific instance, meaning its value is the same across all objects of that class unless explicitly modified. Static variables are defined at the class level, outside of any instance methods.</p> <p>Characteristics of Static Variables: 1. Class-Level Scope:    - Static variables are declared inside the class but outside any instance methods or constructors.    - They are shared across all instances of the class.</p> <ol> <li>Shared State:</li> <li> <p>All instances of the class share the same static variable, and any changes to it are reflected across all instances.</p> </li> <li> <p>Access:</p> </li> <li>Can be accessed using the class name (<code>ClassName.variable</code>) or an instance (<code>object.variable</code>), though the former is preferred for clarity.</li> </ol> <p>Key Points to Remember: 1. Static vs Instance Variables:    - Static variables are shared across all objects of a class.    - Instance variables are unique to each object and defined within methods using <code>self</code>.</p> <ol> <li>Use Cases:</li> <li>Storing values that should remain consistent across all instances, like configuration settings or counters.</li> <li>Tracking shared state or data across all instances.</li> </ol> <pre><code># Write a class for ATM\nclass ATM:\n\n    __counter = 1\n\n    def __init__(self):\n        self.__pin = \"\"\n        self.__balance = 0\n        self.cid = 0\n        self.cid = ATM.__counter\n        ATM.__counter = ATM.__counter + 1\n\n        self.menu()\n\n    # utility functions\n    @staticmethod\n    def get_counter():\n        return ATM.__counter\n\n    def __str__(self):\n        print(\"ATM_instance\")\n\n    def get_pin(self):\n        return self.__pin\n\n    def get_balance(self):\n        return self.__balance\n\n    def set_balance(self, new_balance):\n        self.__balance = new_balance\n        return self.__balance\n\n    def get_cid(self):\n        return self.cid\n\n\n    def menu(self):\n        user_input = input(\n            \"\"\"\n            Hi how can I help you?\n            1. Press 1 to create pin\n            2. Press 2 to change pin\n            3. Press 3 to check balance\n            4. Press 4 to withdraw\n            5. Anything else to exit\n            \"\"\")\n\n        if user_input == \"1\":\n            self.create_pin()\n\n        elif user_input == \"2\":\n            self.change_pin()\n\n        elif user_input == \"3\":\n            self.check_balance()\n\n        elif user_input == \"4\":\n            self.withdraw()\n\n        else:\n            pass\n\n    def create_pin(self):\n        user_pin = int(input(\"Enter your PIN: \"))\n        self.__pin = user_pin\n        print(\"PIN created successfully!\")\n\n        user_balance = int(input(\"Enter your balance: \"))\n        self.__balance = user_balance\n        print(\"Balance stored successfully!\")\n\n        self.menu()\n\n\n    def change_pin(self):\n        new_pin = int(input(\"Enter old PIN: \"))\n        if new_pin == self.__pin:\n            self.__pin = new_pin\n            print(\"PIN changed successfully!\")\n\n        else:\n            print(\"You entered the wrong PIN.\")\n\n        self.menu()\n\n    def check_balance(self):\n        user_pin = int(input(\"Enter old PIN: \"))\n        if user_pin == self.__pin:\n            print(\"Your account balance is:\", self.__balance)\n\n        else:\n            print(\"You entered the wrong PIN.\")\n\n        self.menu()\n\n    def withdrawl(self):\n        user_pin = int(input(\"Enter old PIN: \"))\n        if user_pin == self.__pin:\n            amount = int(input(\"Enter the amount: \"))\n            self.__balance = self.__balance - amount\n            print(\"Your new balance is:\", self.__balance)\n\n        else:\n            print(\"You entered the wrong PIN.\")\n\n        self.menu()\n</code></pre> <pre><code># Create an ATM object\np1 = ATM()\n\n# Print the counter id\nprint(\"Counter ID of the object:\", p1.get_cid())\n</code></pre> <pre><code># Create another ATM object\np2 = ATM()\n\n# Print the counter id\nprint(\"Counter ID of the object:\", p2.get_cid())\n</code></pre> <pre><code># Call the static method with the class name\nATM.get_counter()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/","title":"Object Oriented Programming 3","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#object-oriented-programming-part-3","title":"Object Oriented Programming (Part 3)","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#class-relationship","title":"Class Relationship","text":"<p>In Python, class relationships refer to how classes are related to one another and how they interact in an object-oriented programming context. Here are two primary types of relationships:</p> <ol> <li> <p>Aggregation: Represents a \"has-a\" relationship where the contained object can exist independently of the container.  </p> </li> <li> <p>Inheritance: Represents an \"is-a\" relationship where a child class inherits properties and behaviors from a parent class.</p> </li> </ol>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#aggregation-has-a-relationship","title":"Aggregation (Has-a relationship)","text":"<pre><code># Example\nclass Customer:\n    def __init__(self, name, gender, address):\n        self.name = name\n        self.gender = gender\n        self.address = address\n\n    def print_address(self):\n        print(f\"{self.address.get_city()}, {self.address.state}-{self.address.pin}, {self.address.country}\")\n\n    def edit_profile(self, new_name, new_city, new_pin, new_state):\n        self.name = new_name\n        self.address.edit_addresss(new_city, new_pin, new_state)\n\nclass Address:\n    def __init__(self, city, pin, state, country):\n        self.__city = city # what about private attribute\n        self.pin = pin\n        self.state = state\n        self.country = country\n\n    def get_city(self):\n        return self.__city\n\n    def edit_addresss(self, new_city, new_pin, new_state):\n        self.__city = new_city\n        self.pin = new_pin\n        self.state = new_state\n\naddress1 = Address(city=\"Roorkee\", pin=247667, state=\"Haridwar\", country=\"India\")\ncustomer1 = Customer(name=\"Krishnagopal Halde\", gender=\"Male\", address=address1)\ncustomer1.print_address()\n\ncustomer1.edit_profile(\"Akshat Goel\", \"Mumbai\", 400001, \"Maharashtra\")\ncustomer1.print_address()\n</code></pre> <p>Brief Explanation of Aggregation in the Example:</p> <ul> <li>Aggregation is demonstrated by the <code>Customer</code> class \"having an\" Address as part of its attributes (<code>address</code>).</li> <li>The <code>Customer</code> object does not directly define or manage the properties of the <code>Address</code>. Instead, it uses an independent <code>Address</code> object.</li> <li>Changes to the <code>Address</code> (via <code>edit_addresss</code>) affect the <code>Customer</code> object because the <code>Customer</code> holds a reference to the <code>Address</code> object.</li> </ul> <p>Key Points: 1. Independent <code>Address</code> Object:    The <code>Address</code> object (<code>address1</code>) exists separately and is passed to the <code>Customer</code> constructor.</p> <ol> <li>Interaction with <code>Address</code>: </li> <li>The <code>Customer</code> uses methods like <code>get_city()</code> and <code>edit_addresss()</code> from the <code>Address</code> class to retrieve and modify its data.</li> <li> <p>Modifications to the <code>Address</code> reflect automatically in the <code>Customer</code> as they share the same object.</p> </li> <li> <p>Workflow: </p> </li> <li>Initially, the address is set to <code>\"Roorkee, Haridwar-247667, India\"</code>.</li> <li>After calling <code>edit_profile</code>, the <code>Address</code> object is updated to <code>\"Mumbai, Maharashtra-400001, India\"</code>, and <code>Customer</code> reflects this change.</li> </ol>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#inheritence","title":"Inheritence","text":"<p>Inheritance is a fundamental concept of object-oriented programming (OOP) that allows one class (the child or derived class) to acquire the properties and behaviors of another class (the parent or base class). This enables code reuse, hierarchy creation, and easy extension of existing functionality.</p> <p></p> <p>Key Features of Inheritance:</p> <ol> <li> <p>Code Reusability:    Common features can be defined in the parent class and reused in child classes.</p> </li> <li> <p>Hierarchy:    Inheritance establishes a \"is-a\" relationship between classes, e.g., a <code>Dog</code> \"is-a\" type of <code>Animal</code>.</p> </li> <li> <p>Customization:    Child classes can override or extend the methods and attributes of the parent class.</p> </li> <li> <p>Multiple and Multilevel Inheritance:    Python supports:</p> </li> <li>Single Inheritance: One parent, one child.</li> <li>Multiple Inheritance: One child class inherits from multiple parent classes.</li> <li>Multilevel Inheritance: A child class inherits from another child class.</li> </ol> <p>Benefits of Inheritance - Simplifies code by reducing redundancy. - Promotes modularity and maintainability. - Enables polymorphism, allowing dynamic method overriding.</p> <p>Limitations - Overuse of inheritance can make code harder to debug and maintain. - Alternatives like composition might be more suitable in certain scenarios.</p> <pre><code># Example\n# Parent class\nclass User:\n\n    def __init__(self, name, gender):\n        self.name = name\n        self.gender = gender\n\n    def login(self):\n        print(\"login successfull!\")\n\n# Child class\nclass Student(User):\n\n    def enroll(self):\n        print(\"Enroll in the course.\")\n\nuser1 = User(\"Krishnagopal Halder\", \"Male\")\nprint(\"User's name:\", user1.name)\nprint(\"User's gender:\", user1.gender)\n\nstudent1 = Student(\"Krishnagopal Halder\", \"Male\")\nstudent1.login()\nprint(\"Student's name:\", student1.name)\nprint(\"Student's gender:\", student1.gender)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#class-diagram","title":"Class Diagram","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#what-gets-inherited","title":"What Gets Inherited?","text":"<p>When a child class inherits from a parent class, the following components are inherited:</p> <ol> <li> <p>Constructor: The <code>__init__</code> method (constructor) of the parent class is inherited by the child class.</p> <ul> <li>Behavior: </li> <li>If the child class does not define its own constructor, it will use the parent class's constructor.</li> <li>If the child class defines its own constructor, it overrides the parent class's constructor.</li> </ul> </li> <li> <p>Non Private Attributes: Attributes of the parent class that are not marked as private (e.g., no double underscores like <code>__attr</code>) are inherited by the child class.</p> <ul> <li>Behavior: These attributes can be accessed and modified in the child class.</li> </ul> </li> <li> <p>Non Private Methods: Methods of the parent class that are not private (i.e., without double underscores like <code>__method</code>) are inherited by the child class.</p> </li> <li>Behavior:<ul> <li>The child class can call these methods directly.</li> <li>The child class can override these methods by redefining them.</li> </ul> </li> </ol> <pre><code># Constructor example 1 ( If the child class does not define its own constructor)\n# Parent class\nclass Phone:\n    def __init__(self, price, brand, camera):\n        self.price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\n# Child class\nclass Smartphone(Phone):\n    pass\n\nsmartphone = Smartphone(50000, \"Apple\", 48)\nsmartphone.buy()\n</code></pre> <pre><code># Constructor example 2 ( If the child class defines its own constructor)\n# Parent class\nclass Phone:\n    def __init__(self, price, brand, camera):\n        self.price = price\n        self.brand = brand\n        self.camera = camera\n\nclass Smartphone(Phone):\n    def __init__(self, os, ram):\n        self.os = os\n        self.ram = ram\n        print(\"Inside Smartphone constructor\")\n\nsmartphone = Smartphone(\"Android\", 8)\n# smartphone.brand # will throw error\n</code></pre> <pre><code># Example 3 (Child can't access private members of the class)\n\nclass Phone:\n    def __init__(self, price, brand, camera):\n        print(\"Inside phone constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    # getter method\n    def show(self):\n        print(self.__price)\n\nclass SmartPhone(Phone):\n    def check(self):\n        print(self.__price)\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nprint(smartphone.brand)\n# smartphone.check() # will throw error\nsmartphone.show()\n</code></pre> <pre><code># # Example 4: Guess the output\n# class Parent:\n#     def __init__(self, num):\n#         self.__num = num\n\n#     # getter method\n#     def get_num(self):\n#         return self.__num\n\n# class Child(Parent):\n\n#     def show(self):\n#         print(\"This is in child class\")\n\n# son = Child(100)\n# print(son.get_num())\n# son.show()\n</code></pre> <pre><code># # Example 5: Guess the output\n# class Parent:\n#     def __init__(self, num):\n#         self.__num = num\n\n#     def get_num(self):\n#         return self.__num\n\n# class Child(Parent):\n\n#     def __init__(self, val, num):\n#         self.__val = val\n\n#     def get_val(self):\n#         return self.__val\n\n# son = Child(100, 10)\n# print(\"Parent: Num:\", son.get_num())\n# print(\"Child: Val:\", son.get_val())\n</code></pre> <pre><code># # Example 6: Guess the output\n# class A:\n#     def __init__(self):\n#         self.var1 = 100\n\n#     def display1(self, var1):\n#         print(\"Class A:\", self.var1)\n\n# class B(A):\n\n#     def display2(self, var1):\n#         print(\"Class B:\", self.var1)\n\n# obj = B()\n# obj.display1(200)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#method-overriding","title":"Method Overriding","text":"<p>Method overriding is a feature in object-oriented programming that allows a subclass (child class) to provide a specific implementation for a method that is already defined in its superclass (parent class). The overridden method in the child class must have the same name, parameters, and return type as the method in the parent class.</p> <pre><code>class Phone:\n    def __init__(self, price, brand, camera):\n        print(\"Inside phone constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\nclass SmartPhone(Phone):\n    def buy(self):\n        print(\"Buying a smartphone\")\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nsmartphone.buy()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#super-keyword","title":"<code>super()</code> Keyword","text":"<p>The <code>super()</code> keyword is used in Python to call methods or access attributes from a parent class (also known as the superclass) in the context of a subclass (child class). It provides a way for a child class to refer to and invoke methods or constructors from its parent class, particularly when overriding methods.</p> <p>Key Uses of <code>super()</code>:</p> <ol> <li> <p>Calling the Parent Class's Method:    When a method in a child class overrides a method in a parent class, <code>super()</code> allows you to call the parent class's version of the method.</p> </li> <li> <p>Accessing Parent Class's Constructor:    In a child class, <code>super()</code> can be used to call the parent class's <code>__init__()</code> constructor, enabling the child class to initialize inherited attributes.</p> </li> </ol> <pre><code>class Phone:\n    def __init__(self, price, brand, camera):\n        print(\"Inside phone constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\n\nclass SmartPhone(Phone):\n    def buy(self):\n        print(\"Buying a smartphone.\")\n        # Syntax to call the buy method of parent class\n        super().buy()\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nsmartphone.buy()\n</code></pre> <pre><code># super -&gt; constructor\nclass Phone:\n    def __init__(self, price, brand, camera):\n        self.price = price\n        self.brand = brand\n        self.camera = camera\n\nclass SmartPhone(Phone):\n    def __init__(self, price, brand, camera, os, ram):\n        print(\"Inside phone constructor\")\n        super().__init__(price, brand, camera)\n        self.os = os\n        self.ram = ram\n        print(\"Inside smartphone constructor\")\n\nsmartphone = SmartPhone(50000, \"Samsung\", 12, \"Android\", 4)\n\nprint(smartphone.os)\nprint(smartphone.brand)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#inheritance-in-summary","title":"Inheritance in Summary","text":"<ul> <li>A class can inherit from another class.</li> <li>Inheritance improves code reuse.</li> <li>Constructor, attributes, methods get inherited to the child class.</li> <li>The parent has no access to the child class.</li> <li>Private properties of parent are not accessible directly in child class.</li> <li>Child class can override the attributes or methods. This is called method overriding.</li> <li><code>super()</code> is an inbuilt function which is used to invoke the parent class methods and constructor.</li> </ul> <pre><code># # Guess the output\n# class Parent:\n#     def __init__(self, num):\n#         self.__num = num\n\n#     def get_num(self):\n#         return self.__num\n\n# class Child(Parent):\n#     def __init__(self, num, val):\n#         super().__init__(num)\n#         self.__val = val\n\n#     def get_val(self):\n#         return self.__val\n\n# son = Child(100, 200)\n# print(son.get_num())\n# print(son.get_val())\n</code></pre> <pre><code># # Guess the output\n# class Parent:\n#     def __init__(self):\n#         self.num = 100\n\n# class Child(Parent):\n#     def __init__(self):\n#         super().__init__()\n#         self.var = 200\n\n#     def show(self):\n#         print(self.num)\n#         print(self.var)\n\n# son = Child()\n# son.show()\n</code></pre> <pre><code># # Guess the output\n# class Parent:\n#     def __init__(self):\n#         self.__num = 100\n\n#     def show(self):\n#         print(\"Parent:\", self.__num)\n\n# class Child(Parent):\n#     def __init__(self):\n#         super().__init__()\n#         self.__var = 10\n\n#     def show(self):\n#         print(\"Child:\", self.__var)\n\n# obj = Child()\n# obj.show()\n</code></pre> <pre><code># # Guess the output\n# class Parent:\n#     def __init__(self):\n#         self.__num = 100\n\n#     def show(self):\n#         print(\"Parent:\", self.__num)\n\n# class Child(Parent):\n#     def __init__(self):\n#         super().__init__()\n#         self.__var = 10\n\n#     def show(self):\n#         print(\"Child:\", self.__var)\n\n\n# obj = Child()\n# obj.show()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#types-of-inheritance","title":"Types of Inheritance","text":"<p>Inheritance in Python allows a class (child class) to derive properties and behaviors from another class (parent class). Python supports several types of inheritance:</p> <p>1. Single Inheritance - Definition: A single child class inherits from a single parent class. - Usage: Simplifies code reuse and enhances modularity.</p> <p>2. Multiple Inheritance - Definition: A child class inherits from more than one parent class. - Usage: Useful when a class needs to inherit functionality from multiple classes. - Method Resolution Order (MRO): Determines the order in which methods are called in the inheritance hierarchy.</p> <p>3. Multilevel Inheritance - Definition: A child class inherits from a parent class, and another child class inherits from this child class. - Usage: Creates a chain of inheritance.</p> <p>4. Hierarchical Inheritance - Definition: Multiple child classes inherit from a single parent class. - Usage: Useful for creating subclasses with shared base functionality.</p> <p>5. Hybrid Inheritance - Definition: A combination of two or more types of inheritance (e.g., multiple and multilevel inheritance). - Usage: Allows for flexible and complex hierarchies.</p> <p>Summary Table</p> Type of Inheritance Description Example Single One parent, one child <code>A \u2192 B</code> Multiple Multiple parents, one child <code>A, B \u2192 C</code> Multilevel Chain of inheritance <code>A \u2192 B \u2192 C</code> Hierarchical One parent, multiple children <code>A \u2192 B, C</code> Hybrid Combination of multiple inheritance types Complex combinations <p>Each type of inheritance is suited for specific use cases and offers various levels of code reuse and modularity.</p> <pre><code># Single inheritance\nclass Phone:\n    def __init__(self, price, brand, camera):\n        self.price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\nclass SmartPhone(Phone):\n    pass\n\nSmartPhone(50000, \"Apple\", 13).buy()\n</code></pre> <pre><code># Multilevel inheritance\nclass Product:\n    def review(self):\n        print(\"Product customer review\")\n\nclass Phone(Product):\n    def __init__(self, price, brand, camera):\n        print(\"Inside phone constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\nclass SmartPhone(Phone):\n    pass\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nsmartphone.buy()\nsmartphone.review()\n</code></pre> <pre><code># Hierarchical inheritance\nclass Phone:\n    def __init__(self, price, brand, camera):\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone.\")\n\nclass SmartPhone(Phone):\n    pass\n\nclass FeaturePhone(Phone):\n    pass\n\nSmartPhone(50000, \"Apple\", 13).buy()\nFeaturePhone(5000, \"Nokia\", 2).buy()\n</code></pre> <pre><code># Multiple inheritance\nclass Phone:\n    def __init__(self, price, brand, camera):\n        print(\"Inside price constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\nclass Product:\n    def review(self):\n        print(\"Customer review\")\n\nclass SmartPhone(Phone, Product):\n    pass\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nsmartphone.buy()\nsmartphone.review()\n</code></pre> <p>The Diamond Problem</p> <p>The diamond problem arises in multiple inheritance, where a class inherits from two classes that both inherit from a common base class. It creates ambiguity about which path should be followed when invoking methods or accessing attributes from the common base class.</p> <p>Why is it Called the Diamond Problem? The inheritance hierarchy forms a diamond shape:</p> <pre><code>      A\n     / \\\n    B   C\n     \\ /\n      D\n</code></pre> <ul> <li>Class A: The common base class.</li> <li>Classes B and C: Intermediate classes that inherit from A.</li> <li>Class D: The child class that inherits from both B and C.</li> </ul> <p>When a method in <code>D</code> calls a method or accesses an attribute from <code>A</code>, it's ambiguous whether the call should go through <code>B</code> or <code>C</code>.</p> <p>Method Resolution Order (MRO)</p> <p>To resolve the diamond problem, Python uses the Method Resolution Order (MRO). The MRO determines the order in which classes are searched for methods or attributes.</p> <p>Python uses the C3 linearization algorithm (also called C3 superclass linearization) to compute the MRO. The MRO ensures: 1. Consistency: A method is called from the first valid class found in the order. 2. No Ambiguity: Python resolves the order of calls without ambiguity. 3. Breadth-First Resolution: The child class is searched first, followed by its parents (left to right), and then their parents.</p> <pre><code># The diamond problem\nclass Phone:\n    def __init__(self, price, brand, camera):\n        print(\"Inside phone constructor\")\n        self.__price = price\n        self.brand = brand\n        self.camera = camera\n\n    def buy(self):\n        print(\"Buying a phone\")\n\nclass Product:\n    def buy(self):\n        print(\"Product buy method\")\n\n# Method resolution order\nclass SmartPhone(Product, Phone):\n    pass\n\nsmartphone = SmartPhone(50000, \"Apple\", 13)\nsmartphone.buy()\n</code></pre> <pre><code># # Guess the output\n# class A:\n#     def m1(self):\n#         return 20\n\n# class B(A):\n#     def m1(self):\n#         return 30\n#     def m2(self):\n#         return 40\n\n# class C(B):\n#     def m2(self):\n#         return 20\n\n# obj1 = A()\n# obj2 = B()\n# obj3 = C()\n# print(obj1.m1() + obj3.m1() + obj3.m2())\n</code></pre> <pre><code># # Guess the output\n# class A:\n#     def m1(self):\n#         return 20\n\n# class B(A):\n#     def m1(self):\n#         val = super().m1()+30\n#         return val\n\n# class C(B):\n#     def m1(self):\n#         val = self.m1()+20\n#         return val\n\n# obj = C()\n# print(obj.m1())\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#polymorphism","title":"Polymorphism","text":"<p>Polymorphism is a concept in object-oriented programming that allows objects of different classes to be treated as objects of a common superclass. It enables a single interface (method or function) to operate on different types of objects.</p> <p>Key Points: 1. \"Many forms\": The term \"polymorphism\" means \"many forms.\" A single method or function behaves differently based on the object or class it is acting upon. 2. Dynamic Behavior: The actual implementation executed is determined at runtime, making Python a dynamically-typed and polymorphic language.</p> <p>Types of Polymorphism</p> <p>1. Method Overriding (Runtime Polymorphism) - Definition: A subclass provides a specific implementation for a method already defined in its parent class.</p> <p>2. Method Overloading (Static Polymorphism) - Definition: A method in a class is defined with different parameter configurations. However, Python doesn't support true method overloading. It can be emulated using default arguments or variable-length arguments.</p> <p>3. Operator Overloading - Definition: Operators like <code>+</code>, <code>*</code>, etc., behave differently depending on the operands. This is implemented using special methods (dunder methods).</p> <p>4. Polymorphism with Functions and Objects - Definition: A single function can operate on objects of different classes, provided they share a common interface.</p> <p>Advantages of Polymorphism 1. Code Reusability: Same interface can work with different data types or classes. 2. Flexibility: New functionality can be added without altering existing code. 3. Extensibility: Objects can evolve while maintaining the same interface.</p> <p>Summary Table</p> Type Example Method Overriding Subclasses redefining parent class methods. Method Overloading Same method name with different parameters (emulated in Python). Operator Overloading Operators working differently based on operand type. Polymorphism with Functions Functions accepting objects of different classes. <p>Polymorphism enables flexibility and dynamic behavior in Python, making it a core concept in object-oriented programming.</p> <pre><code># # Method overloading\n# class Shape:\n\n#     def area(self, radius):\n#         return 3.14*radius*radius\n\n#     def area(self, l, b):\n#         return l*b\n\n# s = Shape()\n\n# s.shape(2)\n# s.area(3, 4)\n</code></pre> <pre><code># Alternative way of method overloading in python\nclass Shape:\n    def area(self, a, b=0):\n        if b == 0:\n            return 3.14*a*a\n        else:\n            return a*b\n\ns = Shape()\n\nprint(s.area(2))\nprint(s.area(3, 4))\n</code></pre> <pre><code># Operator overloading\n\"Hello\" + \" World\" # '+' operator is used for concatenation\n</code></pre> <pre><code>4 + 5 # '+' operator is used for addition\n</code></pre> <pre><code>[1, 2, 3] + [4, 5] # '+' operator is used for merginge\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/10_object_oriented_programming_3/#abstraction","title":"Abstraction","text":"<p>Abstraction is an object-oriented programming (OOP) concept that focuses on hiding the implementation details of a feature while exposing only the necessary functionalities. It allows developers to work with high-level interfaces and simplifies the process of designing and using complex systems.</p> <p>Key Characteristics of Abstraction</p> <ol> <li>Hiding Details:</li> <li>The user doesn't need to know how the functionality is implemented; they only interact with the interface.</li> <li> <p>This reduces complexity and enhances security by restricting access to sensitive code.</p> </li> <li> <p>High-Level Interfaces:</p> </li> <li> <p>Only the essential features of an object are shown, while the internal implementation is hidden.</p> </li> <li> <p>Implemented Using Abstract Classes and Interfaces:</p> </li> <li>Abstract classes are blueprints for other classes.</li> <li>They cannot be instantiated directly and are meant to be inherited.</li> </ol> <p>How Abstraction Works in Python</p> <p>Python provides abstraction through the use of: 1. Abstract Base Classes (ABCs):    - Defined using the <code>abc</code> module.    - Classes with one or more abstract methods are abstract classes. 2. Abstract Methods:    - Methods declared but not implemented in an abstract class.</p> <p>Advantages of Abstraction</p> <ol> <li>Reduces Complexity:</li> <li>Users interact with the essential features without worrying about the implementation.</li> <li>Improves Code Reusability:</li> <li>Abstract classes and methods can be reused across different projects.</li> <li>Promotes Flexibility:</li> <li>Allows developers to change implementations without altering the interface.</li> <li>Enhances Security:</li> <li>Hides sensitive or unnecessary details from the end user.</li> </ol> <p>Summary</p> Concept Description Abstraction Hiding implementation details and exposing functionalities. Abstract Class A class with abstract methods that acts as a blueprint. Abstract Method A method declared in an abstract class but not implemented. Concrete Class A class that implements the abstract methods. <p>Abstraction helps to design robust systems by defining clear interfaces, hiding implementation details, and enabling developers to focus on high-level interactions.</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BankApp(ABC):\n    def database(self):\n        print(\"Connected to database\")\n\n    @abstractmethod\n    def security(self):\n        pass\n\n    @abstractmethod\n    def display(self):\n        pass\n</code></pre> <pre><code>class MobileApp(BankApp):\n\n    def mobile_login(self):\n        print(\"login into mobile\")\n\n    def security(self):\n        print(\"mobile security\")\n\n    def display(self):\n        print(\"display\")\n</code></pre> <pre><code>mob = MobileApp()\n</code></pre> <pre><code>mob.security()\nmob.display()\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/11_file_handling_in_python/","title":"File Handling In Python","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/11_file_handling_in_python/#file-handling-in-python","title":"File Handling in Python","text":"<p>In this notebook, we'll explore basic file handling operations as well as using the <code>os</code> and <code>glob</code> modules.</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/11_file_handling_in_python/#1-basic-file-handling-in-python","title":"1. Basic File Handling in Python","text":"<pre><code># Writing to a file\nwith open('sample.txt', 'w') as f:\n    f.write('Hello, World!\\n')\n    f.write('Welcome to file handling in Python.\\n')\nprint(\"File written successfully.\")\n</code></pre> <pre><code># Reading from a file\nwith open('sample.txt', 'r') as f:\n    content = f.read()\nprint(\"File content:\")\nprint(content)\n</code></pre> <pre><code># Appending to a file\nwith open('sample.txt', 'a') as f:\n    f.write('Appending a new line!\\n')\nprint(\"Line appended successfully.\")\n</code></pre> <pre><code># Reading the file again\nwith open('sample.txt', 'r') as f:\n    content = f.read()\nprint(\"Updated file content:\")\nprint(content)\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/11_file_handling_in_python/#2-using-the-os-module","title":"2. Using the <code>os</code> Module","text":"<pre><code>import os\n\n# Get current working directory\ncwd = os.getcwd()\nprint(f\"Current working directory: {cwd}\")\n</code></pre> <pre><code># List all files and directories in the current directory\nentries = os.listdir(cwd)\nprint(\"Directory contents:\")\nfor entry in entries:\n    print(entry)\n</code></pre> <pre><code># Check if a file or directory exists\nfile_exists = os.path.exists('sample.txt')\nprint(f\"Does 'sample.txt' exist? {file_exists}\")\n</code></pre> <pre><code># Create a new directory\nnew_dir = 'test_dir'\nif not os.path.exists(new_dir):\n    os.mkdir(new_dir)\n    print(f\"Directory '{new_dir}' created.\")\nelse:\n    print(f\"Directory '{new_dir}' already exists.\")\n</code></pre> <pre><code># Rename a file\nos.rename('sample.txt', 'renamed_sample.txt')\nprint(\"File renamed to 'renamed_sample.txt'.\")\n</code></pre> <pre><code># Remove a file\nos.remove('renamed_sample.txt')\nprint(\"File 'renamed_sample.txt' removed.\")\n</code></pre> <pre><code># Remove the directory\nos.rmdir('test_dir')\nprint(f\"Directory '{new_dir}' removed.\")\n</code></pre>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/Notebooks/11_file_handling_in_python/#3-using-the-glob-module","title":"3. Using the <code>glob</code> Module","text":"<pre><code>import glob\n\n# Create some files for demonstration\nfor i in range(3):\n    with open(f'file_{i}.txt', 'w') as f:\n        f.write(f\"This is file {i}\\n\")\n\nprint(\"Demo files created: file_0.txt, file_1.txt, file_2.txt\")\n</code></pre> <pre><code># Find all .txt files\ntxt_files = glob.glob('*.txt')\nprint(\"List of .txt files:\")\nprint(txt_files)\n</code></pre> <pre><code># You can also use wildcard patterns\nfile_1_pattern = glob.glob('file_1.*')\nprint(\"Files matching 'file_1.*':\")\nprint(file_1_pattern)\n</code></pre> <pre><code># Cleanup: remove demo files\nfor file in txt_files:\n    os.remove(file)\nprint(\"Demo files removed.\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/","title":"Geospatial Data Science with Python","text":"<p>This project contains several notebooks covering geospatial data analysis with Python.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#01-working-with-projections","title":"01. Working with Projections","text":"<ul> <li>Working with GCS and PCS</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#02-exploring-geospatial-packages","title":"02. Exploring Geospatial Packages","text":"<ul> <li>GeoPandas</li> <li>Spatial Data Structures</li> <li>Spatial Data Manipulation</li> <li>Geocoding</li> <li>Shapely</li> <li>Shapely Properties and Methods</li> <li>Rasterio</li> <li>ipyLeaflet</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#03-exploratory-spatial-data-analysis","title":"03. Exploratory Spatial Data Analysis","text":"<ul> <li>Exploratory Data Visualization</li> <li>Creating Choropleth Map from Points</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/","title":"Working With Gcs And Pcs","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#working-with-geographic-and-projected-coordinate-system","title":"Working with Geographic and Projected Coordinate System","text":"<p>Author: Krishnagopal Halder 1. Geographic Coordinate System: The geographic coordinate system (GCS) uses a three-dimensional spherical surface to represent the Earth's shape. It is based on a datum, which defines the position and orientation of the coordinate system with respect to the Earth. The GCS uses angular units (degrees) to express coordinates. The most common GCS is the WGS84 (World Geodetic System 1984), which is widely used for global positioning and mapping purposes.</p> <ol> <li>Projected Coordinate System: A projected coordinate system (PCS) is a two-dimensional Cartesian coordinate system that flattens the Earth's surface onto a flat map. It uses a map projection to transform the spherical coordinates into x-y coordinates. Map projections mathematically convert the curved Earth's surface onto a flat surface, introducing distortions in distance, shape, area, or direction.</li> </ol> <p>This notebook provides an overview of how to work with geographic and projected coordinate systems using Python. Understanding coordinate systems is crucial for geospatial data analysis, mapping, and spatial analysis tasks. This guide introduces the concepts of geographic and projected coordinate systems and demonstrates how to perform coordinate system transformations, conversions, and visualizations using Python libraries.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#02-setting-up-the-working-directory","title":"02. Setting Up the Working Directory","text":"<pre><code># Checking the current working directory\nprint(os.getcwd())\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\Shapafiles\"\nos.chdir(file_path)\n# Checking the current working directory\nprint(os.getcwd())\n</code></pre> <pre><code># Checking the files in the current working directory\nprint(os.listdir())\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#03-reading-shapefiles-with-geopandas","title":"03. Reading Shapefiles with GeoPandas","text":"<p>GeoPandas is an open-source Python library that extends the capabilities of the popular data manipulation library, pandas, to handle geospatial data. It provides an easy and intuitive way to work with geospatial data, combining the power of pandas' data manipulation and analysis with the geometric operations and spatial functionality of other geospatial libraries, such as Shapely and Fiona.</p> <pre><code># Reading a Natural Earth world shapefile with GeoPandas\nworld = gpd.read_file(\"ne_10m_land.shp\")\n# Reading a Natural Earth Populated Places Point shapefile\npop_cities = gpd.read_file(\"ne_10m_populated_places_simple.shp\")\n</code></pre> <pre><code># Checking the first five rows of the populated places dataset\npop_cities.head()\n</code></pre> <pre><code># Extracting the Admin-0 capitals only\ncapitals = pop_cities[pop_cities[\"featurecla\"]==\"Admin-0 capital\"]\n</code></pre> <pre><code># Checking the Admin-0 capitals\ncapitals.head()\n</code></pre> <pre><code># Reading a Natural Earth 10m interval Graticules shapefile\ngrat = gpd.read_file(\"ne_110m_graticules_10.shp\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#04-checking-the-metadata-of-the-crs","title":"04. Checking the Metadata of the CRS","text":"<pre><code># Checking the CRS metadata of the world shapefile\nworld.crs\n</code></pre> <pre><code># Checking the CRS metadata of the cities shapefile\ncapitals.crs\n</code></pre> <pre><code># Checking the CRS metadata of the graticules shapefile\ngrat.crs\n</code></pre> <pre><code># Checking if all the shapefiles are in same CRS or not\nworld.crs == capitals.crs == grat.crs\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#05-plotting-the-shapefiles-in-a-map","title":"05. Plotting the Shapefiles in a Map","text":"<pre><code>fig, ax = plt.subplots(figsize=(12, 10))\nworld.plot(ax=ax, color=\"darkgray\")\ncapitals.plot(ax=ax, color=\"black\", markersize=8, label=\"Populated Places\")\ngrat.plot(ax=ax, color=\"lightgray\", linewidth=0.5)\nax.set(xlabel=\"Longitude(Degrees)\", ylabel=\"Latitude(Degrees)\", title=\"Populated Places showing in WGS 1984 Datum\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#06-reprojecting-the-data","title":"06. Reprojecting the Data","text":"<p>Reprojection, also known as coordinate transformation or coordinate conversion, refers to the process of converting spatial data from one coordinate system to another. It involves transforming the coordinates of geographic features from their original reference system to a different reference system, often with a different datum or projection.</p> <p>Reprojection is necessary when working with geospatial data that is collected, stored, or analyzed in different coordinate systems. Each coordinate system has its own set of parameters, such as the datum, map projection, units of measurement, and spatial reference. Reprojection ensures that different datasets or layers align properly and can be integrated or overlaid accurately in a consistent spatial reference system.</p> <pre><code># Reprojecting the data from WGS 1984 CRS to Azimuthal Equidistant Projection\nworld_ae = world.to_crs(\"ESRI:54032\")\ncapitals_ae = capitals.to_crs(\"ESRI:54032\")\ngrat_ae = grat.to_crs(\"ESRI:54032\")\n</code></pre> <pre><code># Checking the reprojected CRS\nworld_ae.crs\n</code></pre> <pre><code>capitals_ae.crs\n</code></pre> <pre><code>grat_ae.crs\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/01_Working_with_Projections/01_Working_with_GCS_and_PCS/#07-plotting-the-reprojected-shapefiles-in-a-map","title":"07. Plotting the Reprojected Shapefiles in a Map","text":"<pre><code># Creating a function that will take the files and plot them in a map\ndef plot_map_layers(gdf_1, gdf_2, gdf_3, title, unit, legend):\n    fig, ax = plt.subplots(figsize=(12, 10))\n    gdf_1.plot(ax=ax, color=\"darkgray\")\n    gdf_2.plot(ax=ax, color=\"black\", markersize=8, label=legend)\n    gdf_3.plot(ax=ax, color=\"lightgray\", linewidth=0.5)\n    ax.set(xlabel=\"X Coordinate-\" + unit,\n           ylabel=\"Y Coordinate-\" + unit,\n           title=title\n           )\n    plt.legend()\n    plt.show()\n</code></pre> <pre><code># Using the function to plot the map\nmap_title = \"Populated Places showing in Azimuthal Equidistant Projection\"\nplot_map_layers(world_ae, capitals_ae, grat_ae, map_title, \"Meters\", \"Poulated Places\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/","title":"Geopandas","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#geopandas","title":"GeoPandas","text":"<p>Author: Krishnagopal Halder</p> <p>Geopandas is an open-source Python library that extends the capabilities of the popular data analysis library, pandas, by adding geospatial data processing and manipulation capabilities. It provides a convenient and efficient way to work with geospatial data, such as points, lines, and polygons, within the pandas DataFrame structure.</p> <p>Geopandas leverages the functionalities of other powerful geospatial libraries, including Shapely, Fiona, and Pyproj, to handle geometric operations, file I/O, and coordinate transformations, respectively. By integrating these libraries, Geopandas simplifies the process of reading, manipulating, analyzing, and visualizing geospatial data.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#02-setting-up-the-working-directory","title":"02. Setting Up the Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Change the current working directory\nfile_path =  r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\Shapafiles\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the current working directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#03-reading-and-writing-spatial-data-with-geopandas","title":"03. Reading and Writing Spatial Data with GeoPandas","text":"<pre><code># Reading data from local file\nland = gpd.read_file(file_path+\"\\\\ne_10m_land.shp\")\n</code></pre> <pre><code># Reading data from URL\nurl = \"https://d2ad6b4ur7yvpq.cloudfront.net/naturalearth-3.3.0/ne_110m_land.geojson\"\nland_url = gpd.read_file(url)\n</code></pre> <p>Dataset Description: * TIGER/Line Shapefiles: TIGER/Line Shapefiles, commonly referred to as TIGER shapefiles or simply TIGER files, are a set of geospatial data files provided by the United States Census Bureau. TIGER stands for Topologically Integrated Geographic Encoding and Referencing. In TIGER shapefiles, the \"US TIGER State data\" refers to the boundaries and associated attributes of individual states within the United States. It represents the geographic extent of each state and provides information about their administrative divisions. The core TIGER/Line Files and Shapefiles do not include demographic data, but they do contain geographic entity codes (GEOIDs) that can be linked to the Census Bureau\u2019s demographic data, available on data.census.gov.</p> <ul> <li>Core Based Statistical Areas (CBSA): Core Based Statistical Areas (CBSAs) provides the geographic boundaries and point information for more than 900 statistical regions defined by the U.S. Office of Management and Budget. A CBSA represents a highly populated core area and adjacent communities that have a high degree of economic and social integration with the core. CBSAs consist of counties and county equivalents, and are defined in two categories: (1) Metropolitan Statistical Areas, (2) Micropolitan Statistical Areas.</li> </ul> <pre><code># Reading data stored in a zip file\nzip_path1 = file_path + \"\\\\tl_2021_us_state.zip\"\nus_state = gpd.read_file(zip_path1)\n\nzip_path2 = file_path + \"\\\\tl_2021_us_cbsa.zip\"\nus_cbsa = gpd.read_file(zip_path2)\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#04-filtering-the-data","title":"04. Filtering the Data","text":"<pre><code># Printing the first 5 records of the us_state geodataframe\nus_state.head()\n</code></pre> <pre><code># Filtering the California from us_state file\ncalifornia = us_state[us_state[\"NAME\"]==\"California\"]\n</code></pre> <ul> <li>'mask' Parameter: In Geopandas, the mask parameter is commonly used in spatial operations to select or filter specific geometries based on a spatial relationship with another geometry or a set of geometries.</li> </ul> <pre><code># Creating a new geodataframe that includes cbsa areas of California\nca_cbsas = gpd.read_file(file_path + \"\\\\tl_2021_us_cbsa.zip\", mask=california)\nca_cbsas.head()\n</code></pre> <pre><code># Using a bouning box to filter the data\nbounding_box = (-128.82239, 42.15933, -123.82246, 38.7)\n\n# Filtering the us_cbsa data based on bounding box\nca_cbsas_bbox = gpd.read_file(file_path + \"\\\\tl_2021_us_cbsa.zip\", bbox=bounding_box)\nca_cbsas_bbox.head()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#05-writing-the-data","title":"05. Writing the Data","text":"<pre><code># Define the output path\noutput_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\Shapafiles\"\n\n# Writing the ca_cbsas data as a shapefile\nca_cbsas.to_file(output_path+\"\\ca_cbsas.shp\")\n\n# Writing the ca_cbsas data as GeoJSON\nca_cbsas.to_file(output_path+\"\\ca_cbsas.geojson\", driver=\"GeoJSON\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/01_GeoPandas/#06-spatial-data-visualization","title":"06. Spatial Data Visualization","text":"<pre><code># Plotting the ca_cbsas data\nfig, ax = plt.subplots(figsize=(8, 6))\nca_cbsas.plot(ax=ax)\nplt.xlabel(\"Longitude (X)\")\nplt.ylabel(\"Latitude (Y)\")\nplt.title(\"Core Based Statistical Areas (CBSA) of California\")\nplt.show()\n</code></pre> <pre><code># Plotting simple choropleth map on ca_cbsas data\nfig, ax = plt.subplots(figsize=(8, 6))\nca_cbsas.plot(ax=ax,\n              cmap=\"Spectral\",\n              column=\"ALAND\",\n              edgecolor=\"black\",\n              linewidth=0.5,\n              legend=True\n             )\nplt.title(\"Choropleth Map showing Land Areas of California CBSAs\\n\")\nplt.xlabel(\"Longitude (X)\")\nplt.ylabel(\"Latitude (Y)\")\nplt.show()\n</code></pre> <pre><code># Plotting the choropleth map of world's population\nworld_pop = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\nworld_pop.head()\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(12, 10))\nworld_pop.plot(ax=ax,\n               column=\"pop_est\",\n               cmap=\"Spectral\",\n               edgecolor=\"black\",\n               linewidth=0.5\n              )\nplt.title(\"Choropleth Map showing World Population Estimates\")\nplt.xlabel(\"Longitude (X)\")\nplt.ylabel(\"Latitude (Y)\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/","title":"Spatial Data Structures","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#spatial-data-structures-and-its-methods-and-attributes","title":"Spatial Data Structures and its Methods and Attributes","text":"<p>In Geopandas, the spatial data structure is based on the GeoDataFrame, which is an extension of the pandas DataFrame with additional functionality to handle geospatial data. The spatial data structure in Geopandas allows you to work with spatially referenced data, such as points, lines, and polygons, within a tabular framework. A GeoDataFrame and a GeoSeries are two fundamental components of the Geopandas library, which extends the capabilities of pandas for working with geospatial data.</p> <p>1. GeoDataFrame: A GeoDataFrame is a tabular data structure that extends the functionality of pandas DataFrame by incorporating a \"geometry\" column. This column stores geometric objects associated with each row of the DataFrame. The geometries can represent points, lines, polygons, or other spatial entities.</p> <p>Key features:</p> <ul> <li>DataFrame Structure: A GeoDataFrame retains the tabular structure of a pandas DataFrame, allowing for efficient indexing, filtering, and manipulation of both the attribute data and geometric information.</li> <li>Geometry Column: The \"geometry\" column in a GeoDataFrame contains the geometric objects representing the spatial features. It can store single geometries or collections of geometries.</li> <li>Attribute Data: A GeoDataFrame can have additional columns that store attribute data related to the spatial features. These columns can contain various information such as names, IDs, population figures, or any other relevant attributes.</li> <li>Coordinate Reference System (CRS): A GeoDataFrame includes information about the Coordinate Reference System, defining the spatial reference and coordinate system used by the geometries.</li> <li>Integration with Spatial Operations: The GeoDataFrame integrates with Geopandas' spatial operations, allowing for geometric manipulations, spatial joins, spatial queries, and other spatial analysis tasks.</li> </ul> <p>2. GeoSeries: A GeoSeries, on the other hand, is a one-dimensional array-like object that represents a series of geometric objects. It is based on pandas' Series but is specifically designed to handle spatial data.</p> <p>Key features:</p> <ul> <li>Series-like Behavior: A GeoSeries shares many similarities with a pandas Series, such as indexing, slicing, and applying functions or operations element-wise.</li> <li>Geometry Storage: The primary purpose of a GeoSeries is to store and manage geometric objects. Each element of the series represents a single geometry, such as a point, line, or polygon.</li> <li>Coordinate Reference System (CRS): A GeoSeries also includes information about the Coordinate Reference System, providing spatial reference and coordinate system details for the geometries within the series.</li> <li>Integration with Spatial Operations: Similar to a GeoDataFrame, a GeoSeries integrates with Geopandas' spatial operations, allowing for geometric manipulations, spatial queries, and other spatial analysis tasks at the individual geometry level.</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#02-setting-up-the-working-directory","title":"02. Setting Up the Working Directory","text":"<pre><code># Printing the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\Shapefiles\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the current working directory again\nos.getcwd()\n</code></pre> <pre><code># Printing all the shapefile names of the current working directory\nfor item in os.listdir():\n    if item.endswith(\".shp\"):\n        print(item)\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#03-reading-spatial-data-with-geopandas","title":"03. Reading Spatial Data with GeoPandas","text":"<pre><code># Reading the ne_10m_land dataset\nland = gpd.read_file(file_path + \"\\\\ne_10m_land.shp\")\n</code></pre> <pre><code># Checking the first 5 rows of the shapefile\nland.head()\n</code></pre> <pre><code># Checking the datatype of the land variable\ntype(land)\n</code></pre> <pre><code># Checking the datatype of the geometry column of the land geodataframe\ntype(land[\"geometry\"])\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#04-attributes-of-geoseries-data-structure","title":"04. Attributes of GeoSeries Data Structure","text":"<pre><code># Defining the geometry column in a seperate variable\nland_geometry = land[\"geometry\"]\n</code></pre> <pre><code># Checking the type of land_geometry_variable\ntype(land_geometry)\n</code></pre> <p>crs:  The crs attribute stores the spatial reference and coordinate system information associated with the geometries in the GeoSeries. It provides metadata about how the coordinates are interpreted and projected in the real world.</p> <pre><code># Checking the CRS information of the geometry using crs() attribute\nland_geometry.crs\n</code></pre> <p>geom_type: The geom_type attribute returns the geometry type of each geometry in the GeoSeries. It indicates whether each geometry is a point, line, polygon, or another geometric type.</p> <pre><code># Checking the geometry type of each feature in the GeoSeries\nprint(land_geometry.geom_type)\n</code></pre> <p>area: The area attribute calculates the area of each geometry in the GeoSeries. The area is computed based on the spatial reference system (CRS) of the GeoSeries.</p> <pre><code># Calculating the area of each geometry in the GeoSeries\nland_geometry.area\n</code></pre> <p>bounds: The bounds attribute returns a bounding box (or minimum bounding rectangle) for each geometry in the GeoSeries. The bounding box represents the minimum and maximum x and y coordinates that enclose the geometry.</p> <pre><code># Checking the bounding box for each geometry\nland_geometry.bounds\n</code></pre> <p>total_bounds: The total_bounds attribute returns the overall bounding box that encompasses all geometries in the GeoSeries. It provides the minimum and maximum x and y coordinates that cover the entire collection of geometries.</p> <pre><code># Checking the overall bounding box of the geometries\nland_geometry.total_bounds\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/02_Spatial_Data_Structures/#05-methods-of-geoseries-data-structure","title":"05. Methods of GeoSeries Data Structure","text":"<p>to_crs: The to_crs method allows you to transform the coordinate reference system (CRS) of the geometries in a GeoSeries. It takes a CRS object or a string representation of a CRS as its argument. By applying to_crs, you can reproject the geometries to a different CRS, enabling spatial analysis and visualization in a consistent coordinate system.</p> <pre><code># Plotting the land_geometry map with the default WGS_84 Datum\nfig, ax = plt.subplots(figsize=(8, 6))\nland_geometry.plot(ax=ax)\nplt.title(\"World Map showing in WGS84 Datum\")\nplt.xlabel(\"Longitude (X)\")\nplt.ylabel(\"Latitude (Y)\")\nplt.show()\n</code></pre> <pre><code># Changing the CRS of the land_geometry to the Robinson Projection\nreprojected_land = land_geometry.to_crs(\"ESRI:54030\")\n# Checking the reprojected CRS\nreprojected_land.crs\n</code></pre> <pre><code># Reading a 10 degree graticules file with geopandas\ngrat = gpd.read_file(file_path + \"\\\\ne_110m_graticules_10.shp\").to_crs(\"ESRI:54030\")\n# Plotting the reprojected_land\nfig, ax = plt.subplots(figsize=(8, 6))\nreprojected_land.plot(ax=ax, color=\"darkgray\")\ngrat.plot(ax=ax, color=\"black\", linewidth=0.2)\nplt.title(\"World Map showing in Robinson Projection\")\nplt.xlabel(\"X Coordinate-Meters\")\nplt.ylabel(\"Y Coordinate-Meters\")\nplt.show()\n</code></pre> <p>centroid:  The centroid method computes the centroid (geometric center) of each polygon geometry in a GeoSeries. It returns a new GeoSeries with the centroid points as geometries. This method is applicable only to GeoSeries containing polygon geometries.</p> <pre><code># Defining the bounding box for the North America\nbounds = (-125.0, 24.0, -66.0, 49.0)\n</code></pre> <pre><code># Reading the us_state shapefile with geopandas and filtering with the bounding box\nus_state = gpd.read_file(file_path + \"\\\\tl_2021_us_state.zip\", bbox=bounds)\n</code></pre> <pre><code># Checking the us_state GeoDataframe\nus_state.head()\n</code></pre> <pre><code># Checking the CRS of the us_state file\nus_state.crs\n</code></pre> <pre><code># Calculating the centroids of all the ploygons in the us_state data\nus_centroids = us_state[\"geometry\"].centroid\n</code></pre> <pre><code># Plotting the us_state and us_centroids to a map\nfig, ax = plt.subplots(figsize=(8, 6))\nus_state.plot(ax=ax, \n              cmap=\"Set3\", \n              column=\"GEOID\",\n              linewidth=0.3,\n              edgecolor=\"black\")\nus_centroids.plot(ax=ax, \n                  color=\"red\",\n                  edgecolor=\"black\",\n                  markersize=10,\n                  linewidth=0.5,\n                  label=\"centroid\")\nplt.title(\"USA Map with State Centroids\")\nplt.xlabel(\"Longitude (X)\")\nplt.ylabel(\"Latitude (Y)\")\nplt.legend()\nplt.show()\n</code></pre> <p>distance:  The distance method calculates the Euclidean distance between each geometry in a GeoSeries and a provided geometry. The provided geometry can be a single point, line, or polygon. The method returns a new GeoSeries with the distances calculated for each geometry.</p> <pre><code># Adding the centroid values in a separate column in us_states GeoDataframe\nus_state[\"centroid\"] = us_centroids\n</code></pre> <pre><code># Checking the column names after adding the centroid column\nus_state.columns\n</code></pre> <pre><code># Extracting the centroid value of the California state\ncalifornia_centroid = us_state[\"centroid\"][us_state[\"NAME\"]==\"California\"].to_crs(\"EPSG:32610\").values\n# Extracting the centroid value of the Oregon state\noregon_centroid = us_state[\"centroid\"][us_state[\"NAME\"]==\"Oregon\"].to_crs(\"EPSG:32610\").values\n</code></pre> <pre><code># Checking the CRS of the california_centroid\ncalifornia_centroid.crs\n</code></pre> <pre><code># Printing the centroid value of the California and New York States\nprint(\"California Centroid:\", california_centroid)\nprint(\"Oregon Centroid:\", oregon_centroid)\n</code></pre> <pre><code># Calculating the distance between California and New York using distance() method\nCA_OR_distance = california_centroid.distance(oregon_centroid)\n# Converting the distance into kilometers\ndistance_km = CA_OR_distance[0] / 1000\n# Printing the distance\nprint(\"The euclidean distance between California and Oregon is\", \n      round(distance_km, 2), \n      \"Kilometers.\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/03_Spatial_Data_Manipulation/","title":"Spatial Data Manipulation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/03_Spatial_Data_Manipulation/#spatial-data-manipulations","title":"Spatial Data Manipulations","text":""},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/03_Spatial_Data_Manipulation/#importing-the-required-libraries","title":"Importing the Required Libraries","text":"<pre><code>import os\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/03_Spatial_Data_Manipulation/#setting-up-the-working-directory","title":"Setting Up the Working Directory","text":"<pre><code># Checking the Current Working Directory\nos.getcwd()\n</code></pre> <pre><code># Changing the Current Working Directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\Shapafiles\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the Current Working Directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/03_Spatial_Data_Manipulation/#reading-spatial-data-with-geopandas","title":"Reading Spatial Data with GeoPandas","text":"<pre><code># Reading the natural earth \n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/","title":"Geocoding","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#geocoding","title":"Geocoding","text":""},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import os\nimport pandas as pd\nimport geopandas as gpd\nimport geopy\nfrom geopandas.tools import geocode\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#setting-up-the-current-working-directory","title":"Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\CSVs\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the current working directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#reading-the-csv-files-using-pandas","title":"Reading the CSV Files using Pandas","text":"<pre><code># Reading the Kolkata_City_Attractions.csv file using Pandas\ncsv_name = \"\\\\Kolkata_City_Attractions.csv\"\ncityAttraction = pd.read_csv(file_path + csv_name)\n</code></pre> <pre><code># Checking the first 5 rows of the pandas dataframe\ncityAttraction.head()\n</code></pre> <pre><code># Extracting the 'Address' column from the dataframe as a pandas series\naddresses = cityAttraction[\"Address\"]\n</code></pre> <pre><code># Printing the addresses\naddresses\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#setting-up-the-api-of-mapquest","title":"Setting Up the API of Mapquest","text":"<pre><code># Defining the API Key\napi_key = creds.mapquest_api_key\n# Defining the provider\nprovider = \"openmapquest\"\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/04_Geocoding/#applying-the-geocoding","title":"Applying the Geocoding","text":""},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/","title":"Shapely","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#shapely","title":"Shapely","text":"<p>Author: Krishnagopal Halder</p> <p>Shapely is a powerful Python library for geometric operations and manipulations. It provides a convenient and intuitive interface for working with geometric objects such as points, lines, polygons, and more. Developed by Sean Gillies, Shapely is built on top of the widely-used GEOS (Geometry Engine - Open Source) library, which enables robust and efficient geometric computations.</p> <p>Shapely is particularly useful in the field of spatial analysis and geospatial applications. It allows you to perform a wide range of operations on geometric objects, including spatial relationships, geometric transformations, spatial predicates, and spatial measurements. Whether you need to calculate distances between points, determine if two polygons intersect, or simplify a complex geometry, Shapely provides the tools to accomplish these tasks.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import shapely.geometry\nimport shapely.wkt\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#02-creating-shapely-geometry-from-wkt","title":"02. Creating Shapely Geometry from WKT","text":"<p>In Shapely, WKT stands for \"Well-Known Text,\" which is a standard text-based representation format for geometric objects. Shapely provides support for parsing WKT strings into Shapely geometry objects and generating WKT representations from existing geometries.</p> <p>To work with WKT in Shapely, you need to import the shapely.wkt module.</p> <p>We have a WKT string \"Polygon((0 0, 0 -5, 10 -5, 10 0, 0 0))\", which represents a Polygon with five points. We use wkt.loads() to parse the WKT string and create a Shapely Ploygon geometry object.</p> <p>By leveraging the WKT format, you can easily create Shapely geometries from text-based representations and vice versa. This is particularly useful when you need to store or exchange geometric data in a standard format that is independent of specific programming languages or software packages.</p> <pre><code># Defining WKT strings to represent a polygon, a line string and a point\npoly_wkt = \"Polygon((0 0, 0 -5, 10 -5, 10 0, 0 0))\"\nline_wkt = \"LineString(0 0, 10 0, 10 -5)\"\npoint_wkt = \"Point(0 0)\"\n# Parsing the WKT strings to create Shapely geometry objects\npoly = shapely.wkt.loads(poly_wkt)\nline = shapely.wkt.loads(line_wkt) \npoint = shapely.wkt.loads(point_wkt)\n</code></pre> <pre><code># Printing the Polygon Shapely geometry\nprint(poly)\nprint(type(poly))\npoly\n</code></pre> <pre><code># Printing the LineString Shapely geometry\nprint(line)\nprint(type(line))\nline\n</code></pre> <pre><code># Printing the Point Shapely geometry\nprint(point)\nprint(type(point))\npoint\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#03-creating-shapely-geometry-from-geojson","title":"03. Creating Shapely Geometry from GeoJSON","text":"<p>The GeoJSON object is represented by a dictionary in Python, containing a \"type\" field and a \"coordinates\" field. The \"type\" field indicates the type of geometry, such as \"Point,\" \"Polygon,\" or \"MultiLineString.\" The \"coordinates\" field holds the coordinates defining the geometry.</p> <p>Shapely supports various GeoJSON geometry types, including Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection.</p> <pre><code>from shapely.geometry import Polygon, LineString, Point\n</code></pre> <pre><code># Defining GeoJSON to represent a polygon, a line string and a point\npoly_geojson = {\"type\": \"Polygon\",\n               \"coordinates\": [[[0, 0],\n                               [0, -5],\n                               [10, -5],\n                               [10, 0],\n                               [0, 0]]]}\nline_geojson = {\"type\": \"LineString\",\n               \"coordinates\": [[[0, 0],\n                               [10, 0],\n                               [10, -5]]]}\npoint_geojson = {\"type\": \"Point\",\n                \"coordinates\": [[0, 0]]}\n</code></pre> <pre><code># Parsing the GeoJSON to create Shapely geometry objects\npoly_GJ = Polygon([tuple(i) for i in poly_geojson[\"coordinates\"][0]])\nline_GJ = LineString([tuple(i) for i in line_geojson[\"coordinates\"][0]])\npoint_GJ = Point(tuple(point_geojson[\"coordinates\"][0]))\n</code></pre> <pre><code># Printing the Polygon Shapely geometry\nprint(poly_GJ)\nprint(type(poly_GJ))\npoly_GJ\n</code></pre> <pre><code># Printing the Line Shapely geometry\nprint(line_GJ)\nprint(type(line_GJ))\nline_GJ\n</code></pre> <pre><code># Printing the Point Shapely geometry\nprint(point_GJ)\nprint(type(point_GJ))\npoint_GJ\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#04-creating-shapely-geometry-from-list-of-coordinates","title":"04. Creating Shapely Geometry from List of Coordinates","text":"<p>To create a Shapely geometry from a list of coordinates in Python using the Shapely library, you can utilize the appropriate geometry constructor provided by Shapely. The specific constructor you use depends on the type of geometry you want to create, such as Point, LineString, Polygon, etc.</p> <p>By passing the list of coordinates to the Shapely geometry constructor, you can easily create a Shapely geometry object.</p> <pre><code># Defining Coordinate lists to represent a polygon, a line string and a point\npoly_coords = [(0, 0), (0, -5), (10, -5), (10, 0), (0, 0)]\nline_coords = [(0, 0), (10, 0), (10, -5)]\npoint_coords = [(0, 0)]\n</code></pre> <pre><code># Parsing the coordinate lists to create Shapely geometry objects\npoly_list = Polygon(poly_coords)\nline_list = LineString(line_coords)\npoint_list = Point(point_coords)\n</code></pre> <pre><code># Printing the Polygon Shapely geometry\nprint(poly_list)\nprint(type(poly_list))\npoly_list\n</code></pre> <pre><code># Printing the LineString Shapely geometry\nprint(line_list)\nprint(type(line_list))\nline_list\n</code></pre> <pre><code># Printing the Point Shapely geometry\nprint(point_list)\nprint(type(point_list))\npoint_list\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/05_Shapely/#05-creating-multipolygon-multipoint-and-multilinestring","title":"05. Creating MultiPolygon, MultiPoint and MultiLineString","text":"<pre><code>from shapely.geometry import MultiPolygon, MultiPoint, MultiLineString\n</code></pre> <p>MultiPolygon: A MultiPolygon is a collection of Polygon geometries. Each Polygon represents a closed area defined by a boundary consisting of linear rings. The MultiPolygon can be used to represent complex areas composed of multiple polygons. For instance, a region with multiple islands can be represented as a MultiPolygon.</p> <pre><code># Creating MultiPolygon from WKT\nmultiPoly_wkt = \"MultiPolygon(((0 0, 0 -10, 10 -10, 10 0, 0 0), (3 -2, 6 -8, 8 -2, 3 -2)))\"\nmulti_poly_wkt = shapely.wkt.loads(multiPoly_wkt)\nprint(multi_poly_wkt)\nprint(type(multi_poly_wkt))\nmulti_poly_wkt\n</code></pre> <pre><code># Creating MultiPolygon from List of Coordinates\npolygon1 = Polygon([(0, 0), (0, -10), (10, -10), (10, 0), (0, 0)])\npolygon2 = Polygon([(3, -2), (6, -8), (8, -2), (3, -2)])\nmultiPoly_list = [polygon1, polygon2]\nmulti_poly_list = MultiPolygon(multiPoly_list)\nprint(multi_poly_list)\nprint(type(multi_poly_list))\nmulti_poly_list\n</code></pre> <p>MultiPoint:  A MultiPoint is a collection of Point geometries. Each Point represents a specific location in the 2D space. The MultiPoint can be used to represent multiple discrete points or a set of spatially related locations.</p> <pre><code># Creating MultiPoint from WKT\nmultiPoint_wkt = \"MultiPoint((0 0), (5 5), (0 10))\"\nmulti_point_wkt = shapely.wkt.loads(multiPoint_wkt)\nprint(multi_point_wkt)\nprint(type(multi_point_wkt))\nmulti_point_wkt\n</code></pre> <pre><code># Creating MultiPoint from List of Coordinates\npoint1 = Point((0, 0))\npoint2 = Point((5, 5))\npoint3 = Point((0, 10))\nmultiPoint_list = [point1, point2, point3]\nmulti_point_list = MultiPoint(multiPoint_list)\nprint(multi_point_list)\nprint(type(multi_point_list))\nmulti_point_list\n</code></pre> <p>MultiLineString:  A MultiLineString is a collection of LineString geometries. Each LineString represents a sequence of connected line segments. The MultiLineString can be used to represent complex linear features composed of multiple LineString segments. For example, a road network with multiple interconnected road segments can be represented as a MultiLineString.</p> <pre><code># Creating MultiLineString from WKT\nmultiline_wkt = \"MultiLineString((0 0, 5 0, 5 -5), (10 0, 10 5, 15 5))\"\nmulti_line_wkt = shapely.wkt.loads(multiline_wkt)\nprint(multi_line_wkt)\nprint(type(multi_line_wkt))\nmulti_line_wkt\n</code></pre> <pre><code># Creating MultiLineString from List of Coordinates\nline1 = LineString([(0, 0), (5, 0), (5, -5)])\nline2 = LineString([(10, 0), (10, 5), (15, 5)])\nmultiLine_list = [line1, line2]\nmulti_line_list = MultiLineString(multiLine_list)\nprint(multi_line_list)\nprint(type(multi_line_list))\nmulti_line_list\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/","title":"Shapely Properties And Methods","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#shapely-properties-and-methods","title":"Shapely Properties and Methods","text":"<p>Shapely provides a variety of properties and methods that allow you to perform operations and analyze geometric objects. Here are some commonly used methods in Shapely:</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>from shapely.geometry import Polygon\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#creating-a-shapely-polygon-object","title":"Creating a Shapely Polygon Object","text":"<pre><code>poly = Polygon([(0, 0), (10, 0), (10, 15), (0, 0)])\n</code></pre> <pre><code>poly\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#converting-the-shapely-object-into-geoseries","title":"Converting the Shapely Object into GeoSeries","text":"<pre><code># Converting the Shapely Polygon into GeoPandas GeoSeries\npoly_geo = gpd.GeoSeries(poly)\n</code></pre> <pre><code># Plotting the GeoSeries\npoly_geo.plot()\nplt.show()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#shapely-attributes","title":"Shapely Attributes","text":"<p>In Shapely, geometric objects have several attributes that provide information about their properties. Here are the attributes related to bounds, lengths, and area:</p> <p>bounds Attribute: The bounds attribute of a geometric object returns a tuple representing the bounding box of the object. The bounding box consists of four values: (minx, miny, maxx, maxy), where minx and miny are the minimum x and y coordinates of the object, and maxx and maxy are the maximum x and y coordinates. The bounds attribute is useful for obtaining the extent or spatial coverage of a geometric object.</p> <pre><code># Getting the bounding box of the Shapely Polygon\npoly.bounds\n</code></pre> <p>length Attribute: The length attribute returns the length of a LineString or MultiLineString object. It represents the total length of the line segments that make up the object.</p> <pre><code># Calculating the length of the polygon\npoly.length\n</code></pre> <p>area Attribute: The area attribute returns the area of a Polygon or MultiPolygon object. It represents the surface area enclosed by the polygon.</p> <pre><code># Calculating the area of the polygon\npoly.area\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#shapely-methods","title":"Shapely Methods","text":"<p>Shapely provides a variety of methods that allow you to perform operations and analyze geometric objects. Here are some commonly used methods in Shapely:</p> <pre><code># Creating a Shapely Polygon\npoly2 = Polygon([(0, 0), (2, 0), (1, 1.5), (0, 1)])\npoly2_geo = gpd.GeoSeries(poly2)\npoly2_geo.plot()\nplt.show()\n</code></pre> <p>buffer(distance[, resolution=16, cap_style=CAP_STYLE.round]): Creates a new geometry with a buffer or offset around the original geometry. * The distance parameter specifies the buffer distance. * Optional parameters: resolution determines the number of points in the buffer's approximation, and * cap_style defines the style of the buffer's end caps.</p> <pre><code>poly_buffer = poly2.buffer(distance=2)\npoly_buffer_geo = gpd.GeoSeries(poly_buffer)\npoly_buffer_geo.plot()\nplt.show()\n</code></pre> <p>centroid: Computes the centroid (geometric center) of a geometry. Returns a Point object representing the centroid.</p> <pre><code>poly_centroid = poly2.centroid\npoly_centroid_geo = gpd.GeoSeries(poly_centroid)\npoly_centroid_geo.plot()\nplt.show()\n</code></pre> <p>envelope:  Computes the envelope of a geometry. Returns a Polygon object representing the minimum bounding box (rectangle) that encloses the geometry.</p> <pre><code>poly_envelope = poly2.envelope\npoly_envelope_geo = gpd.GeoSeries(poly_envelope)\npoly_envelope_geo.plot()\nplt.show()\n</code></pre> <p>convex_hull: Computes the convex hull of a geometry. Returns a Polygon object representing the smallest convex polygon that encloses all the points of the input geometry.</p> <pre><code>from shapely.geometry import MultiPoint\n</code></pre> <pre><code># Creating a MultiPoint shapely object\nmultiPoint = MultiPoint([(0, 0), (2, 5), (10, 7), (8, 3)])\nmultiPoint_geo = gpd.GeoSeries(multiPoint)\nmultiPoint_geo.plot()\nplt.show()\n</code></pre> <pre><code># Calculating the convex hull\nmp_convex_hull = multiPoint_geo.convex_hull\nmp_convex_hull.plot()\nplt.show()\n</code></pre> <p>simplify(tolerance, preserve_topology=True):  Creates a simplified version of a geometry by reducing the number of vertices while preserving the shape. * The tolerance parameter determines the maximum allowed distance between the simplified geometry and the original geometry. * The optional preserve_topology parameter specifies whether the topology of the geometry should be preserved during simplification.</p> <pre><code># Reading a shapefile with GeoPandas\nfile_path = r\"D:\\GIS Project\\ShapeFiles\\Bankura District\\Bankura_District.shp\"\ngdf = gpd.read_file(file_path)\n# Selecting a single feature\nfeature = gdf[\"geometry\"][gdf[\"Block\"]==\"Saltora\"]\n# Plotting the geometry\nfeature.plot()\nplt.show()\n</code></pre> <pre><code># Simplifying the feature's geometry\nsimplified_feature = feature.simplify(tolerance=0.002)\nsimplified_feature.plot()\nplt.show()\n</code></pre> <p>intersection(other): * Computes the intersection of two geometries. * Returns a new geometry representing the shared region between the two input geometries.</p> <pre><code># Creating two shapely polygon\npoly1 = Polygon([(0, 0), (10, 0), (10, 10), (0, 10), (0, 0)])\npoly2 = Polygon([(4, 0), (14, 5), (4, 10), (4, 0)])\n# Converting the polygons into GeoSeries\npoly1_geo = gpd.GeoSeries(poly1)\npoly2_geo = gpd.GeoSeries(poly2)\n</code></pre> <pre><code># Plotting the polygons\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.set_title(\"Polygon 1\")\npoly1_geo.plot(ax=ax1)\nax2.set_title(\"Polygon 2\")\npoly2_geo.plot(ax=ax2)\nplt.show()\n</code></pre> <pre><code># Calculating the shared area (intersection) between two polygons\nshared_area = poly1.intersection(poly2)\nshared_area_geo = gpd.GeoSeries(shared_area)\n</code></pre> <pre><code># Plotting the shared area\nshared_area_geo.plot(color=\"darkgrey\")\nplt.title(\"Intersection between \\nPolygon 1 and Polygon 2\")\nplt.show()\n</code></pre> <p>union(other): * Computes the union of two or more geometries. * Returns a new geometry representing the combined shape of the input geometries.</p> <pre><code># Calculating the combined shape (union) of the two polygons\ncombined = poly1.union(poly2)\ncombined_geo = gpd.GeoSeries(combined)\n</code></pre> <pre><code># Plotting the combined shape\ncombined_geo.plot(color=\"darkgrey\")\nplt.title(\"Union of Two Polygons\")\nplt.show()\n</code></pre> <p>difference(other): * Computes the difference between two geometries. * Returns a new geometry representing the region of the first geometry that does not intersect with the second geometry.</p> <pre><code># Calculating the difference between two polygons\ndiff = poly1.difference(poly2)\ndiff_geo = gpd.GeoSeries(diff)\n</code></pre> <pre><code># Plotting the difference area\ndiff_geo.plot(color=\"darkgrey\")\nplt.title(\"Difference of Polygon 1 from Polygon 2\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/06_Shapely_Properties_and_Methods/#shapely-boolean-operations","title":"Shapely Boolean Operations","text":"<p>In Shapely, there are various boolean operations available to compare and analyze geometries. Here are some commonly used boolean operations in Shapely:</p> <pre><code># Displaying the previously created shapely polygons\npoly1\n</code></pre> <pre><code>poly2\n</code></pre> <p>equals(other):  Tests if the geometry is exactly equal to the other geometry.</p> <pre><code>poly1.equals(poly2)\n</code></pre> <p>equals_exact(other):  That allows you to check if two geometries are exactly equal within a specified tolerance</p> <pre><code>poly1.equals_exact(poly2, tolerance=0.001)\n</code></pre> <p>covers(other):  Tests if the geometry covers the other geometry, i.e., if every point of the other geometry is also a point of the geometry or on its boundary.</p> <pre><code>poly1.covers(poly2)\n</code></pre> <p>crosses(other):  Tests if the geometry crosses the other geometry, i.e., if the geometries have some, but not all, interior points in common and their intersection results in a lower-dimensional geometric object.</p> <pre><code>poly1.crosses(poly2)\n</code></pre> <p>contains(other):  Tests if the geometry contains the other geometry, i.e., if every point of the other geometry is also a point of the geometry and their boundaries do not intersect.</p> <pre><code>poly1.contains(poly2)\n</code></pre> <p>covered_by(other):  Tests if the geometry is covered by the other geometry, i.e., if every point of the geometry is also a point of the other geometry or on its boundary.</p> <pre><code>poly1.covered_by(poly2)\n</code></pre> <p>overlaps(other):  Tests if the geometry overlaps the other geometry, i.e., if the geometries share some, but not all, interior points.</p> <pre><code>poly1.overlaps(poly2)\n</code></pre> <p>intersects(other):  Tests if the geometry intersects the other geometry, i.e., if the geometries have at least one point in common.</p> <pre><code>poly1.intersects(poly2)\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/","title":"Rasterio","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#rasterio","title":"Rasterio","text":"<p>Rasterio is a Python library designed to handle geospatial raster datasets. It provides a powerful and efficient way to read, write, manipulate, and analyze raster data. Rasterio builds upon the capabilities of the GDAL (Geospatial Data Abstraction Library) and provides a more user-friendly and Pythonic interface.</p> <p>Here are some key features and functionalities of Rasterio:</p> <ul> <li> <p>Reading and Writing Raster Data: Rasterio supports reading and writing various raster formats, including GeoTIFF, JPEG, PNG, and more. It provides an easy way to access the metadata, spatial reference system (SRS), and other properties of the raster dataset.</p> </li> <li> <p>Data Manipulation: Rasterio allows you to perform various operations on raster data, such as cropping, reprojecting, resampling, warping, and merging. It provides efficient memory-mapped access to the raster data, enabling processing of large datasets.</p> </li> <li> <p>Georeferencing and Coordinate Transformation: Rasterio handles the transformation between pixel coordinates and real-world geographic coordinates using the affine transformation matrix. It provides functionality to convert between different coordinate reference systems (CRS) and perform spatial transformations.</p> </li> <li> <p>Masking and Clipping: Rasterio provides tools to mask out specific areas of a raster using boolean masks or geometries. It allows you to clip rasters using bounding boxes, polygons, or other shapes.</p> </li> <li> <p>Dataset Metadata and Attributes: Rasterio allows you to access and modify the metadata and attributes associated with raster datasets, including band information, nodata values, and color mapping.</p> </li> <li> <p>Parallel Processing: Rasterio supports parallel processing of raster data using Python's multiprocessing module, allowing efficient utilization of multi-core systems for raster operations.</p> </li> <li> <p>Integration with Geospatial Libraries: Rasterio seamlessly integrates with other geospatial libraries such as NumPy, Matplotlib, GeoPandas, and Shapely, enabling powerful geospatial analysis workflows.</p> </li> </ul> <p>Overall, Rasterio provides a convenient and efficient way to work with raster data in Python, making it a valuable tool for geospatial analysis, remote sensing, GIS (Geographic Information System) applications, and other fields that deal with geospatial datasets.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport rasterio\nfrom rasterio.plot import show\nimport numpy as np\nimport geopandas as gpd\nimport shapely.geometry\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#02-setting-up-the-current-working-directory","title":"02. Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\GIS Project\\Raster Files\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the current working directory again\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#03-reading-the-raster-file-with-rasterio","title":"03. Reading the Raster File with Rasterio","text":"<p>Dataset Description: The NASA Shuttle Radar Topography Mission (SRTM) 30m Digital Elevation Model (DEM) dataset provides high-resolution elevation data for the Earth's surface. This dataset is derived from radar measurements collected by the Space Shuttle Endeavour during a 2000 mission.</p> <p>For a specific area, the SRTM 30m DEM dataset offers a detailed representation of the terrain, capturing elevation values at a resolution of approximately 30 meters. This level of detail allows for precise analysis and modeling of the topography, making it valuable for a range of applications, including hydrology, terrain analysis, visualization, and environmental studies.</p> <pre><code># Reading raster from local file with rasterion\nelev = rasterio.open(file_path + \"\\\\Bankura_SRTM_DEM.tif\", mode=\"r\")\n</code></pre> <pre><code># Checking the datatype of the elev variable\ntype(elev)\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#04-rasterio-attributes-and-methods","title":"04. Rasterio Attributes and Methods","text":"<pre><code># Checking the name/path of the raster file\nelev.name\n</code></pre> <pre><code># Checking the metadata of the raster file\nelev.meta\n</code></pre> <pre><code># Checking the driver, crs and count separately\nprint(\"Raster Driver:\", elev.driver)\nprint(\"Raster CRS:\", elev.crs)\nprint(\"Raster Count:\", elev.count)\n</code></pre> <p>bounds Method:  bounds method is used to retrieve the bounding box or extent of a raster dataset. The bounding box represents the minimum and maximum coordinates in the x and y dimensions that encompass the entire raster.</p> <pre><code># Checking the bounding box of the data\nelev.bounds\n</code></pre> <p>By using the asterisk (*) before elev.bounds, it unpacks the values from elev.bounds and passes them as separate arguments to the shapely.geometry.box function.</p> <pre><code>print(*elev.bounds)\n</code></pre> <p>shapely.geometry.box:  This method in Shapely is used to create a rectangular polygon, also known as a bounding box. It creates a Shapely geometry object representing a rectangular region defined by its minimum and maximum x and y coordinates.</p> <pre><code># Converting the bounding box into shapely geometry object\nbbox = shapely.geometry.box(*elev.bounds)\n# Converting the bounding box into geopandas geoseries object\nbbox_geo = gpd.GeoSeries(bbox)\n</code></pre> <pre><code># Plotting the bounding box\nbbox_geo.plot()\nplt.title(\"Bounding Box\")\nplt.xlabel(\"Longitude (DD)\")\nplt.ylabel(\"Latitude (DD)\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#05-creating-a-subset-area","title":"05. Creating a Subset Area","text":"<pre><code># Reading the Shapefile of the region of interest\nfile_path = r\"D:\\GIS Project\\ShapeFiles\\Bankura District\\Bankura_District.shp\"\nroi = gpd.read_file(file_path)\n</code></pre> <pre><code># Checking the first five rows of the geodataframe\nroi.head()\n</code></pre> <pre><code># Printing all the block names\nprint(roi[\"Block\"].unique())\n</code></pre> <pre><code># Extracting the Bankura I for subset area\nroi_subset = roi[\"geometry\"][roi[\"Block\"]==\"Bankura I\"]\n</code></pre> <pre><code># Plotting the roi_subset and bounding box\nroi_subset.plot()\nplt.title(\"ROI Subset\")\nplt.xlabel(\"Longitude (DD)\")\nplt.ylabel(\"Latitude (DD)\")\nplt.show()\n</code></pre> <pre><code># Converting the Bankura I into shapely geometry\nfrom shapely.geometry import Polygon\nroi_subset_geometry = roi_subset.geometry.to_list()\nprint(roi_subset_geometry)\nprint(type(roi_subset_geometry))\n# Getting the first item from the roi_subset_geometry list\nroi_subset_poly = roi_subset_geometry[0]\nroi_subset_poly\n</code></pre> <pre><code># Checking the bounds of the roi_subset_poly\nminx, miny, maxx, maxy = roi_subset_poly.bounds\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/07_Rasterio/#06-visualizing-raster-data","title":"06. Visualizing Raster Data","text":"<pre><code># Visualizing the whole DEM data\nshow(elev)\n</code></pre> <p>from_bounds:  This function is used to create a raster dataset that covers a specific bounding box in the desired CRS (coordinate reference system). transform:In Rasterio, the .transform attribute is used to access the affine transformation matrix of a raster dataset. The transformation matrix defines the spatial relationship between the pixel coordinates of the raster and its real-world coordinates.</p> <pre><code># Visualizing DEM data of the roi_subset area\nfrom rasterio.windows import from_bounds\nraster_path = r\"D:\\GIS Project\\Raster Files\\Bankura_SRTM_DEM.tif\"\nwith rasterio.open(raster_path, mode=\"r\") as src:\n    dem = src.read(1, window=from_bounds(minx, miny, maxx, maxy, src.transform))\n    plt.title(\"ROI Subset DEM\")\n    show(dem)\n</code></pre> <pre><code># Visualizing DEM data of the roi_subset area in grayscale\nplt.title(\"ROI Subset DEM\")\nshow(dem, cmap=\"gray\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/","title":"Ipyleaflet","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#ipyleaflet","title":"ipyLeaflet","text":"<p>ipyleaflet is a Python library that provides interactive mapping capabilities in Jupyter notebooks and JupyterLab. It is built on top of the popular JavaScript mapping library, Leaflet.js. ipyleaflet allows you to create interactive maps, markers, polygons, layers, and other geospatial visualizations directly within the Jupyter environment.</p> <p>With ipyleaflet, you can display maps with various base layers such as OpenStreetMap, Mapbox, and other tile layers. You can add markers and polygons to the map, customize their appearance, and interact with them. The library also supports layers like GeoJSON, WMS (Web Map Service), and Tile layers, allowing you to overlay additional data on the map.</p> <p>One of the key features of ipyleaflet is its interactivity. You can zoom in and out, pan the map, and interact with markers and polygons using mouse events. ipyleaflet also provides widgets that allow you to control and manipulate the map dynamically, such as sliders, checkboxes, and dropdown menus.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport ipyleaflet\nimport pandas as pd\nimport geopandas as gpd\nfrom geopandas.tools import geocode\nimport geopy\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#02-setting-up-the-current-working-directory","title":"02. Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\\CSVs\"\nos.chdir(file_path)\n# Checking the current working directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#03-reading-the-csv-file-with-pandas","title":"03. Reading the CSV File with Pandas","text":"<pre><code># Reading the Kolkata_City_Attractions.csv using pandas\ncity_attractions = pd.read_csv(\"Kolkata_City_Attractions.csv\")\n# Checking the rows of the csv\ncity_attractions.head(10)\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#04-geocoding-addresses-using-nominatim","title":"04. Geocoding Addresses using Nominatim","text":"<pre><code># Geocoding the addresses using nominatim\nkolkata_attractions_gpd = geocode(city_attractions[\"Address\"], provider=\"nominatim\", user_agent=\"nominatim\")\n# Check the geocoded addresses\nkolkata_attractions_gpd\n</code></pre> <pre><code># Joining the two tables\ncity_attractions_gpd = kolkata_attractions_gpd.join(city_attractions[\"Attraction\"])\n# Checking the geodataframe\ncity_attractions_gpd\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#05-data-cleaning","title":"05. Data Cleaning","text":"<p>loc: The loc method in Pandas is used to access and manipulate data based on label-based indexing. It allows you to select specific rows and columns from a DataFrame by specifying the labels of the rows and columns you want to retrieve.</p> <p>The basic syntax for using the loc method is as follows: <pre><code>dataframe.loc[row_labels, column_labels]\n</code></pre> * row_labels: This can be a single label, a list of labels, or a slice object specifying the rows you want to select. The labels can be either the index labels or boolean conditions applied to the index. * column_labels: This can be a single label, a list of labels, or a slice object specifying the columns you want to select. The labels can be either the column names or boolean conditions applied to the columns.</p> <pre><code># Rearranging the columns of the geodataframe\ncity_attractions_gpd = city_attractions_gpd.loc[:, [\"Attraction\", \"address\", \"geometry\"]]\n# Checking the geodataframe\ncity_attractions_gpd\n</code></pre> <pre><code># Removing the rows with null geometry\ncity_attractions_gpd.dropna(inplace=True)\n# Resetting the index\ncity_attractions_gpd.reset_index(inplace=True, drop=True)\n# Checking the geodataframe\ncity_attractions_gpd\n</code></pre> <pre><code># Adding in lat and lon columns\ncity_attractions_gpd[\"lon\"] = city_attractions_gpd[\"geometry\"].x\ncity_attractions_gpd[\"lat\"] = city_attractions_gpd[\"geometry\"].y\n</code></pre> <pre><code># Checking the geodataframe\ncity_attractions_gpd\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/02_Exploring_Geospatial_Packages/08_ipyLeaflet/#06-create-an-interactive-map-with-ipyleaflet","title":"06. Create an Interactive Map with ipyLeaflet","text":"<pre><code>from ipyleaflet import Map, basemaps\n</code></pre> <pre><code># Creating a map using ipyleaflet\ncity_map = Map(basemap = basemap_to_tiles(basemaps.OpenStreetMap.Mapnik),\n               center=(22.5726, 88.3639), \n               zoom=12\n               )\n# Displaying the map\ncity_map\n</code></pre> <p>iterrows(): In geopandas, the iterrows() function allows you to iterate over the rows of a GeoDataFrame. It returns an iterator that yields both the index and the row data for each row in the GeoDataFrame.</p> <pre><code># Mapping the attractions\nfrom ipyleaflet import Marker\nfor index, row in city_attractions_gpd.iterrows():\n    marker = Marker(location=[row.loc[\"lat\"], row.loc[\"lon\"]], title=row.loc[\"Attraction\"])\n    city_map.add_layer(marker)\n\n# Displaying the city map after adding marker points\ncity_map\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/","title":"Exploratory Data Visualization","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#exploratory-data-visualization","title":"Exploratory Data Visualization","text":"<p>Exploratory data visualization is the process of creating visual representations of data to gain insights, discover patterns, and identify relationships within a dataset. It involves the use of various graphical techniques and plots to explore the characteristics and structure of the data. The primary goal of exploratory data visualization is to understand the data and generate hypotheses or ideas for further analysis.</p> <p>Common techniques used in exploratory data visualization include:</p> <ol> <li> <p>Scatter plots: Scatter plots display the relationship between two variables by plotting data points on a two-dimensional graph. They are useful for identifying correlations or patterns between variables.</p> </li> <li> <p>Histograms: Histograms provide a visual representation of the distribution of a single variable. They group data into bins or intervals and display the frequency or count of observations within each bin.</p> </li> <li> <p>Box plots: Box plots, also known as box-and-whisker plots, summarize the distribution of a variable by displaying quartiles, outliers, and the range of the data.</p> </li> <li> <p>Bar charts: Bar charts are used to compare categories or groups by representing the values of different variables as rectangular bars. They are commonly used for categorical data.</p> </li> </ol> <p>Spatial Data Visualization: Spatial data visualization focuses on representing geographic or spatial data in visual form. It involves the use of maps, spatial plots, and other geospatial visualizations to explore and communicate patterns, relationships, and distributions across geographic areas.</p> <p>Common techniques used in spatial data visualization include:</p> <ol> <li> <p>Choropleth maps: Choropleth maps use different colors or shading to represent the intensity or magnitude of a variable across regions or areas. They are effective for displaying data at an aggregated level, such as population density or election results by region.</p> </li> <li> <p>Scatter plots on maps: Scatter plots can be overlaid on maps to visualize the relationship between variables at specific geographic locations. This allows for the exploration of spatial patterns or clusters.</p> </li> <li> <p>Heatmaps: Heatmaps can be used in spatial data visualization to represent the density or intensity of events or occurrences across a geographic area. They provide a visual depiction of hotspots or areas of concentration.</p> </li> <li> <p>Cartograms: Cartograms distort the size or shape of regions on a map based on a variable of interest, allowing for the visualization of relative magnitudes or proportions.</p> </li> </ol>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#01-import-required-libraries","title":"01. Import Required Libraries","text":"<pre><code>import os\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#02-setting-up-the-current-working-directory","title":"0.2 Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\"\nos.chdir(file_path)\ncsv_path = file_path + \"\\\\CSVs\"\nshp_path = file_path + \"\\\\Shapafiles\"\n</code></pre> <pre><code># Checking the new current working directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#03-reading-the-data","title":"03. Reading the Data","text":"<p>Dataset Description: This is the dataset used in the second chapter of Aur\u00e9lien G\u00e9ron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.</p> <p>The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.</p> <p>Content: 1. longitude: A measure of how far west a house is; a higher value is farther west 2. latitude: A measure of how far north a house is; a higher value is farther north 3. housingMedianAge: Median age of a house within a block; a lower number is a newer building 4. totalRooms: Total number of rooms within a block 5. totalBedrooms: Total number of bedrooms within a block 6. population: Total number of people residing within a block 7. households: Total number of households, a group of people residing within a home unit, for a block 8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars) 9. medianHouseValue: Median house value for households within a block (measured in US Dollars) 10. oceanProximity: Location of the house w.r.t ocean/sea</p> <pre><code># Reading the housing.csv data with pandas\nhousing = pd.read_csv(csv_path + \"\\\\housing.csv\")\n# Checking the name of the columns\nhousing.columns\n</code></pre> <pre><code># Checking the first 5 rows of the data\nhousing.head()\n</code></pre> <pre><code># Checking the shape of the dataframe\nhousing.shape\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#04-conducting-exploratory-data-analysis-eda","title":"04. Conducting Exploratory Data Analysis (EDA)","text":"<pre><code># Checking the non-null values in each column\nhousing.info()\n</code></pre> <pre><code># Cleaning the data\nhousing.dropna(inplace=True)\n</code></pre> <pre><code># Checking the shape of the dataframe\nhousing.shape\n</code></pre> <pre><code># Checking the value counts of the ocean_proximity column\nhousing[\"ocean_proximity\"].value_counts()\n</code></pre> <pre><code># Definining a dictionary to encode the values of ocean_proximity column from string to int\nocean_proximity_dict = {\"ISLAND\": 0, \"NEAR BAY\": 1, \"NEAR OCEAN\": 2, \"INLAND\": 3, \"&lt;1H OCEAN\": 4}\n# Encoding the ocean_proximity column\nencoded_ocean_proximity = housing[\"ocean_proximity\"].replace(ocean_proximity_dict)\n</code></pre> <pre><code># Creating a copy of housing dataframe\nhousing_copy = housing.copy()\n</code></pre> <pre><code># Setting the encoded values of ocean_proximity column\nhousing_copy[\"ocean_proximity\"] = encoded_ocean_proximity\n# Checking the first 5 rows of the new housing_copy dataframe\nhousing_copy.head()\n</code></pre> <pre><code># Checking the non-null values in each column of the new housing_copy dataframe\nhousing_copy.info()\n</code></pre> <pre><code># Dropping the rows with null values\nhousing_copy.dropna(inplace=True)\n# Resetting the index\nhousing_copy.reset_index(inplace=True, drop=True)\n</code></pre> <pre><code># Checking the final dataframe\nhousing_copy.head()\n</code></pre> <pre><code># Checking the dataframe information\nhousing_copy.info()\n</code></pre> <pre><code># Describing the dataframe\nhousing_copy.describe()\n</code></pre> <pre><code># Create a visual representation of the data\nhousing_copy.hist(bins=50, figsize=(20, 18))\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/01_Exploratory_Data_Visualization/#05-exploratory-spatial-data-analysis-esda","title":"05. Exploratory Spatial Data Analysis (ESDA)","text":"<pre><code># Converting the pandas dataframe to a geopandas dataframe\nhousing_gdf = gpd.GeoDataFrame(housing_copy, geometry=gpd.points_from_xy(housing_copy.longitude, housing_copy.latitude, crs=4326))\n</code></pre> <pre><code># Checking the CRS of the geodataframe\nhousing_gdf.crs\n</code></pre> <p>Geoplot: Geoplot is a Python library that provides a high-level interface for creating a wide range of geographical visualizations using matplotlib. It is built on top of geopandas, which is a powerful library for working with geospatial data. Geoplot simplifies the process of creating maps, enabling users to quickly generate various types of plots to visualize spatial data.</p> <p>Geoplot offers a set of plot types that are commonly used in geographic data analysis, such as choropleth maps, kernel density estimation (KDE) plots, cartograms, and spatial lags. These plots can be easily customized to suit specific visualization requirements.</p> <p>One of the key features of geoplot is its ability to work seamlessly with geopandas. Geopandas provides data structures to work with geospatial data, such as points, lines, and polygons, and allows users to perform spatial operations on them. Geoplot takes advantage of these data structures and operations, enabling users to create geospatial visualizations by leveraging the power of geopandas.</p> <p>Geoplot provides an intuitive API that allows users to create plots with just a few lines of code. It integrates well with Jupyter notebooks, making it ideal for interactive data exploration and analysis. The library supports various map projections and provides tools for handling geographic coordinate reference systems (CRS).</p> <pre><code># Importing geoplot library\nimport geoplot.crs as gcrs\nimport geoplot as gplt\n</code></pre> <pre><code># Plotting the housing_gdf using geoplot\nax = gplt.webmap(housing_gdf, projection=gcrs.WebMercator())\ngplt.pointplot(housing_gdf, ax=ax, marker=\".\")\n</code></pre> <pre><code># Loading the USA States shapefile\nusa_states = gpd.read_file(shp_path + \"\\\\tl_2021_us_state.zip\")\n# Checking the first five rows of the geodataframe\nusa_states.head()\n</code></pre> <pre><code># Filtering the California geometry\ncalifornia = usa_states[\"geometry\"][usa_states[\"NAME\"]==\"California\"]\n# Checking the CRS of the California geometry\ncalifornia.crs\n</code></pre> <pre><code># Changing the CRS to Web Mercator (4326)\ncalifornia = california.to_crs(4326)\n# Plotting the California map\ncalifornia.plot(color=\"white\", edgecolor=\"black\", linewidth=0.5)\n</code></pre> <pre><code># Checking the housing_gdf dataframe\nhousing_gdf.head()\n</code></pre> <p>Heatmap: A heatmap is a graphical representation of data where values are depicted as colors on a two-dimensional grid. It is particularly useful for visualizing the distribution and intensity of data points across different categories or dimensions.</p> <p>In a heatmap, each cell of the grid represents a combination of two variables, typically displayed along the X and Y axes. The color of each cell corresponds to the value of a third variable, often referred to as the \"intensity\" or \"magnitude\" of the data. The colors used in the heatmap are usually chosen to represent a continuous spectrum, ranging from low to high values.</p> <pre><code># Creating a heatmap of point locations\nax = gplt.kdeplot(housing_gdf,\n                  fill=True, \n                  cmap=\"Reds\", \n                  clip=california.geometry, \n                  projection=gcrs.WebMercator()\n                  )\n# Plotting the California polygon on top of the heatmap\ngplt.polyplot(california, ax=ax, zorder=1)\n</code></pre> <pre><code># Creating a point plot to display the spatial variation of median_house_value\nax = gplt.pointplot(housing_gdf,\n                    hue=\"median_house_value\",\n                    scale=\"median_house_value\",\n                    cmap=\"Reds\",\n                    legend=True,\n                    projection=gcrs.WebMercator()\n                    )\n# Plotting the Califonia polygon on top of the point plot\ngplt.polyplot(california, ax=ax, zorder=0)\nax.set_title(\"Median House Value in California, USA in 1990\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/","title":"Creating Choropleth Map From Points","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: geo     language: python     name: python3</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#creating-choropleth-map-from-points","title":"Creating Choropleth Map from Points","text":""},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#02-setting-up-the-current-working-directory","title":"02. Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\Coding\\Git Repository\\Geospatial_Data_Science_with_Python\\Datasets\"\nos.chdir(file_path)\n# Checking the new current working directory\nos.getcwd()\n</code></pre> <pre><code># Defining the CSV and Shapefile paths\ncsv_path = file_path + \"\\\\CSVs\"\nshp_path = file_path + \"\\\\Shapefiles\"\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#03-reading-the-data","title":"03. Reading the Data","text":"<pre><code># Reading the us_county data using geopandas\nus_county = gpd.read_file(shp_path + \"\\\\tl_2022_us_county.zip\")\nus_county.head()\n</code></pre> <pre><code># Reading the California housing data\nhousing = pd.read_csv(csv_path + \"\\\\housing.csv\")\nhousing.head()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#04-prepairing-the-dataset","title":"04. Prepairing the Dataset","text":"<pre><code># Checking the CRS of the us_county data\nus_county.crs\n</code></pre> <pre><code># Changing the CRS from NAD83 to WGS84\nus_county = us_county.to_crs(4326)\n# Checking the new CRS of the data\nus_county.crs\n</code></pre> <pre><code># Subsetting the US Counties that belong to California state\n# STATFP of California = 06\nca_county = us_county.loc[:,:][us_county[\"STATEFP\"]==\"06\"]\n# Checking the ca_county dataframe\nca_county.head()\n</code></pre> <pre><code># Printing the housing.csv data\nhousing.head()\n</code></pre> <pre><code># Converting the housing dataframe into geodataframe\nhousing_gdf = gpd.GeoDataFrame(housing, \n                               crs=4326, \n                               geometry=gpd.points_from_xy(housing.longitude, housing.latitude))\n# Checking the geodataframe\nhousing_gdf.head()\n</code></pre> <p>In Geopandas, spatial join and aggregation are powerful operations used to combine geospatial datasets based on their spatial relationships and perform calculations or summaries on the resulting joined data. Let's explore each of these operations:</p> <p>Spatial Join: Spatial join is the process of combining two or more geospatial datasets based on their spatial relationships. It involves linking the attributes of one dataset to another dataset based on the spatial proximity or overlap of their geometries. The resulting joined dataset contains attributes from both datasets.</p> <p>In Geopandas, the sjoin() function is used to perform a spatial join. It takes two GeoDataFrames as input and joins them based on a specified spatial relationship, such as \"intersects,\" \"contains,\" or \"within.\" The function returns a new GeoDataFrame with the attributes from both input datasets.</p> <ul> <li>Parameters:</li> <li> <p>how: This parameter determines how the spatial join is performed. The available options are:</p> </li> <li> <p>\"inner\" (default): Only the intersecting or overlapping geometries between the two datasets are retained in the joined result.</p> </li> <li>\"left\": All features from the left GeoDataFrame are kept, and attributes from the right GeoDataFrame are added to the resulting joined dataset based on the spatial relationship.</li> <li>\"right\": All features from the right GeoDataFrame are kept, and attributes from the left GeoDataFrame are added to the resulting joined dataset based on the spatial relationship.</li> <li> <p>\"outer\": All features from both GeoDataFrames are retained, and attributes are added based on the spatial relationship. Non-overlapping features will have missing attribute values.</p> </li> <li> <p>op: This parameter specifies the spatial relationship used for the join. The available options are:</p> </li> <li> <p>\"intersects\" (default): Joins geometries if they intersect or have any spatial overlap.</p> </li> <li>\"contains\": Joins geometries if one completely contains the other.</li> <li>\"within\": Joins geometries if one is completely within the other.</li> <li>\"touches\": Joins geometries if they share a common boundary but do not overlap.</li> </ul> <pre><code># Applying Spatial Join between ca_county and housing_gdf\ncounty_housing_sj = gpd.sjoin(ca_county, housing_gdf, how=\"left\", op=\"contains\")\n# Checking the spatially joined dataframe\ncounty_housing_sj.head()\n</code></pre> <pre><code># Selecting required columns only\ncounty_housing_sj = county_housing_sj[[\"GEOID\", \"NAME\", \"median_house_value\", \"geometry\"]]\n</code></pre> <p>Aggregation: Aggregation in Geopandas involves summarizing or calculating statistics on a group of features based on a specific spatial unit. It allows you to aggregate or dissolve geometries together and compute summary statistics for the combined features.</p> <p>The dissolve() function in Geopandas is commonly used for aggregation. It combines geometries that share a common attribute value and calculates summary statistics for the grouped features. You can specify which attribute to dissolve by, and the function can compute various statistics, such as the sum, mean, maximum, minimum, or count of a specific attribute</p> <pre><code># Aggregating the rows by GEOID\ncounty_housing_agg = county_housing_sj.dissolve(by=[\"GEOID\"], aggfunc=\"mean\")\n</code></pre> <pre><code># Checking the aggregated geodataframe\ncounty_housing_agg.head()\n</code></pre> <pre><code># Manipulating the aggregated data\ncounty_housing_agg.reset_index(inplace=True)\ncounty_housing_agg = county_housing_agg.loc[:, [\"GEOID\", \"median_house_value\", \"geometry\"]]\n# Checking the manipulated aggregated dataframe\ncounty_housing_agg.head()\n</code></pre>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/03_Exploratory_Spatial_Data_Analysis/02_Creating_Choropleth_Map_from_Points/#05-spatial-data-visualization","title":"05. Spatial Data Visualization","text":"<pre><code># Plotting a choropleth map using geopandas\naxes = county_housing_agg.plot(column=\"median_house_value\",\n                               cmap=\"Reds\",\n                               edgecolor=\"black\",\n                               linewidth=0.5,\n                               legend=True)\naxes.set_xlabel(\"Longitude\")\naxes.set_ylabel(\"Latitude\")\naxes.set_title(\"Choropleth Map of California Counties' Median House Value in US$\")\naxes.title.set_size(10)\n</code></pre> <p>Geoviews: Geoviews is a Python library that is built on top of the powerful visualization library called HoloViews. It provides a high-level interface for creating interactive and declarative visualizations of geospatial data. Geoviews is designed to simplify the process of working with complex geospatial datasets and enables easy exploration and analysis of geographic information.</p> <p>Key features and capabilities of Geoviews include:</p> <ul> <li> <p>Declarative syntax: Geoviews allows you to define visualizations using a concise and declarative syntax, which makes it easy to create complex plots with minimal code. You can specify the data, visual attributes, and other parameters using a chainable syntax.</p> </li> <li> <p>Seamless integration: Geoviews seamlessly integrates with popular geospatial libraries such as GeoPandas, Cartopy, and Bokeh. It can ingest data from different formats and sources, including shapefiles, GeoJSON, and raster datasets.</p> </li> <li> <p>Interactive visualizations: Geoviews supports interactive exploration and visualization of geospatial data. It leverages the interactivity features of HoloViews and Bokeh, allowing you to zoom, pan, and explore the data with tooltips, hover effects, and interactive widgets.</p> </li> <li> <p>Dynamic overlays: Geoviews enables the creation of dynamic overlays, where multiple layers of geospatial data can be combined and visualized together. This makes it easy to overlay different data types, such as points, lines, polygons, and raster images.</p> </li> <li> <p>Geospatial projections: Geoviews supports various map projections, allowing you to display and transform geographic data in different coordinate reference systems (CRS). It provides a consistent interface for working with different projections, making it easy to switch between them.</p> </li> <li> <p>Geospatial operations: Geoviews provides functionality for performing common geospatial operations, such as spatial aggregation, spatial joins, and spatial filtering. These operations can be used to analyze and process geospatial data effectively.</p> </li> </ul> <pre><code># Importing geoviews library\nimport geoviews\n</code></pre> <pre><code># Plotting an interactive choropleth map using geoviews\ngeoviews.extension(\"bokeh\")\nchoropleth = geoviews.Polygons(data=county_housing_agg, vdims=[\"median_house_value\", \"GEOID\"])\nchoropleth.options(height=600,\n                   width=500,\n                   title=\"Choropleth Map of California Counties' Median House Value in US$\",\n                   tools=[\"hover\"],\n                   cmap=\"Reds\",\n                   colorbar=True,\n                   colorbar_position = \"bottom\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/","title":"Image Analysis in Remote Sensing with Python","text":"<p>This project focuses on analyzing remote sensing imagery using Python, NumPy, and Scikit-Learn.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/#contents","title":"Contents","text":"<ul> <li>Reading and Displaying Image Band</li> <li>Integration of GEE with NumPy</li> </ul>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/#modules","title":"Modules","text":"<ul> <li> <ol> <li>Images, Arrays, and Matrices</li> </ol> </li> <li>Machine Learning with Scikit Learn</li> </ul>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/","title":"Reading And Displaying Image Band","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#reading-and-displaying-rasters-using-gdal-in-python","title":"Reading and Displaying Rasters Using GDAL in Python","text":"<p>Author: Krishnagopal Halder </p> <p>GDAL (Geospatial Data Abstraction Library) is a powerful open-source library for reading, writing, and manipulating geospatial raster and vector data formats. It provides a wide range of functionalities to work with various GIS (Geographic Information System) data formats, including popular ones like GeoTIFF, Shapefile, and many others. In Python, GDAL can be accessed through the osgeo module, which is a part of the GDAL project.</p> <p>GDAL in Python allows you to perform tasks such as reading and writing raster and vector datasets, extracting metadata, transforming and projecting data, and performing various geospatial analysis operations. With its extensive capabilities, GDAL enables developers and data scientists to process geospatial data efficiently, making it a valuable tool in fields such as remote sensing, environmental modeling, and geospatial analysis.</p> <p>To work with GDAL in Python, you'll need to have the GDAL library installed on your system. You can install it using package managers like pip or conda. Once installed, you can import the necessary modules from osgeo to begin working with geospatial data.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom osgeo import gdal\nfrom osgeo.gdalconst import GA_ReadOnly\nfrom osgeo import osr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#02-setting-up-the-current-working-directory","title":"02. Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nos.getcwd()\n</code></pre> <pre><code># Changing the current working directory\nfile_path = r\"D:\\GIS Project\\Research Projects\\Mangrove_Extent_Mapping\\New Satellite Images\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the current working directory\nos.getcwd()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#03-reading-raster-with-gdal","title":"03. Reading Raster with GDAL","text":"<p>gdal.AllRegister():  gdal.AllRegister() is a function from the GDAL (Geospatial Data Abstraction Library) library. It is used to register all available GDAL drivers. Before using any GDAL functionality, it is necessary to register the available drivers. The gdal.AllRegister() function ensures that all GDAL drivers are registered and available for use in the current session. By registering the drivers, GDAL becomes aware of the formats it can read and write, allowing you to work with different geospatial data formats seamlessly.</p> <pre><code># Registering all available GDAL drivers\ngdal.AllRegister()\n</code></pre> <p>Dataset Description:  The raster data that has been created using the Google Earth Engine platform by compositing cloud masked Landsat 8 images of the year 2022 is a multi-band dataset comprising a total of 9 bands. Each band represents a different type of spectral index derived from the Landsat 8 satellite imagery.</p> <p>Spectral indices are mathematical calculations applied to remote sensing data to extract specific information about the land surface characteristics. These indices are often used to monitor vegetation health, detect changes in land cover, assess water quality, and perform various other types of analysis.</p> <pre><code># Defining the path of the raster image\nraster_name = \"Mangrove_Composite_2022_New.tif\"\nfile = file_path + \"\\\\\" + raster_name\n</code></pre> <pre><code># Checking the metadata of the raster image using command line\n!gdalinfo -nomd Mangrove_Composite_2022_New.tif\n</code></pre> <pre><code># Reading the raster dataset with GDAL\ndataset = gdal.Open(file, GA_ReadOnly)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#04-checking-the-properties-of-the-raster-file","title":"04. Checking the Properties of the Raster File","text":"<pre><code># Checking the type of the dataset variable\ntype(dataset)\n</code></pre> <p>RasterXSize, RasterYSize, and RasterCount are attributes of a GDAL dataset object. These attributes provide information about the size and composition of a raster image.</p> <ul> <li> <p>RasterXSize: This attribute represents the width of the raster image in pixels. It provides the number of columns or the horizontal dimension of the image.</p> </li> <li> <p>RasterYSize: This attribute represents the height of the raster image in pixels. It provides the number of rows or the vertical dimension of the image.</p> </li> <li> <p>RasterCount: This attribute represents the number of bands or layers in the raster image. A band typically represents a specific data type or characteristic of the image, such as red, green, blue (RGB) bands in a color image or different spectral bands in a multispectral image. Each band contains pixel values corresponding to its specific data type or characteristic.</p> </li> </ul> <pre><code># Getting the width, height, number of bands of the image\nnCols = dataset.RasterXSize\nnRows = dataset.RasterYSize\nnBands = dataset.RasterCount\nprint(\"Number of Cloumns (Image Width):\", nCols)\nprint(\"Number of Rows (Image Height):\", nRows)\nprint(\"Number of Bands:\", nBands)\n</code></pre> <p>GetProjection() and GetGeoTransform() are methods of a GDAL dataset object. These methods provide information about the spatial properties and coordinate system of a raster image.</p> <ul> <li> <p>GetProjection(): This method retrieves the projection information of the raster image. The projection defines how the geographic coordinates (latitude and longitude) are mapped to the pixel coordinates in the image. The projection information is typically represented as a well-known text (WKT) string, which describes the coordinate reference system (CRS) used by the image.</p> </li> <li> <p>GetGeoTransform(): This method retrieves the georeferencing information of the raster image. The georeferencing information describes the transformation between the pixel coordinates in the image and real-world geographic coordinates. It consists of six coefficients that define the origin, pixel size, and rotation of the image.</p> </li> </ul> <pre><code># Getting the coordinate information of the raster image\nprint(\"Coordinate System:\", dataset.GetProjection(), sep=\"\\n\")\n</code></pre> <pre><code># Getting the transformation coefficients\ngeoTransform = dataset.GetGeoTransform()\nprint(geoTransform)\n</code></pre> <pre><code>print(\"Origin:\", geoTransform[0], geoTransform[3])\nprint(\"Pixel Size:\", geoTransform[1], geoTransform[5])\n</code></pre> <pre><code>print(\"Upper Left Corner:\", gdal.ApplyGeoTransform(geoTransform, 0, 0))\nprint(\"Upper Right Corner:\", gdal.ApplyGeoTransform(geoTransform, nCols, 0))\nprint(\"Lower Left Corner:\", gdal.ApplyGeoTransform(geoTransform, 0, nRows))\nprint(\"Lower Right Corner:\", gdal.ApplyGeoTransform(geoTransform, nCols, nRows))\nprint(\"Center:\", gdal.ApplyGeoTransform(geoTransform, nCols/2, nRows/2))\n</code></pre> <p>GetMetadata() is a methods of a GDAL dataset object. This method is used to retrieve metadata associated with a raster image.</p> <ul> <li> <p>GetMetadata(): This method retrieves all metadata associated with the dataset. Metadata provides additional information about the dataset, such as the data source, acquisition parameters, processing history, or any other relevant details. The GetMetadata method returns a dictionary object containing key-value pairs of the metadata items.</p> </li> <li> <p>GetMetadata(\"IMAGE_STRUCTURE\"): This method retrieves metadata specifically related to the image structure. The \"IMAGE_STRUCTURE\" parameter is passed to specify the category of metadata to retrieve. This category typically contains information about the structure and organization of the image data, such as the color interpretation, pixel data type, compression, or block size.</p> </li> </ul> <pre><code># Checking the metadata of the raster\nprint(\"Metadata:\", dataset.GetMetadata())\n</code></pre> <pre><code>print(\"Image Structure Metadata:\", dataset.GetMetadata(\"IMAGE_STRUCTURE\"))\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#05-reading-the-bands-of-the-raster","title":"05. Reading the Bands of the Raster","text":"<p>GetRasterBand(): is used to retrieve each band of the dataset within a loop. The band index starts from 1, so nBands + 1 is passed as an argument to GetRasterBand() to obtain each band.</p> <p>The GetDescription() method is then called on each band object (band) to retrieve the band description. The band description can be a user-defined label or a description assigned to the band.</p> <p>After that, the ComputeStatistics() method is used to compute statistics for each band. The False argument passed to ComputeStatistics() indicates that the function should not force computation of statistics if they are not available.</p> <pre><code># Extracting the description of all the bands\nfor i in range(1, nBands + 1):\n    band = dataset.GetRasterBand(i)\n    band_description = band.GetDescription()\n    print(f\"Band{i}: {band_description}\")\n</code></pre> <pre><code># Checking statistics of the raster image using command line\n!gdalinfo -stats Mangrove_Composite_2022_New.tif\n</code></pre> <pre><code># Calculating the statistics of all the bands\nfor i in range(1, nBands + 1):\n    band = dataset.GetRasterBand(i)\n    (minimum, maximum, mean, stdDev) = band.ComputeStatistics(False)\n    print(\"band{:d}, min={:.3f}, max={:.3f}, mean={:.3f}, stdDev={:.3f}\"\\\n          .format(i, minimum, maximum, mean, stdDev))\n</code></pre> <p>The ReadAsArray() method in GDAL is used to read the pixel values of a raster band into a NumPy array. It allows you to access the actual pixel data for further processing or analysis.</p> <pre><code># Reading the first band (NDVI) of the raster image\nndvi = dataset.GetRasterBand(1)\n# Converting the raster band into NumPy array\nndviArray = ndvi.ReadAsArray()\n</code></pre> <pre><code># Printing the NDVI array\nndviArray\n</code></pre> <pre><code># Printing the shape of the NDVI array\nndviArray.shape\n</code></pre> <pre><code># Reading the second band of the raster image\nndwi = dataset.GetRasterBand(2)\n# Converting the raster band into NumPy array\nndwiArray = ndwi.ReadAsArray()\n</code></pre> <pre><code># Checking the shape of the NDWI array\nndwiArray.shape\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#06-plotting-the-raster-band","title":"06. Plotting the Raster Band","text":"<pre><code># Plotting the NDVI band\nplt.figure(figsize=(8, 6))\nplt.imshow(ndviArray, cmap=\"RdYlGn\")\nplt.title(\"Normalized Difference Vegetation Index (NDVI)\")\nplt.xlabel(\"Image X Coordinate\")\nplt.ylabel(\"Image Y Coordinate\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n</code></pre> <pre><code>plt.figure(figsize=(8, 6))\nplt.imshow(ndwiArray, cmap=\"YlGnBu\")\nplt.title(\"Normalized Difference Water Index (NDWI)\")\nplt.xlabel(\"Image X Coordinate\")\nplt.ylabel(\"Image Y Coordinate\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Reading_and_Displaying_Image_Band/#07-automating-the-process-using-custom-function","title":"07. Automating the Process using Custom Function","text":"<pre><code># Creating a function to check the bands description of a raster\ndef checkBands(data):\n    gdal.AllRegister()\n    dataset = gdal.Open(data, GA_ReadOnly)\n\n    # Getting the number of bands in the raster\n    nBands = dataset.RasterCount\n\n    # Printing the raster band description\n    for i in range(1, nBands + 1):\n        band = dataset.GetRasterBand(i)\n        band_info = band.GetDescription()\n        print(f\"Band{i}: {band_info}\")\n</code></pre> <pre><code># Creating a function to plot the bands of a raster\ndef plotBand(data, band_num, palette, title):\n    # Reading the raster band\n    gdal.AllRegister()\n    dataset = gdal.Open(data, GA_ReadOnly)\n    rasterBand = dataset.GetRasterBand(band_num)\n\n    # Convering the raster image to NumPy array\n    bandArray = rasterBand.ReadAsArray()\n\n    # Plotting the raster band\n    plt.figure(figsize=(8, 6))\n    plt.imshow(bandArray, cmap=palette)\n    plt.title(title)\n    plt.xlabel(\"Image X Coordinate\")\n    plt.ylabel(\"Image Y Coordinate\")\n    plt.colorbar(orientation=\"horizontal\")\n    plt.show()\n</code></pre> <pre><code># Testing the functions\ndata_path = r\"D:\\GIS Project\\Research Projects\\Mangrove_Extent_Mapping\\New Satellite Images\\Mangrove_Composite_2022_New.tif\"\n# Checking the bands information using checkbands() function\ncheckBands(data_path)\n</code></pre> <pre><code># Plotting the bands using the plotBand() function\nplotBand(data_path, 7, \"YlGn\", \"Soil Adjusted Vegetation Index (SAVI)\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/","title":"Integration Of Gee With Numpy","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#importing-required-libraries","title":"Importing Required Libraries","text":"<p>```python id=\"IbHHSuYwLNN5\" import ee import geemap import matplotlib.pyplot as plt import seaborn as sns <pre><code>```python id=\"fUanfUHROhbM\"\n# Initializing Earth Engine\nee.Initialize()\n</code></pre></p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#defining-the-region-of-interest","title":"Defining the Region of Interest","text":"<p>```python id=\"4CtS3B7bOkgN\" roi = ee.FeatureCollection(\"users/geonextgis/Bankura_District\") <pre><code>```python id=\"q1AtxYpPPD-w\"\n# Initializing a map\nmap = geemap.Map(height=\"450px\")\nmap.addLayer(roi)\nmap.centerObject(roi, 9)\n</code></pre></p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#preparing-the-dataset","title":"Preparing the Dataset","text":"<p>```python id=\"A0as1uk6Qi3D\"</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#importing-landsat-9-image-collection","title":"Importing Landsat 9 image collection","text":"<p>l9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#filtering-the-landsat-9-image-collection","title":"Filtering the Landsat 9 image collection","text":"<p>filteredL9 = l9.filterBounds(roi)\\                .filterDate(\"2023-01-01\", \"2023-12-31\")\\                .sort(\"CLOUD_COVER\")\\                .first()\\                .select([\"SR_B6\", \"SR_B5\", \"SR_B4\"])\\                .multiply(0.0000275).add(-0.2)\\                .clip(roi)</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#creating-a-fcc-visulalization","title":"Creating a FCC visulalization","text":"<p>fccVis = {     \"min\": 0.0,     \"max\": 0.3,     \"bands\": [\"SR_B6\", \"SR_B5\", \"SR_B4\"] } map.addLayer(filteredL9, fccVis, \"Landsat 9 Image\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 471, \"referenced_widgets\": [\"42d20527513b4e03b3b20d9690d3283a\", \"8d9cfe69e0d6461c9156852077dfc1ed\", \"8ec9cd2c5b70478698aac0fe23ebd57a\", \"d49c6eb44cc34b7fb4e6211e863ec924\", \"864c903719f141ee8bdad82d25bcc41e\", \"95b3f6ca3ef44b239966e8104581c47e\", \"bccac00a96ca467bbddd2cbad21569f2\", \"44ae83655de341dc87424168f58347cf\", \"33eca6f3f4de453e9920f1e77ed0e8be\", \"62584e5182ab4673a3a952ac8fe298a0\", \"5786358660214d3bb206f0d23fac400a\", \"c18015fabdbd4f7ab89cd534d6aaa83f\", \"59cb89040d7b4a7393f4b7b824223bae\", \"eb7c3573f971467ab178dcc9620db23d\", \"31dce48c538c4befaa6d59fdf5a6f802\", \"e4c3fa5003574bf696f92ef0c017622b\", \"66bd6d94a7704692a80aaae8443f478f\", \"99f81972f8c44840975910ff435b6288\", \"cc6314a4ae974effa27623398dcfa1d9\", \"0571fb7435cf4f58b85aa26bd6575d19\", \"4491a309d4134180be09952fc9705e90\", \"9d90f3b2616643f6920e1a0f5a078431\", \"d7de340d8e5d4a3faaa8e003d06215a8\", \"65f8eb0063e544a087fac7ac725374fa\", \"d867e36d41774fc587051e5ea04ad2db\", \"9b90b8eca25241c68025ba7961d6bb1e\", \"311cc9f60c01475a95cb52fad27d15ba\", \"2bea5869901e4db9b4622341557f2474\", \"732d938c98d4484c968bbb0b6826a91f\"]} id=\"MJ43TOqTS_6l\" outputId=\"ecee40c8-d60f-4166-fbc9-bd31178f6d5a\"\n# Extracting a subset area\nroi_subset = ee.Geometry.Polygon([\n                                 [87.011035, 23.179817],\n                                 [87.104422, 23.179817],\n                                 [87.104422, 23.266581],\n                                 [87.011035, 23.266581],\n                                 [87.011035, 23.179817]\n                                 ])\n\n# Clipping the image with the subset area\nsubset_image = filteredL9.clip(roi_subset)\n\n# Add the subset image to the map\nmap.addLayer(subset_image, fccVis, \"Subset Image\")\nmap.addLayer(roi_subset, {\"color\": \"red\"}, \"ROI Subset\")\n\n# Display the map\nmap\n</code></pre></p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#converting-ee-image-to-numpy-array","title":"Converting EE Image to NumPy Array","text":"<p>```python id=\"zoEFPVH8qjn9\"</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#converting-subset-image-to-numpy-array","title":"Converting subset image to numpy array","text":"<p>image_arr = geemap.ee_to_numpy(subset_image) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"OrDx_13Qw2Va\" outputId=\"f2d81652-f574-4fad-9a49-46676479a9bc\"\n# Checking the image array information\nprint(f\"Datatype: {type(image_arr)}\")\nprint(f\"Shape: {image_arr.shape}\")\nprint(f\"Dimensions: {image_arr.ndim}\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DT_p6Pqmw_tD\" outputId=\"a003b683-2d5b-4d3b-c7cb-85fd1b074e1b\"</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#checking-the-numpy-array","title":"Checking the numpy array","text":"<p>image_arr <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"lqW460zk0gcs\" outputId=\"4008f99b-15ee-4ed6-8bdb-f2ae86ed20ff\"\n# Getting the maximum and minimum pixel value in the image\nprint(\"Maximum Pixel Value in the image:\", scaled_arr.max())\nprint(\"Minimum Pixel Value in the image:\", scaled_arr.min())\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"q6eWDxxlzaM-\" outputId=\"7f0f3c1c-537c-4823-990a-f0bb53fe2d26\"</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#scaling-the-value-in-the-range-of-0-to-255","title":"Scaling the value in the range of 0 to 255","text":"<p>scaled_arr = (scaled_arr[:, :, 0:3]/scaled_arr.max())*255</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#printing-the-datatype-of-the-scaled-array","title":"Printing the datatype of the scaled array","text":"<p>type(scaled_arr.dtype)</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#changing-the-datatype-to-unsigned-int8","title":"Changing the datatype to unsigned int8","text":"<p>scaled_arr = scaled_arr.astype(\"uint8\")</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#printing-the-scaled-array","title":"Printing the scaled array","text":"<p>scaled_arr <pre><code>&lt;!-- #region id=\"HH8lfwaa2_PM\" --&gt;\n## **Data Visualization**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 291} id=\"S6ZGQTT23qXz\" outputId=\"1e8c7680-eefb-4939-9a86-5e91574ec3e6\"\n# Writing a function to generate histogram of all bands in the array\ndef plot_histogram(array, bandNames):\n  nBands = array.ndim\n  fig, ax = plt.subplots(nrows=1, ncols=nBands, figsize=(14, 3))\n  fig.subplots_adjust(wspace = 0.28)\n\n  for i in range(nBands):\n    band = array[:, :, i].ravel()\n    sns.histplot(band, bins=50, ax=ax[i])\n    ax[i].set_title(f\"{bandNames[i]} Band's Histogram\")\n    ax[i].set_xlabel(\"Pixel Values\")\n    ax[i].title.set_size(10)\n\n# Defining Band Names in a list\nbandNames = [\"SWIR1\", \"NIR\", \"Red\"]\n\n# Plotting the histogram of all the bands\nplot_histogram(scaled_arr, bandNames)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 354} id=\"8L8r7C282780\" outputId=\"59c346fa-5c89-47ae-8a6a-14ab26142192\"</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#writing-a-function-to-generate-image-display-of-all-bands-in-the-array","title":"Writing a function to generate image display of all bands in the array","text":"<p>def plot_image(array, bandNames):   nBands = array.ndim   fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))</p> <p>for i in range(nBands):     band = array[:, :, i]     axs[i].imshow(band)     axs[i].set_title(bandNames[i] + \" Band\")     axs[i].title.set_size(10)</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Integration_of_GEE_with_NumPy/#plotting-the-image-of-all-the-bands","title":"Plotting the image of all the bands","text":"<p>plot_image(scaled_arr, bandNames) ```</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Untitled/","title":"Untitled","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Untitled/#reading-synthetic-aperture-radar-images-from-gee","title":"Reading Synthetic Aperture Radar Images from GEE","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/","title":"Algebra Of Vectors And Matrices","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#algebra-of-vectors-and-matrices","title":"Algebra of Vectors and Matrices","text":"<p>Author: Krishnagopal Halder</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#02-introduction-to-vectors","title":"02. Introduction to Vectors","text":"<p>In linear algebra, vectors are fundamental objects used to represent quantities that have both magnitude and direction. A vector can be thought of as an ordered collection of numbers, known as components or coordinates, arranged in a specific order. Each component of a vector represents a coordinate along a particular dimension in a coordinate system. The number of components in a vector determines its dimensionality.</p> <p>Vectors are commonly denoted by lowercase bold letters, such as v or u, or by lowercase letters with an arrow on top, such as \u1e8b or \u1e8f. For example, in a two-dimensional space, a vector x can be represented as x = [x\u2081, x\u2082], where x\u2081 and x\u2082 are the components of x along the x and y axes, respectively.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#elementary-properties","title":"Elementary Properties","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#01-vector-creation","title":"01. Vector Creation","text":"<pre><code># Creating a two dimensional vector\nx = np.array([[1], [2]])\n# Checking the dimension\nprint(x.ndim)\n# Printing the x\nx\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#02-sum-of-two-column-vectors","title":"02. Sum of Two Column Vectors","text":"<pre><code># Creating another two dimensional vector\ny = np.array([[3], [4]])\n# Adding two vector x and y\nz = x + y\n</code></pre> <pre><code># Plotting all the vectors x, y, z\nplt.figure(figsize=(6, 4))\nplt.quiver(0, 0, x[0], x[1], angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", label=\"x\")\nplt.quiver(0, 0, y[0], y[1], angles=\"xy\", scale_units=\"xy\", scale=1, color=\"green\", label=\"y\")\nplt.quiver(0, 0, z[0], z[1], angles=\"xy\", scale_units=\"xy\", scale=1, color=\"blue\", label=\"z or (x+y)\")\nplt.xlim([0, 5])\nplt.ylim([0, 7])\nplt.grid()\nplt.legend(loc=\"lower right\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#03-transpose-of-a-column-vector","title":"03. Transpose of a Column Vector","text":"<pre><code># Transpose of x\nx.T\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#04-length-or-euclidean-norm-of-a-vector","title":"04. Length or Euclidean Norm of a Vector","text":"<pre><code># Calculating length of vector x\nlen_x = np.linalg.norm(x)\nprint(\"Length of x:\", round(len_x, 4))\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#05-inner-product-or-dot-product-of-two-vectors","title":"05. Inner Product or Dot Product of Two Vectors","text":"<pre><code># Calculating the dot product between x and y\ndot_product = (x.T).dot(y)\ndot_product\n</code></pre> <pre><code>x.T.dot(y)\n</code></pre> <pre><code># Calculating the dot product by using angle between two vectors\n# Creating a function to calculate angle between two vectors\ndef angle(v1, v2):\n    len_v1 = np.linalg.norm(v1)\n    len_v2 = np.linalg.norm(v2)\n    dot_product = (v1.T).dot(v2)\n    arc_cos = math.acos(dot_product / (len_v1 * len_v2)) # angle in radian\n    return arc_cos\n\n# Deriving the angle between x and y\nangle = angle(x, y)\nprint(\"Angle between x and y:\", round(angle, 4))\nprint(\"Angle between x and y in Degree:\", round(angle * (180 / math.pi), 4))\n</code></pre> <pre><code># Calculating dot product\ndot_product = np.linalg.norm(x) * np.linalg.norm(y) * math.cos(angle)\ndot_product\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#06-vector-representation-in-terms-of-orthogonal-unit-vectors","title":"06. Vector Representation in terms of Orthogonal Unit Vectors","text":"<pre><code># Creating two unit vectors\ni = np.array([[1], [0]])\nj = np.array([[0], [1]])\n# Representing vector x in terms of two unit vectors, i and j, respectively\nx_new = x[0] * i + x[1] * j\nx_new\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#03-introduction-to-matrices","title":"03. Introduction to Matrices","text":"<p>Matrices are a fundamental concept in linear algebra and are used to represent and manipulate linear systems of equations. In linear algebra, a matrix is a rectangular array of numbers or symbols arranged in rows and columns. The size of a matrix is specified by its dimensions, which are given by the number of rows and columns. For example, a matrix with m rows and n columns is said to have dimensions m x n.</p> <p>Each entry in a matrix is called an element and is identified by its position in the matrix using the row and column indices. The row index is typically denoted by i, and the column index is denoted by j. Thus, the element in the i-th row and j-th column of a matrix A is written as A[i, j].</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#elementary-properties_1","title":"Elementary Properties","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#01-matrix-creation","title":"01. Matrix Creation","text":"<pre><code># Creating a 2x2 matrix\nA = np.mat([[3, 2], [1, 4]])\nA\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#02-multiplication-of-a-matrix-with-a-vector","title":"02. Multiplication of a Matrix with a Vector","text":"<pre><code># Multiplying matrix A with vector x\nmatrix_mult_vec = A * x\nmatrix_mult_vec\n</code></pre> <pre><code># Plotting the linear transformation of vector x after multiplying with matrix A\nplt.quiver(0, 0, x[0], x[1], \n           angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", label=\"Original Vector\")\nplt.quiver(0, 0, matrix_mult_vec[0], matrix_mult_vec[1], \n           angles=\"xy\", scale_units=\"xy\", scale=1, color=\"green\", label=\"Tranformed Vector\")\nplt.xlim([0, 8])\nplt.ylim([0, 10])\nplt.grid()\nplt.legend(loc=\"lower right\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#03-product-or-multiplication-of-two-matrices","title":"03. Product or Multiplication of Two Matrices","text":"<p>The product of two matrices is allowed only when the first matrix has the same number of columns as the second matrix has rows.</p> <pre><code># Creating another matrix B\nB = np.mat([[4, 2], [1, 3]])\n# Printing two matrices\nprint(\"Matrix A:\\n\", A)\nprint(\"Matrix B:\\n\", B)\n# Calculating the product of two 2x2 matrices\nprint(\"Product of A and B:\\n\", A * B)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#04-properties-of-matrix-multiplication","title":"04. Properties of Matrix Multiplication","text":"<pre><code># Matrix multiplication is not commutative, AB is not equal to BA\n(A * B) is (B * A)\n</code></pre> <pre><code># Matrix multiplication is associative, (AB)C = A(BC) or (AB)C = A(BC) = ABC\n# Creating another matrix C\nC = np.mat([[2, 3], [4, 1]])\nC\n</code></pre> <pre><code># Multiplying A and B\nA_B = B * A\n# Multiplying B and C\nB_C = C * B\n</code></pre> <pre><code># Checking the associativity\n(C * A_B) == (B_C * A)\n</code></pre> <pre><code>(C * B * A) == (B_C * A)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#05-outer-product-of-two-vectors","title":"05. Outer Product of Two Vectors","text":"<pre><code># Outer product of vectors x and y\nx * y.T\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#06-transpose-of-a-matrix","title":"06. Transpose of a Matrix","text":"<pre><code># Creating a 3x3 matrix\nD = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nD\n</code></pre> <pre><code># Transpose of matrix D\nD.T\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/03_Algebra_of_Vectors_and_Matrices/#07-properties-of-transpose","title":"07. Properties of Transpose","text":"<pre><code>(A + B).T == (A.T) + (B.T)\n</code></pre> <pre><code>(A * B).T == (B.T) * (A.T)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/","title":"Square Matrices","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#square-matrices","title":"Square Matrices","text":"<p>Author: Krishnagopal Halder</p> <p>Square matrices are a fundamental concept in linear algebra, characterized by having an equal number of rows and columns. In other words, a square matrix has the same number of rows as it does columns, resulting in a shape that resembles a square. The size or order of a square matrix is typically denoted by a single positive integer, such as \"n\" or \"m,\" which represents both the number of rows and the number of columns.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#02-elementary-properties","title":"02. Elementary Properties","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#01-creation-of-square-matrix","title":"01. Creation of Square Matrix","text":"<pre><code># Creating a two dimensional square matrix\nA = np.mat([[1, 2], [3, 4]])\n# Printing the matrix\nA\n</code></pre> <pre><code># Checking the dimension of the matrix A\nA.ndim\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#02-determinant-of-a-square-matrix","title":"02. Determinant of a Square Matrix","text":"<pre><code># Calculating the determinant of 2x2 matrix A\ndet_A = np.linalg.det(A)\ndet_A\n</code></pre> <pre><code># Visualization\nplt.figure(figsize=(5, 5))\nplt.quiver(0, 0, 1, 0, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", label=\"i (Unit Vector)\")\nplt.quiver(0, 0, 0, 1, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"green\", label=\"j (Unit Vector)\")\nplt.quiver(0, 0, 1, 2, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"blue\", label=\"Transformed i\")\nplt.quiver(0, 0, 3, 4, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"orange\", label=\"Transformed j\")\nplt.xlim([-4, 5])\nplt.ylim([-4, 5])\nplt.legend(loc=\"lower right\")\nplt.grid()\n</code></pre> <pre><code># Calculating the determinant of a 3x3 matrix B\nB = np.mat([[1, 0, 2], [4, 2, 1], [3, 5, 2]])\ndet_B = np.linalg.det(B)\ndet_B\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#03-properties-of-the-determinant","title":"03. Properties of the Determinant","text":"<pre><code># Creating another 2x2 square matrix B\nB = np.mat([[2, 1], [3, 5]])\nB\n</code></pre> <pre><code># The determinant has the property, |AB| = |A||B|\nAB = B * A\nAB\n</code></pre> <pre><code># Calculating the determinant of AB matrix\ndet_AB = np.linalg.det(AB)\ndet_AB\n</code></pre> <pre><code># Calculating the determinant of B\ndet_B = np.linalg.det(B)\ndet_B\n</code></pre> <pre><code># Proving the property, |AB| = |A||B|\nround(det_AB) == round(det_A * det_B)\n</code></pre> <pre><code># The determinant has the property, |A.T| = |A|\n# Calculating the transpose of A\ntrans_A = A.T\ntrans_A\n</code></pre> <pre><code># Proving the property, |A.T| = |A|\nround(np.linalg.det(trans_A)) == round(np.linalg.det(A))\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#04-identity-matrix","title":"04. Identity Matrix","text":"<pre><code># Creating an identity matrix\nidentity = np.identity(2, dtype=\"int8\")\nidentity\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/01_Images_Arrays_Matrices/04_Square_Matrices/#05-properties-of-identity-matrix","title":"05. Properties of Identity Matrix","text":"<pre><code># For any A, IA = AI = A\nIA = identity * A\nIA\n</code></pre> <pre><code>AI = A * identity\nAI\n</code></pre> <pre><code># Proving the property\nprint(\"IA is AI:\\n\", IA == AI)\nprint(\"AI is A:\\n\", AI == A)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/","title":"Download Landsat 8 Data","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ee\nimport geemap\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/#set-up-earth-engine","title":"Set Up Earth Engine","text":"<pre><code># Authenticating and initializing Earth Engine API\n# ee.Authenticate()\n</code></pre> <pre><code># ee.Initialize()\n</code></pre> <pre><code># Creating a Map object\nm = geemap.Map(width=\"100%\", height=\"550px\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/#add-the-region-of-interest","title":"Add the Region of Interest","text":"<pre><code># Define the region of interest\nroi = ee.Geometry.Polygon([[88.282386, 22.519994], \n                           [88.391925, 22.519994],\n                           [88.391925, 22.623027],\n                           [88.282386, 22.623027],\n                           [88.282386, 22.519994]])\n# Display the roi\nm.addLayer(roi, {}, \"Region of Interest\")\nm.centerObject(roi, 12)\nm\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/#prepare-the-data","title":"Prepare the Data","text":"<pre><code># Import Landsat 8 imagery\nl8 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n\n# Filter the image collection\nl8Filtered = l8.filterBounds(roi)\\\n               .filterDate(\"2022-01-01\", \"2022-12-31\")\\\n               .filterMetadata(\"CLOUD_COVER\", \"less_than\", 10)\\\n               .select(\"SR_B.*\")\n\n# Create a median composite\ncomposite = l8Filtered.median()\\\n                      .clip(roi)\n</code></pre> <pre><code># Create a function to apply scale factor\ndef applyScaleFactor(image):\n    return image.multiply(0.0000275).add(-0.2)\n\n# Apply the function over the composite image\nscaled_composite = applyScaleFactor(composite)\n</code></pre> <pre><code># Display the selected image\nrgbVis = {\n    \"min\": 0.0,\n    \"max\": 0.3,\n    \"bands\": [\"SR_B5\", \"SR_B4\", \"SR_B3\"],\n}\nm.addLayer(scaled_composite, rgbVis, \"Landsat 8 Composite Image\")\nm\n</code></pre> <pre><code># Create a function to add Indices\ndef addIndices(image):\n    ndvi = image.normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename(\"NDVI\")\n    mndwi = image.normalizedDifference([\"SR_B3\", \"SR_B6\"]).rename(\"MNDWI\")\n    ndbi = image.normalizedDifference([\"SR_B6\", \"SR_B5\"]).rename(\"NDBI\")\n    savi = image.select(\"SR_B5\").subtract(image.select(\"SR_B4\"))\\\n                .divide(image.select(\"SR_B5\").add(image.select(\"SR_B4\").add(0.5)))\\\n                .multiply(1.5).rename(\"SAVI\")\n\n    return image.addBands(ndvi)\\\n                .addBands(mndwi)\\\n                .addBands(ndbi)\\\n                .addBands(savi)\n\n# Apply the addIndices function over the scaled image\nwithIndices = addIndices(scaled_composite)\nm.addLayer(withIndices, rgbVis, \"Landsat 8 with Indices\")\nm\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Download_Landsat_8_Data/#export-the-imagery-to-drive","title":"Export the Imagery to Drive","text":"<pre><code># Create a task to export the image\ntask = ee.batch.Export.image.toDrive(image=withIndices.toDouble(),\n                                     description='Landsat_8_Image',\n                                     folder=\"GEE\",\n                                     fileNamePrefix=\"Landsat_8_Image_Kolkata\",\n                                     region=roi,\n                                     scale=30,\n                                     maxPixels=1e10,\n                                     fileFormat=\"GeoTIFF\")\n</code></pre> <pre><code># Start the task\n# task.start()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/","title":"Satellite Image Classification Using Scikit Learn","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#satellite-image-classification-using-scikit-learn","title":"Satellite Image Classification using Scikit-Learn","text":"<p>Author: Krishnagopal Halder</p> <p>In this project, I harness the power of Scikit-Learn's Random Forest algorithm to develop a satellite image classification system. My goal is to automatically classify diverse land cover types within satellite images, including urban areas, water bodies, vegetation, and grass. I begin by collecting and preprocessing a labeled dataset, ensuring consistency in quality and format. To enhance model performance and interpretability, I employ Min-Max scaling to normalize feature values. Random Forest model is trained on these scaled features and evaluated using various metrics, with a particular focus on understanding feature importance, which helps us gain insights into the driving factors behind classification decisions. This project not only delivers an accurate image classification solution but also provides valuable insights into the significant features contributing to land cover classification, making it applicable in diverse fields such as environmental monitoring, urban planning, and land use land cover classification.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#read-the-imagery","title":"Read the Imagery","text":"<pre><code>dataset = rasterio.open(r\"D:\\Research Works\\Dataset\\Raster\\Landsat_8_Image_Kolkata.tif\")\n</code></pre> <pre><code># Visualize the 'RGB' and 'SFCC' image\n\n# Create a function to Normalize the bands\ndef normalize(band):\n    band_min, band_max = band.min(), band.max()\n    return ((band - band_min) / (band_max - band_min))\n\n# Apply the normalize function over the bands\nband_5 = normalize(dataset.read(5))\nband_4 = normalize(dataset.read(4))\nband_3 = normalize(dataset.read(3))\nband_2 = normalize(dataset.read(2))\n\n# Create the 'RGB' and 'SFCC' Image\nrgb = np.dstack((band_4, band_3, band_2))\nsfcc = np.dstack((band_5, band_4, band_3))\n\n# Display the images\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 15))\nax1.imshow(rgb)\nax1.set_title(\"RGB Image of ROI\")\n\nax2.imshow(sfcc)\nax2.set_title(\"SFCC Image of ROI\")\nplt.show()\n</code></pre> <pre><code># Creating a empty pandas dataframe to store the pixel values\ndataset_bands = pd.DataFrame()\n</code></pre> <pre><code># Joining the pixel values of different bands into the dataframe\nfor i in dataset.indexes:\n    temp = dataset.read(i)\n    temp = pd.DataFrame(data=np.array(temp).flatten(), columns=[i])\n    dataset_bands = temp.join(dataset_bands)\n</code></pre> <pre><code>dataset_bands\n</code></pre> <pre><code># Rename the columns\nnew_column_names = {1:\"Coastal\", 2:\"Blue\", 3:\"Green\", 4:\"Red\", 5:\"NIR\", 6:\"SWIR1\",\n                    7:\"SWIR2\", 8:\"NDVI\", 9:\"MNDWI\", 10:\"NDBI\", 11:\"SAVI\", 12:\"Label\"}\ndataset_bands.rename(columns=new_column_names, inplace=True)\n</code></pre> <pre><code>dataset_bands\n</code></pre> <pre><code># Changing the order of columns\ndataset_bands = dataset_bands[dataset_bands.columns[::-1]]\ndataset_bands\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#apply-minmax-scaler","title":"Apply MinMax Scaler","text":"<pre><code>from sklearn.preprocessing import MinMaxScaler\n</code></pre> <pre><code># Create an object of the minmax scaler\nscaler = MinMaxScaler()\n\n# Fit the data\nscaler.fit(dataset_bands.drop(\"Label\", axis=1))\n\n# Transform the data\ndataset_bands_scaled = scaler.transform(dataset_bands.drop(\"Label\", axis=1))\n</code></pre> <pre><code># Convert the scaled array into pandas dataframe\ndataset_bands_scaled = pd.DataFrame(dataset_bands_scaled, columns=dataset_bands.iloc[:, :-1].columns)\ndataset_bands_scaled\n</code></pre> <pre><code># Describe the scaled data\ndataset_bands_scaled.describe()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<pre><code># Plot the Histogram of all the bands\nfig, axes = plt.subplots(nrows=4, ncols=3, figsize=(20, 20))\n\n# Flatten the axes array to make it easier to access each subplot\naxes = axes.flatten()\n\n# Loop through each column and plot its histogram in a subplot\nfor i, column in enumerate(dataset_bands_scaled.columns):\n    ax = axes[i]\n    sns.histplot(dataset_bands_scaled[[column]], ax=ax, bins=50)\n    ax.set_title(column, fontsize=14)\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequecy\")\n\n# Adjust spacing between subplots\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Plot a heatmap to represent the correlation between bands\nplt.figure(figsize=(8, 6))\n\n# Create a mask to hide the upper triangle of the heatmap\nmask = np.triu(np.ones_like(dataset_bands_scaled.corr()))\nsns.heatmap(dataset_bands_scaled.corr(), cmap=\"RdYlGn\", mask=mask, \n            linewidth=1, annot=True, fmt=\".1f\")\nplt.title(\"Correlation Matrix\")\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#prepare-the-training-data","title":"Prepare the Training Data","text":"<pre><code># Create the labeled data\ndataset_bands_scaled[\"Label\"] = dataset_bands[\"Label\"]\ndataset_bands_scaled\n</code></pre> <pre><code># Extract the label data with non null values\nlabeled_data = dataset_bands_scaled.dropna()\nlabeled_data.head()\n</code></pre> <pre><code># Change the datatype of the 'Label' column\nlabeled_data[\"Label\"] = labeled_data[\"Label\"].astype(int)\nlabeled_data.head()\n</code></pre> <pre><code># Plot the training data\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(sfcc, alpha=0.5)\nax.imshow(np.array(dataset_bands_scaled[\"Label\"]).reshape(dataset.shape), cmap=\"jet\")\nplt.title(\"Training Samples\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>x_train, x_test, y_train, y_test = train_test_split(labeled_data.drop(\"Label\", axis=1),\n                                                    labeled_data[\"Label\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nx_train.shape, x_test.shape\n</code></pre> <pre><code>x_train\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#train-a-classifier","title":"Train a Classifier","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n</code></pre> <pre><code># Create an object of the classifir\nclassifier = RandomForestClassifier(n_estimators=75,\n                                    criterion=\"gini\",\n                                    max_depth=8,\n                                    min_samples_split=10,\n                                    random_state=0)\n\n# Fit the training data\nclassifier.fit(x_train, y_train)\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#feature-importance","title":"Feature Importance","text":"<pre><code># Extract the feature importance\nfeature_importance = dict(zip(x_train.columns, classifier.feature_importances_.round(3)))\n# Convert the dictionary into a pandas series\nfeature_importance = pd.Series(feature_importance)\n</code></pre> <pre><code># Sort the feature importance in descending order\nfeature_importance.sort_values(ascending=False, inplace=True)\n</code></pre> <pre><code>feature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(8, 6))\n\n# Reverse the color palette \ncolor_palette = sns.color_palette(\"Reds\", len(feature_importance))\nreversed_palette = color_palette[::-1]\n\nsns.barplot(x=feature_importance, y=feature_importance.index, palette=reversed_palette)\nplt.title(\"Feature Importance\")\nplt.xlabel(\"Feature Importance\")\nplt.show()\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#model-validation","title":"Model Validation","text":"<pre><code># Predict the test data using RF Classifier \ny_pred = classifier.predict(x_test)\n</code></pre> <pre><code>from sklearn.metrics import classification_report, confusion_matrix\n</code></pre> <pre><code># Print the model validation metrics\nprint(classification_report(y_test, y_pred))\n</code></pre> <pre><code># Plot the confusion matrix\nlabels = [\"Built-Up\", \"Water\", \"Vegetation\", \"Grass\"]\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap=\"YlGn\", annot=True, fmt=\"\",\n            xticklabels=labels, yticklabels=labels)\nplt.title(\"Confusion Matrix\")\n</code></pre>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/Machine%20Learning%20with%20Scikit%20Learn/Satellite_Image_Classification_using_Scikit_Learn/#classify-the-imagery","title":"Classify the Imagery","text":"<pre><code># Classify the scaled imagery\nclassified = classifier.predict(dataset_bands_scaled.drop(\"Label\", axis=1))\n</code></pre> <pre><code># Check image dimensions\ndataset.shape\n</code></pre> <pre><code># Reshape the array into image dimensions\nclassified_array = classified.reshape(dataset.shape)\n</code></pre> <pre><code># Plot the SFCC and classified array\nimport matplotlib.colors as mcolors\n\n# Create a list of color\nlulc_color = [\"#EB5353\", \"#1450A3\", \"#285430\", \"#F9D923\"]\ncmap_custom = mcolors.ListedColormap(lulc_color)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 15))\n\nax1.imshow(sfcc)\nax1.set_title(\"Standard False Color Composite\")\n\nax2.imshow(classified_array, cmap=cmap_custom)\nax2.set_title(\"Random Forest Classified Image\")\n\n# Show the figure\nplt.show()\n</code></pre>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/","title":"Mastering Machine Learning and GEE for Earth Science","text":"<p>This repository is designed to empower individuals with advanced expertise in integrating Machine Learning techniques with Google Earth Engine (GEE).</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/","title":"Intro To Geemap","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#intro-to-geemap","title":"Intro to geemap","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#installing-the-updated-version-of-geemap","title":"Installing the Updated Version of geemap","text":"<p>Install geemap version <code>0.29.3</code> or a later release to enable the automatic authentication feature, as this functionality is only supported in these versions. To install other Python packages, you can use the <code>pip install package_name</code> command. To update a package, you can use <code>pip install --upgrade package_name</code> or <code>pip install -U package_name</code>.</p> <p>```python id=\"3Nu69rwRf2ZU\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#pip-install-u-geemap","title":"%pip install -U geemap","text":"<pre><code>&lt;!-- #region id=\"xiAw2gqigGT_\" --&gt;\n## **Import the Required Libraries**\n&lt;!-- #endregion --&gt;\n\n```python id=\"TAvvfdPugQCg\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport ee\nimport geemap\n</code></pre>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#initialize-a-map","title":"Initialize a Map","text":"<p>When initializing a Map object, you may be prompted for authorization. If this occurs, you can obtain the required authorization token by visiting the provided link.</p> <p>Certainly! When working with the Map object in a Python environment, you can play with various parameters to customize the map display.</p> <p>```python id=\"fAyo0CkEiuyY\" Map = geemap.Map() Map <pre><code>```python id=\"jbqnWaFonxPa\"\n# Change the map height and width parameter\nMap = geemap.Map(height=\"400pt\", width=\"800pt\")\nMap\n</code></pre></p> <p>```python id=\"mLurHUdaqBXr\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#print-the-list-of-basemaps","title":"Print the list of basemaps","text":"<p>basemaps = geemap.basemaps</p> <p>for basemap in basemaps:     print(basemap) <pre><code>```python id=\"DMNjQgfnpZ0F\"\n# Change the basemap layer to 'Esri World Imagery'\nMap.add_basemap(basemap=\"Esri.WorldImagery\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#working-with-feature-collection","title":"Working with Feature Collection","text":"<p>In Google Earth Engine, a Feature Collection is a type of data structure that represents a collection of vector features. These features could represent points, lines, polygons, or a combination of these geometries. Feature Collections are fundamental for working with spatial data and conducting geospatial analyses in Earth Engine.</p> <p></p> <p>```python id=\"seisMMFQq14u\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#set-up-a-new-map-object","title":"Set up a new map object","text":"<p>Map = geemap.Map(height=\"400pt\", width=\"100%\") Map <pre><code>```python id=\"1HTb8wvhrHTQ\"\n# Import 'World Administrative Boundary' shapefile layer as a feature collection\nworld = ee.FeatureCollection(\"users/geonextgis/World_Administrative_Boundaries\")\n\n# Set visualization/style parameters\nworld_style = {\n    \"fillColor\": \"00000000\", # transparent color code\n    \"color\": \"black\", # color of the stroke\n    \"width\": 0.5 # stroke width\n}\n\n# Display the layer\nMap.addLayer(world.style(**world_style), {}, \"World Administrative Boundaries\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#filtering-feature-collection","title":"Filtering Feature Collection:","text":"<p>Filtering a Feature Collection in Google Earth Engine involves selecting a subset of features based on specific criteria, such as spatial, attribute, or temporal conditions. This process is essential for isolating relevant data for analysis. Here are the key aspects of filtering a Feature Collection:</p> <ol> <li> <p>Attribute Filtering:</p> <ul> <li>Attribute filtering involves selecting features based on their attribute values, such as properties or characteristics.</li> <li>The <code>filter</code> function is often used in combination with <code>ee.Filter</code> to define attribute-based conditions.</li> </ul> </li> </ol> <p>```python id=\"p-6_3pxI5I-d\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#filter-all-the-asian-countries","title":"Filter all the Asian countries","text":"<p>asian_countries = world.filter(ee.Filter.eq(\"continent\", \"Asia\")) asian_countries_style = {     \"fillColor\": \"93939388\",     \"color\": \"black\",     \"width\": 1 } Map.addLayer(asian_countries.style(**asian_countries_style), {}, \"Asian Countries\") Map.centerObject(asian_countries, 3) <pre><code>&lt;!-- #region id=\"h29pScitxlE6\" --&gt;\n2. **Spatial Filtering:**\n   - Spatial filtering involves selecting features based on their geographic location or proximity to a specified region.\n   - Common spatial filters include `geometry`, `intersects`, `bounds`, and `distance`, allowing users to focus on features within a defined area or at a certain distance from a given point.\n&lt;!-- #endregion --&gt;\n\n```python id=\"7yos-gPEv4jw\"\n# Read 'Gridded Population of the World Version 4' point dataset provided by NASA SEDAC\ngpw = ee.FeatureCollection(\"projects/sat-io/open-datasets/sedac/gpw-v4-admin-unit-center-points-population-estimates-rev11\")\n\n# Filter the feature collection with only Asian countries (Spatial Filtering)\n# Filter the points where population estimates is more than 5000000 in 2020 (Attribute Filtering)\ngpw_asia = gpw.filterBounds(asian_countries)\\\n              .filter(ee.Filter.gt(\"UN_2020_E\", 5000000))\n\ngpw_asia_style = {\n    \"fillColor\": \"C70039\",\n    \"color\": \"black\",\n    \"width\": 1,\n    \"pointSize\": 5\n}\nMap.addLayer(gpw_asia.style(**gpw_asia_style), {}, \"Global Population Estimates &gt; 5 Lakhs (Asia)\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#downloading-feature-collection","title":"Downloading Feature Collection:","text":"<p>\ud83e\udd14 Note: It's always a good practice to comment out the <code>task.start()</code> line when sharing code to avoid unintentional multiple downloads of the same file.</p> <p>```python id=\"myOwTbnCyyaA\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#initialize-an-export-task","title":"Initialize an export task","text":"<p>task = ee.batch.Export.table.toDrive(collection=gpw_asia,                                      description=\"GPW_Asia_Pop_Est_2020\",                                      folder=\"GEE\",                                      fileNamePrefix=\"GPW_Asia_Pop_Est_2020\",                                      fileFormat=\"SHP\")</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#export-the-filtered-feature-collection","title":"Export the filtered feature collection","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#taskstart","title":"task.start()","text":"<pre><code>&lt;!-- #region id=\"dqiPxq5ZB-K7\" --&gt;\n\ud83e\udd14 **Note:** In geemap, the addLayer function is designed to visualize data on the map by adding a layer. However, it's important to note that this function always returns an Image object, not a Feature Collection or individual Feature when using with `style` function.\n\n\ud83d\udd11 **Exercise:**\n - Filter African countries from the `World Administrative Boundary` layer.\n - Filter global population data within the selected African countries.\n - Identify points where population estimates are more than 500,000 (5 lakhs) in 2020.\n - Visualize the filtered layers on the map with custom styling.\n - Download the filtered layers into the Google drive.\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"RHMtRcAzyPyM\" --&gt;\n## **Working with Image Collection**\nIn Google Earth Engine (GEE), an Image Collection is a fundamental data structure used to represent a group or sequence of images. These images can be satellite observations, remotely sensed data, or any other raster data that can be organized over time or space.\n\n&lt;img src=\"https://www.mdpi.com/remotesensing/remotesensing-14-02778/article_deploy/html/images/remotesensing-14-02778-g001.png\"&gt;\n&lt;!-- #endregion --&gt;\n\n```python id=\"yN0bzy4T0DYB\"\n# Set up a new map object\nMap = geemap.Map(height=\"400pt\", width=\"100%\")\nMap\n</code></pre> <p>```python id=\"l423BLL41jTM\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#add-a-marker-to-the-map-and-convert-it-into-an-ee-feature","title":"Add a marker to the map and convert it into an EE feature","text":"<p>marker = Map.draw_last_feature</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#marker","title":"marker","text":"<pre><code>```python id=\"LEzEwbuD7wAO\"\n# Import USGS Landsat 9 Level 2, Collection 2, Tier 1 image collection\nL9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")\n</code></pre>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#filtering-image-collection","title":"Filtering Image Collection","text":"<ul> <li>We can filter Image Collections based on various criteria, such as <code>date range</code>, <code>spatial extent</code>, or <code>metadata</code> properties.</li> <li>To optimize the workflow, it is advisable to follow a specific order when filtering an Image Collection. The recommended sequence includes filtering by boundary first, followed by dates, and then metadata properties.</li> <li>This approach helps reduce computational load and speeds up the execution of operations.</li> </ul> <p>```python id=\"0ZQQXlh08o8r\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#filter-the-landsat-9-image-collection-using-marker-points-filter-with-boundary","title":"Filter the Landsat 9 image collection using marker points (Filter with boundary)","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#select-images-acquired-in-the-year-2022-filter-with-date-range","title":"Select images acquired in the year 2022 (Filter with date range)","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#filter-the-collection-to-include-images-with-less-than-10-cloud-cover-filter-with-metadata","title":"Filter the collection to include images with less than 10% cloud cover (Filter with metadata)","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#choose-the-first-image-from-the-filtered-image-collection","title":"Choose the first image from the filtered image collection","text":"<p>L9Filtered = L9.filterBounds(marker.geometry())\\                .filterDate(\"2022-01-01\", \"2022-12-31\")\\                .filterMetadata(\"CLOUD_COVER\", \"less_than\", 10)\\                .first()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/00_Intro_to_geemap/#display-the-rgb-and-sfcc-images-into-the-map","title":"Display the RGB and SFCC images into the map","text":"<p>rgb_vis = {     \"min\": 8000,     \"max\": 17000,     \"bands\": [\"SR_B4\", \"SR_B3\", \"SR_B2\"] }</p> <p>sfcc_vis = {     \"min\": 8000,     \"max\": 17000,     \"bands\": [\"SR_B5\", \"SR_B4\", \"SR_B3\"] }</p> <p>Map.addLayer(L9Filtered, rgb_vis, \"RGB Composite\") Map.addLayer(L9Filtered, sfcc_vis, \"SFCC Composite\") Map.centerObject(marker, 9) <pre><code>&lt;!-- #region id=\"-G8nSm5rBfDl\" --&gt;\n\ud83e\udd14 **Note:** `getInfo()` is a method in GEE API that allows users to retrieve the values of Earth Engine objects and transfer them from the server-side to the client-side. In the context of GEE, computations often occur on the server-side, which means that the actual data and results reside on Google's servers. To access and work with this information in your local environment, we use `getInfo()`.\n&lt;!-- #endregion --&gt;\n\n```python id=\"OFF4tJTj_huA\"\n# Store the metadata property names in a list\nimage_prop_names = L9Filtered.propertyNames()\n\n# Print the properties information\nimage_props = L9Filtered.toDictionary(image_prop_names).getInfo()\nimage_props\n</code></pre></p> <p>\ud83d\udd11 Exercise:  - Load Landsat 9 image collection for a specific region over a multi-year period (2020-2022).  - Filter the image collection based on the 'CLOUD_COVER' property (&lt; 5%).  - Visualize the image in three different band combinations, such as (Red, Green, Blue), (NIR, Red, Green), and (SWIR2, NIR, Red).  - Print metadata properties of the image.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/","title":"Cloud Masking","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#cloud-masking","title":"Cloud Masking","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#import-the-required-libraries","title":"Import the Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"jtCpeK0V3Dcx\" outputId=\"8cbc8784-d36c-4671-9659-048ed53c97cf\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"LSZlAfcbxddN\" outputId=\"7c189744-8ded-4d83-9980-86ece48b6327\"\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ee\nimport geemap\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#initialize-a-map","title":"Initialize a Map","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"Cp8Jph4818M3\" outputId=\"6b7eb743-2844-4d5b-82f3-c3abf22e6050\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#trigger-the-authentication-flow","title":"# Trigger the authentication flow.","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#eeauthenticate","title":"ee.Authenticate()","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#initialize-the-library","title":"# Initialize the library.","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#eeinitializeprojectmy-project","title":"ee.Initialize(project='my-project')","text":"<pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 554, \"referenced_widgets\": [\"aa90900599f045fe89113f08603f8a58\", \"8d6ae0b4491a485d8a150a474b4f92ae\", \"d3628fd215d04c8794f1cd58e9ae7267\", \"9263df32698c4b0dbed985f150ee5b20\", \"237b11edd249453da0b90fa7d2a74b38\", \"1aca3668a65b4d718e7bc5f687cf29e0\", \"3b305815426c4729b9b2ddce25029b02\", \"1dc6e13a693f41e4afc63d30554573a9\", \"3a1d9a3f210f422c8253727dc2a1021b\", \"297c44e1e8c04e0fb77d5a96c04eac6e\", \"842ccaeac37a409faef4f45391ac1429\", \"cfcdc79bd79b4f1cb6039fcb8d7250e0\", \"f8722942c19e456098fb7bdca65aa6c7\", \"e265a0f26bca490380e04ebe885256fe\", \"e5faff476b6a4747bc3f6b220e7196b8\", \"0d0591a71fde45d69e06d3c838696015\", \"1e46d1659b1144ba9c7cd6dd631bf0a0\", \"cf7343bc47e948fab62a7d79c13454d7\", \"5395da550158409784dc679990a28b9f\", \"e45e192e43a0482586082222f908ebaa\", \"d114815022d54e0dbbec9babc82e027d\", \"04df554b78694b2dbe894674ff5f20eb\", \"3ce841333c6c402a90c3708b24b78f2f\", \"a0d8ed46ac5c40ac9ca9553595795cfc\", \"dbfa87997d3740d1a776512813764d2b\", \"9c825794fb544595a605bb7f06f00304\", \"135972269b9d49c78bcae58967dc988c\", \"40cb076249a046fdb89a7f64a5b4ea64\", \"07fe3cafe88f48cbbcfef09ba767aab3\", \"859497b0d67d48718aa1b00255fdd212\", \"c6aab3c27eb3462ba4b3e000b36f93b7\", \"8f07faee1e6d4b80a8157a1638b6240b\", \"c2168c3ee4e84437b6fba2f9ede1f695\", \"b5fa81f1523649ca80f76656a62242b1\", \"66cc480486e24dd1972d2bab75115b8e\", \"11bd23945493429199b2f78412ddb4cb\", \"13568fb06ac04092846ebd0c1d5826fb\", \"d9b8c4c6b7ce445485765072084f0759\", \"cafed16ceaf14aeb824c950540518830\", \"a1734ea24d074fd99ae14d43854fd65a\", \"9b50f9dfac3748ccb0171ed0cc64bb98\", \"64c1c7d3711b4d0f882406aa6ed08a46\", \"ea3ed3db5343430087ace7ccb3e32050\", \"126da6372d0542da95bed6e1f7590a1c\", \"0f6b42730f7a404bb782e3991ff10319\", \"2e30788c23d64312872a0cbe858e31ef\", \"45cbdda8afe14936a76e17d29d588245\", \"9368db9c62d64ce981fa885b7876b034\", \"11d387980201411894e474c824f0615c\", \"9b14009bb2a0475a9e51578d5323f6cd\", \"a68ea6bbce634c50878fa47157fec829\", \"76896c324d4a4e5896d6bc991a4b42a4\", \"a743ac63720049719d73800e56fc6f06\", \"7c39bae4f09f4df29b3f8caa321b5c50\", \"b5f16d99452a4258a3eba40269a8dafd\", \"a44753a5f6eb47f08125288c0289efb0\", \"19adda7a05e744f5bd7968a6f1311acb\", \"08ff364872a94283bba19d34d34b6d29\", \"cc21a68ffe6b4cf5ae0cbfe66ac15009\", \"fab1b6ec16d344fb821505e875cd3630\", \"46314ffb293041f7b4304046de0d02ce\", \"b9625843322c4a68a0b665258509d84e\", \"23119c034e234b5a98407f3f4f026ab2\", \"3080f2bf265946cf920ed6372e395600\", \"6ad1f747d3654cb3b8e246a500c00892\", \"5818a71f66bf46168c51fea6ce85e9f9\", \"dce54e7d628d43ce93687a3d33de0cef\", \"8e2e6da5894f46f28b12c3aa03ec46ac\", \"60b69a8409de4d5e905b5f3c59c751c0\", \"c047b5560d9b4db7b284446ea7814fc0\", \"d551642d6aaa4ff6b972ac72eb55f8ef\", \"9db69e8c2af3486b971e9462fe8dfff2\", \"063f8efa4b844f6da6d2ab8d11dd0259\", \"564ee9bde6c4497984c5021335f75fae\", \"5b0b46892bde41d8b31743fe610286d3\", \"b4c60a5807be46d48a43803e01270c9f\", \"76c2cda9b624416f8d8e16f91f9c2a8c\", \"23f48b670a934846ac7bb6c847853646\", \"2971b1eeef65488089223fb570374d75\", \"0eb55da115e44e55a16c286cd46cc200\", \"db50f4d17a1a4f329e89e5668c3a1998\", \"1b6fd2b44fb0457cbbef6c07c02df53a\", \"8b24397888ac4405b5b376a60943216c\", \"d4b99aad3d5f490ab9a6543ef09bf6b1\", \"635aac4de8c1470181f63127179a1884\", \"b8bc526123bb40018f886432389470fd\", \"4ccc49da5e9146b594b02466fc405af3\", \"bb97f00e24554351b796d69b436d80d2\", \"d03e5c387ee34baa9c8b8289729e3ccc\", \"5cca1afb82e54a958878e9355c629089\", \"77c04a0c7fc34950870f2c157478ccac\", \"fffeb66f08604a32aac33634dfd7063c\", \"873274bae91e406fb325ec78a94ffb0d\", \"25ce740cc4944b9e936cf3c9ac3c32a7\", \"2cb2b0775f964c0bb684bfb29bdd74e3\", \"12a8497c299e47a799f7169ec3198313\", \"cc1729de4beb456e957ddcb59952d445\", \"64499a7c74fd4aa2835a15358d9a8e09\", \"5dc5aadf7f944bd1873364209aeefd79\", \"3f97ce776c134d52a267233fedb3210f\", \"a61523d9a40b466abce56e64ca46ec78\", \"81ad9647dc234495a78f2613da3ed137\", \"49f23da0974a4b659d68e06d4eecba3a\", \"42d31cbaddf748d89770de3913e83954\", \"c8fd517e1b1c4510bfc316186245b056\", \"bc527cdac37448a8b458e99b984f7640\", \"67b0e0984d3f44d2aba67d9fa0845355\", \"44b5867f660841c09bd93d894c82e462\", \"0b8451c6f8e141c7831bd67610bd2de0\", \"0ac807d8d8474162a9552248dc99e1da\", \"aba40691e3cd40b0a7f4fe4dbbb80486\", \"279d8bcbbd294eacac6884b34b1ff10e\", \"e8138fd6b9774ac1920fa8967559e5e0\", \"7b92c5ed4354419dbcfae2e7cea7e256\", \"2afd338e349e4096bfb5f36cc1c6d68d\", \"d1883c1dd2e54bb1a196380c6ed19b8a\", \"6a669b617c704b13825c68e91708a327\", \"f4c048f77c924c428d92b820ec112869\", \"700c9599ae284ebe86c0232b5e8bd025\", \"3b9593cf10c34da7a80dbf5aec8e02a4\", \"35ad30a4b87a4a249d4078ca52148947\", \"5ebbe173dfe542fd951dbb5ab59ff93b\", \"86ba2ae0b20645b882a19e65d00c0002\", \"5bdcb1ba86c343c9aae1763410d079a6\", \"c36c1d0c911b4f3bb6124196337570c2\", \"126ec5cdb49640239e0aa74325708372\", \"bcf36f1845224f4e851ddcf2b0daebdc\", \"619b03505c8f483baf126fea430e2004\", \"f56d46cee4524658928cd66860181f1e\", \"f1bdb8c090e74996be40e642f682485b\", \"07bba0e9beb9455fa9643b8fc4089678\", \"43d5d72da19549da87e4d7ad18e92444\", \"a7af62ce23574138b2bca92a9fc9105a\", \"f28ec8e595244a6db5f04d2474fe0a8f\", \"8189e5c37f1743aa81bd8709fdd64496\", \"5a71c5c25489433189c9f563b14bd842\", \"cd7fc0baed544e66a8a016e6c75a5b7e\", \"6b525dc2d65b4485925426119302c8ad\", \"d7462d03109a49dcbf68a9d60325c04d\", \"d1735a3ca3cc45f1b945da17d6510941\", \"f11b7a6d47d84148b64adce82efb3827\", \"0a6a91cd89644cdc804cfad429a59326\", \"63edef84b67e4a758edaf269dd51723d\", \"5b747fc05c0742329976dcce16a11851\", \"4a247bc7e7304b78a2319b19f7bab323\", \"310f7edb967e47928b804734a12d390a\", \"ffb2d0746b7649c0bd2f661aa4b3eebd\", \"9ceaa73a1e614c60a36b35b9de5583d9\", \"3fe54d959f4c4d3dafb6f1100ba7d07b\", \"b03620417a9e4e699b3b4c6ee6a99cee\", \"186d522648634d5397f8c4bf38199565\", \"d49f315bb02e4e1cba962ce367afdaf7\", \"01af14b3946849519c98aa035c04319e\", \"057ebc2dbc01473694315e9863b96bda\", \"12e5f5fe96e64248bf1f10408462c2f8\", \"befc07c6f37449929449935b1bd0356f\", \"b31b7e46e1904ac187caad6a41873428\", \"b28adef0ed3f41768a28ffb42f3dc4ab\", \"130d5fa9352a42b2a7c0a7055032f667\", \"9e876da522114d72883d7f1c2c3b565f\", \"2a4d807d46704d0691eef591ec3c6afa\"]} id=\"0txWpZFG1yz3\" outputId=\"0e32fa34-1a19-4ca0-ce52-546bc3b6e6a2\"\nMap = geemap.Map(height=\"400pt\")\nMap\n</code></pre>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#define-a-region-of-interest","title":"Define a Region of Interest","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"t4XvkoER3mx8\" outputId=\"c7236a32-8c27-4868-81a2-db373428000b\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#read-the-shapefile-of-the-west-bengal-state-using-geopandas","title":"Read the shapefile of the West Bengal state using geopandas","text":"<p>shp_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/West_Bengal_Boundary/District_shape_West_Bengal.shp\" wb_gdf = gpd.read_file(shp_path) print(wb_gdf.shape) wb_gdf.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 89} id=\"py8GOFDG4Vcx\" outputId=\"209eaf85-ccec-43bb-c74a-29e3160b74ad\"\n# Filter the 'Bankura' district from the geodataframe\nroi_gdf = wb_gdf[wb_gdf['NAME']==\"Bankura\"]\nroi_gdf\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"q1bvvzFT4tt1\" outputId=\"35753df8-8c69-459a-91ed-2597e27aa851\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#push-the-filtered-geometry-to-the-earth-engine","title":"Push the filtered geometry to the Earth Engine","text":"<p>roi_ee = geemap.gdf_to_ee(roi_gdf)</p> <p>vis_params = {     \"fillColor\": \"00000000\",     \"color\": \"black\",     \"width\": 1 } Map.addLayer(roi_ee.style(**vis_params), {}, \"ROI\") Map.centerObject(roi_ee, 9) <pre><code>&lt;!-- #region id=\"h3Jvfz7K7K7W\" --&gt;\n## **Cloud Masking on Landsat Data**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"KMvvsJdxHTnz\" --&gt;\n### **Filtering Landsat 9 Image Collection**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"TApngSNF7Uze\" outputId=\"0d268ac7-6d97-43cc-c513-508cb3741f65\"\n# Read Landsat 9 image collection from EE\nL9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")\n\n# Filter the image collection with roi, daterange, and cloud cover property\nL9Filtered = L9.filterBounds(roi_ee)\\\n               .filterDate(\"2022-01-01\", \"2022-12-31\")\\\n               .filterMetadata(\"CLOUD_COVER\", \"less_than\", 50)\n\n# Print the size of the filtered image collection\nL9Filtered.size().getInfo()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#function-to-remove-clouds-and-shadows","title":"Function to Remove Clouds and Shadows","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"gxVsL_S98Tx-\" outputId=\"3cc9abfc-38c9-415f-99b1-d6c63b338637\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#write-a-function-to-remove-clouds-from-landsat-9-imagery","title":"Write a function to remove clouds from Landsat 9 imagery","text":"<p>def maskL9CloudsAndShadows(image):</p> <pre><code># Read the 'QA_PIXEL' (Quality Assessment) band\nqa = image.select(\"QA_PIXEL\")\n\n# Define all the variables\ndilated_cloud_bitmask = 1 &lt;&lt; 1\ncirrus_bitmask = 1 &lt;&lt; 2\ncloud_bitmask = 1 &lt;&lt; 3\ncloud_shadow_bitmask = 1 &lt;&lt; 4\n\n# Create a mask\nmask = qa.bitwiseAnd(dilated_cloud_bitmask).eq(0).And(\n       qa.bitwiseAnd(cirrus_bitmask).eq(0)).And(\n       qa.bitwiseAnd(cloud_bitmask).eq(0)).And(\n       qa.bitwiseAnd(cloud_shadow_bitmask).eq(0))\n\nreturn image.updateMask(mask)\n</code></pre> <p><code>``  &lt;!-- #region id=\"mxogLTKiIQzi\" --&gt; \ud83e\udd14 **Note:** &lt;br&gt; 1. **</code>bitwiseAnd<code>Function:**&lt;br&gt; The</code>bitwiseAnd<code>function is used to perform a bitwise</code>AND<code>operation on the bits of two numbers or image bands. It takes two operands and returns a result where each bit position in the output is the logical AND of the corresponding bits in the input operands.  2. **</code>eq<code>Function:**&lt;br&gt; The</code>eq` function is used for element-wise equality comparison. It returns a binary image where each pixel is set to 1 if the corresponding pixels in the input images are equal and 0 otherwise.  </p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#implementation-on-an-image-and-image-collection","title":"Implementation on an Image and Image Collection","text":"<p>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"ipP7tUXVFrQJ\" outputId=\"cd894d10-6665-4ae8-e3a5-0fa5a3de3080\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#check-the-cloud-cover-value-of-the-first-image","title":"Check the cloud cover value of the first image","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#l9filteredfirst","title":"L9Filtered.first()","text":"<p>```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"mmYyHq2jF8Dq\" outputId=\"30f65580-e557-4e9e-f258-93a2b19ad5d6\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#display-the-first-image-of-the-filtered-image-collection","title":"Display the first image of the filtered image collection","text":"<p>L9FilteredFirst = L9Filtered.first()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#display-a-standard-false-color-composite-sfcc","title":"Display a Standard False Color Composite (SFCC)","text":"<p>sfcc_vis = {     \"min\": 8000,     \"max\": 17000,     \"bands\": [\"SR_B5\", \"SR_B4\", \"SR_B3\"] } Map.addLayer(L9FilteredFirst, sfcc_vis, \"Landsat 9 Image\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"qiREfQOiEfRw\" outputId=\"2e58c8cb-49fc-4204-9314-ac866ca78058\"\n# Apply the 'maskL9CloudsAndShadows' function on the first image of the filtered image collection\ncloud_free_image = maskL9CloudsAndShadows(L9FilteredFirst)\nMap.addLayer(cloud_free_image, sfcc_vis, \"Landsat 9 Cloud Free Image\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"NIPsgMeXHo49\" outputId=\"2c4d2268-692c-41b4-cecc-1765b2f783d5\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#apply-the-maskl9cloudsandshadows-function-on-the-filtered-image-collection","title":"Apply the 'maskL9CloudsAndShadows' function on the filtered image collection","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#to-create-a-cloud-free-composite-image-of-the-whole-year","title":"to create a cloud free composite image of the whole year","text":"<p>cloud_free_composite = L9Filtered.map(maskL9CloudsAndShadows)\\                                  .median()\\                                  .clip(roi_ee) Map.addLayer(cloud_free_composite, sfcc_vis, \"Landsat 9 Cloud Free Composite\") <pre><code>&lt;!-- #region id=\"4Ahq7TjlKesy\" --&gt;\n\ud83e\udd14 **Note:** The `map` function in GEE is commonly used for applying a specified function to each element of a collection. This function is particularly useful for processing each image in an image collection or each feature in a feature collection.\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"tsGYYNzE9Tum\" --&gt;\n## **Cloud Masking on Sentinel Data**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"4gjFQZcg95NC\" --&gt;\n### **Filtering Sentinel 2 Image Collection**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"5l0meMYm9arn\" outputId=\"7024ae3d-f16b-42f4-85a5-cbc8bdf1f6ca\"\n# Read Landsat 9 image collection from EE\nS2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n\n# Filter the image collection with roi, daterange, and cloud cover property\nS2Filtered = S2.filterBounds(roi_ee)\\\n               .filterDate(\"2022-01-01\", \"2022-12-31\")\\\n               .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 50)\n\n# Print the size of the filtered image collection\nS2Filtered.size().getInfo()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#function-to-remove-clouds-and-shadows_1","title":"Function to Remove Clouds and Shadows","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"Z1CO_S5R-LDN\" outputId=\"0c7e3367-9605-4eef-aae2-54b7c2cc0bc8\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#write-a-function-to-remove-clouds-from-sentinel-2-imagery","title":"Write a function to remove clouds from Sentinel 2 imagery","text":"<p>def maskS2CloudsAndShadows(image):</p> <pre><code># Select the 'MSK_CLDPRB', 'MSK_SNWPRB', and 'SCL' bands\ncloudProb = image.select(\"MSK_CLDPRB\")\nsnowProb = image.select(\"MSK_SNWPRB\")\nscl = image.select(\"SCL\")\n\n# Define the thresholds for cloud an snow probability\ncloudMask = cloudProb.lt(10)\nsnowMask = snowProb.lt(10)\n\n# Mask the 'cloud shadow' and 'cirrus' pixels from 'SCL' band\ncloudShadowMask = scl.eq(3) # Cloud Shadow = 3\ncirrusMask = scl.eq(10) # Cirrus = 10\n\n# Define the final mask\nmask = cloudMask.And(snowMask)\\\n                .And(cloudShadowMask.neq(1))\\\n                .And(cirrusMask.neq(1))\n\nreturn image.updateMask(mask)\n</code></pre> <p>```  </p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#implementation-on-an-image-and-image-collection_1","title":"Implementation on an Image and Image Collection","text":"<p>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"bheQhqIXCGYu\" outputId=\"38c07264-65b7-46bf-a2b3-21fbba1750ca\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#select-an-image-from-the-filtered-image-collection-of-the-monsoon-time","title":"Select an image from the filtered image collection of the monsoon time","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#where-cloudy_pixel_percentage-value-is-in-between-40-to-50","title":"where 'CLOUDY_PIXEL_PERCENTAGE' value is in between 40 to 50","text":"<p>S2_cloud_image = S2Filtered.filterBounds(roi_ee)\\                         .filterDate(\"2022-06-01\", \"2022-08-31\")\\                         .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"greater_than\", 40)\\                         .first() </p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#display-a-standard-false-color-composite-sfcc-of-the-cloud-image","title":"Display a Standard False Color Composite (SFCC) of the cloud image","text":"<p>S2_sfcc_vis = {     \"min\": 0,     \"max\": 3000,     \"bands\": [\"B8\", \"B4\", \"B3\"] } Map.addLayer(S2_cloud_image, S2_sfcc_vis, \"Sentinel 2 Cloud Image\") ```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"Ft0PMTHzHih2\" outputId=\"c129513e-6e6c-4584-a994-5d270a4520b6\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/01_Cloud_Masking/#apply-the-masks2cloudsandshadows-function-on-the-cloud-image","title":"Apply the 'maskS2CloudsAndShadows' function on the cloud image","text":"<p>S2_cloud_free_image = maskS2CloudsAndShadows(S2_cloud_image) Map.addLayer(S2_cloud_free_image, S2_sfcc_vis, \"Sentinel 2 Cloud Free Image\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"yyV1AtvlIP-K\" outputId=\"8628d05d-c097-4c64-8dc7-dd250d4a3c9e\"\n# Apply the 'maskS2CloudsAndShadows' function on the filtered image collection\n# to create a cloud free composite image of the whole year\nS2_cloud_free_composite = S2Filtered.map(maskS2CloudsAndShadows)\\\n                                    .median()\\\n                                    .clip(roi_ee)\nMap.addLayer(S2_cloud_free_composite, S2_sfcc_vis, \"Sentinel 2 Cloud Free Composite\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/","title":"Band Arithmetic","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#band-arithmetic","title":"Band Arithmetic","text":"<p>Band arithmetic in remote sensing refers to mathematical operations or calculations performed on the digital values of individual bands of remotely sensed satellite or aerial imagery.</p> <p>Two important functions in GEE are <code>normalizedDifference</code> and <code>expression</code>, which are commonly used for image processing and analysis.</p> <ol> <li><code>normalizedDifference</code> Function:</li> <li>The <code>normalizedDifference</code> function in GEE is used to compute the normalized difference between two bands of a satellite image. Normalized differences are often used in remote sensing to highlight specific features or characteristics of the Earth's surface.</li> <li>Syntax: <code>normalizedDifference([bandName1, bandName2])</code></li> <li>Example:      <pre><code>   NDVI = image.normalizedDifference(['NIR', 'Red'])\n</code></pre></li> <li> <p>In this example, NDVI (Normalized Difference Vegetation Index) is calculated by taking the normalized difference between the Near-Infrared (NIR) and Red bands of a satellite image. NDVI is commonly used to assess vegetation health and density.</p> </li> <li> <p>When to Use:</p> <ul> <li>Use <code>normalizedDifference</code> when you want to compute standard normalized difference indices such as NDVI, NDWI, NDBI (Normalized Difference Built-Up Index), etc.</li> <li>It is a convenient function for simple band arithmetic that follows a standard formula.</li> </ul> </li> <li> <p><code>expression</code> Function:</p> </li> <li>The <code>expression</code> function allows users to create custom mathematical expressions to manipulate and combine bands within an image. It provides flexibility for performing complex operations on remote sensing data.</li> <li>Syntax: <code>expression(expressionString, parameters)</code></li> <li>Example:      <pre><code>   NDWI = image.expression('(GREEN - NIR) / (GREEN + NIR)', {\n       'GREEN': image.select('GREEN'),\n       'NIR': image.select('NIR')\n   })\n</code></pre></li> <li> <p>In this example, the NDWI (Normalized Difference Water Index) is calculated using a custom expression that involves the Green and NIR bands. NDWI is commonly used for water detection and monitoring.</p> </li> <li> <p>When to Use:</p> <ul> <li>Use <code>expression</code> when you need to create custom mathematical expressions that go beyond simple normalized differences. This is useful for complex band combinations or mathematical operations.</li> <li>It allows you to define your own index or perform advanced operations on bands, providing more flexibility and control over the image processing workflow.</li> </ul> </li> </ol>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#import-the-required-libraries","title":"Import the Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"jtCpeK0V3Dcx\" outputId=\"ed64d482-3f97-4f77-bbc0-e23609ae455a\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"LSZlAfcbxddN\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} outputId=\"f9371daf-5165-4923-b74c-eb1ecb8b9322\"\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ee\nimport geemap\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#initialize-a-map","title":"Initialize a Map","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"Cp8Jph4818M3\" outputId=\"e73da2da-e08a-48a3-cc9e-3419e1be0216\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#trigger-the-authentication-flow","title":"# Trigger the authentication flow.","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#eeauthenticate","title":"ee.Authenticate()","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#initialize-the-library","title":"# Initialize the library.","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#eeinitializeprojectmy-project","title":"ee.Initialize(project='my-project')","text":"<pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 554, \"referenced_widgets\": [\"8d07c54aca0347ad86203d2ee52f2054\", \"82a6a0f9d55d4358a4c8e0d68a15d285\", \"37506c4546894f3dbac0cf61512325ad\", \"43424ecf4a6c4f5c87b8922329b910c3\", \"1e1c64f413bb45f394def9f539928686\", \"09fc2877d4a04c4bab96f4b451e6c202\", \"cf91367838484eabb72bb31f68358761\", \"3d420d04c0db41a7aead6410d27d341c\", \"3ba332ada2644ed29d6e755c6f2c97c2\", \"c6d3110caf43478c88d9391518ed7284\", \"f37140036d444249864088a6cdf3de5c\", \"41c2c26076fc47f4a302c7e653752cd3\", \"6cc08b431e0f48ad9c9a6a6bef8ab10d\", \"0d7e3ca56a5c408da31cd04a5fff26fa\", \"961ed37951474fcf988498169465fafc\", \"cba218ba97214212b347310558afe558\", \"dc96e908d0824aa38c7388614ea7a158\", \"3f0877c6cfb74130ab363d11d1e00655\", \"d3ea7caa24014fcaa46b3c00ac8ad5fc\", \"7598abc0457c45d2b02d8b9978fd3d43\", \"ea3b45e62b384490be7750167e03c2e9\", \"e7d40bfa50e74e7d955c85fa5c264c25\", \"17af8fa808c74e52bbc5102cd6e837f9\", \"0a9c8cd44ced4a0da547ca1c715d3163\", \"5a98f9da9d324ab48d94282b8c265d6f\", \"8787a141a43b4758a5a582aff241a849\", \"f86ccddca5b24173a57bee8dc4c906e3\", \"05b696e99ab64ed999c5098484d96a94\"]} id=\"0txWpZFG1yz3\" outputId=\"84251dcf-a870-43ef-d14a-8331dba553b4\"\nMap = geemap.Map(height=\"400pt\")\nMap\n</code></pre>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#define-a-region-of-interest","title":"Define a Region of Interest","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"t4XvkoER3mx8\" outputId=\"c60e72c4-9577-4484-8397-0b8cc9c0da65\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#read-the-shapefile-of-the-west-bengal-state-using-geopandas","title":"Read the shapefile of the West Bengal state using geopandas","text":"<p>shp_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/West_Bengal_Boundary/District_shape_West_Bengal.shp\" wb_gdf = gpd.read_file(shp_path) print(wb_gdf.shape) wb_gdf.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 89} id=\"py8GOFDG4Vcx\" outputId=\"e7771025-05c1-4df5-b1a6-e867e702a784\"\n# Filter the 'Bankura' district from the geodataframe\nroi_gdf = wb_gdf[wb_gdf['NAME']==\"Bankura\"]\nroi_gdf\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"q1bvvzFT4tt1\" outputId=\"ec3da44e-0285-4693-ca5e-9a5beda22947\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#push-the-filtered-geometry-to-the-earth-engine","title":"Push the filtered geometry to the Earth Engine","text":"<p>roi_ee = geemap.gdf_to_ee(roi_gdf)</p> <p>vis_params = {     \"fillColor\": \"00000000\",     \"color\": \"black\",     \"width\": 1 } Map.addLayer(roi_ee.style(**vis_params), {}, \"ROI\") Map.centerObject(roi_ee, 9) <pre><code>&lt;!-- #region id=\"h3Jvfz7K7K7W\" --&gt;\n## **Band Arithmetic on Landsat Data**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"KMvvsJdxHTnz\" --&gt;\n### **Filtering Landsat 9 Image Collection**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"TApngSNF7Uze\" outputId=\"04a1af13-416a-4e7f-c11b-5aafd2999e41\"\n# Read Landsat 9 image collection from EE\nL9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")\n\n# Filter the image collection with roi, daterange, and cloud cover property\nL9Filtered = L9.filterBounds(roi_ee)\\\n               .filterDate(\"2022-01-01\", \"2022-12-31\")\\\n               .filterMetadata(\"CLOUD_COVER\", \"less_than\", 50)\n\n# Print the size of the filtered image collection\nL9Filtered.size().getInfo()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#preprocessing-on-landsat-data","title":"Preprocessing on Landsat Data","text":"<p>```python id=\"od1urzmGHKAz\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} outputId=\"220d4701-c910-4218-ef35-04aaca5d2015\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#write-a-function-to-rename-landsat-9-band-names","title":"Write a function to rename Landsat 9 band names","text":"<p>def renameL9(image):     # Define the existing band names     band_names = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6',                   'SR_B7', 'SR_QA_AEROSOL', 'ST_B10', 'ST_ATRAN', 'ST_CDIST', 'ST_DRAD',                   'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', 'QA_PIXEL',                   'QA_RADSAT']</p> <pre><code># Define the new band names\nnew_band_names = ['COASTAL', 'BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1',\n                  'SWIR2', 'AEROSOL', 'THERMAL', 'ST_ATRAN', 'ST_CDIST', 'ST_DRAD',\n                  'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', 'QA_PIXEL',\n                  'QA_RADSAT']\n\n# Rename the band names\nreturn image.rename(new_band_names)\n</code></pre> <p><code></code>python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"gxVsL_S98Tx-\" outputId=\"7ba64f34-09cc-4aba-eb96-98b0fc497940\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#write-a-function-to-remove-clouds-from-landsat-9-imagery","title":"Write a function to remove clouds from Landsat 9 imagery","text":"<p>def maskL9CloudsAndShadows(image):      # Read the 'QA_PIXEL' (Quality Assessment) band     qa = image.select(\"QA_PIXEL\")      # Define all the variables     dilated_cloud_bitmask = 1 &lt;&lt; 1     cirrus_bitmask = 1 &lt;&lt; 2     cloud_bitmask = 1 &lt;&lt; 3     cloud_shadow_bitmask = 1 &lt;&lt; 4      # Create a mask     mask = qa.bitwiseAnd(dilated_cloud_bitmask).eq(0).And(            qa.bitwiseAnd(cirrus_bitmask).eq(0)).And(            qa.bitwiseAnd(cloud_bitmask).eq(0)).And(            qa.bitwiseAnd(cloud_shadow_bitmask).eq(0))      return image.updateMask(mask) ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#function-to-calculate-various-spectral-indices","title":"Function to Calculate Various Spectral Indices","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"bfTmcvEosbpI\" outputId=\"5cd0e819-6dac-4c66-98a5-2c9788e68b95\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#write-a-function-to-calculate-ndvi-ndwi-mndwi-ndbi-bu-savi-evi-gcvi-of-an-image","title":"Write a function to calculate NDVI, NDWI, MNDWI, NDBI, BU, SAVI, EVI, GCVI of an image","text":"<p>def calculateIndices(image):</p> <pre><code># NDVI = (NIR - RED) / (NIR + RED)\nNDVI = image.normalizedDifference([\"NIR\", \"RED\"])\\\n            .rename(\"NDVI\")\n\n# NDWI = (NIR \u2013 SWIR1) / (NIR + SWIR1)\nNDWI = image.normalizedDifference([\"NIR\", \"SWIR1\"])\\\n            .rename(\"NDWI\")\n\n# MNDWI = (Green - SWIR1) / (Green + SWIR1)\nMNDWI = image.normalizedDifference([\"GREEN\", \"SWIR1\"])\\\n             .rename(\"MNDWI\")\n\n# NDBI = (SWIR \u2013 NIR) / (SWIR + NIR)\nNDBI = image.normalizedDifference([\"SWIR1\", \"NIR\"])\\\n            .rename(\"NDBI\")\n\n# BU = NDBI - NDVI\nBU = NDBI.subtract(NDVI)\\\n         .rename(\"BU\")\n\n# SAVI = ((NIR \u2013 RED) / (NIR + RED + 0.5)) * (1.5)\nSAVI = image.expression(\n    \"((NIR - RED) / (NIR + RED + 0.5)) * (1.5)\", {\n        \"NIR\": image.select(\"NIR\"),\n        \"RED\": image.select(\"RED\")\n}).rename(\"SAVI\")\n\n# EVI = 2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\nEVI = image.expression(\n    \"2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\", {\n        \"NIR\": image.select(\"NIR\"),\n        \"RED\": image.select(\"RED\"),\n        \"BLUE\": image.select(\"BLUE\")\n}).rename(\"EVI\")\n\n# GCVI = (NIR/GREEN) \u2212 1\nGCVI = image.expression(\n    \"(NIR / GREEN) - 1\", {\n        \"NIR\": image.select(\"NIR\"),\n        \"GREEN\": image.select(\"GREEN\")\n}).rename(\"GCVI\")\n\n# Add all the indices in a single ee list\nfinal_image = ee.Image([NDVI, NDWI, MNDWI, NDBI, BU, SAVI, EVI, GCVI])\\\n                .copyProperties(image, [\"system:time_start\"])\n\nreturn ee.Image(final_image)\n</code></pre> <p>```  </p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#implementation-on-an-image-and-image-collection","title":"Implementation on an Image and Image Collection","text":"<p>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"ipP7tUXVFrQJ\" outputId=\"98b7bf6f-668d-476b-8c82-78d9f34348c3\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#check-the-cloud-cover-value-of-the-first-image","title":"Check the cloud cover value of the first image","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#l9filteredfirst","title":"L9Filtered.first()","text":"<p>```</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"mmYyHq2jF8Dq\" outputId=\"91ab0b8a-5895-401f-eeb2-0c6aee5e3db6\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#display-the-first-image-of-the-filtered-image-collection","title":"Display the first image of the filtered image collection","text":"<p>L9FilteredFirst = L9Filtered.first()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#apply-renamel9-maskl9cloudsandshadows-and-calculateindices-function-on-the-image","title":"Apply 'renameL9', 'maskL9CloudsAndShadows', and 'calculateIndices' function on the image","text":"<p>renamedL9 = renameL9(L9FilteredFirst) cloudMaksedL9 = maskL9CloudsAndShadows(renamedL9) indicesL9 = calculateIndices(cloudMaksedL9)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#display-the-indices-image","title":"Display the indices image","text":"<p>Map.addLayer(indicesL9, {}, \"L9 Spectral Indices\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"WXV4ze97_-aU\" outputId=\"af4bb4c5-8f3c-4d32-e6f3-61cb40242015\"\n# Display the clipped NDVI image\nNDVI_image = indicesL9.select(\"NDVI\")\\\n                      .clip(roi_ee)\n\nNDVI_vis = {\n    'min': -0.064,\n    'max': 0.332,\n    'palette': ['#a50026', ' #da372a', ' #f67b4a', ' #fdbf6f', ' #feeea2',\n                '#eaf6a2', ' #b7e075', ' #74c365', ' #229c52', ' #006837']\n}\n\nMap.addLayer(NDVI_image, NDVI_vis, \"L9 NDVI Image\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"NIPsgMeXHo49\" outputId=\"59dccde6-bf11-4253-f3ad-a52a2674a15c\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#apply-renamel9-maskl9cloudsandshadows-and-calculateindices-function-on-the-whole-image-collection","title":"Apply 'renameL9', 'maskL9CloudsAndShadows', and 'calculateIndices' function on the whole image collection","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/02_Band_Arithmetic/#to-create-a-cloud-free-median-composite-spectral-indices-image-of-the-whole-year","title":"to create a cloud free median composite spectral indices image of the whole year","text":"<p>medianIndicesL9= L9Filtered.map(renameL9)\\                            .map(maskL9CloudsAndShadows)\\                            .map(calculateIndices)\\                            .median()\\                            .clip(roi_ee)</p> <p>Map.addLayer(medianIndicesL9, {}, \"L9 Median Spectral Indices\") ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/","title":"Download Image And Image Tiles","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#download-image-and-image-tiles-from-earth-engine","title":"Download Image and Image Tiles from Earth Engine","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"MkEGTunYAxLJ\" outputId=\"d82da669-5e60-4239-c089-2452f660830e\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"uKcYkfjABQhN\"\n# %pip install rasterio\n# %pip install geedim\n</code></pre></p> <p>```python id=\"k7zrKDm2A2wK\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} outputId=\"e86f28de-e075-47c1-919f-a952ae81d17a\" import numpy as np import pandas as pd import geopandas as gpd import matplotlib.pyplot as plt import seaborn as sns import ee import geemap import rasterio import geedim <pre><code>&lt;!-- #region id=\"aKbJP6cuBbnX\" --&gt;\n## **Initialize a Map**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"g-pM8eCLBg2-\" outputId=\"fb37512d-86ba-45af-9069-cd447ed4dfff\"\n# # Trigger the authentication flow.\n# ee.Authenticate()\n\n# # Initialize the library.\n# ee.Initialize(project='my-project')\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 554, \"referenced_widgets\": [\"dcc41a0729ee47c78a13bf879d2afa47\", \"bb4b7e08c1424a54af96b3dee23c0714\", \"a4fa12c78fcc4541b966d66d9bf97cb8\", \"c636759c0a5c45f789585174673ee4d4\", \"e9fcdabd48524b94bf66925e9fc48a3b\", \"d6c446c84fe848ae9c72d32795b114fb\", \"065a8c140b9e4383a79cfa1a67dfc739\", \"3adb3d922e274aa5bcb4289ac0e77122\", \"8ad23638fa7445b9b6e50430236e89d1\", \"43f20132ce784cdbaa84287eea4343a6\", \"87bf7925f0664ed79d67ec68fd0ff562\", \"05e83c8f59e8498a8a045f46b2776b2b\", \"88e77d0aa1394110ad5f62e61a898c22\", \"ecf856e512264e54aa97d70527f192ff\", \"3939fde103394f5f9028e8d9b3eeee60\", \"dc0b058cb0914a75b5078895efaec11a\", \"829595534c0d4c20b511d475bda6d3f8\", \"057d2f340b3642a28f5d0d37280742bd\", \"f9e82e7dcf62447a925b7fb217686fab\", \"422b1b348e564e6ea223a8fff4f93b66\", \"44b0a31c7bf14d34a82470ec1b9510b5\", \"625146c091b14afeaec9a0fe5a61de0d\", \"f529b10febb845e6a7fbfe421e259974\", \"163623d0a1d24570a297203e8b5b3923\", \"d146b511de2e47289c91738fe1394f3e\", \"082768304fbc4a7ba9c1104025ff8cb5\", \"fdb41565d6cd496ca231e98779e1c823\"]} id=\"-7EM78vxBh-H\" outputId=\"939b7cb7-5554-4801-8668-8cf122f50719\" Map = geemap.Map(height=\"400pt\") Map <pre><code>&lt;!-- #region id=\"t8u-puRyBzzq\" --&gt;\n## **Define a Region of Interest**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"R15YovkpB1W5\" outputId=\"0602b150-adcf-4aed-ba1a-1b5f67829497\"\n# Read the shapefile of the West Bengal state using geopandas\nshp_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/West_Bengal_Boundary/District_shape_West_Bengal.shp\"\nwb_gdf = gpd.read_file(shp_path)\nprint(wb_gdf.shape)\nwb_gdf.head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 89} id=\"RqFgtywgB53Y\" outputId=\"b0b339cb-ed31-463d-9df7-726a5e76b84e\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#filter-the-bankura-district-from-the-geodataframe","title":"Filter the 'Bankura' district from the geodataframe","text":"<p>roi_gdf = wb_gdf[wb_gdf['NAME']==\"Bankura\"] roi_gdf <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"e6dCxSWUB8MZ\" outputId=\"06e117a4-86d7-4a99-ebef-41ceee103483\"\n# Push the filtered geometry to the Earth Engine\nroi_ee = geemap.gdf_to_ee(roi_gdf)\n\nvis_params = {\n    \"fillColor\": \"00000000\",\n    \"color\": \"black\",\n    \"width\": 1\n}\nMap.addLayer(roi_ee.style(**vis_params), {}, \"ROI\")\nMap.centerObject(roi_ee, 9)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#band-arithmetic-on-landsat-data","title":"Band Arithmetic on Landsat Data","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#filtering-landsat-9-image-collection","title":"Filtering Landsat 9 Image Collection","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"TApngSNF7Uze\" outputId=\"471a0e9e-002f-4adc-af7f-7681e9249bf4\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#read-landsat-9-image-collection-from-ee","title":"Read Landsat 9 image collection from EE","text":"<p>L9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#filter-the-image-collection-with-roi-daterange-and-cloud-cover-property","title":"Filter the image collection with roi, daterange, and cloud cover property","text":"<p>L9Filtered = L9.filterBounds(roi_ee)\\                .filterDate(\"2022-01-01\", \"2022-12-31\")\\                .filterMetadata(\"CLOUD_COVER\", \"less_than\", 50)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#print-the-size-of-the-filtered-image-collection","title":"Print the size of the filtered image collection","text":"<p>L9Filtered.size().getInfo() <pre><code>&lt;!-- #region id=\"K8Y9MoUUHZqy\" --&gt;\n### **Preprocessing on Landsat Data**\n&lt;!-- #endregion --&gt;\n\n```python id=\"od1urzmGHKAz\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} outputId=\"b2365a0e-b0c4-4f6a-c7bb-48ba66d8830e\"\n# Write a function to rename Landsat 9 band names\ndef renameL9(image):\n    # Define the existing band names\n    band_names = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6',\n                  'SR_B7', 'SR_QA_AEROSOL', 'ST_B10', 'ST_ATRAN', 'ST_CDIST', 'ST_DRAD',\n                  'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', 'QA_PIXEL',\n                  'QA_RADSAT']\n\n    # Define the new band names\n    new_band_names = ['COASTAL', 'BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1',\n                      'SWIR2', 'AEROSOL', 'THERMAL', 'ST_ATRAN', 'ST_CDIST', 'ST_DRAD',\n                      'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', 'QA_PIXEL',\n                      'QA_RADSAT']\n\n    # Rename the band names\n    return image.rename(new_band_names)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"gxVsL_S98Tx-\" outputId=\"1465d714-819e-4ba7-cd43-0c008275e639\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#write-a-function-to-remove-clouds-from-landsat-9-imagery","title":"Write a function to remove clouds from Landsat 9 imagery","text":"<p>def maskL9CloudsAndShadows(image):</p> <pre><code># Read the 'QA_PIXEL' (Quality Assessment) band\nqa = image.select(\"QA_PIXEL\")\n\n# Define all the variables\ndilated_cloud_bitmask = 1 &lt;&lt; 1\ncirrus_bitmask = 1 &lt;&lt; 2\ncloud_bitmask = 1 &lt;&lt; 3\ncloud_shadow_bitmask = 1 &lt;&lt; 4\n\n# Create a mask\nmask = qa.bitwiseAnd(dilated_cloud_bitmask).eq(0).And(\n       qa.bitwiseAnd(cirrus_bitmask).eq(0)).And(\n       qa.bitwiseAnd(cloud_bitmask).eq(0)).And(\n       qa.bitwiseAnd(cloud_shadow_bitmask).eq(0))\n\nreturn image.updateMask(mask)\n</code></pre> <p>```  </p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#function-to-calculate-various-spectral-indices","title":"Function to Calculate Various Spectral Indices","text":"<p>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"bfTmcvEosbpI\" outputId=\"2826f98d-09f6-4398-a8ce-da6252a7ff88\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#write-a-function-to-calculate-ndvi-ndwi-mndwi-ndbi-bu-savi-evi-gcvi-of-an-image","title":"Write a function to calculate NDVI, NDWI, MNDWI, NDBI, BU, SAVI, EVI, GCVI of an image","text":"<p>def calculateIndices(image):      # NDVI = (NIR - RED) / (NIR + RED)     NDVI = image.normalizedDifference([\"NIR\", \"RED\"])\\                 .rename(\"NDVI\")      # NDWI = (NIR \u2013 SWIR1) / (NIR + SWIR1)     NDWI = image.normalizedDifference([\"NIR\", \"SWIR1\"])\\                 .rename(\"NDWI\")      # MNDWI = (Green - SWIR1) / (Green + SWIR1)     MNDWI = image.normalizedDifference([\"GREEN\", \"SWIR1\"])\\                  .rename(\"MNDWI\")      # NDBI = (SWIR \u2013 NIR) / (SWIR + NIR)     NDBI = image.normalizedDifference([\"SWIR1\", \"NIR\"])\\                 .rename(\"NDBI\")      # BU = NDBI - NDVI     BU = NDBI.subtract(NDVI)\\              .rename(\"BU\")      # SAVI = ((NIR \u2013 RED) / (NIR + RED + 0.5)) * (1.5)     SAVI = image.expression(         \"((NIR - RED) / (NIR + RED + 0.5)) * (1.5)\", {             \"NIR\": image.select(\"NIR\"),             \"RED\": image.select(\"RED\")     }).rename(\"SAVI\")      # EVI = 2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))     EVI = image.expression(         \"2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\", {             \"NIR\": image.select(\"NIR\"),             \"RED\": image.select(\"RED\"),             \"BLUE\": image.select(\"BLUE\")     }).rename(\"EVI\")      # GCVI = (NIR/GREEN) \u2212 1     GCVI = image.expression(         \"(NIR / GREEN) - 1\", {             \"NIR\": image.select(\"NIR\"),             \"GREEN\": image.select(\"GREEN\")     }).rename(\"GCVI\")      # Add all the indices in a single ee list     final_image = ee.Image([NDVI, NDWI, MNDWI, NDBI, BU, SAVI, EVI, GCVI])\\                     .copyProperties(image, [\"system:time_start\"])      return ee.Image(final_image) ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#implementation-on-an-image-collection","title":"Implementation on an Image Collection","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"NIPsgMeXHo49\" outputId=\"ed23e391-5533-4e8c-81b4-2a5ba712fcf3\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#apply-renamel9-maskl9cloudsandshadows-and-calculateindices-function-on-the-whole-image-collection","title":"Apply 'renameL9', 'maskL9CloudsAndShadows', and 'calculateIndices' function on the whole image collection","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#to-create-a-cloud-free-median-composite-spectral-indices-image-of-the-whole-year","title":"to create a cloud free median composite spectral indices image of the whole year","text":"<p>medianIndicesL9= L9Filtered.map(renameL9)\\                            .map(maskL9CloudsAndShadows)\\                            .map(calculateIndices)\\                            .median()\\                            .clip(roi_ee)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#display-only-the-ndvi-image","title":"Display only the NDVI image","text":"<p>NDVI_vis = {     \"min\": 0,     \"max\": 0.3,     \"bands\": [\"NDVI\"],     \"palette\": [\"#a50026\", \"#da372a\", \"#f67b4a\", \"#fdbf6f\", \"#feeea2\",                 \"#eaf6a2\", \"#b7e075\", \"#74c365\", \"#229c52\", \"#006837\"] }</p> <p>Map.addLayer(medianIndicesL9, NDVI_vis, \"L9 Median NDVI\") <pre><code>&lt;!-- #region id=\"bmuvhj3ZDRc7\" --&gt;\n## **Download the Full Image to Drive**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"gglVrDz3FqN2\" --&gt;\n\ud83e\udd14 **Note:** Prior to downloading the image, it is crucial to explicitly specify the projection of the image.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"u-yxJ4s2Fz13\" outputId=\"f658cad4-20d5-411f-e214-4de69c6eeeb7\"\n# Define the projection\nproj = ee.Projection(\"EPSG:32645\")\n\n# Reproject the 'medianIndicesL9' image\nmedianIndicesL9 = medianIndicesL9.reproject(proj, crsTransform=None, scale=30)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 52} id=\"kR6h42LeGRQB\" outputId=\"1de40de8-fb36-4cee-ba16-8a310406e24b\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#check-the-current-projection-and-scale-of-the-image","title":"Check the current projection and scale of the image","text":"<p>print(\"Projection Information:\", medianIndicesL9.projection().getInfo()) print(\"Spatial Resolution:\", medianIndicesL9.projection().nominalScale().getInfo()) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 84, \"referenced_widgets\": [\"78e6d5a3c19e4d0fa4f4d520e989a5ee\", \"12b5483d9716435d8fc75d0d5e471da0\", \"bfb05cf8a6ba4d2c9320210f07c3e5a6\", \"0de7a765594e489e94ddcd555b6af3f4\", \"f529871ce08b4ef6b6fcc9fe361194af\", \"2c854e9a053a48139b2f3a57152e04dc\", \"0f3303a4c4b3442baf571040ca286e7a\", \"9ba7c2dda5514e6ba2969e0566ff50d7\", \"5df0ac03709a4d36970f1eaf2f6e4e6b\", \"bb35f32ed61944e2b591ad4111002984\", \"ce85141d757740fab005ce3aeab9a659\"]} id=\"hJtxYYkqEQ7M\" outputId=\"b60532c1-fd32-4d6b-98f9-35ae520778b6\"\n# Specify the output file path and file name\noutput_path = \"/content/drive/MyDrive/GEE//\"\nfile_name = \"Median_Indices_2022.tif\"\ngeemap.download_ee_image(image=medianIndicesL9,\n                         filename=output_path+file_name,\n                         region=roi_ee.geometry(),\n                         scale=30)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#read-the-image-using-rasterio","title":"Read the Image using Rasterio","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 173} id=\"MwPqqW9qJdpR\" outputId=\"2993115a-7617-4794-d661-9c5759cdf199\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#read-the-median_indices_2022tif-image","title":"Read the 'Median_Indices_2022.tif' image","text":"<p>src = rasterio.open(output_path+file_name)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#store-all-the-metadata","title":"Store all the metadata","text":"<p>raster_meta = src.meta driver = raster_meta[\"driver\"] dtype = raster_meta[\"dtype\"] nodata = raster_meta[\"nodata\"] width = raster_meta[\"width\"] height = raster_meta[\"height\"] count = raster_meta[\"count\"] crs = raster_meta[\"crs\"] transform = raster_meta[\"transform\"]</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#print-the-metadata-of-the-raster","title":"Print the metadata of the raster","text":"<p>src.meta <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 34} id=\"YmhCqEPaLtuo\" outputId=\"846dfb2c-2cb7-40ff-faa1-28043f2d7f67\"\n# Store all the band names in a variable\nband_names = src.descriptions\nband_names\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"J4a42pArKf7K\" outputId=\"baecd843-590d-4cc1-cc65-d3071fee2bfb\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#convert-the-raster-image-into-an-array","title":"Convert the raster image into an array","text":"<p>raster_arr = src.read()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#close-the-source-file","title":"Close the source file","text":"<p>src.close() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 452} id=\"d8wVTgmtKrA8\" outputId=\"731f696a-e79d-4977-d72d-6e469bd70564\"\n# Plot the NDVI image\nplt.figure()\nplt.imshow(raster_arr[0], cmap=\"RdYlGn\")\nplt.colorbar(label=\"NDVI\")\nplt.title(\"Median NDVI 2022\")\nplt.show()\n</code></pre></p> <p>\ud83e\udd14 Note: When working with raster images, especially in scenarios involving larger regions of interest (ROIs) and high spatial resolution, it's imperative to optimize the processing strategy for efficiency. Processing the entire image at once can be computationally intensive, leading to increased memory usage and slower performance. To overcome these challenges, a more effective approach involves dividing the image into smaller tiles and processing each tile individually. This not only conserves memory but also significantly improves processing speed.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#download-the-image-by-tiles","title":"Download the Image by Tiles","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 17} id=\"TYfMqk7MSlA6\" outputId=\"45cbafc9-987f-4e19-bf61-8f38d9792722\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#extract-the-bounding-box-of-the-roi-as-a-polygon","title":"Extract the bounding box of the ROI as a polygon","text":"<p>roi_boundary = roi_ee.geometry().bounds()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/00_geemap/03_Download_Image_and_Image_Tiles/#create-a-mesh-grid","title":"Create a mesh grid","text":"<p>mesh = geemap.fishnet(roi_boundary, rows=2, cols=2, delta=0) Map.addLayer(mesh.style(**vis_params), {}, \"Mesh Grid\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 232, \"referenced_widgets\": [\"b6f95862fb2b46ee9434f9683cea279e\", \"4abc7204e00747a494eb601d3c6775e0\", \"7191b33441c243709bbc4a424b8c4090\", \"7c5ac1bc7ad64a6097f5fad3bc321e98\", \"2b731bfcfdcd47d8839490d1fdb4d1cb\", \"3b8d39c9654241bdb27db0afb09155d1\", \"534c8f3a541543c3a9faf223f20646c6\", \"f11b2e63806f48e08807458470a6ea5a\", \"22d5f253b19a4d3c8a5f1238620edfa9\", \"2ba9c97265fa4250ad424349aea7ddcf\", \"4591fabbbb1140128538f9f03c375729\", \"7347c40cf2c749e68851d029ade7c553\", \"2173c4c33cc84b77b36ee47858aebfe1\", \"acf813a7d5c144e88679a84fe1d746ef\", \"cec96e00778d46fbb8d9c08305b18ce8\", \"0ca50f2d9eab4099a3215d46d03e89ca\", \"fd6c5d312825494482f46715f64fdebd\", \"2c7dccccebf146f891ade2b83059c1fa\", \"224293e188e44ed9b47652c2415adf4c\", \"44043d51897b44d8a73b39b589f0dc7b\", \"e14a832eee834959b875debe8f1611c5\", \"99892987244247fcb6913473995f7139\", \"b4e88e23c12e4256912ce07dd32eff2c\", \"cb441c5a492b47d4930df43fc62db27b\", \"30dcec8c51534a04a7a220003a8e5025\", \"297c032db6fc4c03bf4c52996f71dc91\", \"4947659d336d4d3ca20edea8c8cc6e4d\", \"7a0c8678f1264fdd8fc9d2d1176aa362\", \"46dd069111374266b1e5d2a58568b7b9\", \"31a5ac368301435f89bb15cf1ae23059\", \"b54f81d877d643719426a1503be72211\", \"30fdc48b97e748558893a4be307bb848\", \"3af11acd9c0b421ab4da83ffd83c894e\", \"d9c93ace142a4169981ebdfac06826e2\", \"30bc3a95026840788211be12f10d7bbd\", \"b9bf2e1fe0014a9a9824a40e6c47e208\", \"128e8609d7ae44f48bd201e32ac02db5\", \"d30a07aa7bed4e6182a665db4e19ff69\", \"767782f77b6947798c54213afa637b63\", \"883d03614e7647c48c29776d6e076bb3\", \"c5c6a92f959f48b092597b905a32e59a\", \"3468eff9945b4189bb6b7661dfed9045\", \"555732f35ba8416d896f228a164e9c40\", \"fe5dd8e6817e47c4adc233727a8c38d0\"]} id=\"91I7l0WPUBsl\" outputId=\"a21e65d0-72ff-40fb-de3a-89512c7a2e4d\"\n# Download the image by tiles\ngeemap.download_ee_image_tiles(image=medianIndicesL9,\n                               features=mesh,\n                               out_dir=output_path,\n                               prefix=\"Median_Indices_2022_\",\n                               crs=\"EPSG:32645\",\n                               scale=30)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/","title":"Working With Csv Files","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#working-with-csv-files","title":"Working with CSV Files","text":"<p>Working with CSV (Comma-Separated Values) files in Python using the Pandas library is a common task in data analysis and manipulation. Pandas is a powerful data manipulation library that provides data structures like DataFrame, which is particularly useful for handling tabular data, such as that found in CSV files.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"8vFe6W8p0RCl\" outputId=\"a8055e59-4d55-48d9-c4bc-26def9d526a8\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#mount-google-drive","title":"Mount google drive","text":"<p>from google.colab import drive drive.mount(\"/content/drive/\") <pre><code>```python id=\"jXantEUmzVPb\"\nimport pandas as pd\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#open-a-local-csv-file","title":"Open a Local CSV File","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"dMADUGvXzkat\" outputId=\"51c509d8-5113-48f6-de42-e60f4117d67f\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#define-the-csv-path","title":"Define the CSV path","text":"<p>csv_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Iris.csv\" df = pd.read_csv(csv_path) df <pre><code>&lt;!-- #region id=\"2UinvT5Y1HV2\" --&gt;\n## **Open a CSV File from an URL**\n\n - **`requests` Module**: &lt;br&gt;\nThe `requests` module is used for making HTTP requests to interact with web resources. It simplifies the process of sending HTTP requests and handling the responses. You can use it to make GET, POST, PUT, DELETE, and other types of HTTP requests.\n\n- **`io` Module**: &lt;br&gt;\nThe `io` module provides classes for working with streams and file-like objects. It's often used to handle input/output operations in a generic way. You can use it to work with in-memory streams, files, and other data sources.\n&lt;!-- #endregion --&gt;\n\n```python id=\"c9ZpIe1R1K0Y\"\nimport requests\nfrom io import StringIO\n</code></pre></p> <p>```python id=\"i_YWR-3q2FXF\" url = r\"https://raw.githubusercontent.com/codeforamerica/ohana-api/master/data/sample-csv/addresses.csv\" req = requests.get(url) data = StringIO(req.text) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"VtD_ozK63xD5\" outputId=\"fa71548b-7fb5-4e20-987a-58a632f83720\"\ndf = pd.read_csv(data)\ndf.head() # by default 'head' function prints the first five rows of the dataframe\n</code></pre></p> <p>\ud83d\udd11 Note: A response with a status code of 200 indicates a successful HTTP request. The HTTP status code 200 <code>OK</code> is one of the standard HTTP response codes, and it means that the request was successful, the server processed the request, and the requested information is contained in the response body.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#sep-parameter","title":"<code>sep</code> Parameter","text":"<p>The <code>sep</code> parameter specifies the delimiter or separator used in the CSV file to distinguish between different fields or columns. By default, the comma (,) is used as the separator. However, in some cases, CSV files might use other delimiters such as tabs (\\t) or semicolons (;).</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"BFjgCPVr4WIG\" outputId=\"1e13880e-eef5-48cc-8f00-fdee81f555bc\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#read-a-tab-separated-file-with-pandas","title":"Read a Tab Separated File with pandas","text":"<p>tsv_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/movie_titles_metadata.tsv\" df = pd.read_csv(tsv_path, delimiter=\"\\t\") df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"ktwuGRnX44Q5\" outputId=\"a1c25044-7d63-4fbb-d623-f775bfc5cf73\"\n# Giving the name of the columns\ncolumn_names = [\"sl.no\", \"name\", \"release_year\", \"rating\", \"votes\", \"genres\"]\npd.read_csv(tsv_path, delimiter=\"\\t\", names=column_names)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#index_col-parameter","title":"<code>index_col</code> Parameter","text":"<p>The <code>index_col</code> parameter specifies which column(s) should be used as the DataFrame's index. The index is used to uniquely label rows in the DataFrame. By default, pandas assigns a numeric index starting from 0. Specifying <code>index_col</code> allows you to use one or more columns as the index.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 237} id=\"3-hqM8pG57c3\" outputId=\"e3b7c364-f0ed-403d-e3b3-1cc88a3703bf\" pd.read_csv(csv_path, index_col=\"Id\").head() <pre><code>&lt;!-- #region id=\"F3rS-HCg6JW5\" --&gt;\n## **`header` Parameter**\nThe `header` parameter specifies which row should be considered as the header (column names) when reading the CSV file. By default, the first row is treated as the header. If you set header=None, pandas will not use any row as the header, and columns will be labeled with numeric indices. You can also provide an integer row number to use as the header, or a list of integers to skip multiple initial rows.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 313} id=\"4SY2XG1b6YJ5\" outputId=\"78598b33-0739-4e3e-c8ef-96bedefd8c79\"\ntest_csv_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/test.csv\"\npd.read_csv(test_csv_path)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 212} id=\"V4dhKRjW67KQ\" outputId=\"d556ddc4-1bdf-4c61-d143-ed64fa51a8bc\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#change-the-header-row","title":"Change the header row","text":"<p>pd.read_csv(test_csv_path, header=1) <pre><code>&lt;!-- #region id=\"qY2YjnJP7Rb5\" --&gt;\n## **`usecols` Parameter**\nThe `usecols` parameter in the pandas `read_csv()` function is used to specify which columns from the CSV file should be read into the DataFrame. This parameter allows you to selectively read only a subset of columns, which can be useful when dealing with large datasets where not all columns are needed or when you want to focus on specific data.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 261} id=\"NurJ7L427wgU\" outputId=\"a6fbed3c-3800-4d47-a1d5-070f5a66b0d1\"\ntrain_csv_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/aug_train.csv\"\npd.read_csv(train_csv_path).head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"rZ-MK6mt8FeW\" outputId=\"8c8d7197-0486-4658-8576-87fcbfcf24f3\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#create-a-list-of-required-columns","title":"Create a list of required columns","text":"<p>required_columns = [\"enrollee_id\", \"gender\", \"education_level\"] pd.read_csv(train_csv_path, usecols=required_columns).head() <pre><code>&lt;!-- #region id=\"CoTv1oEC9OLp\" --&gt;\n## **`skiprows`/`nrows` Parameter**\n - **`skiprows` Parameter:** &lt;br&gt;\nThe `skiprows` parameter allows you to specify the number of rows at the beginning of the CSV file to skip while reading. You can pass an integer representing the number of rows to skip or a list of row indices (0-based) that should be skipped.\n\n - **`nrows` Parameter:** &lt;br&gt;\nThe `nrows` parameter is used to limit the number of rows read from the CSV file. You can pass an integer representing the maximum number of rows to read.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 530} id=\"pONxGtPx9huQ\" outputId=\"829ee8eb-bd42-488a-9f3c-e706d7a86a89\"\npd.read_csv(train_csv_path)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 278} id=\"2bLw3HC79uty\" outputId=\"2f7985ba-0cf7-4bbf-f0a6-919a7bdb12cf\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#skip-the-rows-at-index-1-and-3","title":"Skip the rows at index 1 and 3","text":"<p>pd.read_csv(train_csv_path, skiprows=[1, 3]).head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 530} id=\"S-TSlgIu-WP-\" outputId=\"d3989919-8127-41b7-ea54-49e25047aa21\"\n# Read only the first 100 rows\npd.read_csv(train_csv_path, nrows=100)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#encoding-parameter","title":"<code>encoding</code> Parameter","text":"<p>The <code>encoding</code> parameter is used to specify the character encoding of the CSV file being read. Character encoding defines how characters are represented as bytes in a file. Different encodings are used for different languages and writing systems.</p> <p>When reading a CSV file, it's important to use the correct encoding to ensure that the data is interpreted correctly. If you encounter issues where characters are not displayed correctly or you see encoding-related errors, specifying the appropriate encoding can help resolve these problems.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 810} id=\"IvPcPt7M-4Bq\" outputId=\"69a2d6fe-883e-4317-f244-85bfb3575740\" zomato_csv_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/zomato.csv\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#pdread_csvzomato_csv_path-throws-and-error","title":"pd.read_csv(zomato_csv_path)  # Throws and error","text":"<p>pd.read_csv(zomato_csv_path, encoding=\"latin-1\").head() <pre><code>&lt;!-- #region id=\"nAhGA_1eAs4F\" --&gt;\n## **`dtype` Parameter**\nThe `dtype` parameter allows you to explicitly specify the data types for columns when reading a CSV file into a DataFrame. This can be useful when you want to ensure that specific columns are interpreted with the correct data types.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 261} id=\"9T-XAHp1A2KP\" outputId=\"2ab6d4c9-588c-4e17-d548-8a90ce8be64c\"\ndf = pd.read_csv(train_csv_path)\ndf.head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"3obvlAojBAe6\" outputId=\"76362c24-868e-4a8f-d23e-76c90f55ef4e\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#check-the-columns-information-of-the-dataframe","title":"Check the columns information of the dataframe","text":"<p>df.info() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"LrDr3KB0BMET\" outputId=\"16b31182-74ce-454f-ad41-953d665d44f2\"\n# Convert the datatype of the target column into integer\ndf = pd.read_csv(train_csv_path, dtype={\"target\": \"int8\"})\ndf.info()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#handling-dates","title":"Handling Dates","text":"<p>The <code>parse_dates</code> parameter allows you to specify columns that should be parsed as datetime objects when reading a CSV file. This can be especially useful when you have date or datetime information in your CSV file that you want to directly interpret as datetime objects in your DataFrame.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 417} id=\"7u0iFSY0Cv1j\" outputId=\"477f62ac-7520-41a0-ab3c-6c7cee786240\" ipl_csv_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/IPL Matches 2008-2020.csv\" df = pd.read_csv(ipl_csv_path) df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"RlClXO4WC6LV\" outputId=\"b4f372b3-1d64-4924-8137-ba733ab4ae35\"\ndf.info()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"HmjAURzIDCHF\" outputId=\"4a5efc48-283a-40f5-902c-92b71b0c86d9\" df = pd.read_csv(ipl_csv_path, parse_dates=[\"date\"]) df.info() <pre><code>&lt;!-- #region id=\"yteIcgdYDZix\" --&gt;\n## **`converters` Parameter**\nThe `converters` parameter in the pandas `read_csv()` function is used to provide a dictionary of functions that allow you to customize the way specific columns are converted or transformed during the reading process. It's a powerful tool when you need more control over how the data is processed as it's being read from the CSV file.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 417} id=\"s_GHvt29D7wU\" outputId=\"c6daf762-e764-4aec-a918-151c4116a665\"\ndf = pd.read_csv(ipl_csv_path)\ndf.head()\n</code></pre></p> <p>```python id=\"5yMS-SAdEI1N\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#create-a-function-to-convert-the-name-of-the-team-into-its-acronym","title":"Create a function to convert the name of the team into its acronym","text":"<p>def renameTeam(name):     if name == \"Kolkata Knight Riders\":         return \"KKR\"     else:         return name <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 417} id=\"03NToR88ElNI\" outputId=\"11a2f997-1f2f-4fa2-a04a-92862865bb20\"\n# Apply converters\ndf = pd.read_csv(ipl_csv_path, converters={\"team2\": renameTeam})\ndf.head()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#na_values-parameters","title":"<code>na_values</code> Parameters","text":"<p>The <code>na_values</code> parameter is used to specify a list of values that should be treated as missing or NaN (Not a Number) values when reading a CSV file into a DataFrame. This parameter allows you to define how specific values in your CSV file should be interpreted as missing data.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 261} id=\"ihKFlrgjFQVV\" outputId=\"a25771e7-5323-4c88-e043-dadc59887048\" pd.read_csv(train_csv_path).head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 261} id=\"fzK175vuFZLv\" outputId=\"3ce0aa41-2256-4d15-9824-67fd3f140eba\"\n# Select 'Male' values as 'NaN' for example\npd.read_csv(train_csv_path, na_values=\"Male\").head()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/00_Working_with_CSV_Files/#load-a-huge-dataset-in-chunks","title":"Load a Huge Dataset in Chunks","text":"<p>The <code>chunksize</code> parameter in the pandas <code>read_csv()</code> function is used to read a large CSV file in smaller, manageable chunks rather than reading the entire file into memory at once. This can be very useful when dealing with datasets that are too large to fit entirely in memory.</p> <p>```python id=\"_6EOFQqeF8S6\" dfs = pd.read_csv(train_csv_path, chunksize=5000) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"Rb2-W65RGHVr\" outputId=\"75983d3b-fead-4912-fce4-cba366de806b\"\nfor chunk in dfs:\n    print(chunk.shape)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/","title":"Understanding The Data","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#understanding-the-data","title":"Understanding the Data","text":"<p>Understanding the data is a critical step in the data science process. It involves gaining insights into the structure, content, quality, and characteristics of the data you're working with. Properly understanding the data sets the foundation for making informed decisions, building accurate models, and deriving meaningful insights.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"PawDZoMwpXnE\" outputId=\"1e3a0968-9f4c-4cef-9cd4-7ea488ab1dd5\" from google.colab import drive drive.mount('/content/drive') <pre><code>```python id=\"0729fb1c-c7ed-4699-8a41-f60b407862c0\"\nimport pandas as pd\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#read-the-data","title":"Read the Data","text":"<p>```python id=\"cab24e9c-40aa-4bbc-ad8c-43d1e89b9e45\" df = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Global YouTube Statistics.csv\", encoding=\"latin\") <pre><code>&lt;!-- #region id=\"eacd944c-2f70-427c-9a97-0dd3bf73b29b\" --&gt;\n## **How Big is the Data?**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"4cfdbea8-2ca8-4730-8779-28de98befb57\" outputId=\"828f9d22-111a-444e-bdc2-0a449b051ce1\"\ndf.shape\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#how-does-the-data-look-like","title":"How does the Data look like?","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 394} id=\"933de3d1-318d-4155-b327-116664f8fa6c\" outputId=\"166bf67d-06c1-4fc8-c5d0-adfffdefffee\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#print-first-5-rows-of-the-dataframe","title":"Print first 5 rows of the dataframe","text":"<p>df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 429} id=\"18828cf0-83a9-44e6-a020-6b690d1882fe\" outputId=\"83e3ddd6-40b8-4425-9f28-59364fd697d2\"\n# Randomly choose 5 rows and print it\ndf.sample(5)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#what-are-the-data-types-of-the-columns","title":"What are the Data Types of the Columns?","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1c963531-1111-492a-a377-e5333c6d0f92\" outputId=\"a8c928b1-c92e-44c4-bc4a-2469e59f81f2\" df.info() <pre><code>&lt;!-- #region id=\"46dcd22b-4b66-4490-8251-d6241c610625\" --&gt;\n## **Are there any Missing Values?**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"9d5e693a-5a4f-44c0-a9d9-7937c472c654\" outputId=\"acc64ed2-bee9-48fc-86cf-d8db1d2e837d\"\n# Checking the number of missing values for each column\ndf.isnull().sum()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#how-does-the-data-look-mathematically","title":"How does the Data look Mathematically?","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 419} id=\"06b175e1-fae3-4441-b6b1-4f9a57987bc7\" outputId=\"d238093a-e7d3-4773-d911-c3d23dad8fae\" df.describe() <pre><code>&lt;!-- #region id=\"25783af0-db46-4863-880d-63dbbef1cc2a\" --&gt;\n## **Are there any Duplicate Rows?**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"554b97e7-8d80-4e36-ba46-836c8ea891d9\" outputId=\"91d651a4-8bb8-4123-f95e-2c09cf126e6f\"\ndf.duplicated().sum()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#how-is-the-correlation-between-columns","title":"How is the Correlation between Columns?","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 844} id=\"7d3810d1-c41c-40da-8f9d-ae1a84bbf91c\" outputId=\"bb37e174-d589-4a41-b38d-adba9c855f68\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/01_Data_Gathering/01_Understanding_the_Data/#extract-the-correlation-between-all-the-variables","title":"Extract the correlation between all the variables","text":"<p>df.corr() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"97318239-dc13-4b34-99ad-bf80ec0fd148\" outputId=\"237931f3-99c3-472f-d2a6-b2c5da883da8\"\n# Extract the correlation between the 'subscribers' and other numerical columns\ndf.corr()[\"subscribers\"]\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/","title":"Eda Using Univariate Analysis","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#eda-using-univariate-analysis","title":"EDA Using Univariate Analysis","text":"<p>Exploratory Data Analysis (EDA) using univariate analysis focuses on examining individual variables in your dataset to gain insights into their distribution, characteristics, and potential outliers. Univariate analysis helps you understand the basic properties of each variable and identify patterns that can guide further analysis.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"mNqCUtBj3toW\" outputId=\"a36c99b9-2241-40bd-9e9d-3c5534e0a12d\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"d60ea2b2-da57-4889-9fb3-03b07578e068\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#read-the-data","title":"Read the Data","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#dataset-description","title":"Dataset Description","text":"<p>The Titanic dataset is a well-known and frequently used dataset in the field of machine learning. It contains information about passengers who were on board the RMS Titanic during its ill-fated maiden voyage, which tragically ended in a sinking on April 15, 1912. This dataset is often employed for practicing and demonstrating various machine learning techniques, particularly in the context of binary classification.</p> <p>The dataset typically includes the following features:</p> <ol> <li>PassengerID: A unique identifier for each passenger.</li> <li>Survived: A binary variable indicating whether the passenger survived the sinking (1) or did not survive (0).</li> <li>Pclass (Ticket Class): The socio-economic class of the passenger, with values 1, 2, or 3.</li> <li>Name: The name of the passenger.</li> <li>Sex: The gender of the passenger.</li> <li>Age: The age of the passenger. This feature may contain missing values.</li> <li>SibSp: The number of siblings/spouses aboard the Titanic.</li> <li>Parch: The number of parents/children aboard the Titanic.</li> <li>Ticket: The ticket number.</li> <li>Fare: The amount of money paid for the ticket.</li> <li>Cabin: The cabin number where the passenger stayed. This feature may contain missing values.</li> <li>Embarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).</li> </ol> <p>The main objective when working with the Titanic dataset in a machine learning context is often to predict whether a passenger survived or not based on the given features. This is a binary classification problem. Machine learning algorithms can be trained on a subset of the data to learn patterns and relationships between the features and the target variable (Survived), and then these models can be evaluated on unseen data to assess their predictive performance.</p> <p>It's important to note that the dataset might require preprocessing, such as handling missing values, encoding categorical variables, and scaling numerical features, to prepare it for machine learning models. The Titanic dataset serves as a valuable resource for beginners to practice and showcase their machine learning skills.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 275} id=\"ce3f9e36-f504-450e-8546-f3571bca146b\" outputId=\"30ab3e7c-8dfa-40cf-c887-a621c95b757e\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#read-the-data-into-a-dataframe","title":"Read the data into a dataframe","text":"<p>df = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/titanic.csv\") df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"w7UDtvyo579l\" outputId=\"c77031d0-840a-4e6e-d6e6-3726e6c0bd70\"\n# Print the shape of the data\ndf.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"h6XmRppa54Qs\" outputId=\"512332a2-1d4c-40cf-f8d9-de54c4adb404\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-the-information-of-all-the-columns","title":"Check the information of all the columns","text":"<p>df.info() <pre><code>&lt;!-- #region id=\"8a3105a8-e2ba-4052-b507-40a081e09090\" --&gt;\n## **EDA on Categorical Data**\nExploratory Data Analysis (EDA) on categorical data involves analyzing and visualizing the patterns, relationships, and distributions within categorical variables. Here are some common techniques and tools used for EDA on categorical data:\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"4c07b3d6-e57a-4d6f-84bd-6ffbd4096fec\" --&gt;\n### **Count Plot**\n\nA count plot is a type of categorical data visualization that displays the counts of observations in each category of a categorical variable. It is particularly useful for understanding the distribution and frequency of different categories within a dataset.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 503} id=\"wVrKZoOi7vk6\" outputId=\"00f41bc5-1f79-4956-c100-c4f3d69a3554\"\n# Check the number of male and female in the 'Sex' column\nsns.countplot(x=df[\"Sex\"])\ndf[\"Sex\"].value_counts()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 501} id=\"0f96ca45-bea3-4621-9765-f3035e421b2c\" outputId=\"6eed1db2-5605-4e1b-b348-58857a5555b1\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-the-number-of-people-died-and-survived","title":"Check the number of people died and survived","text":"<p>sns.countplot(x=df[\"Survived\"]) df[\"Survived\"].value_counts() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 518} id=\"b2420234-ac50-4147-b537-8bb2a8b8f7e8\" outputId=\"c486e2b8-7d16-459e-9df5-7f985e9a911c\"\n# Check the number of people boared at each class\nsns.countplot(x=df[\"Pclass\"])\ndf[\"Pclass\"].value_counts()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 518} id=\"d6f4829c-aa21-478c-ae9b-efca2d5da57c\" outputId=\"e6643997-424d-49f9-9931-163994c3e9c0\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-the-travellers-embarked-location","title":"Check the travellers embarked location","text":"<p>sns.countplot(x=df[\"Embarked\"]) df[\"Embarked\"].value_counts() <pre><code>&lt;!-- #region id=\"3690778d-3b24-4649-97f9-64b863119fa9\" --&gt;\n### **Pie Chart**\nA pie chart is a circular statistical graphic that is divided into slices to illustrate numerical proportions. It is a popular visualization tool for representing the distribution of a categorical variable as a whole. Each slice of the pie represents a specific category, and the size of each slice corresponds to the proportion or percentage of the whole that the category represents.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 458} id=\"557bafb2-dc44-42b1-b647-1ebe5c6b2003\" outputId=\"448ff52d-398b-4485-e58f-d12f384e4c95\"\n# Plot the percent of people died and survived in a pie chart\ndf[\"Survived\"].value_counts().plot(kind=\"pie\", autopct=\"%.2f\")\n\n# Calculate the percentage\n(df[\"Survived\"].value_counts()/len(df[\"Survived\"])) * 100\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#eda-on-numerical-data","title":"EDA on Numerical Data","text":"<p>Exploratory Data Analysis (EDA) on numerical data involves analyzing and visualizing the patterns, distributions, and relationships within quantitative variables. Here are some common techniques and tools used for EDA on numerical data:</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#histogram","title":"Histogram","text":"<p>Create histograms to visualize the distribution of numerical variables. Histograms display the frequency or probability density of different ranges or bins.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"64251ba4-5cf0-425e-a79a-1bfe728e4504\" outputId=\"03302c37-db29-4c9e-dff6-a1bd2befd0e4\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#plot-the-histogram-of-the-age-column","title":"Plot the histogram of the 'Age' column","text":"<p>sns.histplot(x=df[\"Age\"], bins=50) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 468} id=\"iyKKstT8B5wP\" outputId=\"c1a7c435-3192-4c61-9580-1f89ce52bcb6\"\n# Plot the histogram of the 'Age' column\nsns.histplot(df[\"Fare\"], bins=50)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#dist-plotkde-plot","title":"Dist Plot/KDE Plot","text":"<p>Kernel density plots provide a smooth estimation of the probability density function for a numerical variable, offering insights into the underlying distribution.</p> <p>Interpreting the Probability Density Function (PDF) of the 'Age' column in the Titanic dataset involves analyzing the smoothed estimate of the likelihood of different age values occurring among the passengers. Here's how you can interpret the PDF of the 'Age' column in the Titanic dataset:</p> <ol> <li>Central Tendency:</li> <li>Look for the central tendency in the PDF. Peaks in the PDF may indicate modes or clusters of age values that are more prevalent among the passengers.</li> <li> <p>The highest point in the PDF might correspond to the average or median age of the passengers.</p> </li> <li> <p>Distribution Shape:</p> </li> <li>Observe the overall shape of the PDF. A symmetric distribution suggests a balanced age distribution, while asymmetry may indicate skewness.</li> <li> <p>Skewness to the right (positive skew) suggests a longer right tail, indicating a higher presence of older passengers. Skewness to the left (negative skew) suggests a higher presence of younger passengers.</p> </li> <li> <p>Variability and Spread:</p> </li> <li> <p>Examine the width of the PDF at different points. A wider PDF indicates greater variability or dispersion in age values, while a narrower PDF suggests less variability.</p> </li> <li> <p>Distinct Age Groups:</p> </li> <li>Identify if there are distinct peaks or modes in the PDF, which may represent specific age groups.</li> <li> <p>For example, there might be peaks around certain ages corresponding to children, adults, or elderly passengers.</p> </li> <li> <p>Outliers:</p> </li> <li> <p>Look for isolated peaks or regions with unusually low probability density, which may indicate the presence of outliers or unique age values.</p> </li> <li> <p>Comparison with Other Variables:</p> </li> <li> <p>Compare the 'Age' PDF with other relevant variables (e.g., 'Survived', 'Class', 'Gender') to identify potential relationships or patterns. For instance, you might compare the age distribution between survivors and non-survivors.</p> </li> <li> <p>Handling Missing Values:</p> </li> <li>Address any missing values in the 'Age' column, as these may impact the shape and interpretation of the PDF. You can choose to impute missing ages or visualize the distribution excluding missing values.</li> </ol> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"f4dc04f0-40c8-4f6f-a242-4f0a67fd5613\" outputId=\"6b317dd2-4fe6-4a46-c11f-c075f23fc007\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#plot-the-pdf-of-the-age-column","title":"Plot the PDF of the 'Age' column","text":"<p>sns.kdeplot(data=df[\"Age\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"2qlZhW3RFPrj\" outputId=\"3901c442-259e-4544-d4b3-3fe42ec7ec1c\"\n# Plot the PDF of the 'Age' column by the gender\nsns.kdeplot(data=df, x=\"Age\", hue=\"Sex\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 468} id=\"Q4xYBn_5HCdk\" outputId=\"bd6856ee-314c-48c9-89fc-7e6c002e8377\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#compare-the-age-pdf-with-survived-to-identify-potential-relationships-or-patterns","title":"Compare the 'Age' PDF with 'Survived' to identify potential relationships or patterns","text":"<p>sns.kdeplot(data=df, x=\"Age\", hue=\"Survived\", palette=[\"red\", \"green\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"HL-Ha3hOCcNO\" outputId=\"7bc16a2b-11e8-4bb4-ba32-ee8cf4b4a7d3\"\n# Plot the PDF of the 'Fare' column\nsns.kdeplot(data=df[\"Fare\"])\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#box-plot","title":"Box Plot","text":"<p>A boxplot, also known as a box-and-whisker plot, is a graphical representation of the distribution of a dataset. It provides a visual summary of the central tendency, spread, and potential outliers within the data. Boxplots are particularly useful for comparing the distribution of numerical variables across different groups or categories.</p> <p>Here are the key components and interpretations of a boxplot:</p> <ol> <li>Box:</li> <li>The central box represents the interquartile range (IQR), which spans the middle 50% of the data. The box's bottom and top edges denote the first quartile (Q1) and third quartile (Q3), respectively.</li> <li> <p>The width of the box indicates the spread of the middle 50% of the data.</p> </li> <li> <p>Median (Q2):</p> </li> <li> <p>A line inside the box represents the median (Q2), which is the middle value when the data is sorted.</p> </li> <li> <p>Whiskers:</p> </li> <li>Whiskers extend from the box to the minimum and maximum values within a specified range. The range can be determined by various methods, such as 1.5 times the IQR or extending to the minimum and maximum values within a certain percentage of the data.</li> <li> <p>Outliers beyond the whiskers are often represented as individual points.</p> </li> <li> <p>Outliers:</p> </li> <li> <p>Individual data points outside the whiskers are considered potential outliers. These points are plotted individually, providing a clear visual indication of values that significantly deviate from the central tendency.</p> </li> <li> <p>Symmetry and Skewness:</p> </li> <li>The symmetry or skewness of the distribution can be inferred by observing the relative lengths of the whiskers and the position of the median.</li> <li> <p>An asymmetric boxplot with a longer tail on one side suggests skewness.</p> </li> <li> <p>Comparison Across Groups:</p> </li> <li> <p>Boxplots are particularly useful for comparing the distribution of numerical variables across different categories or groups. Multiple boxes can be displayed side by side for visual comparison.</p> </li> <li> <p>Seaborn's <code>boxplot</code>:</p> </li> <li>In Python, the Seaborn library provides the <code>boxplot</code> function for creating boxplots.</li> <li>This function offers various customization options, including the ability to create grouped or categorical boxplots.</li> </ol> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"218c51f7-e9dd-453a-afe2-ed1eeff39268\" outputId=\"11a64ec0-ee49-43c4-ab67-04fbd8c38601\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#plot-the-box-plot-of-age-column","title":"Plot the box plot of 'Age' column","text":"<p>sns.boxplot(x=df[\"Age\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"cKkfQktsK8Fj\" outputId=\"66d7bc93-2f55-44e7-ef94-f7b960de7457\"\n# Plot the 'box' plot of 'Fare' column\nsns.boxplot(x=df[\"Fare\"])\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7b4617f2-e56b-4071-9270-94a652525839\" outputId=\"5a9f45fb-0b42-436f-a63f-fe0d3b8a78dd\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-the-minimum-age","title":"Check the minimum 'Age'","text":"<p>df[\"Age\"].min() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1308f2f2-908d-4a68-a4a5-73d0bb686952\" outputId=\"771674d9-0158-4f73-aa7a-643e3c165589\"\n# Check the maximum 'Age'\ndf[\"Age\"].max()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cddc35aa-bd83-421b-a39b-515f87db238c\" outputId=\"e5d48509-9b39-4449-e8ae-9c64f687d914\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-the-mean-age","title":"Check the mean 'Age'","text":"<p>df[\"Age\"].mean() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"c19a8ed5-9f73-455e-931d-7a823c73eeb1\" outputId=\"dfd4a3be-51b2-4b90-d89d-58f003510464\"\n# Check the skewness of the 'Age' column\ndf[\"Age\"].skew()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DXLI6hnDNXCa\" outputId=\"392e80e0-3211-4f3c-9a8d-b5de46ab58ac\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/00_EDA_using_Univariate_Analysis/#check-all-the-descriptive-statistics","title":"Check all the descriptive statistics","text":"<p>df[\"Age\"].describe() ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/","title":"Eda Using Bivariate And Multivariate Analysis","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#eda-using-bivariate-and-multivariate-analysis","title":"EDA Using Bivariate and Multivariate Analysis","text":"<p>Exploratory Data Analysis (EDA) is a critical initial step in the data analysis process that involves summarizing, visualizing, and understanding the main characteristics and patterns present in a dataset. Bivariate and multivariate analysis are two important components of EDA that help you explore relationships and interactions between multiple variables in your dataset.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"pcxAMNWNgyhr\" outputId=\"30b5b519-acd9-41e1-803c-6944f1e7a96c\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"f75bf1bd-718f-4410-a14e-45f5a480d5db\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#read-the-data","title":"Read the Data","text":"<p>```python id=\"5e1048c1-995c-4b4b-98a0-3a2c3fe7932b\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#reading-the-tips-data-from-seaborn","title":"Reading the 'tips' data from seaborn","text":"<p>tips = sns.load_dataset(\"tips\") <pre><code>```python id=\"bdd7b6f0-af43-49e0-a4f1-863f74b4c622\"\n# Read the 'titanic' data\ntitanic = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/titanic.csv\")\n</code></pre></p> <p>```python id=\"fa67470c-84a4-448a-9145-b873ebfeff35\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#read-the-iris-dataset","title":"Read the 'iris' dataset","text":"<p>iris = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Iris.csv\") <pre><code>```python id=\"c1795708-8211-4a9d-8902-fb01c5f471bd\"\n# Read the 'flight' data\nflights = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/flights.csv\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"ef12d752-7c37-407c-a3f5-d4f819ab296b\" outputId=\"6457270d-3c2c-4f98-a8fb-252672d5fcce\" tips.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"62f0b648-9412-445e-b3d5-797bf715ce88\" outputId=\"6e670ef3-fcb6-4ace-e8a5-63cd70a0e7c1\"\ntips.shape\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#scatter-plot-numerical-numerical","title":"Scatter Plot (Numerical - Numerical)","text":"<p>A scatterplot is a fundamental and widely used data visualization technique that displays the relationship between two numerical variables. It is particularly effective for revealing patterns, trends, and potential correlations between the variables. In a scatterplot, each data point is represented as a dot, and the position of the dot corresponds to the values of the two numerical variables on the x and y axes.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 450} id=\"522c71fa-b44b-471d-8205-50a0e8c0d862\" outputId=\"d6504594-8e05-4b65-b597-b42979855789\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plotting-a-scatterplot-between-total_bill-and-tip-using-univariate-analysis","title":"Plotting a scatterplot between 'total_bill' and 'tip' using univariate analysis","text":"<p>sns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\") plt.show() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 467} id=\"65513b77-41f7-492b-b05a-97ad0d952ed4\" outputId=\"574dd82d-21c8-4862-c043-80250256ef8f\"\n# Plotting a scatterplot between 'total_bill' and 'tip' using multivariate analysis\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"sex\", style=\"smoker\", size=\"size\")\nplt.legend(bbox_to_anchor=(1, 1))\n</code></pre></p> <p>\ud83e\udd14 Note: </p> <ol> <li><code>sns.scatterplot()</code>:</li> <li>This function from the Seaborn library is used to create a scatterplot.</li> <li>The <code>data</code> parameter specifies the DataFrame (<code>tips</code>) containing the data.</li> <li>The <code>x</code> and <code>y</code> parameters define the variables to be plotted on the x-axis and y-axis, respectively (<code>total_bill</code> and <code>tip</code>).</li> <li>The <code>hue</code> parameter introduces color differentiation based on the values of the \"sex\" variable.</li> <li>The <code>style</code> parameter introduces different marker styles based on the values of the \"smoker\" variable.</li> <li> <p>The <code>size</code> parameter introduces different marker sizes based on the values of the \"size\" variable.</p> </li> <li> <p><code>plt.legend()</code>:</p> </li> <li>This function from Matplotlib is used to display a legend for the scatterplot.</li> <li>The <code>bbox_to_anchor</code> parameter specifies the location of the legend box relative to the axes. In this case, it is set to the top-right corner of the plot (<code>bbox_to_anchor=(1, 1)</code>).</li> </ol>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#bar-plot-numerical-categorical","title":"Bar Plot (Numerical - Categorical)","text":"<p>A bar plot (or bar chart) is a common and effective data visualization technique used to represent the relationship between a numerical variable and a categorical variable. It is particularly useful for displaying and comparing the distribution of a numerical variable across different categories. In a bar plot, rectangular bars are drawn to represent the values of the numerical variable for each category in the categorical variable.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 258} id=\"613fcc56-2238-4806-8385-e45c10a1f7f3\" outputId=\"56249efa-0774-4705-ad0b-f956a0853b66\" titanic.head() <pre><code>&lt;!-- #region id=\"obXCB6lQkqCZ\" --&gt;\n**Plot Interpretation**:\n   - The bar plot aims to show the average age of passengers traveling in each passenger class.\n   - Each bar represents a different passenger class (1st, 2nd, or 3rd class).\n   - The height of each bar corresponds to the average age of passengers in the respective class.\n   - The black lines (error bars) extending from each bar represent one standard deviation from the mean age for each passenger class.\n   - The length of the error bars provides a visual indication of the variability or spread of ages within each class.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"ed8ffb5e-8841-4817-97a8-d3f82cf23db2\" outputId=\"395facbb-3a29-41b2-e0b4-d9fcf59fb9fc\"\n# Plot the avergae age of a passsenger traveling in a particular class\nsns.barplot(data=titanic, x=\"Pclass\", y=\"Age\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"831e748e-11d7-43cc-9141-f6871b096228\" outputId=\"6f37c767-f92d-4055-cc4b-a48cfc0c3f70\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plot-the-avergae-age-of-a-passsenger-male-and-female-traveling-in-a-particular-class","title":"Plot the avergae age of a passsenger (male and female) traveling in a particular class","text":"<p>sns.barplot(data=titanic, x=\"Pclass\", y=\"Age\", hue=\"Sex\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"d9e00cab-be15-47a7-9f22-167194cbfa66\" outputId=\"77c70ef1-5bb8-4eaa-fb95-e63fea42ed0c\"\n# Plot the avergae fare in a particular class\nsns.barplot(data=titanic, x=\"Pclass\", y=\"Fare\", hue=\"Sex\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#box-plot-numerical-categorical","title":"Box Plot (Numerical - Categorical)","text":"<p>A box plot (or box-and-whisker plot) is a powerful and widely used data visualization tool for representing the distribution of a numerical variable across different categories of a categorical variable. Box plots provide insights into the central tendency, spread, and potential outliers within each category, making them especially valuable for comparing the distributions of numerical data across different groups.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"43692ed3-71c0-4fda-bbf6-5b1468f85818\" outputId=\"4e364a23-f418-4a69-a61e-bf7ba2dce552\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plot-a-boxplot-between-sex-and-age","title":"Plot a boxplot between 'Sex' and 'Age'","text":"<p>sns.boxplot(data=titanic, x=\"Sex\", y=\"Age\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"e582cd5d-6c6a-4177-8f74-a59b7f4d21e7\" outputId=\"56eec88f-0582-4b89-86cf-bebb9fcffb69\"\n# Plot a boxplot between 'Sex' and 'Age' also show 'Survived'\nsns.boxplot(data=titanic, x=\"Sex\", y=\"Age\", hue=\"Survived\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#kde-plot-numerical-categorical","title":"KDE Plot (Numerical - Categorical)","text":"<p>A KDE (Kernel Density Estimate) plot is a data visualization technique that provides an estimation of the probability density function of a continuous random variable. In the context of a numerical-categorical relationship, a KDE plot can be used to visualize the distribution of a numerical variable within different categories of a categorical variable. Seaborn's <code>kdeplot</code> function is commonly used to create KDE plots in Python.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"3dfc90b0-4629-43f1-afa7-69abca976cef\" outputId=\"ac1df408-0a52-41c6-dc94-0b32e0668621\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plot-the-relationship-between-age-and-survived","title":"Plot the relationship between 'Age' and 'Survived'","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plotting-the-age-of-the-people-who-could-not-survive","title":"Plotting the Age of the people who could not survive","text":"<p>sns.kdeplot(titanic[titanic[\"Survived\"]==0][\"Age\"])</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#plotting-the-age-of-the-people-who-could-survive","title":"Plotting the Age of the people who could survive","text":"<p>sns.kdeplot(titanic[titanic[\"Survived\"]==1][\"Age\"]) <pre><code>&lt;!-- #region id=\"a70406f7-9ba6-42b5-ac01-1beb6c8f6ae5\" --&gt;\n## **Heat Map (Categorical - Categorical)**\nA heatmap is a data visualization technique used to represent the magnitude of a variable in a two-dimensional space, typically through color intensity. In the context of categorical-categorical relationships, a heatmap is particularly useful for visualizing the frequency, proportions, or relationships between two categorical variables. Seaborn's `heatmap` function in Python is commonly used for creating categorical heatmaps.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 258} id=\"b8bd3533-9a06-4521-a1cc-9e1338ebef4e\" outputId=\"c19b7de5-503d-43aa-fbb9-4377ac3419fb\"\ntitanic.head()\n</code></pre></p> <p>\ud83e\udd14 Note: <code>crosstab</code> is a function in the Pandas library in Python that computes a cross-tabulation (also known as contingency table) of two or more factors. It is particularly useful for analyzing the relationships between categorical variables by summarizing the frequency of occurrences of different combinations of categories.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 174} id=\"5bbb9ecd-2d17-45d0-90ba-e6b8ce4e97a9\" outputId=\"b8f14136-d29c-4811-bbea-9645fe7d9538\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#get-the-number-of-people-died-and-survived-in-each-pclass","title":"Get the number of people died and survived in each Pclass","text":"<p>pd.crosstab(index=titanic[\"Pclass\"], columns=titanic[\"Survived\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"912ca60f-71e3-441e-96e7-e34101c089a0\" outputId=\"b8549f61-9b37-41f8-c0b2-ef7d70ee77cb\"\nsns.heatmap(pd.crosstab(index=titanic[\"Pclass\"], columns=titanic[\"Survived\"]), cmap=\"Reds\",\n            annot=True, fmt=\"\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#group-by-function","title":"Group By Function","text":"<p>The <code>groupby</code> function in Pandas is a powerful tool for splitting a DataFrame into groups based on one or more criteria, applying a function to each group independently, and then combining the results into a new DataFrame. It is a key feature for data analysis and manipulation in Pandas.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 174} id=\"8eaf7930-27b3-4b02-87a5-254aa46f2f5d\" outputId=\"d6b670ed-e970-4d04-94fa-eef7a1586fa7\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#group-titanic-dataframe-based-on-pclass","title":"Group titanic dataframe based on 'Pclass'","text":"<p>titanic.groupby(by=\"Pclass\").mean() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 461} id=\"507ca8a7-9c5d-4fa5-9655-2b06b5d53d08\" outputId=\"fb19b967-08ca-4541-bffb-aa666f153cda\"\n# Percentage of people survived in each Pclass\n(titanic.groupby(\"Pclass\").mean()[\"Survived\"]*100).plot(kind=\"bar\")\nplt.ylabel(\"% of People Survived\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#pair-plot","title":"Pair Plot","text":"<p><code>pairplot</code> is a function in the Seaborn library for creating a grid of scatterplots that visualize the relationships between pairs of variables in a DataFrame. It's a convenient tool for exploring the pairwise relationships in a dataset, especially when dealing with multiple numerical variables.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"4c2ec4fa-aff0-4538-bfbf-1cd19091ad67\" outputId=\"5bf08490-b25e-4639-ab89-2ba46d623bc6\" iris.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 902} id=\"110a8cf0-650c-4f74-abe5-303070c03733\" outputId=\"afd859d5-8016-4fde-aeaf-4baa1556a672\"\nsns.pairplot(iris.iloc[:, 1:], hue=\"Species\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#cluster-map","title":"Cluster Map","text":"<p><code>clustermap</code> is a function in the Seaborn library that creates a clustered heatmap, which is a heatmap with dendrograms on both the rows and columns. It is a powerful visualization tool for exploring patterns and relationships in a dataset by hierarchically clustering both the observations (rows) and variables (columns) based on their similarities.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"0tvIjYfUiAIU\" outputId=\"8e6fce4b-de9a-481d-a1a8-7dff14de7023\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#get-the-number-of-people-died-and-survived-for-each-sibsp","title":"Get the number of people died and survived for each SibSp","text":"<p>pd.crosstab(index=titanic[\"Parch\"], columns=titanic[\"Survived\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 1000} id=\"SeDa4Kc5iJ92\" outputId=\"b43269ab-25bc-4b2d-b1ab-0100410a8570\"\nsns.clustermap(pd.crosstab(index=titanic[\"Parch\"], columns=titanic[\"Survived\"]), cmap=\"Reds\",\n               annot=True, fmt=\"\")\n</code></pre></p> <p>\ud83e\udd14 Note: <code>clustermap</code> is particularly useful for identifying patterns and relationships in datasets with multiple variables, helping to uncover hidden structures in the data. It is a valuable tool in exploratory data analysis and can provide insights into the underlying organization of complex datasets.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#line-plot-numerical-numerical","title":"Line Plot (Numerical - Numerical)","text":"<p>A line plot, also known as a line chart or line graph, is a type of data visualization that displays data points connected by straight line segments. It is commonly used to illustrate the trend or pattern in a dataset over a continuous interval, such as time. Line plots are particularly effective for showing how a variable changes in relation to another variable or over a sequential period.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"TeUARe3Xko93\" outputId=\"258a7319-b6d8-408a-b585-624e2280baeb\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#print-the-first-5-rows-of-the-flights-data","title":"Print the first 5 rows of the flights data","text":"<p>flights.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 425} id=\"D0bFhKlxkvJ3\" outputId=\"50332fe2-0a9e-4265-d936-986491e9b4fd\"\n# Group the number of passengers and calculate total for each year\nflights_stat = flights.groupby(\"year\").sum().reset_index()\nflights_stat\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 466} id=\"KPi-qtLolB-A\" outputId=\"d8ffc22b-1b36-4ece-eb8c-e74e5f758f75\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#create-a-simple-line-plot-between-years-and-passengers","title":"Create a simple line plot between years and passengers","text":"<p>sns.lineplot(data=flights_stat, x=\"year\", y=\"passengers\", marker=\"o\") <pre><code>&lt;!-- #region id=\"s-9aRpDBlHMh\" --&gt;\n## **Miscellaneous**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 457} id=\"xIV70ixmlOya\" outputId=\"86ac7d2c-e8a2-44a8-dab6-b2d7b11cc342\"\n# Create a pivot table\nflights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 488} id=\"dQ0AU_rglnt2\" outputId=\"3e2500b3-cdc0-49d2-ed39-83f68cefe2c9\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/02_Exploratory_Data_Analysis/01_EDA_using_Bivariate_and_Multivariate_Analysis/#create-a-heatmap","title":"Create a heatmap","text":"<p>sns.heatmap(flights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\"),             cmap=\"Reds\") <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 1000} id=\"cAhYw43Dm7S1\" outputId=\"a4391b2b-5457-4765-9dfa-09b7b7a7f2f0\"\n# Create a cluster map\nsns.clustermap(flights.pivot_table(values=\"passengers\", index=\"month\", columns=\"year\"),\n            cmap=\"Reds\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/","title":"Standardization","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#standardization-feature-scaling","title":"Standardization - Feature Scaling","text":"<p>Standardization, also known as z-score normalization or standard scaling, is a technique used in data preprocessing to rescale the features of a dataset. The goal of standardization is to transform the data so that it has a mean of 0 and a standard deviation of 1. This process helps to make the features more comparable and can be particularly useful in machine learning algorithms that are sensitive to the scale of the input features, such as gradient descent-based optimization algorithms.</p> <p>1. Standard Deviation:  The standard deviation (\\(\\sigma\\)) is a measure of the amount of variation or dispersion in a set of values. The formula for calculating the standard deviation of a sample is different from the formula for calculating the standard deviation of a population.</p> <p>For a <code>Sample</code>:</p> \\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\] <p>where: - \\(s\\) is the sample standard deviation, - \\(n\\) is the number of observations in the sample, - \\(x_i\\) is each individual observation in the sample, - \\(\\bar{x}\\) is the sample mean.</p> <p>For a <code>Population</code>:</p> \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\\] <p>where: - \\(\\sigma\\) is the population standard deviation, - \\(N\\) is the number of observations in the population, - \\(x_i\\) is each individual observation in the population, - \\(\\mu\\) is the population mean.</p> <p></p> <p>2. Z-score: A Z-score, also known as a standard score, is a statistical measure that quantifies how far away a particular data point is from the mean (average) of a dataset when measured in terms of standard deviations. It's a way to standardize and normalize data, making it easier to compare values from different datasets or different parts of the same dataset.</p> \\[Z = \\frac{(X - \\mu)}{\\sigma}\\] <p>where: - \\(Z\\) is the standardized value, - \\(X\\) is the original value of the feature, - \\(\\mu\\) is the mean of the feature, - \\(\\sigma\\) is the standard deviation of the feature.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#when-to-use-standardization","title":"When to use Standardization?","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#example","title":"Example","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"SoywawFvcrtf\" outputId=\"63bada6e-632e-42b8-ef07-b2c7d614fcf7\" from google.colab import drive drive.mount('/content/drive') <pre><code>```python id=\"7c25c2b2-736c-4ef3-a2c1-fc0841bd0b77\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#read-the-data","title":"Read the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"5b70dfb3-487f-4872-82fd-17cfdbf8837d\" outputId=\"91f1be78-33eb-46c8-a785-e729b4ed6652\" df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Social_Network_Ads.csv\") df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"68c64ef5-73c6-484f-ba0f-e07353e7b167\" outputId=\"3d021044-1463-4288-ed38-69f083e27c9f\"\ndf.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"37651496-bee0-49f7-972d-4903a01e4a8b\" outputId=\"22070045-923e-45c1-d017-9db93a90dba2\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#check-the-null-values","title":"Check the null values","text":"<p>df.isnull().sum() <pre><code>&lt;!-- #region id=\"84547301-2fcf-4d84-83b2-600d666d9945\" --&gt;\n## **Train Test Split**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"26eb9133-8fb8-4162-8f5d-30d4abc5d0ef\" outputId=\"814cb56a-dabe-4c11-a638-5d1344e21b43\"\n# Split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(df.drop(\"Purchased\", axis=1),\n                                                    df[\"Purchased\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#standard-scaler","title":"Standard Scaler","text":"<p>In scikit-learn, the <code>StandardScaler</code> is a preprocessing technique provided by the library for standardizing or scaling features in your dataset. It follows the standardization process discussed earlier, where it scales the features to have a mean of 0 and a standard deviation of 1. This is done to ensure that all features have the same scale, making them more suitable for machine learning algorithms that are sensitive to feature scales.</p> <p>The <code>fit</code> method is typically called on a machine learning model or a data preprocessing object to adapt it to the specific dataset you are working with. Its purpose is to learn from the data and update the internal state of the object.</p> <p>```python id=\"afd045be-dc46-4c4f-994e-3ebfa5c17877\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#instantiate-a-standardscaler-object","title":"Instantiate a 'StandardScaler' object","text":"<p>scaler = StandardScaler()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#fit-the-scaler-to-the-train-set-it-will-learn-the-parameters","title":"Fit the scaler to the train set, it will learn the parameters","text":"<p>scaler.fit(X_train)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#transform-train-and-test-sets","title":"Transform train and test sets","text":"<p>X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) <pre><code>```python id=\"09667611-c74f-4d93-a48f-277ca9ef43ed\"\n# transform always returns a numpy array\n# Converting the scaled numpy arrays into pandas dataframes\nX_train_scaled = pd.DataFrame(data=X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(data=X_test_scaled, columns=X_test.columns)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"50920ad7-6e6e-4911-b9c4-6955fc75c0e9\" outputId=\"e66d6328-ccd3-4270-cf06-348bd2a0b939\" X_train_scaled.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"38511e54-d48f-444f-9539-5dfc453438d4\" outputId=\"af0e40e3-36f8-4049-c919-fff466dac0a5\"\n# Describe the training data\nX_train.describe()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"6ad06ab1-2bd3-455b-8d0c-839b1334acad\" outputId=\"3d323b7d-3087-4021-e631-4cad9510fb68\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#describe-the-scaled-training-data","title":"Describe the scaled training data","text":"<p>X_train_scaled.describe().round(2) <pre><code>&lt;!-- #region id=\"2cfbddf0-474b-4225-a48d-af5314343669\" --&gt;\n## **Effect of Scaling**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 486} id=\"31edeac9-64b4-4e8c-a67d-30b122036edd\" outputId=\"403719d5-30f5-4b1d-931c-77bcd18bdb94\"\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a scatter plot of the training data\nax1.scatter(x=X_train[\"Age\"], y=X_train[\"EstimatedSalary\"])\nax1.set_title(\"Before Standardization\")\nax1.set_xlabel(\"Age\")\nax1.set_ylabel(\"Estimated Salary\")\n\n# Creating a scatter plot of the scaled training data\nax2.scatter(x=X_train_scaled[\"Age\"], y=X_train_scaled[\"EstimatedSalary\"], color=\"red\")\nax2.set_title(\"After Standardization\")\nax2.set_xlabel(\"Standardized Age\")\nax2.set_ylabel(\"Standardized Estimated Salary\")\nplt.show()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"aeebc391-ca9f-48d5-aa89-2d3f6f80b7aa\" outputId=\"bd0a912d-cb5d-47f8-c450-7bcf39abb7f8\" fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#creating-a-probabilty-density-plot-of-the-training-data","title":"Creating a probabilty density plot of the training data","text":"<p>ax1.set_title(\"Before Standardization\") sns.kdeplot(data=X_train[\"Age\"], ax=ax1, label=\"Age\") sns.kdeplot(data=X_train[\"EstimatedSalary\"], ax=ax1, label=\"Estimated Salary\") ax1.legend()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#creating-a-probabilty-density-plot-of-the-scaled-training-data","title":"Creating a probabilty density plot of the scaled training data","text":"<p>ax2.set_title(\"After Standardization\") sns.kdeplot(data=X_train_scaled[\"Age\"], ax=ax2, label=\"Age\") sns.kdeplot(data=X_train_scaled[\"EstimatedSalary\"], ax=ax2, label=\"Estimated Salary\") ax2.legend()</p> <p>plt.show() <pre><code>&lt;!-- #region id=\"e21c2343-3d3e-416a-adbb-5a44fa8ce4a3\" --&gt;\n## **Comparison of Distribution**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"8b6a9532-34b9-4e3d-96f3-62a5db9e49b6\" outputId=\"877fedbd-ce57-414f-9150-c9e7ac72d0aa\"\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the 'Age' column from the training data\nsns.kdeplot(X_train[\"Age\"], ax=ax1)\nax1.set_title(\"Age Distribution before Scaling\")\n\n# Creating a probabilty density plot of the 'Age' column from the scaled training data\nsns.kdeplot(X_train_scaled['Age'], ax=ax2)\nax2.set_title(\"Age Distribution after Scaling\")\n\nplt.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#why-scaling-is-important","title":"Why Scaling is Important?","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#comparison-on-logistic-regression-model","title":"Comparison on Logistic Regression Model","text":"<p>```python id=\"eed5a239-3803-48e9-af0c-f40ed7da3d54\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#creating-two-logistic-regression-model-for-the-training-data-and-scaled-training-data-respectively","title":"Creating two logistic regression model for the training data and scaled training data respectively","text":"<p>lr = LogisticRegression() lr_scaled = LogisticRegression() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"bc41bf94-d582-4707-9a33-bfe627b89a70\" outputId=\"825eb414-7ccd-40e2-8eb6-5a534fc1aae9\"\n# Fitting the data to the models\nlr.fit(X=X_train, y=y_train)\nlr_scaled.fit(X=X_train_scaled, y=y_train)\n</code></pre></p> <p>```python id=\"0bcccd8a-a3c8-4f18-9372-eafdc05e5a3a\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#predict-the-testing-data","title":"Predict the testing data","text":"<p>y_pred = lr.predict(X_test) y_pred_scaled = lr_scaled.predict(X_test_scaled) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cbe756fa-d4b3-4c52-9ee1-3fead7878310\" outputId=\"b1826d3e-6bfe-4283-d8d4-b756280c5fbb\"\n# Calculating the accuracy\nprint(\"Accuracy Score on Actual Data:\", accuracy_score(y_test, y_pred).round(2))\nprint(\"Accuracy Score on Scaled Data:\", accuracy_score(y_test, y_pred_scaled).round(2))\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#comparison-on-decision-tree-model","title":"Comparison on Decision Tree Model","text":"<p>```python id=\"a74c6dbc-3a78-4a85-9091-78d9a080d3a7\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#creating-two-decision-tree-model-for-the-training-data-and-scaled-training-data-respectively","title":"Creating two decision tree model for the training data and scaled training data respectively","text":"<p>dt = DecisionTreeClassifier() dt_scaled = DecisionTreeClassifier() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"c5460e78-4aec-498f-9362-9d6a471ebea6\" outputId=\"f56554d0-e0e7-4994-d313-d2b1555bc3cd\"\n# Fitting the data to the models\ndt.fit(X=X_train, y=y_train)\ndt_scaled.fit(X=X_train_scaled, y=y_train)\n</code></pre></p> <p>```python id=\"3d736bd1-52f8-44d3-99b2-bf8afcb14beb\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#predict-the-testing-data_1","title":"Predict the testing data","text":"<p>y_pred = dt.predict(X_test) y_pred_scaled = dt_scaled.predict(X_test_scaled) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"14399866-ecf6-48fa-96fb-fc332332cc4f\" outputId=\"53809d6a-94a1-45e7-bea5-44fef19b7151\"\n# Calculating the accuracy\nprint(\"Accuracy Score on Actual Data:\", accuracy_score(y_test, y_pred).round(2))\nprint(\"Accuracy Score on Scaled Data:\", accuracy_score(y_test, y_pred_scaled).round(2))\n</code></pre></p> <p>\ud83e\udd14 Note: The accuracy improved after applying normalization to the Logistic Regression model because Logistic Regression is sensitive to the scale of input features, and normalization ensures a consistent and effective learning process by bringing all features to a similar scale. On the other hand, the Decision Tree model's accuracy remained unchanged because Decision Trees are inherently less sensitive to the scale of input features, as their splitting criteria depend on feature thresholds rather than absolute values. Therefore, normalization did not significantly impact the Decision Tree model's performance.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#effect-of-outlier","title":"Effect of Outlier","text":"<p>Standardization, by itself, does not handle outliers in the data. In fact, standardization can sometimes exacerbate the impact of outliers, making them more prominent in the scaled data. Standardization can be affected by outliers, and its sensitivity to extreme values may impact the resulting standardized values. Depending on the context and goals of the analysis, alternative scaling methods that are more robust to outliers might be considered.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 143} id=\"c89b5dd4-0b1c-4861-ba1d-a4bcce3bface\" outputId=\"768d41da-ed94-461b-98d9-adf476a7fc3a\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#adding-some-outliers-to-the-data","title":"Adding some outliers to the data","text":"<p>outliers = pd.DataFrame({\"Age\":[10, 90, 97], \"EstimatedSalary\":[1000, 250000, 350000], \"Purchased\": [0, 1, 1]}) outliers <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"d9487bd4-68d4-4304-b9ac-7acde6efd6e3\" outputId=\"f6043ef6-85c7-4d7d-efbf-52f5a65e97d3\"\n# Concat the outliers in the previous dataframe\nnew_df = pd.concat([df, outliers])\nnew_df\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"2133020a-9076-4dc4-9f0b-b72249890f68\" outputId=\"c3411229-caab-4e29-b4a3-8db1e5122cec\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#create-a-scatter-plot","title":"Create a scatter plot","text":"<p>plt.scatter(x=new_df[\"Age\"], y=new_df[\"EstimatedSalary\"]) plt.show() <pre><code>&lt;!-- #region id=\"d1d0133a-a59e-4803-9675-d7e59efd5e89\" --&gt;\n### **Applying Standardization**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cdebac6b-e52d-4d05-9e84-acef2968ec32\" outputId=\"dcc948a8-ed4f-466e-8ee2-7aba167ba22a\"\n# Splitting the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(new_df.drop([\"Purchased\"], axis=1),\n                                                    new_df[\"Purchased\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre></p> <p>```python id=\"f4179cef-201a-4d3e-99fe-bdfcb33fcc30\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#creating-a-standard-scaler-object","title":"Creating a Standard Scaler object","text":"<p>scaler = StandardScaler()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#fitting-the-data","title":"Fitting the data","text":"<p>scaler.fit(X_train)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#applying-standardization-on-the-training-and-testing-data","title":"Applying Standardization on the training and testing data","text":"<p>X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"9922875d-06bb-40aa-a042-70d5b48dacf9\" outputId=\"d0f53390-65a7-4f89-939b-e2da55ec0a1a\"\n# Converting the scaled data into a pandas dataframe\nX_train_scaled = pd.DataFrame(data=X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(data=X_test_scaled, columns=X_test.columns)\nX_train_scaled.head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 503} id=\"622d9bb8-b3df-4b8a-916f-cf5fec5ee84b\" outputId=\"f3152c67-29cb-472a-9202-cc4487636aa0\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#plot-the-data","title":"Plot the data","text":"<p>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#plotting-the-x_train-data","title":"Plotting the x_train data","text":"<p>ax1.scatter(x=X_train[\"Age\"], y=X_train[\"EstimatedSalary\"]) ax1.set_title(\"Before Standardization\") ax1.set_xlabel(\"Age\") ax1.set_ylabel(\"Estimated Salary\")</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/00_Standardization/#plotting-the-scaled-x_train-data","title":"Plotting the scaled x_train data","text":"<p>ax2.scatter(x=X_train_scaled[\"Age\"], y=X_train_scaled[\"EstimatedSalary\"], color=\"red\") ax2.set_title(\"After Standardization\") ax2.set_xlabel(\"Standardized Age\") ax2.set_ylabel(\"Standardized Estimated Salary\") ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/01_Normalization/","title":"Normalization","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/01_Normalization/#normalization-feature-scaling","title":"Normalization - Feature Scaling","text":"<p>Normalization, also known as Min-Max scaling, is a feature scaling technique used in data preprocessing to rescale numerical features to a specific range, typically between 0 and 1. The goal of normalization is to ensure that all feature values have similar scales, making them more suitable for machine learning algorithms that are sensitive to the magnitude of input features.</p>  ## **When to use Normalization?**   # **Example**   ## **Import Required Libraries**   ```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"27KLSOV_g-Q4\" outputId=\"6077e63d-ac57-4d59-a86b-fc98e045a465\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"d2ae2692-7dfe-4067-9a4f-27ce661465dd\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n</code></pre>  ## **Read the Data**   ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"c52ff947-954e-436f-8b13-17bc76091f95\" outputId=\"525c79a6-0902-4842-bd25-3518b5818d0c\" # Read some specific column from the data csv_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/wine_data.csv\" df = pd.read_csv(csv_path, usecols=[\"Class\", \"Alcohol\", \"Malic acid\"]) df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2a74fec9-2403-429a-9151-36f2ce595fca\" outputId=\"db605697-b94d-4806-e354-b916abb4bf69\"\ndf.shape\n</code></pre>  ## **Data Visualization**   ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"75695559-c12b-43c4-8192-3648a7a18b5b\" outputId=\"a19c21d5-46bc-4382-cc99-3befd9fde03a\" # Creating probabilty density plots of the data fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))  sns.kdeplot(data=df[\"Alcohol\"], ax=ax1, c=\"red\", label=\"Alcohol\") ax1.set_title(\"Probability Distribution of Alcohol\") ax1.legend()  sns.kdeplot(data=df[\"Malic acid\"], ax=ax2, c=\"blue\", label=\"Malic acid\") ax2.set_title(\"Probability Distribution of Malic acid\") ax2.legend()  plt.show() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 468} id=\"0f2a3aa2-b018-46fd-b450-a3cb4418bb7b\" outputId=\"484f74ad-13f8-46e1-cb03-7cd1df62b597\"\n# Creating a scatter plot of the data\nsns.scatterplot(x=df[\"Alcohol\"], y=df[\"Malic acid\"], hue=df[\"Class\"], palette=[\"red\", \"blue\", \"green\"])\n</code></pre>  ## **Train Test Split**   ```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"d7bbdbed-83ed-40ee-99a1-67763bc3c900\" outputId=\"65dc23ec-b032-40f7-9d9e-0c9a923207b8\" X_train, X_test, y_train, y_test = train_test_split(df.drop(\"Class\", axis=1),                                                     df[\"Class\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>&lt;!-- #region id=\"0cda6d24-1504-4538-9d96-909f838dacf2\" --&gt;\n## **MinMax Scaler**\nThe Min-Max Scaler is a feature scaling technique used in machine learning to transform and normalize the values of features within a specific range. It is particularly useful when dealing with features that have varying scales, as it helps ensure that all features contribute equally to the learning process. Min-Max scaling transforms the original values of features to a specified range, typically between 0 and 1.\n&lt;!-- #endregion --&gt;\n\n```python id=\"365d98cf-a27a-4bd8-b1e1-ae22da20479b\"\n# Creating a MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit the scaler to the train set, it will learn the parameters\nscaler.fit(X_train)\n\n# Transform train and test sets\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>  ```python id=\"6ee2db16-6b58-42e5-bff4-806cb84424d8\" # transform always returns a numpy array # Converting the scaled numpy arrays into pandas dataframes X_train_scaled = pd.DataFrame(data=X_train_scaled, columns=[\"Alcohol\", \"Malic acid\"]) X_test_scaled = pd.DataFrame(data=X_test_scaled, columns=[\"Alcohol\", \"Malic acid\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"cKl8RjDWijtV\" outputId=\"329e14d2-3648-4225-c6b5-f848a741b821\"\nX_train.head()\n</code></pre>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"b895afde-0b26-4073-b83e-871af4ecbc89\" outputId=\"bd7306e0-0e76-405e-f516-38d4976f741b\" X_train_scaled.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"32e2c777-3ba7-4a3b-9eaa-434070cf0dca\" outputId=\"4510c793-6220-44ac-916a-58516ad20a92\"\n# Describe the training data\nnp.round(X_train.describe(), 2)\n</code></pre>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 300} id=\"93541294-b3f9-49b0-86c8-b63d51a74077\" outputId=\"d1324f70-86d4-431c-8799-13f7bdc383f3\" # Describe the scaled training data np.round(X_train_scaled.describe(), 2) <pre><code>&lt;!-- #region id=\"b22a40e5-e280-4658-a955-fa6da7f31925\" --&gt;\n## **Effect of Scaling**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"fa5097dd-b418-424d-a8a8-cde1931345c1\" outputId=\"d09c531a-d58c-4d7f-b5d4-da07ced2149b\"\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a scatter plot of the training data\nsns.scatterplot(data=X_train, x=\"Alcohol\", y=\"Malic acid\", ax=ax1, c=y_train)\nax1.set_title(\"Before Scaling\")\n\n# Creating a scatter plot of the scaled training data\nsns.scatterplot(data=X_train_scaled, x=\"Alcohol\", y=\"Malic acid\", ax=ax2, c=y_train)\nax2.set_title(\"After Scaling\")\n\nplt.show()\n</code></pre>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 468} id=\"b333b729-bb11-4537-8d1d-66fa49d2c757\" outputId=\"ade9f8f7-397a-4d89-c247-7edc05358a8f\" fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))  # Creating a probability density plot of the training data sns.kdeplot(data=X_train[\"Alcohol\"], ax=ax1, label=\"Alcohol\") sns.kdeplot(data=X_train[\"Malic acid\"], ax=ax1, label=\"Malic acid\") ax1.legend() ax1.set_xlabel(\"\") ax1.set_title(\"Before Scaling\")  # Creating a probability density plot of the scaled training data sns.kdeplot(data=X_train_scaled[\"Alcohol\"], ax=ax2, label=\"Alcohol\") sns.kdeplot(data=X_train_scaled[\"Malic acid\"], ax=ax2, label=\"Malic acid\") ax2.legend() ax2.set_xlabel(\"\") ax2.set_title(\"After Scaling\")  plt.show() <pre><code>&lt;!-- #region id=\"f558e0ca-45b9-4307-872f-a94775e1a148\" --&gt;\n## **Comparison of Distribution**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"8513abf7-2cd2-461c-b088-65610d0fc78b\" outputId=\"e0f0273e-b09c-4da8-bebe-db0100c07b6a\"\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# Creating a probabilty density plot of the 'Alcohol' column from the training data\nsns.kdeplot(X_train[\"Alcohol\"], ax=ax1)\nax1.set_title(\"Alcohol Distribution before Scaling\")\n\n# Creating a probabilty density plot of the 'Alcohol' column from the scaled training data\nsns.kdeplot(X_train_scaled['Alcohol'], ax=ax2)\nax2.set_title(\"Alcohol Distribution after Scaling\")\n\nplt.show()\n</code></pre>  ```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 487} id=\"738bf89c-356b-4bf5-b8f6-3dbae8032b50\" outputId=\"a0b517f6-9609-49fc-bf37-f6283255419a\" fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))  # Creating a probabilty density plot of the 'Malic acid' column from the training data sns.kdeplot(X_train[\"Malic acid\"], ax=ax1) ax1.set_title(\"Malic acid Distribution before Scaling\")  # Creating a probabilty density plot of the 'Malic acid' column from the scaled training data sns.kdeplot(X_train_scaled['Malic acid'], ax=ax2) ax2.set_title(\"Malic acid Distribution after Scaling\")  plt.show() ```   \ud83e\udd14 **Note:** **Difference between Standardization and Normalization** Standardization and normalization are two common techniques for feature scaling in data preprocessing, and they have different approaches and effects on the data:  **Goal:** * **Standardization:** The goal of standardization is to transform the data in such a way that it has a mean of 0 and a standard deviation of 1. It centers the data around zero and scales it by the standard deviation.  * **Normalization:** The goal of normalization is to rescale the data to a specific range, typically between 0 and 1. It preserves the relative relationships between data points but scales them to fit within the chosen range."},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/","title":"Encoding Categorical Data","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#encoding-categorical-data","title":"Encoding Categorical Data","text":"<p>Encoding categorical data is an essential step in preparing data for machine learning models since most machine learning algorithms require numerical input data. Categorical data represents non-numeric data such as categories, labels, or classes.</p> <p>In Python, you can use various techniques to encode categorical data, and the choice of encoding method depends on the nature of your data and the machine learning algorithm you plan to use.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7q6G4UrQXyft\" outputId=\"2e6e164c-3199-4dc5-efd2-8e70b2469090\" from google.colab import drive drive.mount(\"/content/drive/\") <pre><code>```python id=\"9b663240-d39a-4652-a6db-e339c66c790e\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#read-the-data","title":"Read the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"4c0911b0-638c-41e8-ac84-ec1eb31653c1\" outputId=\"98d3d3cd-d0fa-44be-a917-7e316952a862\" df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/customer.csv\") df.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7279d5bc-1143-46ed-8202-31e1aae8a05a\" outputId=\"d1f05fea-1eb4-4db6-9d46-6289b957c40d\"\ndf.shape\n</code></pre></p> <p>```python id=\"c0442293-408a-4647-9233-f37e9ef4015d\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#extrcting-the-review-education-and-purchased-colums-from-the-dataframe","title":"Extrcting the 'review', 'education' and 'purchased' colums from the dataframe","text":"<p>df = df.iloc[:, 2:] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"7a598d4d-0988-4b21-82ad-9a5b193a823a\" outputId=\"b22a1f77-b9d6-4949-ae36-3bd3fbd2d377\"\ndf.head()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#train-test-split","title":"Train Test Split","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"c2ffb602-1945-4019-9b35-6f7f74330029\" outputId=\"2f0c4d23-3735-4016-9f4d-a780da205b88\" X_train, X_test, y_train, y_test = train_test_split(df.drop(\"purchased\", axis=1),                                                     df[\"purchased\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>&lt;!-- #region id=\"3b728b3e-63a3-4e58-9f6f-e61a7ecaeb9f\" --&gt;\n## **Ordinal Encoding**\nOrdinal encoding is a technique for encoding categorical data where the categories have a meaningful order or ranking. This method assigns a unique integer value to each category based on its order or priority. Ordinal encoding is appropriate when the categorical data represents ordered or ranked values, such as \"low,\" \"medium,\" and \"high\" or \"small,\" \"medium,\" \"large.\"\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"T7vU07eKZG97\" --&gt;\n&lt;center&gt;&lt;img src=\"https://miro.medium.com/v2/resize:fit:654/1*NUzgzszTdpLPZpeKPPf0kQ.png\" width=\"40%\"&gt;&lt;/center&gt;\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"GtGQLmodZn2R\" outputId=\"c45e4e59-c7c4-4b63-ed26-7ea3da8433f0\"\n# Checking the unique values in each column\nprint(\"Unique values in each column:\")\nfor col in df.columns:\n    print(f\"{col}: {df[col].unique()}\")\n</code></pre></p> <p>```python id=\"c4354c9b-627b-4a8f-8b83-272c7fa33200\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#creating-an-object-of-ordinal-encoder-class","title":"Creating an object of ordinal encoder class","text":"<p>ordinal_encoder = OrdinalEncoder(categories=[[\"Poor\", \"Average\", \"Good\"], [\"School\", \"UG\", \"PG\"]],                                  dtype=np.int8)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#fit-the-training-data","title":"Fit the training data","text":"<p>ordinal_encoder.fit(X_train)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#transform-the-training-and-testing-data","title":"Transform the training and testing data","text":"<p>X_train_encoded = ordinal_encoder.transform(X_train) X_test_encoded = ordinal_encoder.transform(X_test) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"de9f248e-da76-48ed-af6d-e700173f563e\" outputId=\"c0fe784a-d126-4e62-9680-05a38ac638f8\"\nordinal_encoder.categories_\n</code></pre></p> <p>```python id=\"92c263a9-674e-4f3f-a452-8d4d2cbd8a2f\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#converting-the-encoded-array-into-pandas-dataframe","title":"Converting the encoded array into pandas dataframe","text":"<p>X_train_encoded = pd.DataFrame(X_train_encoded, columns=[\"review\", \"education\"]) X_test_encoded = pd.DataFrame(X_test_encoded, columns=[\"review\", \"eucation\"]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"75ab867f-a3f6-4bbe-9af7-b948f2df3729\" outputId=\"b47e511c-bd46-4a18-90f8-461a72b0e1e4\"\n# Print the non-encoded training data\nX_train.head()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"27495f5c-4ebb-4322-81d5-9f12f66cccc0\" outputId=\"a7bc83b6-4b65-495e-addf-94b9e1d0b10c\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#print-the-encoded-training-data","title":"Print the encoded training data","text":"<p>X_train_encoded.head() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} id=\"0ef4bab4-ae34-4035-a73d-cf5c2b358ac8\" outputId=\"eea2fc6f-ad3d-4a80-db58-7091520b1c74\"\n# Print the encoded testing data\nX_test_encoded.head()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#label-encoding","title":"Label Encoding","text":"<p>Label encoding is a technique for encoding categorical data into numerical values, where each category is assigned a unique integer label. This encoding is suitable for categorical data where there is no inherent order or ranking among the categories.</p> <p>You can use the <code>LabelEncoder</code> class from the sklearn.preprocessing module to perform label encoding. This encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y and not the input X.</p> <p>```python id=\"f4e0c0e3-2914-4c62-9ee6-2f9bdd136577\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#creating-an-object-of-the-label-encoder-class","title":"Creating an object of the label encoder class","text":"<p>label_encoder = LabelEncoder()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#fit-the-training-data_1","title":"Fit the training data","text":"<p>label_encoder.fit(y_train)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#transform-the-training-and-testing-data_1","title":"Transform the training and testing data","text":"<p>y_train_encoded = label_encoder.transform(y_train) y_test_encoded = label_encoder.transform(y_test) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"8a3f0535-f4cd-4459-8972-1432ad01f1b2\" outputId=\"2c6b4a59-5c3a-477d-c0bc-907c80406b4a\"\nlabel_encoder.classes_\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"e05df3bf-0702-4a0f-92c2-7edf97dc595c\" outputId=\"bb996a70-950e-46d1-c8d2-240bf5e80879\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#print-the-y_train-data","title":"Print the y_train data","text":"<p>y_train.head(10) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"88a79a59-c49f-44b9-a376-583f2988e617\" outputId=\"3f42f60e-a6d1-4d22-8e35-48e523e57266\"\n# Print the y_train_encoded data\ny_train_encoded\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"c179ac7c-6114-4800-b092-7c3a098a37fc\" outputId=\"862eae9e-6cf6-48ba-bfbc-44e4a4e99b09\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/02_Encoding_Categorical_Data/#print-the-y_test_encoded-data","title":"Print the y_test_encoded data","text":"<p>y_test_encoded ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/","title":"One Hot Encoding","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding","title":"One Hot Encoding","text":"<p>One-Hot Encoding is a popular technique used in machine learning and data preprocessing, especially when dealing with categorical data. It is used to represent categorical variables as binary vectors or matrices, where each category is mapped to a unique binary value.</p> <p>This transformation is necessary because many machine learning algorithms and models require numerical input, and categorical data in its raw form cannot be directly used in these algorithms.</p> <p></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"vywY7cnJfjE2\" outputId=\"904d28da-cf60-48fd-a550-9b53317b744d\" from google.colab import drive drive.mount(\"/content/drive/\") <pre><code>```python id=\"8df890ce-c77a-4a58-aa7f-cbe26712a7e6\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#read-the-data","title":"Read the Data","text":"<p>```python id=\"5ecc074a-8716-404b-93a0-0e56f39cd863\" outputId=\"3ff06609-9786-43b3-d0d8-b5914e290e18\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/cars.csv\") df.head() <pre><code>```python id=\"3d98f302-de65-467a-ac5a-28235fd04d5c\" outputId=\"2a5e53a0-1868-49e3-ea5e-dbcc35d03c7a\" colab={\"base_uri\": \"https://localhost:8080/\"}\ndf.shape\n</code></pre></p> <p>```python id=\"8d7fc924-1386-481f-88a4-12f742a88cb4\" outputId=\"f4229462-69bc-4c66-ed7f-e2f17a40c019\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#check-the-number-of-unique-brand-names","title":"Check the number of unique brand names","text":"<p>df[\"brand\"].nunique() <pre><code>```python id=\"4f084779-4a43-4e4b-ae62-73a5a2aa8d46\" outputId=\"b1fc186f-a787-4e26-a112-33758dae8ce0\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Count the values for each brand in 'brand' column\ndf[\"brand\"].value_counts()\n</code></pre></p> <p>```python id=\"1e2ea2a1-a7da-440e-aaf9-ea9f114b64b7\" outputId=\"1b8b131e-a6f1-433e-824c-bee284a41caa\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#count-the-values-for-each-unique-name-in-fuel-column","title":"Count the values for each unique name in 'fuel' column","text":"<p>df[\"fuel\"].value_counts() <pre><code>```python id=\"0fa49577-16e8-4650-906b-8d93a9ddbcba\" outputId=\"3f828a5d-856b-4858-e3f6-05fa6aafd018\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Count the values for each unique name in 'owner' column\ndf[\"owner\"].value_counts()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding-with-pandas","title":"One Hot Encoding with Pandas","text":"<p>```python id=\"6294b599-a14c-46d1-9669-0e975421e300\" outputId=\"aee4fc44-4896-4c95-844d-18635d8a38ff\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 478}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#applying-one-hot-encoding-on-fuel-and-owner-columns","title":"Applying One Hot Encoding on 'fuel' and 'owner' columns","text":"<p>pd.get_dummies(data=df, columns=[\"fuel\", \"owner\"]) <pre><code>&lt;!-- #region id=\"68f66cd2-69c2-4284-bafc-e1993c5d0fee\" --&gt;\n## **K-1 One Hot Encoding with Pandas**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"8106db49-aec3-4092-b4fa-617611c66faa\" --&gt;\nWhen using the `pd.get_dummies()` function in Pandas, you can drop the first category (column) of each categorical variable to avoid multicollinearity, which can be useful in certain situations. This is done using the `drop_first` parameter. Setting `drop_first=True` will drop the first category from each categorical variable after one-hot encoding.\n&lt;!-- #endregion --&gt;\n\n```python id=\"54d231b2-7b77-4c65-81f5-283f6d828955\" outputId=\"8baf4391-e7d2-467d-ba30-eadeaacc2ba5\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 441}\n# Applying One Hot Encoding on 'fuel' and 'owner' columns\n# Removing the first categorical variable to avoid multicolinearity\npd.get_dummies(data=df, columns=[\"fuel\", \"owner\"], drop_first=True)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#one-hot-encoding-using-sklearn","title":"One Hot Encoding using Sklearn","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#train-test-split","title":"Train Test Split","text":"<p>```python id=\"b34cc2a2-7692-4c3c-8420-0a2d9f69181a\" outputId=\"11c067ad-8f9b-4466-c12a-0a20bf2071b9\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#print-the-dataframe","title":"Print the dataframe","text":"<p>df.head() <pre><code>```python id=\"d5d1d1f0-21f4-49a0-a2bd-ffc47e1e9e8b\" outputId=\"2da27c1a-6ec4-48bc-ac3c-4e17f86c450e\" colab={\"base_uri\": \"https://localhost:8080/\"}\nX_train, X_test, y_train, y_test = train_test_split(df.drop(\"selling_price\", axis=1),\n                                                    df[\"selling_price\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, y_train.shape\n</code></pre></p> <p>```python id=\"16f19237-de20-4b22-840e-c26e6f1d1d84\" outputId=\"47dfdab0-40e2-4802-bb25-ffda83530463\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} X_train.head() <pre><code>```python id=\"98044aa4-1fbb-4bd5-930d-3229f20f2423\" outputId=\"bb8d51a4-69be-492e-98ad-41b0f4bf58f3\" colab={\"base_uri\": \"https://localhost:8080/\"}\nX_train.shape\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#apply-ohe-on-fuel-and-owner-columns","title":"Apply OHE on 'fuel' and 'owner' Columns","text":"<p>\ud83e\udd14 Note: The <code>sparse_output</code> parameter in the <code>OneHotEncoder</code> controls the format of the output matrix. When <code>sparse_output</code> is set to <code>True</code>, the output matrix will be represented as a sparse matrix (SciPy sparse matrix format), which is a memory-efficient way to store matrices with a large number of zero values. This can be beneficial when dealing with high-dimensional one-hot encoded matrices, as it saves memory compared to using a dense matrix representation. On the other hand, when <code>sparse_output</code> is set to <code>False</code> (the default), the output matrix will be a dense NumPy array.</p> <p>```python id=\"7af538f2-16c0-4cd9-92d1-fc75e7d92c30\" outputId=\"998893dd-0bde-45b2-f239-11e93c33a311\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#creating-an-object-of-the-one-hot-encode-class","title":"Creating an object of the One Hot Encode class","text":"<p>one_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False, dtype=np.int8)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#separating-the-fuel-and-owner-columns-from-the-x_train-dataframe","title":"Separating the 'fuel' and 'owner' columns from the X_train dataframe","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#fit-the-separated-training-data","title":"Fit the separated training data","text":"<p>one_hot_encoder.fit(X_train[[\"fuel\", \"owner\"]])</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#transform-the-separated-training-data","title":"Transform the separated training data","text":"<p>X_train_encoded = one_hot_encoder.transform(X_train[[\"fuel\", \"owner\"]]) X_train_encoded <pre><code>```python id=\"c9c9a5e4-3584-4786-8450-f0e26964cbb8\" outputId=\"15edf17e-dd7c-4aa8-ea8b-fad7d8fdaf2d\" colab={\"base_uri\": \"https://localhost:8080/\"}\nX_train_encoded.shape\n</code></pre></p> <p>```python id=\"56c09bbd-14fd-420f-9ecb-07656f0697b0\" outputId=\"d228d914-5afa-493b-c639-4a0d72eee8a5\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#merge-the-x_train_encoded-columns-with-the-brand-and-km_driven-columns","title":"Merge the X_train_encoded columns with the 'brand' and 'km_driven' columns","text":"<p>X_train_merged = np.hstack((X_train[[\"brand\", \"km_driven\"]], X_train_encoded)) X_train_merged <pre><code>```python id=\"bfd54d97-10f0-4c3a-99ec-80c13a79cdb7\" outputId=\"f22d45f0-ffd0-462a-ba51-ed8d44d71eb7\" colab={\"base_uri\": \"https://localhost:8080/\"}\nX_train_merged.shape\n</code></pre></p> <p>```python id=\"a1e833ce-9ac8-4b8e-abbe-af968a4c248b\" outputId=\"98260aa8-185c-45a9-b95d-3d2562d20086\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#print-the-column-names-of-the-encoded-x_train-data","title":"Print the column names of the encoded x_train data","text":"<p>one_hot_encoder.get_feature_names_out() <pre><code>```python id=\"3dfcee27-04be-47ec-8167-e9fcfe8d1611\" outputId=\"ef06429f-b9a2-4bc2-f93c-3c94ba0c27f5\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Define the column names in an array\ncolumn_names = np.concatenate((X_train.columns[0:2], one_hot_encoder.get_feature_names_out()), axis=0)\nprint(len(column_names))\ncolumn_names\n</code></pre></p> <p>```python id=\"9c91d3b0-07e1-4897-9e39-a0ba836780cb\" outputId=\"4aedf6da-7f34-482e-8510-504bee5848cc\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 441}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#convert-the-x_train_merged-array-into-pandas-dataframe","title":"Convert the x_train_merged array into pandas dataframe","text":"<p>X_train_encoded = pd.DataFrame(data=X_train_merged, columns=column_names) X_train_encoded <pre><code>```python id=\"de07eec4-e6ec-4b03-9fdb-2e60d34204c1\" outputId=\"b9f07f80-47bd-4aaf-ad40-d4b732db111e\" colab={\"base_uri\": \"https://localhost:8080/\"}\nX_train_encoded.shape\n</code></pre></p> <p>```python id=\"7c94761a-6567-4350-a2e4-5286efef176a\" outputId=\"2f516e6e-3cbe-40a2-9a72-11bde6342c4f\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#print-the-x_test-data","title":"Print the x_test data","text":"<p>X_test.head() <pre><code>```python id=\"c5639c46-4bc4-43cf-ab73-4677391bc99e\" outputId=\"7251cb8a-03f5-4485-9b11-d1e04f665ec6\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Encode x_test data\nX_test_encoded = one_hot_encoder.transform(X_test[[\"fuel\", \"owner\"]])\nX_test_encoded\n</code></pre></p> <p>```python id=\"d7c02894-d6c8-4f86-aa7b-6e5ab4a33c2d\" outputId=\"2dcb6c68-9aac-4721-fe32-52019580e9e7\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#merge-the-x_test_encoded-columns-with-the-brand-and-km_driven-columns","title":"Merge the x_test_encoded columns with the 'brand' and 'km_driven' columns","text":"<p>X_test_merged = np.hstack((X_test.iloc[:, 0:2], X_test_encoded)) X_test_merged <pre><code>```python id=\"6f6d04cc-35f1-4150-8f7d-42e600151da6\" outputId=\"986b8b4e-5385-42ea-988c-2cb47caef826\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 441}\n# Convert the x_test_merged array into pandas dataframe\nX_test_encoded = pd.DataFrame(data=X_test_merged, columns=column_names)\nX_test_encoded\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#apply-ohe-on-brand-column-using-pandas","title":"Apply OHE on 'brand' Column using Pandas","text":"<p>```python id=\"0a01b063-da22-4738-acf1-60a05856c66d\" outputId=\"c73fd51d-db7f-4dec-9e9a-4993de415991\" colab={\"base_uri\": \"https://localhost:8080/\"}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#count-the-values-for-each-brand-in-brand-column","title":"Count the values for each brand in 'brand' column","text":"<p>counts = df[\"brand\"].value_counts() counts <pre><code>```python id=\"be7a1463-81ff-4d9e-af2e-ccf554bdb20e\" outputId=\"5541518a-b922-4ec8-c265-5a5e5e982a1c\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check the total number of unique brands\ndf[\"brand\"].nunique()\n</code></pre></p> <p>```python id=\"8ba47626-20ed-433a-9f47-ca4e0596ba14\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#define-a-threshold","title":"Define a threshold","text":"<p>threshold = 100 <pre><code>```python id=\"6a186820-a8f7-4229-aed9-c11c43fb902a\" outputId=\"a7c61f7e-78e4-47ab-cfe2-2a469f10c529\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Store the name of brands in a list where the value count is less than 100\nrepl = counts[counts &lt;= threshold].index\nrepl\n</code></pre></p> <p>```python id=\"f4402431-aa57-458f-93fe-bc6505ed6dba\" outputId=\"c61ec684-f221-44ec-8932-8f5ea22de056\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#replace-the-name-of-the-brand-with-others","title":"Replace the name of the brand with 'others'","text":"<p>new_df = df.replace(to_replace=repl, value=\"Others\") new_df <pre><code>```python id=\"76b41c92-08a4-4d7c-869e-6a4d1fe4ab2a\" outputId=\"e3c2da61-e496-460e-f4af-078bfe04244a\" colab={\"base_uri\": \"https://localhost:8080/\"}\nnew_df[\"brand\"].value_counts()\n</code></pre></p> <p>```python id=\"896eb779-396f-4d94-8642-4052dd760309\" outputId=\"6fef356f-8d21-4598-f9b5-389ba4931259\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 676}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/03_One_Hot_Encoding/#apply-ohe-on-brand-column-of-the-new-dataframe","title":"Apply OHE on 'brand' column of the new dataframe","text":"<p>pd.get_dummies(data=new_df[\"brand\"]).sample(20) ```</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/","title":"Column Transformer","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#column-transformer","title":"Column Transformer","text":"<p>The ColumnTransformer is a feature in scikit-learn, a popular Python machine learning library, that allows you to apply different preprocessing steps to different subsets of the columns (features) in your dataset. It is particularly useful when you have a dataset with a mix of numerical and categorical features, and you want to apply different transformations to these feature types.</p> <p>Here's an overview of how the ColumnTransformer works:</p> <ol> <li> <p>Specify Transformers: First, you define a list of transformers, where each transformer specifies a particular preprocessing step to be applied to a subset of the columns. For example, you might have one transformer for numerical columns (e.g., scaling), another for categorical columns (e.g., one-hot encoding), and maybe even other transformers for specific subsets of columns.</p> </li> <li> <p>Specify Columns: For each transformer, you also specify which columns it should be applied to. This is done using the columns parameter, where you can specify either column indices or column names.</p> </li> <li> <p>Combine Transformers: You create a ColumnTransformer object and pass in the list of transformers. You can also specify what to do with the remaining columns that are not specified in any of the transformers, using the remainder parameter. Options include dropping them or passing them through without any transformation.</p> </li> <li> <p>Fit and Transform: You can then fit the ColumnTransformer on your dataset using the fit method, and subsequently transform your dataset using the transform method. The ColumnTransformer applies the specified transformations to the designated columns and returns a transformed dataset.</p> </li> </ol>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cef0fd0d-7536-4eb0-978b-7323a5417ee9\" outputId=\"56adefd9-b6a3-44c7-dd64-cd6b3993b426\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"e6d0edce-9ca7-47a1-b4ac-efe9ae7c1352\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#read-the-data","title":"Read the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"a208816c-5f6e-4b1c-98f2-2188ffb7bcd2\" outputId=\"43109656-d869-4796-f8e9-95b6cb211e49\" df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/covid_toy.csv\") df <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"453d72e1-778d-471c-b001-dab4a238cafd\" outputId=\"2fb127d0-a594-487b-8b23-18ea6d6758fa\"\n# Check the information of the columns\ndf.info()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"c37008a3-8bea-4c9b-a763-b7a82dfda3ed\" outputId=\"cf0da5d6-0704-426e-bedc-ee158fc70c34\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#check-the-number-of-null-values-in-each-column","title":"Check the number of null values in each column","text":"<p>df.isnull().sum() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"4d5b0d2b-c706-456b-85ab-5a1992708e81\" outputId=\"2efdf8f1-8b25-4a7a-dda1-6948fe7328a2\"\n# Check all the unique values of the categorical columns\nfor column in df.select_dtypes(include=\"object\").columns:\n    unique_values = df[column].unique()\n    print(f\"{column}: {unique_values}\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#preprocessing-without-columntransformer","title":"Preprocessing without <code>ColumnTransformer</code>","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#train-test-split","title":"Train Test Split","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"4ed2d3e7-0f3c-43cb-8479-eacf4a2c688b\" outputId=\"1777e94a-5c44-4abe-ec80-9568d7915b88\" X_train, X_test, y_train, y_test = train_test_split(df.drop(\"has_covid\", axis=1),                                                     df[\"has_covid\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 363} id=\"e9e080a0-7f73-42f2-af69-0443b6a60931\" outputId=\"959167dd-8ba1-42f0-b506-d9256c2cc416\"\nX_train.head(10)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#fill-the-null-values-of-fever-column-using-simpleimputer","title":"Fill the Null Values of <code>fever</code> Column using <code>SimpleImputer</code>","text":"<ul> <li>SimpleImputer:  It is an univariate imputer for filling missing values with simple strategies. It replaces missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value.</li> </ul> <p>```python id=\"f13c1455-6d3e-42e7-9ed9-c3404ae9c0ba\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#create-a-simpleimputer-object","title":"Create a SimpleImputer object","text":"<p>simple_imputer = SimpleImputer(strategy='mean')</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#fit-the-fever-column-of-the-training-data","title":"Fit the 'fever' column of the training data","text":"<p>simple_imputer.fit(X_train[[\"fever\"]])</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#transform-the-fever-column-of-the-training-and-testing-data","title":"Transform the 'fever' column of the training and testing data","text":"<p>X_train_fever = simple_imputer.transform(X_train[[\"fever\"]]) X_test_fever = simple_imputer.transform(X_test[[\"fever\"]]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"655d6e9c-374c-4605-b825-95dafb9f3a80\" outputId=\"ef0116d9-b4bf-490f-97ec-245182ef2a99\"\n# Print the first ten values of the x_train_fever\nX_train_fever[:10]\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#apply-ordinalencdoer-to-cough-column","title":"Apply <code>OrdinalEncdoer</code> to <code>cough</code> Column","text":"<p>```python id=\"7bdca428-4b99-4b01-957d-a2a4c1263846\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#create-an-object-of-the-ordinalencoder","title":"Create an object of the OrdinalEncoder","text":"<p>ordinal_encoder = OrdinalEncoder(categories=[[\"Mild\", \"Strong\"]], dtype=int)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#fit-the-cough-column-of-the-training-data","title":"Fit the 'cough' column of the training data","text":"<p>ordinal_encoder.fit(X_train[[\"cough\"]])</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#transform-the-cough-column-of-the-training-and-testing-data","title":"Transform the 'cough' column of the training and testing data","text":"<p>X_train_cough = ordinal_encoder.transform(X_train[[\"cough\"]]) X_test_cough = ordinal_encoder.transform(X_test[[\"cough\"]]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"412fb5d2-8c09-4160-a684-1250d01d5fa1\" outputId=\"fd3c79bb-9c1f-429d-cc5b-c9b55b2fff28\"\n# Print the first ten values of the x_train_cough\nX_train_cough[:10]\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#apply-onehotencdoer-to-gender-and-city-columns","title":"Apply <code>OneHotEncdoer</code> to <code>gender</code> and <code>city</code> Columns","text":"<p>```python id=\"04163d54-e874-4a67-bece-fe6af0a45c97\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#create-an-object-of-the-onehotencoder","title":"Create an object of the OneHotencoder","text":"<p>one_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False, dtype=int)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#fit-the-genedr-and-city-columns-of-the-training-data","title":"Fit the 'genedr' and 'city' columns of the training data","text":"<p>one_hot_encoder.fit(X_train[[\"gender\", \"city\"]])</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#transform-the-genedr-and-city-columns-of-the-training-and-testing-data","title":"Transform the 'genedr' and 'city' columns of the training and testing data","text":"<p>X_train_gender_city = one_hot_encoder.transform(X_train[[\"gender\", \"city\"]]) X_test_gender_city = one_hot_encoder.transform(X_test[[\"gender\", \"city\"]]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"2a57d9c3-e81a-4848-9a3f-c661f5110c69\" outputId=\"38d0a748-838d-4a8c-e3aa-090b300b1055\"\n# Check the new column names after applying One Hot Encoding\none_hot_encoder.get_feature_names_out()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1592c5ac-1b95-46df-899a-04fd7c68c211\" outputId=\"c540245b-06eb-490d-89f9-a38cc1bd2aaa\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#print-the-first-ten-values-of-the-x_train_gender_city","title":"Print the first ten values of the x_train_gender_city","text":"<p>X_train_gender_city[:10] <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"6ed46656-b7af-4e36-9764-c7338758d148\" outputId=\"1a62d012-0053-4f18-e5f8-eaf2b4cbd59b\"\nX_train_cough.shape\n</code></pre></p> <p>```python id=\"82b8cbd3-ff65-47e8-9a91-1ab8e6857925\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#convert-the-age-column-into-numpy-array","title":"Convert the 'age' column into numpy array","text":"<p>X_train_age = np.array(X_train[\"age\"]).reshape((70, 1)) X_test_age = np.array(X_test[\"age\"]).reshape((30, 1)) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7f2962fd-4c3c-4b4c-a174-85c92aafae24\" outputId=\"ab6a4655-183b-4e99-ca71-823d12aad734\"\n# Print the first ten values of the x_train_age\nX_train_age[:10]\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#concatenating-all-the-arrays-for-the-training-and-testing-data","title":"Concatenating all the Arrays for the Training and Testing Data","text":"<p>```python id=\"6bbc7b84-3409-426a-992a-bfe249780de7\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#concatenating-all-the-columns-of-the-training-data","title":"Concatenating all the columns of the training data","text":"<p>X_train_transformed = np.concatenate((X_train_age, X_train_fever, X_train_cough, X_train_gender_city), axis=1)</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#concatenating-all-the-columns-of-the-training-data_1","title":"Concatenating all the columns of the training data","text":"<p>X_test_transformed = np.concatenate((X_test_age, X_test_fever, X_test_cough, X_test_gender_city), axis=1) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"30eb309f-e6fc-4a75-b484-451dcb931574\" outputId=\"d3444b87-1b17-4498-d4cf-32bb8a1a79e1\"\n# Defining the column names of the transformed dataframe\ncolumn_names = np.concatenate((np.array([\"age\", \"fever\", \"cough\"]), one_hot_encoder.get_feature_names_out()))\ncolumn_names\n</code></pre></p> <p>```python id=\"d3801242-6f64-4a33-80c7-4155936b4795\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#convert-transformed-data-into-pandas-dataframe","title":"Convert transformed data into pandas dataframe","text":"<p>X_train_transformed = pd.DataFrame(X_train_transformed, columns=column_names) X_test_transformed = pd.DataFrame(X_test_transformed, columns=column_names) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"10c2297c-8c9e-42d2-a715-94005c20a524\" outputId=\"e7333907-9f54-4aac-9a90-b526633b16f2\"\n# Print the transformed data\nX_train_transformed\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"5f36b93a-2b39-444e-9294-e217acee8bec\" outputId=\"e3950e72-a145-454b-9507-e9302f9bb60d\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#print-the-information-of-the-transformed-training-data","title":"Print the information of the transformed training data","text":"<p>X_train_transformed.info() <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 363} id=\"3627419d-67ba-485a-b5f3-ea4474d4b41b\" outputId=\"ab4bfec8-28e2-4409-8386-807ef05f7327\"\nX_test_transformed.head(10)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"a73ba1c6-0e98-426e-82e5-0a529fa58836\" outputId=\"354b0230-5408-425a-8298-e3c189eed9f4\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#print-the-information-of-the-transformed-testing-data","title":"Print the information of the transformed testing data","text":"<p>X_test_transformed.info() <pre><code>&lt;!-- #region id=\"3acd212e-c256-4508-a2a2-ab45ff822808\" --&gt;\n## **Preprocessing with `ColumnTransformer`**\n&lt;!-- #endregion --&gt;\n\n```python id=\"08d4ed97-54cb-4598-84f6-953be18e6ff8\"\n# Create an object of the ColumnTransformer\ntransformer = ColumnTransformer(transformers=[\n    (\"tranformer_1\", SimpleImputer(strategy='mean'), [\"fever\"]),\n    (\"transformer_2\", OrdinalEncoder(categories=[[\"Mild\", \"Strong\"]]), [\"cough\"]),\n    (\"transformer_3\", OneHotEncoder(drop=\"first\", sparse_output=False), [\"gender\", \"city\"])\n], remainder=\"passthrough\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"184d6720-0d29-4f25-8b80-c3d781c56d64\" outputId=\"f900c279-7480-47ac-c3af-45a48fba22ba\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#fit-and-transform-the-training-data","title":"Fit and transform the training data","text":"<p>X_train_transformed = transformer.fit_transform(X_train) X_train_transformed.shape <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"3f70033f-bb03-42c1-8c5c-9fa58a242bd2\" outputId=\"15664858-6ee5-4182-8c70-dfb465c53ee8\"\n# Transform the testing data\nX_test_transformed = transformer.transform(X_test)\nX_test_transformed.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"dde17647-0287-4823-98ea-7db0360d29ec\" outputId=\"2e38af5a-42ba-44dc-cf1a-5be5826c064a\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/03_Feature_Engineering/04_Column_Transformer/#checking-the-new-column-names-of-the-transformed-data","title":"Checking the new column names of the transformed data","text":"<p>transformer.get_feature_names_out() <pre><code>```python id=\"24bec896-474e-45d7-bd75-2a27b0f1f6e7\"\n# Convert the transformed array into pandas dataframe\nX_train_transformed = pd.DataFrame(X_train_transformed, columns=transformer.get_feature_names_out())\nX_test_transformed = pd.DataFrame(X_test_transformed, columns=transformer.get_feature_names_out())\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 443} id=\"da00b5ca-cf7d-4fbd-b00d-59117e153503\" outputId=\"43397990-9efa-465a-f063-0e22613131bd\" X_train_transformed <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 383} id=\"6daba95a-d809-4653-b743-c7124b89af3a\" outputId=\"8b7f163a-ac27-433e-f79c-d2cb4ed985a4\"\nX_test_transformed.head(10)\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/","title":"Simple Linear Regression","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#simple-linear-regression","title":"Simple Linear Regression","text":"<p>Simple linear regression is a statistical method used to model the relationship between two variables, typically denoted as X and Y. It is a straightforward approach to understanding how changes in one variable (X) are associated with changes in another variable (Y). The goal of simple linear regression is to find a linear equation that best represents this relationship.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"wcGCl3-qyif1\" outputId=\"5371da08-6333-404f-c5d4-db4800074b64\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"d38d3d6b-30d7-4845-a298-6223bc76568c\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#read-the-data","title":"Read the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"76b55681-c21a-4003-b5d5-64f2353a2a4e\" outputId=\"8acc74e8-a16b-463e-c4a6-9101214ded6d\" df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Placement_SLR.csv\") df <pre><code>&lt;!-- #region id=\"0a04f3dd-91f1-4c11-b6e2-6e9e851ca22f\" --&gt;\n## **Plot the Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472} id=\"2e07ae22-b215-492d-a111-d552badc3246\" outputId=\"e59bc0e8-242e-49e3-ea1e-c8440f7c759d\"\nsns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#train-test-split","title":"Train Test Split","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"13db0206-bf93-4cd3-9237-a41c0084f9ae\" outputId=\"35e0458c-5504-4d12-ae42-017f731adca3\" X_train, X_test, y_train, y_test = train_test_split(df[\"cgpa\"],                                                     df[\"package\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape, X_test.shape <pre><code>&lt;!-- #region id=\"7772fa64-424d-4330-8374-4e41ea5af701\" --&gt;\n## **Train a Simple Linear Regression Model**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"dc710d01-3674-46ef-95a0-b283c42abeee\" outputId=\"118d97be-7c8a-4224-ee50-0843296153c3\"\n# Instantiate a LinearRegression object\nlr = LinearRegression()\n\n# Fit the training data\nlr.fit(X_train.values.reshape(140, 1), y_train)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"475dd1be-18df-4c1d-bac1-0354a3e154e1\" outputId=\"e5ba6f17-6821-45fa-bb67-6288faa49e5a\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#predict-the-test-data","title":"Predict the Test data","text":"<p>y_pred = lr.predict(X_test.values.reshape(60, 1)) y_pred <pre><code>&lt;!-- #region id=\"3f2750d9-622b-4777-9579-c0e7f2e3da4f\" --&gt;\n## **Plot the Best Fit Line**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472} id=\"89ca947a-241c-4ea0-b7b4-115d6515667d\" outputId=\"055d4bd6-39eb-4e49-d35d-f8316b2f9dd1\"\nsns.scatterplot(x=df[\"cgpa\"], y=df[\"package\"])\nsns.lineplot(x=X_train, y=lr.predict(X_train.values.reshape(140, 1)), c=\"red\",\n             label=\"Regression Line\")\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#fetch-the-slope-and-y-intercept-value","title":"Fetch the Slope and Y-intercept Value","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"f2af85e5-08c4-45ef-9b4f-3089e59ddf8a\" outputId=\"b2114144-1d78-40a2-b807-835cfcf215fb\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#extract-the-slope-value","title":"Extract the slope value","text":"<p>m = lr.coef_[0]</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/00_Simple_Linear_Regression/#extract-the-y-intercept-value","title":"Extract the y-intercept value","text":"<p>c = lr.intercept_</p> <p>print(\"Slope (m):\", m) print(\"Y-intercept (c):\", c) <pre><code>&lt;!-- #region id=\"952d63ca-10b0-4b8f-8907-7a7f438f1cd4\" --&gt;\n## **Check the RMSE**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"33e6b592-9f78-4ace-85f4-f530a0b22404\" outputId=\"31623739-cf4a-4dc3-f580-0aae0ffd6cac\"\n# Calculate the Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"Mean Squared Error (MSE):\", mse.round(2))\nprint(\"Root Mean Squared Error (RMSE):\", rmse.round(2))\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/","title":"Regression Metrics","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Regression metrics are used to evaluate the performance of predictive models that aim to estimate a continuous target variable. These metrics help assess how well the model's predictions align with the actual values, allowing you to understand the accuracy, precision, and goodness of fit of your regression model. Here are some commonly used regression metrics:</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"7XDqnfZD-AKe\" outputId=\"d178172b-c2fd-4b70-99fd-4a09b1492261\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"92fa9d73-a883-461a-be23-6ff065abd927\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#read-the-data","title":"Read the Data","text":"<p>```python id=\"0c90745f-be6c-4520-a1ef-1d1aaab8e30a\" outputId=\"a9b40ed5-1244-4c2f-d003-32796b27b8c3\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 206} df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/Mastering-Machine-Learning-and-GEE-for-Earth-Science/Datasets/Placement_SLR.csv\") df.head() <pre><code>```python id=\"71b20f08-238d-4103-8a81-0018d371d418\" outputId=\"adf8d71d-ea83-48e3-9142-af4b3117623f\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Check for the null values\ndf.isnull().sum()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#train-test-split","title":"Train Test Split","text":"<p>```python id=\"0d8d40ae-1e86-431b-bdf8-e1a047659f11\" outputId=\"a5781459-fb88-4f27-f00c-5217595aa6b3\" colab={\"base_uri\": \"https://localhost:8080/\"} X_train, X_test, y_train, y_test = train_test_split(df.drop(\"package\", axis=1),                                                     df[\"package\"],                                                     test_size=0.3,                                                     random_state=0) X_train.shape , X_test.shape <pre><code>&lt;!-- #region id=\"2efd8c32-fe7d-4674-8853-fc0008b6b88b\" --&gt;\n## **Plot the Data**\n&lt;!-- #endregion --&gt;\n\n```python id=\"26fd9ae8-e051-4c95-a84e-fb65e532ce8c\" outputId=\"3c760943-94cb-495b-bc89-fc062d3c9dcd\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472}\nsns.scatterplot(x=X_train[\"cgpa\"], y=y_train)\nplt.title(\"Scatterplot between CGPA and Package\")\nplt.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#train-a-simple-linear-regression-model","title":"Train a Simple Linear Regression Model","text":"<p>```python id=\"4df83756-2eb4-489c-b513-755c8228e9e8\" outputId=\"01fc8a15-5896-45f2-eefb-96212fe4f791\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74}</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#instantiate-an-object-of-the-linearregression-class","title":"Instantiate an object of the LinearRegression class","text":"<p>lr = LinearRegression()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#fit-the-training-data","title":"Fit the training data","text":"<p>lr.fit(X_train, y_train) <pre><code>```python id=\"ac6c7219-eddf-474c-9764-c6a2c312a95d\" outputId=\"012abb3d-44cd-4076-da64-7e6366fd09a0\" colab={\"base_uri\": \"https://localhost:8080/\"}\n# Predict the test data\ny_pred = lr.predict(X_test)\ny_pred\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#plot-the-regression-line","title":"Plot the Regression Line","text":"<p>```python id=\"866160c1-d413-46b0-a1b7-6f62ae7bd853\" outputId=\"c9a526fe-e518-44b8-990e-a0dd87fffcc1\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 472} sns.scatterplot(x=X_train[\"cgpa\"], y=y_train) sns.lineplot(x=X_test[\"cgpa\"], y=lr.predict(X_test), c=\"red\", label=\"Regression Line\") plt.title(\"Scatterplot between CGPA and Package\") plt.show() <pre><code>&lt;!-- #region id=\"bc21deeb-702b-405a-bfca-b8d739d6fb06\" --&gt;\n## **Check the Accuracy using Regression Metrics**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"4e3945c7-3068-46cd-be62-7bdc17c80740\" --&gt;\n### **Mean Absolute Error (MAE):**\nMAE is the average of the absolute differences between the predicted and actual values. It measures the average magnitude of errors and is easy to understand.\n\n**Advantages:**\n* Easy to interpret as it represents the average absolute error.\n* Resistant to outliers, as it does not square errors.\n\n**Disadvantages:**\n* Does not penalize larger errors more heavily.\n* May not work well if the error distribution is not symmetric.\n\n**Formula:**&lt;br&gt;\n&lt;center&gt;&lt;img src=\"https://editor.analyticsvidhya.com/uploads/42439Screenshot%202021-10-26%20at%209.34.08%20PM.png\" width=\"40%\"&gt;&lt;/center&gt;\n&lt;!-- #endregion --&gt;\n\n```python id=\"08dfb84d-645b-4c15-88a7-2d30096ce3d3\" outputId=\"81c25808-f665-40e3-e24e-9bbf7defd170\" colab={\"base_uri\": \"https://localhost:8080/\"}\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae.round(2))\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE):","text":"<p>MSE measures the average of the squared differences between the predicted and actual values. It gives more weight to larger errors and penalizes them.</p> <p>Advantages: * Provides a measure of how well the model performs while penalizing larger errors. * Mathematically convenient and commonly used in optimization algorithms.</p> <p>Disadvantages: * The squared nature of the metric makes it sensitive to outliers. * The units of MSE are not the same as the target variable, making it less interpretable.</p> <p>Formula:</p> <p>```python id=\"b7e7a16c-3857-40c5-a624-e53725d30e6b\" outputId=\"7b324f5a-a2e3-4166-ff4c-65799a48a92c\" colab={\"base_uri\": \"https://localhost:8080/\"} mse = mean_squared_error(y_test, y_pred) print(\"Mean Squared Error (MSE):\", mse.round(2)) <pre><code>&lt;!-- #region id=\"d37a581c-b5b4-4e52-bf46-b6fa31da7e20\" --&gt;\n### **Root Mean Squared Error (RMSE):**\nRMSE is the square root of the MSE. It provides a more interpretable metric in the same units as the target variable.\n\n**Advantages:**\n* Shares the same unit as the target variable, which makes it more interpretable than MSE.\n* Balances the sensitivity to outliers found in MSE.\n\n**Disadvantages:**\n* Like MSE, it can still be sensitive to outliers.\n* Not as intuitive as MAE.\n\n**Formula:**&lt;br&gt;\n&lt;center&gt;&lt;img src=\"https://miro.medium.com/v2/resize:fit:966/1*lqDsPkfXPGen32Uem1PTNg.png\" width=\"40%\"&gt; &lt;/center&gt;\n&lt;!-- #endregion --&gt;\n\n```python id=\"000db115-7940-4475-b8dd-274b706a9e23\" outputId=\"31558fb9-5372-4fb4-bad5-473ce0ccc718\" colab={\"base_uri\": \"https://localhost:8080/\"}\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error (RMSE):\", rmse.round(2))\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/02_Regression_Metrics/#r-squared-r2","title":"R-squared (R\u00b2):","text":"<p>R-squared measures the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, with higher values indicating a better fit.</p> <p>Advantages: * Provides a measure of goodness of fit, indicating how well the model explains the variance in the data. * Values range from 0 to 1, where higher values suggest a better fit.</p> <p>Disadvantages: * It may increase when adding more predictors, even if they are irrelevant (overfitting). * R-squared alone doesn't reveal the direction or magnitude of individual errors.</p> <p>Formula:</p> <p>```python id=\"a1f28b62-2f25-4fdc-9d0d-a6547bd72e4f\" outputId=\"6268239b-1947-48c2-f131-a77de12fddf3\" colab={\"base_uri\": \"https://localhost:8080/\"} r2 = r2_score(y_test, y_pred) print(\"R2 Score:\", r2.round(2)) <pre><code>&lt;!-- #region id=\"5e67c7ac-44b3-4361-92d1-6ecb99381664\" --&gt;\n### **Adjusted R-squared:**\nAdjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. It penalizes the addition of irrelevant predictors.\n\n**Advantages:**\n* Adjusts R-squared to account for the number of predictors, helping to mitigate overfitting.\n* Offers a more realistic assessment of model fit in multiple regression.\n\n**Disadvantages:**\n* It can still be influenced by outliers and unrepresentative samples.\n\n**Formula:**&lt;br&gt;\n&lt;center&gt;&lt;img src=\"https://i.stack.imgur.com/RcGf6.png\" width=\"50%\"&gt; &lt;/center&gt;\n&lt;!-- #endregion --&gt;\n\n```python id=\"8e5d2a56-bcc8-4603-948c-f0958f2c56c0\"\nn = len(X_test) # Total Sample Size\np = len(X_test.columns) # Number of independent variable\n</code></pre></p> <p><code>python id=\"5410b9bc-be16-4b66-8faa-812635d462ef\" outputId=\"c7396f77-113c-41c5-95a3-0692c8061be6\" colab={\"base_uri\": \"https://localhost:8080/\"} adusted_r2 = 1 - ((1 - r2)*(n - 1) / (n - p - 1)) print(\"Adjusted R2 Score:\", adusted_r2.round(2))</code></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/","title":"Multiple Linear Regression","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<p>Multiple linear regression is a statistical method used in predictive modeling and data analysis. It extends simple linear regression, which involves modeling the relationship between a dependent variable (also known as the response variable) and a single independent variable (predictor), to cases where there are multiple independent variables. In multiple linear regression, you have more than one predictor variable.</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"SmwCeFYMeEg5\" outputId=\"f19d2f71-68af-48df-e11f-64511a6c0a37\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"ce7070f2-9e93-4722-87b1-86362f9c8bcd\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#generate-a-data-for-regression","title":"Generate a Data for Regression","text":"<p>```python id=\"8b042bfd-f474-4b4a-882a-682f81e16080\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#generate-a-data-for-regression_1","title":"Generate a data for regression","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#x-independent-featues","title":"X = independent featues","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#y-dependent-feature","title":"y = dependent feature","text":"<p>X, y = make_regression(n_samples=100,                        n_features=2,                        n_informative=2,                        n_targets=1,                        noise=50) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 223} id=\"e60f7f09-257f-476f-9661-cb2a1a1ffcf2\" outputId=\"efad2867-007d-40a3-d6b3-0b82a89ee20e\"\n# Create a dataframe\ndata_dict = {\"feature1\": X[:, 0], \"feature2\": X[:, 1], \"target\":y}\ndf = pd.DataFrame(data=data_dict)\nprint(df.shape)\ndf.head()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#plot-the-data","title":"Plot the Data","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 617} id=\"20b43982-e95d-4912-ab0b-48e07cb10725\" outputId=\"f1cdb3bb-ea58-4cbc-b548-158973c1ee4a\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#plot-a-3-dimensional-scatter-plot","title":"Plot a 3-dimensional scatter plot","text":"<p>fig = px.scatter_3d(data_frame=df, x=\"feature1\", y=\"feature2\",                     z=\"target\", width=600, height=600) fig.update_traces(marker={'size': 4}) fig.show() <pre><code>&lt;!-- #region id=\"910fbb6d-fd5e-4e99-916b-d4abe87803e6\" --&gt;\n## **Train Test Split**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"5b45d5b7-60b3-4957-80b9-1d40930f0057\" outputId=\"42e62175-130d-4021-ea06-e6e3d9bb27f4\"\nX_train, X_test, y_train, y_test = train_test_split(df.drop(\"target\", axis=1),\n                                                    df[\"target\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#train-a-linear-regression-model","title":"Train a Linear Regression Model","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"5589bca6-c63b-4756-b2ff-6fe7e6f4ce4e\" outputId=\"c8761826-446f-4bc5-81b7-14d8fa8c95c3\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#instantiate-a-linear-regression-object","title":"Instantiate a linear Regression object","text":"<p>lr = LinearRegression()</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#fit-the-training-data","title":"Fit the training data","text":"<p>lr.fit(X_train, y_train) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"d41bfcff-3ce3-446b-9787-1acdd82c1fb3\" outputId=\"a23162cf-ded5-46ef-b8dd-05dacd031ffb\"\n# Print the coefficients\nprint(\"Coefficients:\", lr.coef_)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"b3bbab38-d22d-421b-8876-b44e0ef2f0ea\" outputId=\"094edf3f-c1b4-4e20-e626-bdfc440b4f8d\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#print-the-intercept-value","title":"Print the intercept value","text":"<p>print(\"Intercept:\", lr.intercept_) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"cf692a92-d1e0-418b-ad8c-eb120497af9c\" outputId=\"66638a39-10a9-469f-c9d7-b46f835ab5a2\"\n# Predict the test data\ny_pred = lr.predict(X_test)\ny_pred\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#accuracy-assessment","title":"Accuracy Assessment","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"404fef22-da23-4ec3-9f7e-60b46a8af64b\" outputId=\"cd69c8ba-a6b9-4be7-eb23-2a711c76a073\" print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred)) print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred)) print(\"R2 Score:\", r2_score(y_test, y_pred)) <pre><code>&lt;!-- #region id=\"2b6a4230-f855-4a7c-b5bf-d7dcae582b7f\" --&gt;\n## **Plot the Regression Plane**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"1ec1f958-7584-4a94-af2d-10b58bd4e0a2\" outputId=\"1765c27f-c269-4fb9-f3d6-094d3960b921\"\n# Check the minimum value of the data\ndf.min()\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"b5fbd295-00ae-4e4a-9f25-011dbce3adbc\" outputId=\"5a7aacea-72c6-496e-c11d-368c54f205a1\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#check-the-maximum-value-of-the-data","title":"Check the maximum value of the data","text":"<p>df.max() <pre><code>```python id=\"cb8f49e9-9be0-414f-8c43-565486ff4d48\"\n# Make a mesh grid\nx = np.linspace(start=-3, stop=3, num=10)\ny = np.linspace(start=-3, stop=3, num=10)\nxGrid, yGrid = np.meshgrid(y, x)\n</code></pre></p> <p>```python id=\"b1752ad3-95fe-4201-a87a-0b1be3f7082b\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#combine-x-and-y-corordinates-grid","title":"Combine x and y cor=ordinates grid","text":"<p>final = np.vstack((xGrid.ravel().reshape(1, 100), yGrid.ravel().reshape(1, 100))).T</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/03_Multiple_Linear_Regression/#predict-the-z-value","title":"Predict the z value","text":"<p>final_z = lr.predict(final).reshape(10, 10) z = final_z <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 617} id=\"8540897f-c374-48fb-8419-22a43916e7ba\" outputId=\"628a7e34-babc-488f-afaf-caf9c771563c\"\nfig = px.scatter_3d(data_frame=df, x=\"feature1\", y=\"feature2\",\n                    z=\"target\", width=600, height=600)\nfig.update_traces(marker={'size': 4})\nfig.add_trace(go.Surface(x=x, y=y, z=z))\nfig.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/","title":"Perceptron Trick","text":""},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#perceptron-trick","title":"Perceptron Trick","text":"<p>A perceptron is one of the simplest and fundamental building blocks in deep learning and artificial neural networks. It was developed by Frank Rosenblatt in the late 1950s and is a type of artificial neuron or node that can be used for binary classification tasks. While perceptrons are limited in their capabilities compared to more complex neural network architectures, they serve as a foundational concept for understanding how neural networks work.</p> <p>Here's an introduction to perceptrons in deep learning:</p> <ol> <li>Basic Structure: A perceptron takes multiple binary inputs (0 or 1) and produces a single binary output (0 or 1). Each input is associated with a weight, and there is also an additional parameter called the bias. Mathematically, the output of a perceptron is calculated as the weighted sum of inputs plus the bias, followed by applying a step function (often the Heaviside step function or a similar activation function) to the sum.</li> </ol> \\[y = \\text{Activation Function}\\left(\\sum_{i=1}^{n} \\text{weight}_i \\cdot \\text{input}_i + \\text{bias}\\right)\\] <ol> <li> <p>Weights and Bias: The weights in a perceptron represent the strength of the connection between the inputs and the output. A larger weight means that the corresponding input has a stronger influence on the output. The bias acts as an offset, allowing the perceptron to produce different outputs even when all inputs are zero.</p> </li> <li> <p>Activation Function: The activation function determines whether the perceptron should fire (output 1) or not (output 0) based on the weighted sum of inputs plus the bias. The choice of activation function is crucial, as it introduces non-linearity into the model. Common activation functions include the step function, sigmoid, ReLU (Rectified Linear Unit), and others.</p> </li> </ol>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"2a6b1568-306f-4105-8ba0-c3a014073875\" import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression</p> <p>import warnings warnings.filterwarnings(\"ignore\") <pre><code>&lt;!-- #region id=\"5c0e0687-e8b2-48b9-a003-d6f6a400b99a\" --&gt;\n## **Make a Data**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"83591f0d-f6e7-4591-a4b6-abde048366c5\" outputId=\"88d2482d-91d0-4664-faf7-bfc4a779c1f1\"\n# Make a sample classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=1, n_redundant=0, n_classes=2,\n                           n_clusters_per_class=1, random_state=0, hypercube=False, class_sep=1.5)\n\n# Plot the data\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\nplt.show()\n</code></pre></p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#build-the-perceptron-algorithm","title":"Build the Perceptron Algorithm","text":"<p>```python id=\"878142c1-a751-44a6-835d-bfa927ad07e7\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#write-a-function-to-build-the-algorithm-of-a-step-function","title":"Write a function to build the algorithm of a step function","text":"<p>def step(z):     \"\"\"     This function returns 0 if value is less than or equals to 0 and returns 1     if value is greater than 0.     \"\"\"     return 0 if z &lt;= 0 else 1 <pre><code>```python id=\"24c0abae-e61a-4353-a915-9e27009ec698\"\n# Write a function to build the algorithm of a perceptron\ndef perceptron(X, y, epochs):\n\n    # Add an extra column for intercept term\n    X = np.insert(X, 0, 1, axis=1)\n\n    # Initialize the weights\n    weights = np.ones(X.shape[1])\n\n    # Initialize a learning rate\n    lr = 0.01\n\n    for i in range(epochs):\n        # Select a random index\n        n = np.random.randint(len(X))\n        # Calculate the y-predicted\n        y_hat = step(np.dot(X[n], weights))\n        # Update the weights\n        weights = weights + lr * (y[n]-y_hat) * X[n]\n\n    return weights[0], weights[1:]\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"561f2bbf-515d-4632-bbd3-a04bcd37f15c\" outputId=\"24f78a20-79ea-4202-c3db-a1004f7ed3b3\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#calculate-the-intercept-and-coefficients","title":"Calculate the intercept and coefficients","text":"<p>intercept_, coef_ = perceptron(X, y, epochs=1000)</p> <p>print(\"Intercept(w0):\", intercept_) print(\"Coefficients(w1, w2):\", coef_) <pre><code>&lt;!-- #region id=\"a47deb8f-0f5a-4302-a19f-2c8f2e1f653a\" --&gt;\nGeneral Equation of a Line is:&lt;br&gt;\n$$ \\ Ax + By + C = 0 \\ $$\n\nWe an also write it like:&lt;br&gt;\n$$ \\ y = mx + c \\ $$ where $ \\ m\\ $ is the slope and $ \\ c\\ $ is the y-intercept.&lt;br&gt;\nor,\n$$ \\ y = -\\frac{A}{B}x - \\frac{C}{B} \\ $$\nwhere $ -\\frac{A}{B} \\ $ is the slope and $ -\\frac{C}{B} \\ $ is the intercept.&lt;br&gt;\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"85be8ff3-ddc9-4a31-92e8-066528790a9e\" outputId=\"dfe73a20-2e6a-4a4b-bc10-d1df790d9ff2\"\nm = -(coef_[0]/coef_[1])\nc = -(intercept_/coef_[1])\nprint(\"Slope(m):\", m)\nprint(\"Y-Intercept(c):\", c)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"c593620d-012c-4895-88d7-1553df7357ed\" outputId=\"c297ffaf-2bd3-49dd-de43-e54060e07d1e\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#plot-the-decision-boundary","title":"Plot the decision boundary","text":"<p>X_line = np.linspace(-1, 1, 50) y_line = X_line * m + c</p> <p>sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y) sns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Decision Boundary\") plt.ylim((-2.5, 2.5)) plt.show() <pre><code>&lt;!-- #region id=\"f3ab6cbb-d3c1-4c99-9b22-aff819d6020a\" --&gt;\n## **Apply the Logistic Regression**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 74} id=\"d6fa6dfd-ea12-436e-81e0-24febdb464f2\" outputId=\"2faee3ff-6d15-4b3d-d277-eed660b92a42\"\n# Instantiate a Logistic Regression model\nlr = LogisticRegression()\n\n# Fit the data\nlr.fit(X, y)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"6c468496-495b-4ae7-b515-ef0b97d59f55\" outputId=\"6c9403cb-dc80-448b-c960-d604446dd808\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#print-intercept-and-coefficients","title":"Print intercept and coefficients","text":"<p>print(\"Intercept of LR Model (w0):\", lr.intercept_) print(\"Coefficients of LR Model (w1, w2):\", lr.coef_) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"a654fc03-16e6-4535-9c37-6dc404d481e0\" outputId=\"cca29d24-60cc-4889-d2b4-9cb06a126a5f\"\n# Calculate the slope(m) and y-intercept(c) of the LR model\nm_lr = -(lr.coef_[0][0] / lr.coef_[0][1])\nc_lr = -(lr.intercept_[0] / lr.coef_[0][1])\n\nprint(\"Slope(m) of LR:\", m_lr)\nprint(\"Y-Intercept(c) of LR:\", c_lr)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"5988bebf-ea8f-4efc-8645-2f035c846570\" outputId=\"a8fc18e8-ec31-4659-9cca-c5e26af130e6\"</p>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/04_Machine_Learning_Algorithms/11_Perceptron_Trick/#plot-the-decision-boundary_1","title":"Plot the decision boundary","text":"<p>X_line = np.linspace(-1, 1, 50) y_line_lr = X_line * m_lr + c_lr</p> <p>sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y) sns.lineplot(x=X_line, y=y_line, c=\"red\", label=\"Perceptron\") sns.lineplot(x=X_line, y=y_line_lr, c=\"green\", label=\"Logistic Regression\") plt.ylim((-2.5, 2.5)) plt.show() ```</p>"},{"location":"geospatial/climate-data-downscaling/","title":"Climate Data Downscaling","text":"<p>This project implements climate data downscaling techniques using deep learning.</p>"},{"location":"geospatial/climate-data-downscaling/#files","title":"Files","text":"<ul> <li>Training Notebook</li> <li><code>model.py</code>: Model architecture</li> <li><code>train.py</code>: Training script</li> <li><code>dataset.py</code>: Data loading utilities</li> <li><code>config.py</code>: Configuration settings</li> </ul>"},{"location":"geospatial/climate-data-downscaling/#automated-notebooks-list","title":"\ud83d\udcd4 Automated Notebooks List","text":"<ul> <li>Train</li> </ul>"},{"location":"geospatial/climate-data-downscaling/#data-assets","title":"\ud83d\udcca Data &amp; Assets","text":"<ul> <li>config.py</li> <li>dataset.py</li> <li>loss.py</li> <li>model.py</li> <li>train.py</li> <li>utils.py</li> </ul>"},{"location":"geospatial/climate-data-downscaling/train/","title":"Train","text":""},{"location":"geospatial/climate-data-downscaling/train/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport logging\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom utils import compute_mean_std, EarlyStopping, setup_logger\nimport config as cfg\nfrom dataset import ClimateDataset\nfrom model import QuantileDownscaler\nfrom loss import QuantileLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n</code></pre>"},{"location":"geospatial/climate-data-downscaling/train/#dataset-and-dataloader","title":"Dataset and dataloader","text":"<pre><code># Collect and sort file paths\nlr_paths = sorted(glob(os.path.join(cfg.DATA_DIR, 'lr_images', '*.nc')))\nhr_paths = sorted(glob(os.path.join(cfg.DATA_DIR, 'hr_images', '*.nc')))\nassert len(lr_paths) == len(hr_paths), \"LR and HR directories must contain same number of files\"\n\n# Random shuffle of indices\nrng = np.random.default_rng(42)\nindices = np.arange(len(lr_paths))\nrng.shuffle(indices)\n\n# 80:20 split index\nsplit_idx = int(0.8 * len(indices))\n\ntrain_idx = indices[:split_idx]\ntest_idx  = indices[split_idx:]\n\n# Gather file lists\ntrain_lr_paths = [lr_paths[i] for i in train_idx]\ntrain_hr_paths = [hr_paths[i] for i in train_idx]\n\ntest_lr_paths = [lr_paths[i] for i in test_idx]\ntest_hr_paths = [hr_paths[i] for i in test_idx]\n\nprint(f\"Train: {len(train_lr_paths)}, Test: {len(test_lr_paths)}\")\n</code></pre> <pre><code># Precompute the normalization stats for all the channels\nmean, std = compute_mean_std(train_lr_paths, num_samples=int(len(train_lr_paths) * 0.5), num_workers=30)\n\ntrain_dataset = ClimateDataset(\n    lr_paths=train_lr_paths, \n    hr_paths=train_hr_paths, \n    mean=mean, std=std, size=128\n)\ntest_dataset = ClimateDataset(\n    lr_paths=test_lr_paths, \n    hr_paths=test_hr_paths, \n    mean=mean, std=std, size=128\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n</code></pre>"},{"location":"geospatial/climate-data-downscaling/train/#model-loss-optimizer","title":"Model, Loss, Optimizer","text":"<pre><code>model = QuantileDownscaler(\n    in_channels=cfg.IMG_CHANNELS, \n    num_channels=64, \n    num_blocks=16, \n    quantiles=[0.05, 0.5, 0.95]\n)\nmodel = nn.DataParallel(model)\n\ncriterion = QuantileLoss(quantiles=[0.05, 0.5, 0.95])\n\noptimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n</code></pre> <pre><code>def train_model(\n    model,\n    train_loader,\n    val_loader,\n    criterion,\n    optimizer,\n    num_epochs=50,\n    device=\"cuda\",\n    checkpoint_path=\"best_model.pth\",\n    patience=10,\n    log_file=\"train.log\",\n):\n    \"\"\"\n    Train model with early stopping, checkpointing, and logging.\n\n    Args:\n        model: PyTorch nn.Module\n        train_loader: DataLoader\n        val_loader: DataLoader\n        criterion: loss function\n        optimizer: optimizer\n        num_epochs: max epochs\n        device: \"cuda\" or \"cpu\"\n        checkpoint_path: file to save best model\n        patience: early stopping patience\n        log_file: log file path\n    \"\"\"\n\n    logger = setup_logger(log_file)\n    early_stopping = EarlyStopping(patience=patience, path=checkpoint_path)\n\n    model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        # -------------------\n        # Training\n        # -------------------\n        model.train()\n        train_loss = 0.0\n        for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n            X, y = X.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            preds = model(X)\n            print(preds.shape)\n            loss = criterion(preds, y)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * X.size(0)\n\n        train_loss /= len(train_loader.dataset)\n\n        # -------------------\n        # Validation\n        # -------------------\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X, y in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n                X, y = X.to(device), y.to(device)\n                preds = model(X)\n                loss = criterion(preds, y)\n                val_loss += loss.item() * X.size(0)\n\n        val_loss /= len(val_loader.dataset)\n\n        logger.info(\n            f\"Epoch [{epoch}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n        )\n\n        # -------------------\n        # Checkpoint &amp; Early Stopping\n        # -------------------\n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            logger.info(\"Early stopping triggered.\")\n            break\n</code></pre> <pre><code>if __name__ == '__main__':\n    train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=test_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=cfg.NUM_EPOCHS,\n        device=cfg.DEVICE,\n        checkpoint_path=os.path.join(cfg.CHECKPOINT_DIR, \"best_model.pth\"),\n        patience=10,\n        log_file=os.path.join(cfg.LOG_DIR, \"train.log\"),\n    )\n</code></pre> <pre><code>X, y = train_dataset[0]\n</code></pre> <pre><code>X_hat = model(X.unsqueeze(dim=0))\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(X.cpu().detach().numpy()[12])\nplt.colorbar(shrink=0.8);\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(X_hat[0].mean(axis=0).cpu().detach().numpy())\nplt.colorbar(shrink=0.8);\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(y.cpu().detach().numpy()[0])\nplt.colorbar(shrink=0.8);\n</code></pre>"},{"location":"geospatial/geonext-handbook/","title":"GeoNext Handbook","text":"<p>Welcome to the GeoNext Handbook, a comprehensive guide for geospatial analysis and machine learning.</p>"},{"location":"geospatial/geonext-handbook/#contents","title":"Contents","text":"<ul> <li>Markdown Guide</li> <li>Jupyter Notebooks Example</li> <li>Maize Leaf Disease Classification using ViT</li> <li>Bibliography</li> </ul>"},{"location":"geospatial/geonext-handbook/CONDUCT/","title":"Conduct","text":""},{"location":"geospatial/geonext-handbook/CONDUCT/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"geospatial/geonext-handbook/CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"geospatial/geonext-handbook/CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/","title":"Contributing","text":""},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in the ways listed below.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs using GitHub issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>GeoNext Handbook could always use more documentation, whether as part of the official GeoNext Handbook docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue on GitHub.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here's how to set up <code>GeoNext Handbook</code> for local development.</p> <ol> <li>Fork the repo on GitHub.</li> <li>Clone your fork locally.</li> <li>Install your local copy into a virtualenv, e.g., using <code>conda</code>.</li> <li>Create a branch for local development and make changes locally.</li> <li>Commit your changes and push your branch to GitHub.</li> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that the GeoNext Handbook project is released with a Contributor Code of Conduct. By contributing to this project you agree to abide by its terms.</p>"},{"location":"geospatial/geonext-handbook/book/bibliography/","title":"Bibliography","text":""},{"location":"geospatial/geonext-handbook/book/bibliography/#bibliography","title":"Bibliography","text":""},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/","title":"Maize Leaf Disease Classification Using Vit","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: py310     language: python     name: python3</p>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#maize-leaf-disease-classification-using-vision-transformer-vit","title":"Maize Leaf Disease Classification using Vision Transformer (ViT)","text":""},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\n\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport argparse\nimport random\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchinfo import summary\nfrom going_modular.going_modular import engine\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device, torch.__version__)\n\n# Define the data directory\ndata_dir = 'data'\nout_model_dir = 'models'\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#define-the-configurations","title":"Define the Configurations","text":"<pre><code>IMG_SIZE = 256\nNUM_CHANNELS = 3\nBATCH_SIZE = 32\nRANDOM_SEED = 42\n\n# Set the seeds\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#dataset-and-dataloader","title":"Dataset and DataLoader","text":"<pre><code># Custom image dataset class\nclass ImageDataset(Dataset):\n    \"\"\"\n    Class for loading simple text-based images into PyTorch datasets.\n\n    A text-based dataset where each image is associated with a label from a given\n    list of labels.\n\n    Args:\n        data_dir: Path to directory containing 'images' folder.\n        transform: Optional transformation function or list. Applied on each input\n        example (image) before being fed into the model.\n\n    Attributes:\n    - `len`: Number of images in dataset.\n    - `image_paths`: List of file paths for all images in the dataset.\n    - `labels`: List of corresponding labels for the images.\n    - `class_to_idx`: Dictionary mapping class labels to their respective indices.\n\n    To use this dataset, you would create an instance via:\n    ```python\n    dataset = ImageDataset(\"path/to/data\")\n    ```\n\n    The `transform` parameter is optional and can be used to process each input example before feeding it into a\n    model. The returned examples are a tuple containing the transformed image and its corresponding label.\n    \"\"\"\n\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        self.image_paths = glob(os.path.join(data_dir, 'images', '*', '*.jpg'))\n        self.labels = []\n        self.class_to_idx = {}\n\n        for path in self.image_paths:\n            label = path.split('\\\\')[-2]\n            self.labels.append(label)\n\n        for i, c in enumerate(set(self.labels)):\n            self.class_to_idx[c] = i\n\n        self.labels = [self.class_to_idx[l] for l in self.labels]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        image = Image.open(image_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n</code></pre> <pre><code># Define the transforms\nmanual_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor()\n])\n</code></pre> <pre><code># Create an image dataset object\ndataset = ImageDataset(os.path.join(data_dir), transform=manual_transforms)\n\n# Split the data into training and testing\ntrain_size = int(len(dataset) * 0.8)\ntest_size = len(dataset) - train_size\n\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\nprint('Number of training images:', len(train_dataset))\nprint('Number of testing images:', len(test_dataset))\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\nprint(f'{len(train_dataset)//BATCH_SIZE} batches of {BATCH_SIZE} images in the training data.')\nprint(f'{len(test_dataset)//BATCH_SIZE} batches of {BATCH_SIZE} images in the testing data.')\n</code></pre> <pre><code># Plot sample image\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(6, 6))\naxes = axes.flatten()\n\nfor ax in axes:\n    random_id = random.randint(0, len(train_dataset))\n    img, label = train_dataset.__getitem__(random_id)\n    img = img.permute(1, 2, 0).numpy()\n    label_name = [k for k, v in dataset.class_to_idx.items() if label == v][0]\n    ax.imshow(img)\n    ax.set_title(label_name)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#model-building","title":"Model Building","text":""},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#patchembedding-class","title":"<code>PatchEmbedding</code> class","text":"<p>Patch embedding transforms an input image into a sequence of patch vectors, enabling the use of transformers for vision tasks.</p> <ul> <li>The image is split into non-overlapping patches (e.g., 16\u00d716).</li> <li>Each patch is flattened and projected into a vector of fixed dimension using a <code>Conv2d</code> layer with <code>kernel_size = stride = patch_size</code>.</li> <li>The output is a sequence of patch embeddings with shape <code>(batch_size, num_patches, embedding_dim)</code>.</li> </ul> <p>This allows transformers to process images similarly to how they handle word tokens in NLP.</p> <pre><code>class PatchEmbedding(nn.Module):\n    \"\"\"\n    Converts an input image into a sequence of patch embeddings for Vision Transformers.\n\n    Args:\n        in_channels (int): Number of input image channels. Default is 3 (RGB).\n        patch_size (int): Size of each square patch (e.g., 16 for 16x16 patches).\n        embedding_dim (int): Dimension of the linear patch embedding.\n        image_size (int): Height/Width of the input image (assumes square input).\n        dropout (float): Dropout probability after embedding.\n    \"\"\"\n    def __init__(self, \n                 in_channels: int = 3,\n                 patch_size: int = 16,\n                 embedding_dim: int = 768,\n                 image_size: int = 256,\n                 dropout: float = 0.0):\n        super().__init__()\n\n        assert image_size % patch_size == 0, \\\n            f\"Image size ({image_size}) must be divisible by patch size ({patch_size})\"\n\n        self.patch_size = patch_size\n        self.embedding_dim = embedding_dim\n        self.num_patches = (image_size // patch_size) ** 2\n\n        # Conv2d turns image into patch embeddings\n        self.projection = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=embedding_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        # Learnable class token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n\n        # Learnable positional embeddings (1 for [CLS] + num_patches)\n        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): Input tensor of shape (B, C, H, W)\n\n        Returns:\n            Tensor: Patch + class token embeddings of shape (B, N+1, D)\n        \"\"\"\n        B = x.shape[0]\n\n        # Project patches\n        x = self.projection(x)                # (B, D, H/P, W/P)\n        x = x.flatten(2).transpose(1, 2)      # (B, N, D), where N = num_patches\n\n        # Add class token\n        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, D)\n        x = torch.cat((cls_token, x), dim=1)          # (B, N+1, D)\n\n        # Add positional encoding and apply dropout\n        x = x + self.position_embedding               # (B, N+1, D)\n        return self.dropout(x)\n</code></pre> <pre><code># Let's test it on a single image\n# Create an object of the PatchEmbedding class \npatchify = PatchEmbedding(\n    in_channels=3,\n    patch_size=16,\n    embedding_dim=768,\n    image_size=256,\n    dropout=0.3\n)\n\n# Pass a single image through\nimage, label = train_dataset.__getitem__(random.randint(0, len(train_dataset)))\nprint('Input image shape:', image.shape)\n\npatch_embedded_image = patchify(image.unsqueeze(0))\nprint('Output patch embedding shape:', patch_embedded_image.shape)\n</code></pre> <pre><code># View the patch embedding and patch embedding shape\nprint(patch_embedded_image)\nprint(f'Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimensions]')\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#multi-head-attention-mha-block","title":"<code>Multi-Head Attention</code> (MHA) Block","text":"<p>Multi-Head Attention is a key component of transformer models that allows the network to attend to different parts of a sequence simultaneously from multiple representation subspaces.</p> <ul> <li>Multiple heads learn different types of relationships in parallel.</li> <li>Each head performs scaled dot-product attention independently.</li> <li>The results from all heads are concatenated and linearly transformed.</li> </ul> <p>In Vision Transformers (ViT), MHA enables the model to learn relationships between image patches, allowing for global context understanding.</p> <pre><code>class MultiHeadAttentionBlock(nn.Module):\n    \"\"\"\n    A single Multi-Head Self-Attention block with pre-layer normalization \n    and residual connection, as used in Transformer encoders.\n\n    Args:\n        embedding_dim (int): Dimensionality of input embeddings.\n        num_heads (int): Number of attention heads.\n        attn_dropout (float): Dropout rate applied within attention mechanism.\n    \"\"\"\n    def __init__(self, embedding_dim: int = 768, num_heads: int = 12, attn_dropout: float = 0.0):\n        super().__init__()\n\n        # Layer normalization before attention (Pre-Norm)\n        self.norm = nn.LayerNorm(embedding_dim)\n\n        # Multi-head self-attention layer\n        self.attn = nn.MultiheadAttention(\n            embed_dim=embedding_dim,\n            num_heads=num_heads,\n            dropout=attn_dropout,\n            batch_first=True  # Output shape: (B, N, D)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): Input tensor of shape (B, N, D)\n\n        Returns:\n            Tensor: Output tensor of shape (B, N, D)\n        \"\"\"\n        x_norm = self.norm(x)\n        attn_output, _ = self.attn(\n            query=x_norm,\n            key=x_norm,\n            value=x_norm\n        )\n\n        return x + attn_output  # Residual connection\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#mlp-block","title":"<code>MLP</code> Block","text":"<p>The MLP Block in Vision Transformers follows each attention layer and helps the model learn complex transformations.</p> <ul> <li>Consists of two linear layers with a non-linear activation (typically GELU).</li> <li>Applies Layer Normalization before the MLP (Pre-Norm).</li> <li>Uses Dropout for regularization.</li> <li>Includes a residual connection: output = input + MLP(normalized input)</li> </ul> <pre><code>class MLPBlock(nn.Module):\n    \"\"\"\n    Feed-forward block used in Vision Transformers.\n    Applies LayerNorm, two linear layers with GELU activation, and dropout.\n\n    Args:\n        embedding_dim (int): Input and output embedding dimension.\n        mlp_size (int): Hidden layer size in the MLP (usually 4x embedding_dim).\n        dropout (float): Dropout rate after each linear layer.\n    \"\"\"\n    def __init__(self, embedding_dim: int = 768, mlp_size: int = 3072, dropout: float = 0.1):\n        super().__init__()\n\n        self.norm = nn.LayerNorm(embedding_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(embedding_dim, mlp_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_size, embedding_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): Input tensor of shape (B, N, D)\n\n        Returns:\n            Tensor: Output tensor of same shape (B, N, D) after residual connection.\n        \"\"\"\n        x_norm = self.norm(x)\n        return x + self.mlp(x_norm)  # Residual connection\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#transformerencoder-block","title":"<code>TransformerEncoder</code> Block","text":"<pre><code>class TransformerEncoderBlock(nn.Module):\n    \"\"\"\n    A single Transformer encoder block composed of:\n    - Multi-head self-attention with residual connection\n    - MLP block with residual connection\n\n    Args:\n        embedding_dim (int): Dimensionality of input embeddings.\n        num_heads (int): Number of attention heads.\n        mlp_size (int): Hidden layer size in the MLP.\n        mlp_dropout (float): Dropout rate in the MLP.\n        attn_dropout (float): Dropout rate in the attention layer.\n    \"\"\"\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n        num_heads: int = 12,\n        mlp_size: int = 3072,\n        mlp_dropout: float = 0.1,\n        attn_dropout: float = 0.0\n    ):\n        super().__init__()\n        self.attention = MultiHeadAttentionBlock(embedding_dim, num_heads, attn_dropout)\n        self.mlp = MLPBlock(embedding_dim, mlp_size, mlp_dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): Input of shape (B, N, D)\n\n        Returns:\n            Tensor: Output of shape (B, N, D)\n        \"\"\"\n        x = self.attention(x)\n        x = self.mlp(x)\n        return x\n</code></pre> <pre><code># Plot the summary of the transformer encoder\ntransformer_encoder_block = TransformerEncoderBlock()\n\nsummary(\n    model=transformer_encoder_block,\n    input_size=(1, 257, 768), # (batch_size, num_patches + 1, embedding dimension),\n    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n    col_width=20,\n    row_settings=['var_names']\n)\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#visiontransformer","title":"<code>VisionTransformer</code>","text":"<p>The Vision Transformer (ViT) model applies transformer architecture to image classification tasks. Instead of using convolutions, the image is divided into non-overlapping patches, each treated as a token, which is then processed by transformer layers.</p> <p>Key Components: 1. Patch Embedding: The image is split into fixed-size patches, and each patch is embedded into a vector using a linear projection. 2. Transformer Encoder: A series of transformer encoder blocks processes the embedded patches. These blocks consist of multi-head self-attention and MLP blocks, both with residual connections. 3. Classification Head: The output of the class token (added at the beginning of the sequence) is passed through a linear layer to predict the class label.</p> <p>Output: Class logits for classification.</p> <p>ViT offers a powerful alternative to convolutional neural networks (CNNs) by leveraging self-attention to learn global relationships between image patches.</p> <pre><code>class ViT(nn.Module):\n    \"\"\"\n        Vision Transformer (ViT) model for image classification.\n        The model uses Transformer encoder layers with patch-based image embeddings.\n\n        Args:\n            img_size (int): Image size (height and width). Should be divisible by patch_size.\n            in_channels (int): Number of input channels in the image (default: 3 for RGB).\n            patch_size (int): Size of each patch (e.g., 16 for 16x16 patches).\n            embedding_dim (int): Embedding dimension for patch embeddings and transformer layers.\n            embedding_dropout (float): Dropout rate applied to patch embeddings.\n            num_transformer_layers (int): Number of transformer encoder layers.\n            num_heads (int): Number of attention heads in multi-head attention.\n            attn_dropout (float): Dropout rate applied in multi-head attention.\n            mlp_size (int): Hidden size of MLP layers in transformer blocks.\n            mlp_dropout (float): Dropout rate applied in the MLP layers.\n            num_classes (int): Number of output classes for classification.\n        \"\"\"\n\n    def __init__(self,\n                 img_size: int = 256,\n                 in_channels: int = 3,\n                 patch_size: int = 16,\n                 embedding_dim: int = 768,\n                 embedding_dropout: float = 0.1,\n                 num_transformer_layers: int = 12,\n                 num_heads: int = 12,\n                 attn_dropout: float = 0.0,\n                 mlp_size: int = 3072,\n                 mlp_dropout: float = 0.1,\n                 num_classes: int = 4):\n        super().__init__()\n\n        # Assert image size is divisible by patch size\n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size. Got image size: {img_size}, patch size: {patch_size}\"\n\n        # Dropout layer for embeddings\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)  # Ensure p is a float value (probability)\n\n        # Patch embedding layer\n        self.patch_embedding = PatchEmbedding(\n            in_channels=in_channels,\n            patch_size=patch_size,\n            embedding_dim=embedding_dim,\n            image_size=img_size,\n            dropout=embedding_dropout  # Make sure this is passed as a float, not a layer object\n        )\n\n        # Transformer encoder layers\n        self.transformer_encoder = nn.Sequential(\n            *[TransformerEncoderBlock(\n                embedding_dim=embedding_dim,\n                num_heads=num_heads,\n                mlp_size=mlp_size,\n                mlp_dropout=mlp_dropout,\n                attn_dropout=attn_dropout) for _ in range(num_transformer_layers)]\n        )\n\n        # Classifier head: Final Linear Layer for Classification\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embedding_dim),\n            nn.Linear(embedding_dim, num_classes)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the Vision Transformer.\n\n        Args:\n            x (Tensor): Input tensor of shape (B, C, H, W), where B is batch size, \n                        C is number of channels, and H, W are image height and width.\n\n        Returns:\n            Tensor: Predicted class logits of shape (B, num_classes)\n        \"\"\"\n        # Apply patch embedding to the input\n        x = self.patch_embedding(x)\n\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n\n        # Use the output from the class token for classification (the first token in the sequence)\n        return self.classifier(x[:, 0])  # The class token is the first token (index 0)\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#training","title":"Training","text":"<pre><code># Get pretrained weights for ViT-Base\npretrained_vit = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\npretrained_state_dict = pretrained_vit.state_dict()\n\n# Create ViT model instance\nvit = ViT(num_classes=len(dataset.class_to_idx)).to(device)\n\n# Build a new state dict by remapping keys\nremapped_state_dict = {}\nfor new_key, old_key in zip(vit.state_dict().keys(), pretrained_state_dict.keys()):\n    if pretrained_state_dict[old_key].shape == vit.state_dict()[new_key].shape:\n        remapped_state_dict[new_key] = pretrained_state_dict[old_key]\n\n    else:\n        print(f\"Skipping {old_key} \u2192 {new_key} due to shape mismatch: \"\n              f\"{pretrained_state_dict[old_key].shape} vs {vit.state_dict()[new_key].shape}\")\n\n# Load the model weights\nvit.load_state_dict(remapped_state_dict, strict=False)\n\n# Print the summary of the model\nsummary(\n    model=vit,\n    input_size=(BATCH_SIZE, NUM_CHANNELS, IMG_SIZE, IMG_SIZE), # (batch_size, num of channels, height, width),\n    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n    col_width=20,\n    row_settings=['var_names']\n)\n</code></pre> <pre><code># Setup optimizer with a better learning rate\noptimizer = torch.optim.AdamW(params=vit.parameters(),\n                              lr=3e-4,  # more stable\n                              betas=(0.9, 0.999),\n                              weight_decay=0.3)\n\n# Define loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train the model\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=50,\n                       device=device)\n</code></pre> <pre><code># Save the model\n# torch.save(vit.state_dict(), os.path.join(out_model_dir, 'maize_leaf_disease_vit_model_weights.pth'))\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/maize_leaf_disease_classification_using_ViT/#make-predictions","title":"Make Predictions","text":"<pre><code>random_image_id = np.random.randint(0, len(dataset))\nrandom_image_path = dataset.image_paths[random_image_id]\nprint('Random Image Path:', random_image_path)\n\npred_and_plot_image(\n    model=vit,\n    class_names=list(dataset.class_to_idx.keys()),\n    image_path=random_image_path,\n    transform=manual_transforms\n)\n</code></pre> <pre><code>random_image_id = np.random.randint(0, len(dataset))\nrandom_image_path = dataset.image_paths[random_image_id]\nprint('Random Image Path:', random_image_path)\n\npred_and_plot_image(\n    model=vit,\n    class_names=list(dataset.class_to_idx.keys()),\n    image_path=random_image_path,\n    transform=manual_transforms\n)\n</code></pre>"},{"location":"geospatial/geonext-handbook/book/markdown/","title":"Markdown","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#markdown-files","title":"Markdown Files","text":"<p>Whether you write your book's content in Jupyter Notebooks (<code>.ipynb</code>) or in regular markdown files (<code>.md</code>), you'll write in the same flavor of markdown called MyST Markdown.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#what-is-myst","title":"What is MyST?","text":"<p>MyST stands for \"Markedly Structured Text\". It is a slight variation on a flavor of markdown called \"CommonMark\" markdown, with small syntax extensions to allow you to write roles and directives in the Sphinx ecosystem.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#what-are-roles-and-directives","title":"What are roles and directives?","text":"<p>Roles and directives are two of the most powerful tools in Jupyter Book. They are kind of like functions, but written in a markup language. They both serve a similar purpose, but roles are written in one line, whereas directives span many lines. They both accept different kinds of inputs, and what they do with those inputs depends on the specific role or directive that is being called.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#using-a-directive","title":"Using a directive","text":"<p>At its simplest, you can insert a directive into your book's content like so:</p> <pre><code>```{mydirectivename}\nMy directive content\n```\n</code></pre> <p>This will only work if a directive with name <code>mydirectivename</code> already exists (which it doesn't). There are many pre-defined directives associated with Jupyter Book. For example, to insert a note box into your content, you can use the following directive:</p> <pre><code>```{note}\nHere is a note\n```\n</code></pre> <p>This results in:</p> <pre><code>Here is a note\n</code></pre> <p>In your built book.</p> <p>For more information on writing directives, see the MyST documentation.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#using-a-role","title":"Using a role","text":"<p>Roles are very similar to directives, but they are less-complex and written entirely on one line. You can insert a role into your book's content with this pattern:</p> <pre><code>Some content {rolename}`and here is my role's content!`\n</code></pre> <p>Again, roles will only work if <code>rolename</code> is a valid role's name. For example, the <code>doc</code> role can be used to refer to another page in your book. You can refer directly to another page by its relative path. For example, the role syntax <code>{doc}`index`</code> will result in: {doc}<code>index</code>.</p> <p>For more information on writing roles, see the MyST documentation.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#adding-a-citation","title":"Adding a citation","text":"<p>You can also cite references that are stored in a <code>bibtex</code> file. For example, the following syntax: <code>{cite}`holdgraf_evidence_2014`</code> will render like this: {cite}<code>holdgraf_evidence_2014</code>.</p> <p>Multiple citations can be used like this:<code>holdgraf_rapid_2016, holdgraf_encoding_2017</code></p> <p>Moreover, you can insert a bibliography into your page with this syntax: The <code>{bibliography}</code> directive must be used for all the <code>{cite}</code> roles to render properly. For example, if the references for your book are stored in <code>references.bib</code>, then the bibliography is inserted with:</p> <pre><code>```{bibliography}\n```\n</code></pre> <p>Resulting in a rendered bibliography that looks like:</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#executing-code-in-your-markdown-files","title":"Executing code in your markdown files","text":"<p>If you'd like to include computational content inside these markdown files, you can use MyST Markdown to define cells that will be executed when your book is built. Jupyter Book uses jupytext to do this.</p> <p>First, add Jupytext metadata to the file. For example, to add Jupytext metadata to this markdown page, run this command:</p> <pre><code>jupyter-book myst init markdown.md\n</code></pre> <p>Once a markdown file has Jupytext metadata in it, you can add the following directive to run the code at build time:</p> <pre><code>```{code-cell}\nprint(\"Here is some code to execute\")\n```\n</code></pre> <p>When your book is built, the contents of any <code>{code-cell}</code> blocks will be executed with your default Jupyter kernel, and their outputs will be displayed in-line with the rest of your content.</p> <p>For more information about executing computational content with Jupyter Book, see The MyST-NB documentation.</p>"},{"location":"geospatial/geonext-handbook/book/notebooks/","title":"Notebooks","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3     language: python     name: python3</p>"},{"location":"geospatial/geonext-handbook/book/notebooks/#content-with-notebooks","title":"Content with notebooks","text":"<p>You can also create content with Jupyter Notebooks. This means that you can include code blocks and their outputs in your book.</p>"},{"location":"geospatial/geonext-handbook/book/notebooks/#markdown-notebooks","title":"Markdown + notebooks","text":"<p>As it is markdown, you can embed images, HTML, etc into your posts!</p> <p></p> <p>You can also \\(add_{math}\\) and</p> \\[ math^{blocks} \\] <p>or</p> \\[ \\begin{aligned} \\mbox{mean} la_{tex} \\\\ \\\\ math blocks \\end{aligned} \\] <p>But make sure you $Escape $your $dollar signs $you want to keep!</p>"},{"location":"geospatial/geonext-handbook/book/notebooks/#myst-markdown","title":"MyST markdown","text":"<p>MyST markdown works in Jupyter Notebooks as well. For more information about MyST markdown, check out the MyST guide in Jupyter Book, or see the MyST markdown documentation.</p>"},{"location":"geospatial/geonext-handbook/book/notebooks/#code-blocks-and-outputs","title":"Code blocks and outputs","text":"<p>Jupyter Book will also embed your code blocks and output in your book. For example, here's some sample Matplotlib code:</p> <pre><code>from matplotlib import rcParams, cycler\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()\n</code></pre> <pre><code># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nN = 10\ndata = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)]\ndata = np.array(data).T\ncmap = plt.cm.coolwarm\nrcParams[\"axes.prop_cycle\"] = cycler(color=cmap(np.linspace(0, 1, N)))\n\n\nfrom matplotlib.lines import Line2D\n\ncustom_lines = [\n    Line2D([0], [0], color=cmap(0.0), lw=4),\n    Line2D([0], [0], color=cmap(0.5), lw=4),\n    Line2D([0], [0], color=cmap(1.0), lw=4),\n]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nlines = ax.plot(data)\nax.legend(custom_lines, [\"Cold\", \"Medium\", \"Hot\"]);\n</code></pre> <p>There is a lot more that you can do with outputs (such as including interactive outputs) with your book. For more information about this, see the Jupyter Book documentation</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/","title":"Crop-Yield-Forecasting-Germany","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/#automated-notebooks-list","title":"\ud83d\udcd4 Automated Notebooks List","text":"<ul> <li>00 Yield Data Preparation</li> <li>01 Data Preparation For Pbms</li> <li>02 Data Preparation For Pbms</li> </ul>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/","title":"Yield Data Preparation","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#de-yield-data-preparation","title":"DE Yield Data Preparation","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom difflib import SequenceMatcher\nimport os\nimport geopandas as gpd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the shapefile for DE NUTS3\nde_nuts_gdf = gpd.read_file(r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\shapefiles\\DE_NUTS\\DE_NUTS_3.shp\")\nprint(de_nuts_gdf.shape)\nde_nuts_gdf.head()\n</code></pre> <pre><code>(455, 9)\n</code></pre> NUTS_ID LEVL_CODE CNTR_CODE NAME_LATN NUTS_NAME MOUNT_TYPE URBN_TYPE COAST_TYPE geometry 0 DE11B 3 DE Main-Tauber-Kreis Main-Tauber-Kreis None None None POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 3 DE Heidenheim Heidenheim None None None MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 3 DE Ostalbkreis Ostalbkreis None None None MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 3 DE Baden-Baden, Stadtkreis Baden-Baden, Stadtkreis None None None MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 3 DE Karlsruhe, Stadtkreis Karlsruhe, Stadtkreis None None None POLYGON ((938225.711 6286986.826, 940668.057 6... <pre><code># Read the latest yield data for Germany for the year 2022, and 2023\nde_yield_2023 = pd.read_excel(\n       r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\csvs\\DE_Yield_Latest.xlsx\", \n       skiprows=[i for i in range(7)], nrows=537,\n       header=None,\n       names=[\"district_no\", \"district\", \"ww\", \"rye\",\n              \"wb\", \"sb\", \"oats\", \"triticale\",\n              \"pota_tot\", \"sugarbeet\", \"wrape\", \"silage_maize\"])\n\nde_yield_2023.replace([\"-\", \"\", \"/\", \".\", \"...\"], np.nan, inplace=True) # replace the special characters\nde_yield_2023[\"district_no\"] = de_yield_2023[\"district_no\"].astype(\"int\") # change the datatype of district no into 'int'\nde_yield_2023 = pd.melt(\n       de_yield_2023,\n       id_vars='district_no', \n       value_vars=de_yield_2023.columns[2:], \n       var_name='var', ignore_index=True\n) # melt the dataframe\nde_yield_2023['year'] = 2023 # add the 'year' info\nde_yield_2023['measure'] = 'yield' # add the 'measure' column\nde_yield_2023['outlier'] = np.nan # add the outlier columns\nprint(de_yield_2023.shape)\nde_yield_2023.head()\n</code></pre> <pre><code>(5370, 6)\n</code></pre> district_no var value year measure outlier 0 1 ww 83.2 2023 yield NaN 1 1001 ww NaN 2023 yield NaN 2 1002 ww NaN 2023 yield NaN 3 1003 ww 85.2 2023 yield NaN 4 1004 ww NaN 2023 yield NaN <pre><code>de_yield_2022 = pd.read_excel(\n       r\"D:\\Research Works\\Agriculture\\Germany_Multiple_Crops_Cimate_Change\\Datasets\\Yield_Statistitics_WholeGermany_1999-2022\\Yield_Statistitics_WholeGermany_2022.xlsx\", \n       skiprows=[i for i in range(7)], nrows=537,\n       header=None,\n       names=[\"district_no\", \"district\", \"ww\", \"rye\",\n              \"wb\", \"sb\", \"oats\", \"triticale\",\n              \"pota_tot\", \"sugarbeet\", \"wrape\", \"silage_maize\"])\n\nde_yield_2022.replace([\"-\", \"\", \"/\", \".\", \"...\"], np.nan, inplace=True) # replace the special characters\nde_yield_2022[\"district_no\"] = de_yield_2022[\"district_no\"].astype(\"int\") # change the datatype of district no into 'int'\nde_yield_2022 = pd.melt(\n       de_yield_2022,\n       id_vars='district_no', \n       value_vars=de_yield_2022.columns[2:], \n       var_name='var', ignore_index=True\n) # melt the dataframe\nde_yield_2022['year'] = 2022 # add the 'year' info\nde_yield_2022['measure'] = 'yield' # add the 'measure' column\nde_yield_2022['outlier'] = np.nan # add the outlier columns\nprint(de_yield_2022.shape)\nde_yield_2022.head()\n</code></pre> <pre><code>(5370, 6)\n</code></pre> district_no var value year measure outlier 0 1 ww 95.8 2022 yield NaN 1 1001 ww NaN 2022 yield NaN 2 1002 ww NaN 2022 yield NaN 3 1003 ww 101.7 2022 yield NaN 4 1004 ww NaN 2022 yield NaN <pre><code># Read the yield dataset available for DE from 1979 to 2021\nde_yield_1979_21 = pd.read_csv(r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\csvs\\openagrar_derivate_00056476\\Final_data.csv\")\n\n# Filter the dataframe for yield only\nde_yield_1979_21 = de_yield_1979_21[de_yield_1979_21['measure']=='yield']\n\n# Create a seperate df to store only 'district_no', 'district', and 'nuts_id'\nde_nuts = de_yield_1979_21[['district_no', 'district', 'nuts_id']].drop_duplicates()\n\nprint(de_yield_1979_21.shape)\nde_yield_1979_21.head()\n</code></pre> <pre><code>(179691, 8)\n</code></pre> district_no district nuts_id year var measure value outlier 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 grain_maize yield NaN 0 5 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0 7 1001 Flensburg, kreisfreie Stadt DEF01 1979 potat_tot yield NaN 0 9 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0 11 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#data-processsing","title":"Data Processsing","text":"<pre><code># Merge the district information to the latest dataset\nde_yield_2023 = pd.merge(left=de_yield_2023, right=de_nuts, on='district_no', how='inner')\nde_yield_2022 = pd.merge(left=de_yield_2022, right=de_nuts, on='district_no', how='inner')\n\n# Convert the yield values from dt/ha to t/ha\nde_yield_2023['value'] = de_yield_2023['value'] / 10\nde_yield_2022['value'] = de_yield_2022['value'] / 10\n\n# Reorder the columns based on the data from 1971-2021\nde_yield_2023 = de_yield_2023[de_yield_1979_21.columns]\nde_yield_2022 = de_yield_2022[de_yield_1979_21.columns]\n\nprint('Shape of the data for 2023:', de_yield_2023.shape)\nprint('Shape of the data for 2022:', de_yield_2022.shape)\n</code></pre> <pre><code>Shape of the data for 2023: (3970, 8)\nShape of the data for 2022: (3970, 8)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#merge-all-the-datasets","title":"Merge All the Datasets","text":"<pre><code># Concat all the datasets\nmerged_df = pd.concat((de_yield_1979_21, de_yield_2022, de_yield_2023), ignore_index=True)\n\n# Sort the dataframe based on district number, year, and var\nmerged_df.sort_values(by=['district_no', 'year', 'var'], inplace=True)\nprint(merged_df.shape)\nmerged_df.head()\n</code></pre> <pre><code>(187631, 8)\n</code></pre> district_no district nuts_id year var measure value outlier 0 1001 Flensburg, kreisfreie Stadt DEF01 1979 grain_maize yield NaN 0.0 1 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0.0 2 1001 Flensburg, kreisfreie Stadt DEF01 1979 potat_tot yield NaN 0.0 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0.0 4 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0.0 <pre><code>merged_df[(merged_df['year']==2022) &amp; (merged_df['var']=='ww')]\n</code></pre> district_no district nuts_id year var measure value outlier 179691 1001 Flensburg, kreisfreie Stadt DEF01 2022 ww yield NaN NaN 179692 1002 Kiel, kreisfreie Stadt DEF02 2022 ww yield NaN NaN 179693 1003 L\u00fcbeck, kreisfreie Stadt DEF03 2022 ww yield 10.17 NaN 179694 1004 Neum\u00fcnster, kreisfreie Stadt DEF04 2022 ww yield NaN NaN 179695 1051 Dithmarschen, Landkreis DEF05 2022 ww yield 9.47 NaN ... ... ... ... ... ... ... ... ... 180083 16073 Saalfeld-Rudolstadt, Landkreis DEG0I 2022 ww yield 4.86 NaN 180084 16074 Saale-Holzland-Kreis DEG0J 2022 ww yield 6.16 NaN 180085 16075 Saale-Orla-Kreis DEG0K 2022 ww yield 5.69 NaN 180086 16076 Greiz, Landkreis DEG0L 2022 ww yield 6.64 NaN 180087 16077 Altenburger Land, Landkreis DEG0M 2022 ww yield 8.38 NaN <p>397 rows \u00d7 8 columns</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#plot-the-dataset-in-maps","title":"Plot the Dataset in Maps","text":"<pre><code>def plot_var_map(dataframe, shapefile, var, year):\n\n    # prepare the data \n    dataframe = dataframe[(dataframe['year']==year) &amp; (dataframe['var']==var)]\n    dataframe = dataframe[['nuts_id', 'district_no', 'district', 'year', 'var', 'value']]\n\n    shapefile.rename(columns={'NUTS_ID': 'nuts_id'}, inplace=True)\n\n    # merged the dataframe with the shapefile\n    shapefile_merged = pd.merge(left=shapefile, right=dataframe, on='nuts_id', how='left')\n    shapefile_merged = shapefile_merged[['nuts_id', 'district_no', 'NUTS_NAME', 'district', 'year', 'var', 'value', 'geometry']]\n\n    return shapefile_merged\n</code></pre> <pre><code>test_df = plot_var_map(merged_df, de_nuts_gdf, var='wb', year=2023)\ntest_df\n</code></pre> nuts_id district_no NUTS_NAME district year var value geometry 0 DE11B 8128.0 Main-Tauber-Kreis Main-Tauber-Kreis 2023.0 wb 6.15 POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 8135.0 Heidenheim Heidenheim, Landkreis 2023.0 wb NaN MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 8136.0 Ostalbkreis Ostalbkreis 2023.0 wb 7.19 MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 8211.0 Baden-Baden, Stadtkreis Baden-Baden, kreisfreie Stadt 2023.0 wb NaN MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 8212.0 Karlsruhe, Stadtkreis Karlsruhe, kreisfreie Stadt 2023.0 wb NaN POLYGON ((938225.711 6286986.826, 940668.057 6... ... ... ... ... ... ... ... ... ... 450 DE5 NaN Bremen NaN NaN NaN NaN MULTIPOLYGON (((949166.140 7023798.944, 952179... 451 DE6 NaN Hamburg NaN NaN NaN NaN MULTIPOLYGON (((1134475.694 7117788.896, 11336... 452 DEE NaN Sachsen-Anhalt NaN NaN NaN NaN MULTIPOLYGON (((1294568.737 6984897.844, 12963... 453 DE7 NaN Hessen NaN NaN NaN NaN MULTIPOLYGON (((1057294.287 6737363.291, 10568... 454 DE NaN Deutschland NaN NaN NaN NaN MULTIPOLYGON (((1163782.809 6033270.891, 11632... <p>455 rows \u00d7 8 columns</p> <pre><code>merged_df.shape\n</code></pre> <pre><code>(187631, 8)\n</code></pre> <pre><code>merged_df['value'].isnull().sum()\n</code></pre> <pre><code>43290\n</code></pre> <pre><code>merged_df[merged_df['value']&gt;0]\n</code></pre> district_no district nuts_id year var measure value outlier 1 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0.0 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0.0 4 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0.0 5 1001 Flensburg, kreisfreie Stadt DEF01 1979 silage_maize yield 43.60 0.0 7 1001 Flensburg, kreisfreie Stadt DEF01 1979 wb yield 4.52 0.0 ... ... ... ... ... ... ... ... ... 187630 16077 Altenburger Land, Landkreis DEG0M 2023 silage_maize yield 41.94 NaN 186836 16077 Altenburger Land, Landkreis DEG0M 2023 sugarbeet yield 76.63 NaN 184851 16077 Altenburger Land, Landkreis DEG0M 2023 wb yield 10.04 NaN 187233 16077 Altenburger Land, Landkreis DEG0M 2023 wrape yield 4.34 NaN 184057 16077 Altenburger Land, Landkreis DEG0M 2023 ww yield 9.32 NaN <p>144341 rows \u00d7 8 columns</p> <pre><code>144341 + 43290\n</code></pre> <pre><code>187631\n</code></pre> <pre><code>test_df.plot(column='value', edgecolor='k', linewidth=0.3, figsize=(8, 8), legend=True)\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/","title":"Data Preparation For Pbms","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#data-preparation-for-pbms","title":"Data Preparation for PBMs","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport os\nfrom glob import glob\nimport json\nfrom tqdm.auto import tqdm\nimport ee\nimport geemap\nfrom sklearn.neighbors import BallTree\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nout_master_dir = r'datasets\\master'\nout_temp_dir = r'temp_data'\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#instantiate-a-map-object","title":"Instantiate a Map Object","text":"<pre><code># ee.Authenticate()\n# ee.Initialize()\n</code></pre> <pre><code># Import Germany Shapefile\nde_roi = ee.FeatureCollection('users/geonextgis/Germany_Administrative_Level_2')\npoly_style = {'fillColor': '00000000', 'color': 'black', 'width': 1}\nMap = geemap.Map(basemap='Esri.WorldImagery')\nMap.addLayer(de_roi.style(**poly_style), {}, 'DE ROI')\nMap.centerObject(de_roi, 6)\nMap\n</code></pre> <pre><code>Map(center=[51.055719127031935, 10.373828310619029], controls=(WidgetControl(options=['position', 'transparent\u2026\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the DE NUTS3 Shapefile\nde_nuts3_gdf = gpd.read_file(\"datasets\\shapefiles\\DE_NUTS\\DE_NUTS_3.shp\")\nde_nuts3_gdf = de_nuts3_gdf[de_nuts3_gdf['LEVL_CODE']==3] # Filter for NUTS3\nprint(de_nuts3_gdf.shape)\nde_nuts3_gdf.head()\n</code></pre> <pre><code>(400, 6)\n</code></pre> NUTS_ID LEVL_CODE CNTR_CODE NAME_LATN NUTS_NAME geometry 0 DE11B 3 DE Main-Tauber-Kreis Main-Tauber-Kreis POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 3 DE Heidenheim Heidenheim MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 3 DE Ostalbkreis Ostalbkreis MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 3 DE Baden-Baden, Stadtkreis Baden-Baden, Stadtkreis MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 3 DE Karlsruhe, Stadtkreis Karlsruhe, Stadtkreis POLYGON ((938225.711 6286986.826, 940668.057 6... <pre><code># Read the soil coordinates\nsoil_coords_df = pd.read_csv(\"datasets\\csvs\\Site_Soil_BZE_WGS84_Coords.csv\")\n\n# Read the soil attributes\nsoil_attributes_df = pd.read_excel(\n    \"datasets\\csvs\\SoilData.xlsx\",\n    sheet_name='SoilData'\n)\nsoil_attributes_df.drop(columns=['Lon', 'Lat'], inplace=True)\nsoil_attributes_df.rename(columns={'Location_id': 'PointID'}, inplace=True)\n\nprint('Soil coordinates data shape:', soil_coords_df.shape)\nprint('Soil coordinates attributes shape:', soil_attributes_df.shape)\n</code></pre> <pre><code>Soil coordinates data shape: (3104, 3)\nSoil coordinates attributes shape: (3087, 31)\n</code></pre> <pre><code># Read the ESA WorldCover dataset\nesa_lulc = ee.ImageCollection(\"ESA/WorldCover/v100\").first()\nvisualization = {\n  'bands': ['Map'],\n}\nMap.addLayer(esa_lulc, visualization, 'ESA LULC', opacity=0.6)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#soil-data-preparation","title":"Soil Data Preparation","text":"<pre><code># Convert the soil coordinates into geodataframe\nsoil_coords_geometry = gpd.points_from_xy(soil_coords_df['Longitude'], soil_coords_df['Latitude'], crs=4326)\nsoil_coords_gdf = gpd.GeoDataFrame(soil_coords_df, geometry=soil_coords_geometry).to_crs(epsg=3857)\nprint(soil_coords_gdf.shape)\nsoil_coords_gdf.head()\n</code></pre> <pre><code>(3104, 4)\n</code></pre> PointID Longitude Latitude geometry 0 2 8.411608 54.859923 POINT (936375.919 7334727.347) 1 3 8.697143 54.864382 POINT (968161.53 7335589.787) 2 4 8.765298 54.866979 POINT (975748.51 7336092.132) 3 5 8.959032 54.863920 POINT (997314.88 7335500.425) 4 6 9.078456 54.870685 POINT (1010609.099 7336809.049) <pre><code># Merge the NUTS3 information in the soil coordinates dataframe\nsoil_coords_gdf = soil_coords_gdf.sjoin(de_nuts3_gdf[['NUTS_ID', 'NUTS_NAME', 'geometry']], how='left', predicate='intersects')\nprint(soil_coords_gdf.shape)\nsoil_coords_gdf.head()\n</code></pre> <pre><code>(3104, 7)\n</code></pre> PointID Longitude Latitude geometry index_right NUTS_ID NUTS_NAME 0 2 8.411608 54.859923 POINT (936375.919 7334727.347) 166.0 DEF07 Nordfriesland 1 3 8.697143 54.864382 POINT (968161.53 7335589.787) 166.0 DEF07 Nordfriesland 2 4 8.765298 54.866979 POINT (975748.51 7336092.132) 166.0 DEF07 Nordfriesland 3 5 8.959032 54.863920 POINT (997314.88 7335500.425) 166.0 DEF07 Nordfriesland 4 6 9.078456 54.870685 POINT (1010609.099 7336809.049) 166.0 DEF07 Nordfriesland <pre><code># Merge the soil coordinate information in the soil attributes dataframe\nsoil_attributes_gdf = pd.merge(\n    left=soil_coords_gdf[['PointID', 'Longitude', 'Latitude', 'NUTS_ID', 'NUTS_NAME', 'geometry']], \n    right=soil_attributes_df, \n    on='PointID', \n    how='inner')\n\nsoil_attributes_gdf.columns = [col.replace('.0', '') for col in soil_attributes_gdf.columns]\nsoil_attributes_gdf.to_crs(crs='epsg:31467', inplace=True) # change the CRS\nsoil_attributes_ee = geemap.gdf_to_ee(soil_attributes_gdf, geodesic=False)\n\nsoil_style = {'color': 'green', 'pointSize': 5}\nMap.addLayer(soil_attributes_ee.style(**soil_style), {}, 'Soil Points')\n\nprint(soil_attributes_gdf.shape)\nsoil_attributes_gdf.head()\n</code></pre> <pre><code>(3087, 36)\n</code></pre> PointID Longitude Latitude NUTS_ID NUTS_NAME geometry SoilLayerDepth_10cm SoilLayerDepth_30cm SoilLayerDepth_50cm SoilLayerDepth_70cm ... BD_50cm BD_70cm BD_1m BD_2m OC_10cm OC_30cm OC_50cm OC_70cm OC_1m OC_2m 0 2 8.411608 54.859923 DEF07 Nordfriesland POINT (3462285.321 6081355.028) 0.1 0.3 0.5 0.7 ... 1.45 1.30 1.54 1.54 2.183 2.075 1.151 0.691 0.157 0.157 1 3 8.697143 54.864382 DEF07 Nordfriesland POINT (3480623.416 6081734.949) 0.1 0.3 0.5 0.7 ... 1.57 1.39 1.47 1.47 2.064 1.589 0.574 0.579 0.581 0.581 2 4 8.765298 54.866979 DEF07 Nordfriesland POINT (3485000.58 6082007.3) 0.1 0.3 0.5 0.7 ... 1.37 1.48 1.49 1.49 1.915 1.517 2.220 1.028 0.652 0.652 3 5 8.959032 54.863920 DEF07 Nordfriesland POINT (3497439.177 6081642.411) 0.1 0.3 0.5 0.7 ... 1.45 1.48 1.60 1.60 3.375 1.701 1.233 1.089 0.674 0.674 4 6 9.078456 54.870685 DEF07 Nordfriesland POINT (3505106.596 6082397.618) 0.1 0.3 0.5 0.7 ... 1.42 0.89 1.35 1.35 2.447 1.166 0.398 3.570 0.241 0.241 <p>5 rows \u00d7 36 columns</p> <pre><code># Save the Soil data\n# soil_attributes_gdf.to_csv(os.path.join(out_csv_dir, 'DE_Soil_BZE_Master.csv'), index=False)\n# soil_attributes_gdf.to_file(os.path.join(out_master_dir, 'DE_Soil_BZE_Master.shp'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#climate-grid-preparation","title":"Climate Grid Preparation","text":"<pre><code># Read the DWD Climate JSON file\ndwd_json_path = \"datasets\\shapefiles\\dwd_ubn_latlon_to_rowcol.json\"\n\nwith open(dwd_json_path, 'r') as file:\n    dwd_json = json.load(file)\n\n# Function to convert the json data into a table\ndef json_to_table(data):\n    coords_info = [i[0] for i in dwd_json]\n    row_col_info = [i[1] for i in dwd_json]\n\n    coords_info = pd.DataFrame(coords_info, columns=['Latitude', 'Longitude'])\n    row_col_info = pd.DataFrame(row_col_info, columns=['Row', 'Column'])\n\n    final_df = pd.concat((coords_info, row_col_info), axis=1)\n\n    return final_df\n\ndwd_grid_df = json_to_table(dwd_json)\nprint(dwd_grid_df.shape)\ndwd_grid_df.head()\n</code></pre> <pre><code>(358303, 4)\n</code></pre> Latitude Longitude Row Column 0 55.054328 8.402969 0 181 1 55.054404 8.418616 0 182 2 55.045268 8.387459 1 180 3 55.045346 8.403102 1 181 4 55.036286 8.387596 2 180 <pre><code># Convert the data into geodataframe\ndwd_grid_gdf = gpd.GeoDataFrame(\n    dwd_grid_df, \n    geometry=gpd.points_from_xy(dwd_grid_df['Longitude'], dwd_grid_df['Latitude']),\n    crs=4326)\nprint(dwd_grid_gdf.shape)\ndwd_grid_gdf.head()\n</code></pre> <pre><code>(358303, 5)\n</code></pre> Latitude Longitude Row Column geometry 0 55.054328 8.402969 0 181 POINT (8.40297 55.05433) 1 55.054404 8.418616 0 182 POINT (8.41862 55.0544) 2 55.045268 8.387459 1 180 POINT (8.38746 55.04527) 3 55.045346 8.403102 1 181 POINT (8.4031 55.04535) 4 55.036286 8.387596 2 180 POINT (8.3876 55.03629) <pre><code># # Save the data\n# dwd_grid_df.to_csv(os.path.join(out_csv_dir, 'DE_DWD_UBN_Centroids.csv'), index=False)\n\n# dwd_grid_gdf.to_crs(31467).to_file(\"datasets\\shapefiles\\DE_DWD_UBN_GRIDS\\DE_DWD_UBN_Centroids_EPSG_31467.shp\")\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#filter-points-falling-on-cropland","title":"Filter Points falling on Cropland","text":"<pre><code># Load the DWD grid centroids on EE Map object\ndwd_centroids_ee = ee.FeatureCollection('projects/ee-geonextgis/assets/DE_DWD_UBN_Centroids_EPSG_31467') \ndwd_style = {\n    'color': 'red',\n    'pointSize': 1,\n    'width': 1,\n    'fillColor': '00000000'\n}\n\n# Create a 500m buffer for each feature and get bounds\ndwd_grids_ee = dwd_centroids_ee.map(lambda f: f.buffer(500).bounds())\n\nMap.addLayer(dwd_centroids_ee.style(**dwd_style), {}, 'DWD Centroids', False)\nMap.addLayer(dwd_grids_ee.style(**dwd_style), {}, 'DWD Grids', False)\n</code></pre> <pre><code># # Iterate over the columns and extract the data in the temporary folder\n# for column_id in tqdm(sorted(dwd_grid_gdf['Column'].unique())):\n\n#     filered_column_cells = dwd_grids_ee.filter(ee.Filter.eq('Column', int(column_id)))\n\n#     try:\n#         out_data_path = os.path.join(out_temp_dir, f'Col_{column_id}_DWD_LULC.csv')\n\n#         # Extract LULC info for all the DWD cells\n#         dwd_lulc_zonal_stat = geemap.zonal_statistics_by_group(\n#             esa_lulc, filered_column_cells, out_data_path, statistics_type='SUM'\n#         )\n\n#         print(f'Column ID: {column_id} | Data saved at {out_data_path}')\n\n#     except:\n#         print(f'Column ID: {column_id} | Error.')\n</code></pre> <pre><code># Merge all the data in a single file\nconcatenated_df = pd.DataFrame()\n\nlulc_class_columns = ['Class_10', 'Class_20', 'Class_30', 'Class_40', 'Class_50', 'Class_60',\n                      'Class_70', 'Class_80', 'Class_90', 'Class_95', 'Class_100']\n\ntemp_file_paths = glob(out_temp_dir + '\\\\*.csv')\n\nfor path in tqdm(temp_file_paths):\n    temp_df = pd.read_csv(path)\n\n    for col in lulc_class_columns:\n        if col not in list(temp_df.columns):\n            temp_df[col] = 0\n\n        temp_df[col] = np.round((temp_df[col] / temp_df['Class_sum']) * 100, 4)\n\n    temp_df = temp_df[['Row', 'Column', 'Longitude', 'Latitude'] + lulc_class_columns]\n\n    concatenated_df = pd.concat((concatenated_df, temp_df), axis=0)\n\n# Filter the grid cell where cropland ('Class_40') area is more than 20%\ndwd_cropland_df = concatenated_df[concatenated_df['Class_40']&gt;=20].iloc[:, :4]\ndwd_cropland_gdf = pd.merge(left=dwd_cropland_df, right=dwd_grid_gdf[['Row', 'Column', 'geometry']], on=['Row', 'Column'], how='left')\ndwd_cropland_gdf = gpd.GeoDataFrame(dwd_cropland_gdf)\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>  0%|          | 0/640 [00:00&lt;?, ?it/s]\n\n\n(190658, 5)\n</code></pre> Row Column Longitude Latitude geometry 0 444 0 5.876024 51.024361 POINT (5.87602 51.02436) 1 477 100 7.311274 50.757258 POINT (7.31127 50.75726) 2 481 100 7.312566 50.721317 POINT (7.31257 50.72132) 3 482 100 7.312888 50.712331 POINT (7.31289 50.71233) 4 511 100 7.322172 50.451747 POINT (7.32217 50.45175) <pre><code># Add the NUTS information\ndwd_cropland_gdf = dwd_cropland_gdf.to_crs(crs='epsg:3857')\ndwd_cropland_gdf = dwd_cropland_gdf.sjoin(de_nuts3_gdf[['NUTS_ID', 'NUTS_NAME', 'geometry']], how='left', predicate='intersects')\ndwd_cropland_gdf.dropna(inplace=True)\ndwd_cropland_gdf.sort_values(by=['NUTS_ID'], inplace=True)\ndwd_cropland_gdf.reset_index(drop=True, inplace=True)\ndwd_cropland_gdf['Cell_ID'] = dwd_cropland_gdf.index\ndwd_cropland_gdf = dwd_cropland_gdf[['Cell_ID', 'Row', 'Column', 'Latitude', 'Longitude', 'NUTS_ID', 'NUTS_NAME', 'geometry']]\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>(190364, 8)\n</code></pre> Cell_ID Row Column Latitude Longitude NUTS_ID NUTS_NAME geometry 0 0 703 234 48.737371 9.201742 DE111 Stuttgart, Stadtkreis POINT (1024333.233 6230415.593) 1 1 691 236 48.845227 9.229425 DE111 Stuttgart, Stadtkreis POINT (1027414.872 6248640.293) 2 2 707 233 48.701424 9.188012 DE111 Stuttgart, Stadtkreis POINT (1022804.862 6224350.32) 3 3 706 233 48.710416 9.188046 DE111 Stuttgart, Stadtkreis POINT (1022808.604 6225867.209) 4 4 705 233 48.719409 9.188080 DE111 Stuttgart, Stadtkreis POINT (1022812.348 6227384.366) <pre><code># Save the data\n# dwd_cropland_gdf.to_csv(os.path.join(out_master_dir, 'DE_DWD_UBN_Crop.csv'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#find-k-nearest-soil-points-for-each-climate-grid-cell","title":"Find k-Nearest Soil Points for Each Climate Grid Cell","text":"<pre><code># Convert the soil data in the same coordinate system\nsoil_attributes_gdf.to_crs(crs='epsg:3857', inplace=True)\nsoil_attributes_gdf.crs == dwd_cropland_gdf.crs\n</code></pre> <pre><code>True\n</code></pre> <pre><code># Extract coordinates\ndwd_cropland_coords = np.array(list(zip(dwd_cropland_gdf.geometry.x, dwd_cropland_gdf.geometry.y)))\nsoil_coords = np.array(list(zip(soil_attributes_gdf.geometry.x, soil_attributes_gdf.geometry.y)))\n\n# Build BallTree for fast nearest neighbor search\ntree = BallTree(soil_coords, metric='euclidean')\n\n# Define number of neighbors (k)\nk = 5  # Adjust based on data density\n\n# Query k-nearest neighbors for each climate point\ndistances, indices = tree.query(dwd_cropland_coords, k=k)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#compute-inverse-distance-weighted-idw-average","title":"Compute Inverse Distance Weighted (IDW) Average","text":"<pre><code># Define soil properties to interpolate\nsoil_properties = soil_attributes_gdf.columns[12:]  # Add all required properties\npower = 2  # IDW power parameter\n\n# Dictionary to store weighted values for each property\nweighted_results = {prop: [] for prop in soil_properties}\n\n# Compute IDW for each climate grid cell\nfor i, climate_point in tqdm(enumerate(dwd_cropland_gdf.geometry)):\n    nearest_soil_points = soil_attributes_gdf.iloc[indices[i]]  # Get k nearest neighbors\n    nearest_distances = distances[i]\n\n    # Avoid division by zero\n    nearest_distances[nearest_distances == 0] = 1e-6\n\n    # Compute weights (w = 1/d^p)\n    weights = 1 / (nearest_distances ** power)\n\n    # Compute weighted average for each soil property\n    for prop in soil_properties:\n        weighted_avg = np.round(np.sum(weights * nearest_soil_points[prop].values) / np.sum(weights), 4)\n        weighted_results[prop].append(weighted_avg)\n\n# Assign weighted values to climate GeoDataFrame\nfor prop in soil_properties:\n    dwd_cropland_gdf[f\"{prop}\"] = weighted_results[prop]\n</code></pre> <pre><code>0it [00:00, ?it/s]\n</code></pre> <pre><code># Add soil layer depth columns (needed for PBMs)\ndwd_cropland_gdf['SoilLayerDepth_10cm'] = 0.1\ndwd_cropland_gdf['SoilLayerDepth_30cm'] = 0.3\ndwd_cropland_gdf['SoilLayerDepth_50cm'] = 0.5\ndwd_cropland_gdf['SoilLayerDepth_70cm'] = 0.7\ndwd_cropland_gdf['SoilLayerDepth_1m'] = 1\ndwd_cropland_gdf['SoilLayerDepth_2m'] = 2\n\n# Reorder the columns\ndwd_cropland_gdf = dwd_cropland_gdf[list(dwd_cropland_gdf.columns[:7]) + list(soil_attributes_gdf.columns[6:12]) + list(soil_properties) + ['geometry']]\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>(190364, 38)\n</code></pre> Cell_ID Row Column Latitude Longitude NUTS_ID NUTS_NAME SoilLayerDepth_10cm SoilLayerDepth_30cm SoilLayerDepth_50cm ... BD_70cm BD_1m BD_2m OC_10cm OC_30cm OC_50cm OC_70cm OC_1m OC_2m geometry 0 0 703 234 48.737371 9.201742 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5407 1.5394 1.5394 1.2970 1.1749 0.3660 0.3316 0.1847 0.1847 POINT (1024333.233 6230415.593) 1 1 691 236 48.845227 9.229425 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.4462 1.4607 1.4607 2.2463 1.8119 0.6600 0.5122 0.2873 0.2873 POINT (1027414.872 6248640.293) 2 2 707 233 48.701424 9.188012 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5569 1.4867 1.4867 1.3170 1.1219 0.3438 0.3209 0.1713 0.1713 POINT (1022804.862 6224350.32) 3 3 706 233 48.710416 9.188046 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5501 1.5007 1.5007 1.2979 1.1741 0.3449 0.3250 0.1761 0.1761 POINT (1022808.604 6225867.209) 4 4 705 233 48.719409 9.188080 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5484 1.5136 1.5136 1.3000 1.1741 0.3491 0.3254 0.1780 0.1780 POINT (1022812.348 6227384.366) <p>5 rows \u00d7 38 columns</p> <pre><code># Save the data\n# dwd_cropland_gdf.drop(columns='geometry').to_csv(os.path.join(out_master_dir, 'DE_DWD_UBN_Crop_Soil.csv'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/","title":"Data Preparation For Pbms","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#data-preparation-for-pbms-2","title":"Data Preparation for PBMs - 2","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport os\nfrom glob import glob\nimport json\nfrom tqdm.auto import tqdm\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nout_master_dir = r'datasets\\master'\nout_temp_dir = r'temp_data'\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the Soil Hydraulic Property dataset\npbm_data = pd.read_csv('datasets\\csvs\\soilhydraulic_property_Germany_Points_Amit.csv', delimiter=';')\npbm_data = pbm_data.iloc[:, :-2]\nprint(pbm_data.shape)\npbm_data.head()\n</code></pre> <pre><code>(190364, 144)\n</code></pre> location dampingdepth soilwater_fc_global soilwater_sat_global drainage_rate deltatheta DZF depth_1 depth_2 depth_3 ... InitialFixedPConcentration_6 slimalfa_1 slimalfa_2 slimalfa_3 slimalfa_4 slimalfa_5 slimalfa_6 Nitrogen Phosphorous Potassium 0 0 6 0.305742 0.41441 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 1 1 6 0.352146 0.447852 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 2 2 6 0.292897 0.422044 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 3 3 6 0.300213 0.420899 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 4 4 6 0.30233 0.418748 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 <p>5 rows \u00d7 144 columns</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#data-processing","title":"Data Processing","text":"<pre><code># Divide the Nitrogen, Phosphorous, and Potassium data by 10\npbm_data[['Nitrogen', 'Phosphorous', 'Potassium']] = pbm_data[['Nitrogen', 'Phosphorous', 'Potassium']] / 10\nprint(pbm_data.shape)\npbm_data.head()\n</code></pre> <pre><code>(190364, 144)\n</code></pre> location dampingdepth soilwater_fc_global soilwater_sat_global drainage_rate deltatheta DZF depth_1 depth_2 depth_3 ... InitialFixedPConcentration_6 slimalfa_1 slimalfa_2 slimalfa_3 slimalfa_4 slimalfa_5 slimalfa_6 Nitrogen Phosphorous Potassium 0 0 6 0.305742 0.41441 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 1 1 6 0.352146 0.447852 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 2 2 6 0.292897 0.422044 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 3 3 6 0.300213 0.420899 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 4 4 6 0.30233 0.418748 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 <p>5 rows \u00d7 144 columns</p> <pre><code># Define the scenarios\ndef process_data(row, fertilizer_scenario=2, crop_cyle_count=0, crop='winter wheat'):\n\n    location_values = [row['location']] * 6 \n    fertilizer_scenarios = [fertilizer_scenario] * 6\n    crop_cyle_counts = [crop_cyle_count] * 6\n    crop_values = [crop] * 6\n    type_values = ['PTotal', 'KTotal', 'NTotal'] * 2\n    dvs_values = [0.001, 0.001, 0.25, 0.4, 0.4, 0.9]\n    event_values = [1, 2, 3, 4, 5, 6]\n    fertilizer_value = [round(float(v), 6) for v in [row['Phosphorous'], row['Potassium'], row['Nitrogen']]]\n    fertilizer_values =  [round((v/2), 6) for v  in (fertilizer_value * 2)]\n\n    final_df = pd.DataFrame({\n        'location': location_values,\n        'FertilizerScenario': fertilizer_scenarios,\n        'CropCycleCount': crop_cyle_counts,\n        'crop': crop_values,\n        'Event': event_values,\n        'vType': type_values,\n        'DVS': dvs_values,\n        'Amount': fertilizer_values\n    })\n\n    return final_df\n</code></pre> <pre><code># Apply the algorithm on each rows\npbm_data_processed = pbm_data.apply(process_data, axis=1)\npbm_data_processed = pd.concat(pbm_data_processed.tolist(), ignore_index=True)\nprint(pbm_data_processed.shape)\npbm_data_processed.head()\n</code></pre> <pre><code>(1142184, 8)\n</code></pre> location FertilizerScenario CropCycleCount crop Event vType DVS Amount 0 0 2 0 winter wheat 1 PTotal 0.001 0.358030 1 0 2 0 winter wheat 2 KTotal 0.001 0.849858 2 0 2 0 winter wheat 3 NTotal 0.250 6.022877 3 0 2 0 winter wheat 4 PTotal 0.400 0.358030 4 0 2 0 winter wheat 5 KTotal 0.400 0.849858 <pre><code># Save the data \n# pbm_data_processed.to_csv(os.path.join(out_master_dir, 'fertilizer_Soil3_AllKreis_Krishna.csv'), index=False)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/","title":"Flood Susceptibility Zonation of Malda District","text":"<p>This project focuses on identifying flood-prone areas in the Malda District using various machine learning algorithms.</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/#analysis-notebooks","title":"Analysis Notebooks","text":"<ul> <li>Training and Testing Data Preparation</li> <li>Random Forest Application</li> <li>XGBoost Application</li> <li>Logistic Regression Application</li> <li>SVM Application</li> <li>Convert Image to CSV</li> <li>Classify the Image</li> <li>Assessment of Validation Metrics</li> </ul>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/","title":"Classify The Image","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image","title":"Classify the Image","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#load-the-original-parameters-image","title":"Load the Original Parameters Image","text":"<pre><code>parameter_img = rasterio.open(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\Rasters\\Maldah_Flood_Parameters.tif\")\n</code></pre> <pre><code># Store the image parameters in separate variables\nbandNum = parameter_img.count\nheight = parameter_img.height\nwidth = parameter_img.width\ncrs = parameter_img.crs\ntransform = parameter_img.transform\nshape = (height, width)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#read-the-image-as-dataframe","title":"Read the Image as DataFrame","text":"<pre><code>image = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Image_CSV.csv\")\nimage\n</code></pre> <pre><code># Remove unnecessary column\nimage.drop(\"Unnamed: 0\", axis=1, inplace=True)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#load-the-models","title":"Load the Models","text":"<pre><code>import pickle\n</code></pre> <pre><code># Import the models\nrf_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\rf_model.pkl\"\nxgb_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\xgb_model.pkl\"\nlog_reg_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\log_reg_model.pkl\"\nsvm_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\svm_model.pkl\"\n\nrf_model = pickle.load(open(rf_model_path, \"rb\"))\nxgb_model = pickle.load(open(xgb_model_path, \"rb\"))\nlog_reg_model = pickle.load(open(log_reg_model_path, \"rb\"))\nsvm_model = pickle.load(open(svm_model_path, \"rb\"))\n</code></pre> <pre><code>rf_model\n</code></pre> <pre><code>xgb_model\n</code></pre> <pre><code>log_reg_model\n</code></pre> <pre><code>svm_model\n</code></pre> <pre><code>rf_model.feature_names_in_\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image_1","title":"Classify the Image","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image-with-random-forest","title":"Classify the Image with Random Forest","text":"<pre><code># Classify the image with RF Model\nrf_predict = rf_model.predict(image)\n</code></pre> <pre><code># Predict the Probability of Classification \nrf_predict_prob = rf_model.predict_proba(image)\nrf_predict_prob = rf_predict_prob[:, 1]\nrf_predict_prob\n</code></pre> <pre><code># Reshape the array\nrf_classified_image = rf_predict_prob.reshape((3267, 2351))\nrf_classified_image\n</code></pre> <pre><code># Plot the image\nplt.figure(figsize=(8, 6))\nplt.imshow(rf_classified_image, cmap=\"coolwarm\")\nplt.title(\"RF Classified Flood Susceptibility Map\")\nplt.colorbar(label=\"Flood Probability\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image-with-xgboost","title":"Classify the Image with Xgboost","text":"<pre><code># Classify the image with Xgboost Model\nxgb_predict = xgb_model.predict(image)\n</code></pre> <pre><code># Predict the Probability of Classification \nxgb_predict_prob = xgb_model.predict_proba(image)\nxgb_predict_prob = xgb_predict_prob[:, 1]\nxgb_predict_prob\n</code></pre> <pre><code># Reshape the array\nxgb_classified_image = xgb_predict_prob.reshape((3267, 2351))\nxgb_classified_image\n</code></pre> <pre><code># Plot the image\nplt.figure(figsize=(8, 6))\nplt.imshow(xgb_classified_image, cmap=\"coolwarm\")\nplt.title(\"Xgboost Classified Flood Susceptibility Map\")\nplt.colorbar(label=\"Flood Probability\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image-with-logistic-regression","title":"Classify the Image with Logistic Regression","text":"<pre><code># Classify the image with Logistic Regression Model\nlog_reg_predict = log_reg_model.predict(image)\n</code></pre> <pre><code># Predict the Probability of Classification \nlog_reg_predict_prob = log_reg_model.predict_proba(image)\nlog_reg_predict_prob = log_reg_predict_prob[:, 1]\nlog_reg_predict_prob\n</code></pre> <pre><code># Reshape the array\nlog_reg_classified_image = log_reg_predict_prob.reshape((3267, 2351))\nlog_reg_classified_image\n</code></pre> <pre><code># Plot the image\nplt.figure(figsize=(8, 6))\nplt.imshow(log_reg_classified_image, cmap=\"coolwarm\")\nplt.title(\"Logistic Regression Classified Flood Susceptibility Map\")\nplt.colorbar(label=\"Flood Probability\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#classify-the-image-with-support-vector-machine","title":"Classify the Image with Support Vector Machine","text":"<pre><code># Classify the image with SVM Model\nsvm_predict = svm_model.predict(image)\n</code></pre> <pre><code># Predict the Probability of Classification \nsvm_predict_prob = svm_model.predict_proba(image)\nsvm_predict_prob = svm_predict_prob[:, 1]\nsvm_predict_prob\n</code></pre> <pre><code># Reshape the array\nsvm_classified_image = svm_predict_prob.reshape((3267, 2351))\nsvm_classified_image\n</code></pre> <pre><code># Plot the image\nplt.figure(figsize=(8, 6))\nplt.imshow(svm_classified_image, cmap=\"coolwarm\")\nplt.title(\"SVM Classified Flood Susceptibility Map\")\nplt.colorbar(label=\"Flood Probability\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/10_Classify_the_Image/#export-the-images","title":"Export the Images","text":"<pre><code># # Save the image to the file\n# folder_path = r\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\Rasters\\Outputs\"\n# file_name = \"\\SVM_Flood_Prob.tif\"\n# location = folder_path + file_name\n\n# output = rasterio.open(\n#     location,\n#     mode='w',\n#     driver=\"GTiff\",\n#     width=parameter_img.shape[1],\n#     height=parameter_img.shape[0],\n#     count=1,\n#     crs=crs,\n#     transform=transform,\n#     dtype=str(svm_classified_image.dtype),\n# )\n\n# output.write(svm_classified_image, 1)\n# output.close()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/","title":"Assessment Of Validation Metrics","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#assessment-of-validation-metrics","title":"Assessment of Validation Metrics","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#load-all-the-models","title":"Load all the Models","text":"<pre><code># Import the models\n# Import the models\nrf_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\rf_model.pkl\"\nxgb_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\xgb_model.pkl\"\nlog_reg_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\log_reg_model.pkl\"\nsvm_model_path = r\"D:\\Coding\\Git Repository\\Research Repo\\Flood\\Flood-Susceptibility-Zonation-of-Maldah\\Model\\svm_model.pkl\"\n\nrf_model = pickle.load(open(rf_model_path, \"rb\"))\nxgb_model = pickle.load(open(xgb_model_path, \"rb\"))\nlog_reg_model = pickle.load(open(log_reg_model_path, \"rb\"))\nsvm_model = pickle.load(open(svm_model_path, \"rb\"))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#import-test-data","title":"Import Test Data","text":"<pre><code># Import the testing data\ntesting_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Testing_Data.csv\")\ntesting_df.head()\n</code></pre> <pre><code># Select the best feature\nX_test = testing_df[rf_model.feature_names_in_]\ny_test = testing_df[\"Flood\"]\n</code></pre> <pre><code>X_test\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#plot-the-roc-and-auc","title":"Plot the ROC and AUC","text":"<pre><code>from sklearn.metrics import roc_curve, roc_auc_score\n</code></pre> <pre><code># Store the classifiers in a list\nclassifiers = [\n    (\"Logistic Regression\", log_reg_model),\n    (\"Support Vector Machine\", svm_model),\n    (\"Random Forest\", rf_model),\n    (\"XGBoost\", xgb_model)\n]\n</code></pre> <pre><code># Create a plot for ROC curves\nplt.figure(figsize=(6, 6), dpi=100)\n\nsns.set(style=\"whitegrid\")\n# sns.set(font='Times New Roman')\n\n# Loop through each classifier\nfor name, classifier in classifiers:\n    # Predict the test data\n    classifier.predict(X_test)\n\n    # Predict probabilities\n    y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n\n    # Calculate the ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n    # Calculate the AUC score\n    auc = roc_auc_score(y_test, y_pred_prob)\n\n    # Plot the ROC curve\n    sns.lineplot(x=fpr, y=tpr, label=f\"{name} (AUC = {auc:.3f})\")\n\n# Add labels and legend\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate', fontname=\"Times New Roman\")\nplt.ylabel('True Positive Rate', fontname=\"Times New Roman\")\nplt.xticks(fontname=\"Times New Roman\")\nplt.yticks(fontname=\"Times New Roman\")\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontname=\"Times New Roman\")\nlegend = plt.legend(loc='best')\n\nfor text in legend.get_texts():\n    text.set_fontname('Times New Roman')\n    text.set_fontsize(12)\n\n# Show the plot\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#plot-the-validation-metrics","title":"Plot the Validation Metrics","text":"<pre><code>from sklearn.metrics import classification_report\n</code></pre> <pre><code>for name, classifier in classifiers:\n    print(name)\n    print(classification_report(y_test, classifier.predict(X_test)))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#plot-shap-values","title":"Plot SHAP Values","text":"<pre><code>import shap\n</code></pre> <pre><code>order = X_test.columns\ncol2num = {col: i for i, col in enumerate(X_test.columns)}\n\norder = list(map(col2num.get, order))\n</code></pre> <pre><code>order\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#shap-values-of-logistic-regression","title":"Shap Values of Logistic Regression","text":"<pre><code># Fits the explainer\nlog_reg_explainer = shap.Explainer(log_reg_model.predict, X_test)\n\n# Calculates the SHAP values - It takes some time\nlog_shap_values = log_reg_explainer(X_test)\n</code></pre> <pre><code>plt.figure()\nshap.plots.beeswarm(log_shap_values, max_display=None, order=order, plot_size=(6, 10))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#shap-values-of-support-vector-machine","title":"SHAP Values of Support Vector Machine","text":"<pre><code># Fits the explainer\nsvm_explainer = shap.Explainer(svm_model.predict, X_test)\n\n# Calculates the SHAP values - It takes some time\nsvm_shap_values = svm_explainer(X_test)\n</code></pre> <pre><code>plt.figure()\nshap.plots.beeswarm(svm_shap_values, max_display=None, order=order, plot_size=(6, 10))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#shap-values-of-random-forest","title":"SHAP Values of Random Forest","text":"<pre><code># Fits the explainer\nrf_explainer = shap.Explainer(rf_model.predict, X_test)\n\n# Calculates the SHAP values - It takes some time\nrf_shap_values = rf_explainer(X_test)\n</code></pre> <pre><code>plt.figure()\nshap.plots.beeswarm(rf_shap_values, max_display=None, order=order, plot_size=(6, 10))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/11_Assessment_of_Validation_Metrics/#shap-values-of-xgboost","title":"SHAP Values of XGBoost","text":"<pre><code># Fits the explainer\nxgb_explainer = shap.Explainer(xgb_model.predict, X_test)\n\n# Calculates the SHAP values - It takes some time\nxgb_shap_values = xgb_explainer(X_test)\n</code></pre> <pre><code>plt.figure()\nshap.plots.beeswarm(xgb_shap_values, max_display=None, order=order, plot_size=(6, 10))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/","title":"Training And Testing Data Preparation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#training-and-testing-data-preparation","title":"Training and Testing Data Preparation","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#read-the-data","title":"Read the Data","text":"<pre><code># Read the flood samples data\ngdf = gpd.read_file(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\Shapefiles\\Flood_Sample_Data.shp\")\nprint(gdf.shape)\ngdf.head()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#preprocess-the-data","title":"Preprocess the Data","text":"<pre><code># Check the column informations\ngdf.info()\n</code></pre> <pre><code># Change the name of the columns\ngdf.columns\n</code></pre> <pre><code># Change the name of the columns\nnew_col_names = ['Relief_Amplitude', 'Dist_to_River', 'LULC', 'TWI', 'Rainfall', \n                 'Clay_Content', 'STI', 'TRI', 'TPI',\n                 'SPI', 'NDVI', 'Slope', 'MFI', 'Elevation', 'Flood', 'MNDWI',\n                 'Drainage_Density', 'Lithology', 'Geomorphology', 'geometry']\n\n# Create a dictionary\nnew_col_dict = dict(zip(gdf.columns, new_col_names))\n\n# Change the name of the columns\ngdf.rename(columns=new_col_dict, inplace=True)\n</code></pre> <pre><code>gdf.head()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#rename-the-values-of-the-categorical-variables","title":"Rename the Values of the Categorical Variables","text":"<pre><code># Define the values for the geomorphology\ngeomorpholoy_dict = {1: \"Active_Flood_Plain\",\n                     2: \"Embankment\",\n                     3: \"Older_Alluvial_Plain\",\n                     4: \"Older_Flood_Plain\",\n                     5: \"Pond\",\n                     6: \"River\",\n                     7: \"WatBod_Lake\",\n                     8: \"Younger_Alluvial_Plain\"}\n\n# Define the values for the lithology\nlithology_dict = {1: \"Cl_wi_S_Si_Ir_N\",\n                  2: \"Fe_Ox_S_Si_Cl\",\n                  3: \"S_Si_Gr\",\n                  4: \"S_Si_Cl\",\n                  5: \"S_Si_Cl_wi_Cal_Co\"}\n\n# Define the values for the LULC\nlulc_dict = {1: \"Waterbodies\",\n             2: \"Natural_Vegetation\",\n             3: \"Agricultural_Field\",\n             4: \"Bare_Ground\",\n             5: \"Built_UP_Area\"}\n</code></pre> <pre><code>gdf.replace({\"Geomorphology\": geomorpholoy_dict, \"Lithology\": lithology_dict, \"LULC\": lulc_dict},\n             inplace=True)\ngdf.head()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#train-test-split","title":"Train Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(gdf.drop(\"Flood\", axis=1),\n                                                    gdf[\"Flood\"],\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#apply-ohe-on-geomorphology-lithology-and-lulc-columns","title":"Apply OHE on 'Geomorphology', 'Lithology' and 'LULC' Columns","text":"<pre><code># Apply One Hot Encoding on the training data\nX_train_encoded = pd.get_dummies(X_train, columns=[\"Geomorphology\", \"Lithology\", \"LULC\"])\nX_train_encoded.info()\n</code></pre> <pre><code># Apply One Hot Encoding on the testing data\nX_test_encoded = pd.get_dummies(X_test, columns=[\"Geomorphology\", \"Lithology\", \"LULC\"])\nX_test_encoded.info()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#reset-the-index-of-the-dataframe-and-series","title":"Reset the Index of the Dataframe and Series","text":"<pre><code>X_train_encoded.reset_index(drop=True, inplace=True)\nX_test_encoded.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#finalize-the-training-and-testing-data","title":"Finalize the Training and Testing Data","text":"<pre><code># Add the y_train to X_train_encoded\nX_train_encoded[\"Flood\"] = y_train\ntraining_df = X_train_encoded\n</code></pre> <pre><code>training_df.info()\n</code></pre> <pre><code># Add the y_test to X_test_encoded\nX_test_encoded[\"Flood\"] = y_test\ntesting_df = X_test_encoded\n</code></pre> <pre><code>testing_df.info()\n</code></pre> <pre><code>training_df.head()\n</code></pre> <pre><code>testing_df.head()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/4_Training_and_Testing_Data_Preparation/#export-the-training-and-testing-data","title":"Export the Training and Testing Data","text":"<pre><code>output_folder_csv = \"D:\\\\Research Works\\\\Flood\\\\Flood_Risk_Zonation_of_Maldah\\\\Datasets\\\\CSVs\\\\\"\noutput_folder_shp = \"D:\\\\Research Works\\\\Flood\\\\Flood_Risk_Zonation_of_Maldah\\\\Datasets\\\\Shapefiles\\\\\"\n</code></pre> <pre><code># Export as CSV files\n# training_df.to_csv(output_folder_csv+\"Training_Data.csv\")\n# testing_df.to_csv(output_folder_csv+\"Testing_Data.csv\")\n</code></pre> <pre><code># Export as SHP files\ntraining_gdf = gpd.GeoDataFrame(training_df, geometry=training_df.geometry)\ntesting_gdf = gpd.GeoDataFrame(testing_df, geometry=testing_df.geometry)\n\n# training_gdf.to_file(output_folder_shp+\"Training_Data.shp\", driver=\"ESRI Shapefile\")\n# testing_gdf.to_file(output_folder_shp+\"Testing_Data.shp\", driver=\"ESRI Shapefile\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/","title":"Application Of Random Forest For Flood Susceptibility Zonation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#application-of-random-forest-for-flood-susceptibility-zonation","title":"Application of Random Forest for Flood Susceptibility Zonation","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"whitegrid\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#read-the-data","title":"Read the Data","text":"<pre><code>training_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Training_Data.csv\")\ntesting_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Testing_Data.csv\")\n</code></pre> <pre><code>training_df.head()\n</code></pre> <pre><code># Drop the geometry columns\ntraining_df.drop(\"geometry\", axis=1, inplace=True)\ntesting_df.drop(\"geometry\", axis=1, inplace=True)\n\ntraining_df.columns == testing_df.columns\n</code></pre> <pre><code>training_df.shape\n</code></pre> <pre><code># Check for the null values\ntraining_df.isnull().sum().sum()\n</code></pre> <pre><code>testing_df.isnull().sum().sum()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#specify-the-train-test-data","title":"Specify the Train Test Data","text":"<pre><code>X_train, y_train = training_df.drop(\"Flood\", axis=1), training_df[\"Flood\"]\nX_test, y_test = testing_df.drop(\"Flood\", axis=1), testing_df[\"Flood\"]\n\nX_train.shape, X_test.shape\n</code></pre> <pre><code>X_train.head()\n</code></pre> <pre><code># Change the datatype into float\nX_train = X_train.astype(float)\nX_test = X_test.astype(float)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#feature-selection-using-information-gain","title":"Feature Selection using Information Gain","text":"<pre><code>from sklearn.feature_selection import mutual_info_classif\n</code></pre> <pre><code># Determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info\n</code></pre> <pre><code># Convert the mutual info array into a pandas series\nmutual_info = pd.Series(mutual_info, index=X_train.columns).sort_values(ascending=False)\nmutual_info\n</code></pre> <pre><code># Plot the orderd mutual_info values per feature\nplt.figure(figsize=(12, 4), dpi=100)\n\n# Define a color palette\ncolor_palette = sns.color_palette(palette=\"coolwarm\", n_colors=len(mutual_info))\n\nsns.barplot(x=mutual_info.index, y=mutual_info, palette=color_palette,\n            edgecolor=\"black\", linewidth=0.5)\nplt.title(\"Mutual Information\", fontname=\"Times New Roman\", color=\"black\", fontsize=12)\nplt.xticks(rotation=90)\nplt.xticks(fontname=\"Times New Roman\")\nplt.yticks(fontname=\"Times New Roman\")\nplt.ylabel(\"Information Gain Value\", fontname=\"Times New Roman\")\nplt.show()\n</code></pre> <pre><code>from sklearn.feature_selection import SelectKBest\n</code></pre> <pre><code># Select the top 20 important features\nselected_features = SelectKBest(mutual_info_classif, k=20)\nselected_features.fit(X_train, y_train)\nselected_features = X_train.columns[selected_features.get_support()]\nselected_features\n</code></pre> <pre><code>X_train = X_train[selected_features]\nX_train\n</code></pre> <pre><code>X_test = X_test[selected_features]\nX_test\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#apply-random-forest-classification","title":"Apply Random Forest Classification","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#build-a-random-forest-classification-model","title":"Build a Random Forest Classification Model","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n</code></pre> <pre><code># Instantiate a RandomForestClassifier object\nrf = RandomForestClassifier()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Define all the hyperparameters\n\n# Number of trees in random forest\nn_estimators = [50, 75, 100, 150, 200, 300]\n\n# Criterion\ncriterion = [\"gini\", \"entropy\"]\n\n# Maximum depth of each tree\nmax_depth = [2, 4, 6, 8, 10, None]\n\n# Number of features to consider at each split\nmax_features = [0.2, 0.4, 0.6, 0.8, 1.0]\n\n# Minimum number of samples required to split an internal node\nmin_samples_split = [2, 4, 8, 10]\n\n# Minimumn number of samples required to be a leaf node\nmin_samples_leaf = [1, 2, 4, 8]\n\n# Number of samples\nmax_samples = [0.25, 0.5, 0.75, 1.0]\n\n# Bootstrap\nbootstrap = [True, False]\n</code></pre> <pre><code># Define the parameter grid in a dictionary\nrf_param_grid = {\"n_estimators\": n_estimators,\n                 \"criterion\": criterion,\n                 \"max_depth\": max_depth,\n                 \"max_features\": max_features,\n                 \"min_samples_split\": min_samples_split,\n                 \"min_samples_leaf\": min_samples_leaf,\n                 \"max_samples\": max_samples,\n                 \"bootstrap\": bootstrap}\nrf_param_grid\n</code></pre> <pre><code>from sklearn.model_selection import RandomizedSearchCV\n</code></pre> <pre><code># Apply Randomized Search CV\nrf_grid = RandomizedSearchCV(estimator=rf,\n                             param_distributions=rf_param_grid,\n                             n_iter=1000,\n                             scoring=\"accuracy\",\n                             n_jobs=-1,\n                             cv=5,\n                             verbose=1)\n</code></pre> <pre><code># Fit the training data to GridSearcCV\nrf_grid.fit(X_train, y_train)\n</code></pre> <pre><code>rf_grid.best_params_\n</code></pre> <pre><code># Check the best score\nrf_grid.best_score_\n</code></pre> <pre><code># Build a Random Forest Model with best estimators\nrf_final = rf_grid.best_estimator_\nrf_final\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = rf_final.predict(X_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#feature-importance","title":"Feature Importance","text":"<pre><code># Get the feature importance\nfeature_importance = rf_final.feature_importances_\n\n# Convert the feature importance into a pandas series\nfeature_importance = pd.Series(feature_importance, index=X_train.columns)\n\n# Sort the values in descending order\nfeature_importance = feature_importance.sort_values(ascending=False)\nfeature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(6, 8), dpi=100)\n\n# Define a color palette\ncolor_palette = sns.color_palette(palette=\"coolwarm\", n_colors=len(feature_importance))\n\nsns.barplot(x=feature_importance, y=feature_importance.index, palette=color_palette, \n            edgecolor=\"black\", linewidth=0.5)\nplt.title(\"Feature Importance\", fontname=\"Times New Roman\", color=\"black\", fontsize=12)\nplt.xticks(fontname=\"Times New Roman\")\nplt.yticks(fontname=\"Times New Roman\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/5_Application_of_Random_Forest_for_Flood_Susceptibility_Zonation/#export-the-model","title":"Export the Model","text":"<pre><code>import pickle\n</code></pre> <pre><code>output_folder = \"D:\\\\Coding\\\\Git Repository\\\\Research Repo\\\\Flood\\\\Flood-Susceptibility-Zonation-of-Maldah\\\\Model\\\\\"\nmodel_name = \"rf_model.pkl\"\n</code></pre> <pre><code># Export the model\n# pickle.dump(rf_final, file=open(output_folder+model_name, \"wb\"))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/","title":"Application Of Xgboost For Flood Susceptibility Zonation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#application-of-xgboost-for-flood-susceptibility-zonation","title":"Application of Xgboost for Flood Susceptibility Zonation","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"whitegrid\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#read-the-data","title":"Read the Data","text":"<pre><code>training_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Training_Data.csv\")\ntesting_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Testing_Data.csv\")\n</code></pre> <pre><code>training_df.head()\n</code></pre> <pre><code># Drop the geometry columns\ntraining_df.drop(\"geometry\", axis=1, inplace=True)\ntesting_df.drop(\"geometry\", axis=1, inplace=True)\n\ntraining_df.columns == testing_df.columns\n</code></pre> <pre><code>training_df.shape\n</code></pre> <pre><code># Check for the null values\ntraining_df.isnull().sum().sum()\n</code></pre> <pre><code>testing_df.isnull().sum().sum()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#specify-the-train-test-data","title":"Specify the Train Test Data","text":"<pre><code>X_train, y_train = training_df.drop(\"Flood\", axis=1), training_df[\"Flood\"]\nX_test, y_test = testing_df.drop(\"Flood\", axis=1), testing_df[\"Flood\"]\n\nX_train.shape, X_test.shape\n</code></pre> <pre><code>X_train.head()\n</code></pre> <pre><code># Change the datatype into float\nX_train = X_train.astype(float)\nX_test = X_test.astype(float)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#feature-selection-using-information-gain","title":"Feature Selection using Information Gain","text":"<pre><code># Select the important features\nselected_features = ['Dist_to_River', 'TWI', 'Rainfall', 'Clay_Content', 'TRI', 'NDVI',\n                     'MFI', 'Elevation', 'MNDWI', 'Drainage_Density', 'Geomorphology_Active_Flood_Plain',\n                     'Geomorphology_Older_Alluvial_Plain', 'Geomorphology_Older_Flood_Plain',\n                     'Lithology_Cl_wi_S_Si_Ir_N', 'Lithology_Fe_Ox_S_Si_Cl',\n                     'Lithology_S_Si_Cl', 'Lithology_S_Si_Cl_wi_Cal_Co',\n                     'LULC_Agricultural_Field', 'LULC_Built_UP_Area',\n                     'LULC_Natural_Vegetation']\n</code></pre> <pre><code>X_train = X_train[selected_features]\nX_train\n</code></pre> <pre><code>X_test = X_test[selected_features]\nX_test\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#apply-xgboost-classification","title":"Apply Xgboost Classification","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#build-an-xgboost-model","title":"Build an Xgboost Model","text":"<pre><code>from xgboost import XGBClassifier\n</code></pre> <pre><code># Instantiate a XGBClassifier object\nxgb = XGBClassifier()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Define all the hyperparameters for Xgboost Model\n\n# Number of boosting rounds\nn_estimators = [25, 50, 100, 150, 200, 300]\n\n# Step size shrinkage\nlearning_rate = [0.05, 0.1, 0.15, 0.2, 0.3]\n\n# Maximum depth of the trees\nmax_depth = [2, 4, 6, 8, None]\n\n# Minimum sum of instance weight (hessian) needed in a child\nmin_child_weight = [1, 3, 5, 7]\n\n# Minimum loss reduction required to make a further partition on a leaf node\ngamma = [0.0, 0.1, 0.2, 0.3, 0.4]\n\n# Fraction of features used for fitting the trees\ncolsample_bytree = [0.3, 0.5, 0.7, 1.0]\n\n# Subsample ratio of the training instance\nsubsample = [0.3, 0.5, 0.7, 1]\n</code></pre> <pre><code># Define the parameter grid in a dictionary\nxgb_param_grid = {\"n_estimators\": n_estimators,\n                  \"learning_rate\": learning_rate,\n                  \"max_depth\": max_depth,\n                  \"min_child_weight\": min_child_weight,\n                  \"gamma\": gamma,\n                  \"colsample_bytree\": colsample_bytree,\n                  \"subsample\": subsample}\nxgb_param_grid\n</code></pre> <pre><code>from sklearn.model_selection import RandomizedSearchCV\n</code></pre> <pre><code># Apply Randomized Search CV\nxgb_grid = RandomizedSearchCV(estimator=xgb,\n                              param_distributions=xgb_param_grid,\n                              n_iter=1000,\n                              scoring=\"accuracy\",\n                              n_jobs=-1,\n                              cv=5,\n                              verbose=1)\n</code></pre> <pre><code># Fit the training data to Randomized Search CV\nxgb_grid.fit(X_train, y_train)\n</code></pre> <pre><code>xgb_grid.best_params_\n</code></pre> <pre><code># Check the best score\nxgb_grid.best_score_\n</code></pre> <pre><code># Build a Xgboost Model with best estimators\nXgb_final = xgb_grid.best_estimator_\nXgb_final\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = Xgb_final.predict(X_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#feature-importance","title":"Feature Importance","text":"<pre><code># Get the feature importance\nfeature_importance = Xgb_final.feature_importances_\n\n# Convert the feature importance into a pandas series\nfeature_importance = pd.Series(feature_importance, index=X_train.columns)\n\n# Sort the values in descending order\nfeature_importance = feature_importance.sort_values(ascending=False)\nfeature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(6, 8), dpi=100)\n\n# Define a color palette\ncolor_palette = sns.color_palette(palette=\"coolwarm\", n_colors=len(feature_importance))\n\nsns.barplot(x=feature_importance, y=feature_importance.index, palette=color_palette, \n            edgecolor=\"black\", linewidth=0.5)\nplt.title(\"Feature Importance\", fontname=\"Times New Roman\", color=\"black\", fontsize=12)\nplt.xticks(fontname=\"Times New Roman\")\nplt.yticks(fontname=\"Times New Roman\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/6_Application_of_Xgboost_for_Flood_Susceptibility_Zonation/#export-the-model","title":"Export the Model","text":"<pre><code>import pickle\n</code></pre> <pre><code>output_folder = \"D:\\\\Coding\\\\Git Repository\\\\Research Repo\\\\Flood\\\\Flood-Susceptibility-Zonation-of-Maldah\\\\Model\\\\\"\nmodel_name = \"xgb_model.pkl\"\n</code></pre> <pre><code># Export the model\n# pickle.dump(Xgb_final, file=open(output_folder+model_name, \"wb\"))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/","title":"Application Of Logistic Regression For Flood Susceptibility Zonation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#application-of-logistic-regression-for-flood-susceptibility-zonation","title":"Application of Logistic Regression for Flood Susceptibility Zonation","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"whitegrid\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#read-the-data","title":"Read the Data","text":"<pre><code>training_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Training_Data.csv\")\ntesting_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Testing_Data.csv\")\n</code></pre> <pre><code>training_df.head()\n</code></pre> <pre><code># Drop the geometry columns\ntraining_df.drop(\"geometry\", axis=1, inplace=True)\ntesting_df.drop(\"geometry\", axis=1, inplace=True)\n\ntraining_df.columns == testing_df.columns\n</code></pre> <pre><code>training_df.shape\n</code></pre> <pre><code># Check for the null values\ntraining_df.isnull().sum().sum()\n</code></pre> <pre><code>testing_df.isnull().sum().sum()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#specify-the-train-test-data","title":"Specify the Train Test Data","text":"<pre><code>X_train, y_train = training_df.drop(\"Flood\", axis=1), training_df[\"Flood\"]\nX_test, y_test = testing_df.drop(\"Flood\", axis=1), testing_df[\"Flood\"]\n\nX_train.shape, X_test.shape\n</code></pre> <pre><code>X_train.head()\n</code></pre> <pre><code># Change the datatype into float\nX_train = X_train.astype(float)\nX_test = X_test.astype(float)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#feature-selection-using-information-gain","title":"Feature Selection using Information Gain","text":"<p>```python jupyter={\"source_hidden\": true}</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#select-the-important-features","title":"Select the important features","text":"<p>selected_features = ['Dist_to_River', 'TWI', 'Rainfall', 'Clay_Content', 'TRI', 'NDVI',                      'MFI', 'Elevation', 'MNDWI', 'Drainage_Density', 'Geomorphology_Active_Flood_Plain',                      'Geomorphology_Older_Alluvial_Plain', 'Geomorphology_Older_Flood_Plain',                      'Lithology_Cl_wi_S_Si_Ir_N', 'Lithology_Fe_Ox_S_Si_Cl',                      'Lithology_S_Si_Cl', 'Lithology_S_Si_Cl_wi_Cal_Co',                      'LULC_Agricultural_Field', 'LULC_Built_UP_Area',                      'LULC_Natural_Vegetation'] <pre><code>```python\nX_train = X_train[selected_features]\nX_train\n</code></pre></p> <pre><code>X_test = X_test[selected_features]\nX_test\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#apply-logistic-regression-classification","title":"Apply Logistic Regression Classification","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#build-a-logistic-regression-model","title":"Build a Logistic Regression Model","text":"<pre><code>from sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code># Instantiate a LogisticRegression object\nlog_reg = LogisticRegression()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Define all the hyperparameters for Logistic Regression Model\nlog_reg_param_grid = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n    'max_iter': [100, 200, 300],\n    'class_weight': ['balanced', None]\n}\n</code></pre> <pre><code>from sklearn.model_selection import RandomizedSearchCV\n</code></pre> <pre><code># Apply Randomized Search CV\nlog_reg_grid = RandomizedSearchCV(estimator=log_reg,\n                                  param_distributions=log_reg_param_grid,\n                                  n_iter=1000,\n                                  scoring=\"accuracy\",\n                                  n_jobs=-1,\n                                  cv=5,\n                                  verbose=1)\n</code></pre> <pre><code># Fit the training data to Randomized Search CV\nlog_reg_grid.fit(X_train, y_train)\n</code></pre> <pre><code>log_reg_grid.best_params_\n</code></pre> <pre><code># Check the best score\nlog_reg_grid.best_score_\n</code></pre> <pre><code># Build a Logistic Regression Model with best estimators\nlog_reg_final = log_reg_grid.best_estimator_\nlog_reg_final\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = log_reg_final.predict(X_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#feature-importance","title":"Feature Importance","text":"<pre><code># Get the feature importance\nfeature_importance = log_reg_final.coef_[0]\n\n# Convert the feature importance into a pandas series\nfeature_importance = pd.Series(feature_importance, index=X_train.columns)\n\n# Sort the values in descending order\nfeature_importance = feature_importance.sort_values(ascending=False)\nfeature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(8, 4), dpi=100)\n\n# Define a color palette\ncolor_palette = sns.color_palette(palette=\"coolwarm\", n_colors=len(feature_importance))\n\nsns.barplot(x=feature_importance.index, y=feature_importance, palette=color_palette, \n            edgecolor=\"black\", linewidth=0.5)\nplt.title(\"Feature Importance\", fontname=\"Times New Roman\", color=\"black\", fontsize=12)\nplt.xticks(fontname=\"Times New Roman\", rotation=90)\nplt.yticks(fontname=\"Times New Roman\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/7_Application_of_Logistic_Regression_for_Flood_Susceptibility_Zonation/#export-the-model","title":"Export the Model","text":"<pre><code>import pickle\n</code></pre> <pre><code>output_folder = \"D:\\\\Coding\\\\Git Repository\\\\Research Repo\\\\Flood\\\\Flood-Susceptibility-Zonation-of-Maldah\\\\Model\\\\\"\nmodel_name = \"log_reg_model.pkl\"\n</code></pre> <pre><code># Export the model\n# pickle.dump(log_reg_final, file=open(output_folder+model_name, \"wb\"))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/","title":"Application Of Svm For Flood Susceptibility Zonation","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#application-of-support-vector-machine-for-flood-susceptibility-zonation","title":"Application of Support Vector Machine for Flood Susceptibility Zonation","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#import-required-libraries","title":"Import Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"whitegrid\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#read-the-data","title":"Read the Data","text":"<pre><code>training_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Training_Data.csv\")\ntesting_df = pd.read_csv(\"D:\\Research Works\\Flood\\Flood_Risk_Zonation_of_Maldah\\Datasets\\CSVs\\Testing_Data.csv\")\n</code></pre> <pre><code>training_df.head()\n</code></pre> <pre><code># Drop the geometry columns\ntraining_df.drop(\"geometry\", axis=1, inplace=True)\ntesting_df.drop(\"geometry\", axis=1, inplace=True)\n\ntraining_df.columns == testing_df.columns\n</code></pre> <pre><code>training_df.shape\n</code></pre> <pre><code># Check for the null values\ntraining_df.isnull().sum().sum()\n</code></pre> <pre><code>testing_df.isnull().sum().sum()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#specify-the-train-test-data","title":"Specify the Train Test Data","text":"<pre><code>X_train, y_train = training_df.drop(\"Flood\", axis=1), training_df[\"Flood\"]\nX_test, y_test = testing_df.drop(\"Flood\", axis=1), testing_df[\"Flood\"]\n\nX_train.shape, X_test.shape\n</code></pre> <pre><code>X_train.head()\n</code></pre> <pre><code># Change the datatype into float\nX_train = X_train.astype(float)\nX_test = X_test.astype(float)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#feature-selection-using-information-gain","title":"Feature Selection using Information Gain","text":"<pre><code># Select the important features\nselected_features = ['Dist_to_River', 'TWI', 'Rainfall', 'Clay_Content', 'TRI', 'NDVI',\n                     'MFI', 'Elevation', 'MNDWI', 'Drainage_Density', 'Geomorphology_Active_Flood_Plain',\n                     'Geomorphology_Older_Alluvial_Plain', 'Geomorphology_Older_Flood_Plain',\n                     'Lithology_Cl_wi_S_Si_Ir_N', 'Lithology_Fe_Ox_S_Si_Cl',\n                     'Lithology_S_Si_Cl', 'Lithology_S_Si_Cl_wi_Cal_Co',\n                     'LULC_Agricultural_Field', 'LULC_Built_UP_Area',\n                     'LULC_Natural_Vegetation']\n</code></pre> <pre><code>X_train = X_train[selected_features]\nX_train\n</code></pre> <pre><code>X_test = X_test[selected_features]\nX_test\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#apply-support-vector-classification","title":"Apply Support Vector Classification","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#build-a-support-vector-classification-model","title":"Build a Support Vector Classification Model","text":"<pre><code>from sklearn.svm import SVC\n</code></pre> <pre><code># Instantiate a SVC object\nsvc = SVC(probability=True)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Define all the hyperparameters for Support Vector Classification Model\nsvc_param_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'gamma': [0.1, 1, 10]\n}\n</code></pre> <pre><code>from sklearn.model_selection import RandomizedSearchCV\n</code></pre> <pre><code># Apply Randomized Search CV\nsvc_grid = RandomizedSearchCV(estimator=svc,\n                              param_distributions=svc_param_grid,\n                              n_iter=1000,\n                              scoring=\"accuracy\",\n                              n_jobs=-1,\n                              cv=5,\n                              verbose=1)\n</code></pre> <pre><code># Fit the training data to Randomized Search CV\nsvc_grid.fit(X_train, y_train)\n</code></pre> <pre><code>svc_grid.best_params_\n</code></pre> <pre><code># Check the best score\nsvc_grid.best_score_\n</code></pre> <pre><code># Build a Logistic Regression Model with best estimators\nsvc_final = svc_grid.best_estimator_\nsvc_final\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#accuracy-assessment","title":"Accuracy Assessment","text":"<pre><code># Predict the test data\ny_pred = svc_final.predict(X_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#feature-importance","title":"Feature Importance","text":"<pre><code>from sklearn.inspection import permutation_importance\n\n# Perform permutation feature importance\nperm_importance = permutation_importance(svc_final, X_test, y_test, n_repeats=30)\n\n# Get the importance scores\nimportance_scores = perm_importance.importances_mean\n</code></pre> <pre><code># Get the feature importance\nfeature_importance = importance_scores\n\n# Convert the feature importance into a pandas series\nfeature_importance = pd.Series(feature_importance, index=X_train.columns)\n\n# Sort the values in descending order\nfeature_importance = feature_importance.sort_values(ascending=False)\nfeature_importance\n</code></pre> <pre><code># Plot the feature importance\nplt.figure(figsize=(6, 8), dpi=100)\n\n# Define a color palette\ncolor_palette = sns.color_palette(palette=\"coolwarm\", n_colors=len(feature_importance))\n\nsns.barplot(x=feature_importance, y=feature_importance.index, palette=color_palette, \n            edgecolor=\"black\", linewidth=0.5)\nplt.title(\"Feature Importance\", fontname=\"Times New Roman\", color=\"black\", fontsize=12)\nplt.xticks(fontname=\"Times New Roman\")\nplt.yticks(fontname=\"Times New Roman\")\nplt.show()\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/8_Application_of_SVM_for_Flood_Susceptibility_Zonation/#export-the-model","title":"Export the Model","text":"<pre><code>import pickle\n</code></pre> <pre><code>output_folder = \"D:\\\\Coding\\\\Git Repository\\\\Research Repo\\\\Flood\\\\Flood-Susceptibility-Zonation-of-Maldah\\\\Model\\\\\"\nmodel_name = \"svm_model.pkl\"\n</code></pre> <pre><code># Export the model\n# pickle.dump(svc_final, file=open(output_folder+model_name, \"wb\"))\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/","title":"Convert The Image Into Csv","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: Python 3 (ipykernel)     language: python     name: python3</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#convert-the-image-into-csv","title":"Convert the Image into CSV","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"mOG7H7d9KZxO\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#pip-install-rasterio","title":"!pip install rasterio","text":"<pre><code>```python id=\"c1KFEd57Hkbc\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#prepare-the-image-for-classification","title":"Prepare the Image for Classification","text":"<p>```python id=\"9Z85JbvsIeOR\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#read-the-image-with-rasterio","title":"Read the image with Rasterio","text":"<p>image = rasterio.open(\"/content/drive/MyDrive/ML &amp; DL/Flood Data/Maldah_Flood_Parameters.tif\") <pre><code>```python id=\"Yxxng30-I2GH\"\n# Store the image parameters in separate variables\nbandNum = image.count\nheight = image.height\nwidth = image.width\ncrs = image.crs\ntransform = image.transform\nshape = (height, width)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"BOeJwyzST-0N\" outputId=\"cefae966-ef54-4f18-935c-228174ef6987\" print(\"Band Number:\", bandNum) print(\"Image Height:\", height) print(\"Image Width:\", width) print(\"CRS:\", crs) print(\"Transform:\\n\", transform) print(\"Shape:\", shape) <pre><code>```python id=\"dl8W7IFpI6-L\"\n# Create an empty pandas dataframe to store the pixel values\nimage_bands = pd.DataFrame()\n</code></pre></p> <p>```python id=\"jnEZHEaXI-mg\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#joining-the-pixel-values-of-different-bands-into-the-dataframe","title":"Joining the pixel values of different bands into the dataframe","text":"<p>for i in image.indexes:     temp = image.read(i)     temp = pd.DataFrame(data=np.array(temp).flatten(), columns=[i])     image_bands = temp.join(image_bands) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 423} id=\"s9f6eEtJJK6G\" outputId=\"8a4ccc2d-ddc5-4500-e2e1-89d9af71006c\"\nimage_bands\n</code></pre></p> <p>```python id=\"ArD9T7RuKl6o\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#store-all-the-band-names-in-a-list","title":"Store all the band names in a list","text":"<p>bandNames = [\"Elevation\", \"Slope\", \"Dist_to_River\", \"Drainage_Density\",              \"Geomorphology\", \"Lithology\", \"Relief_Amplitude\", \"Rainfall\",              \"MFI\", \"NDVI\", \"MNDWI\", \"SPI\", \"STI\", \"TPI\", \"TRI\", \"TWI\",              \"LULC\", \"Clay_Content\"]; <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 226} id=\"cSHWAzKpKyKI\" outputId=\"49c2bf32-8fd3-4262-ae3c-a33bba8e0cba\"\n# Change the column names\nimage_bands.columns = bandNames[::-1]\nimage_bands.head()\n</code></pre></p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#data-preprocessing","title":"Data Preprocessing","text":"<p>```python id=\"eVPJ6oqRLfyh\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#fill-the-null-values-of-the-clay_content-column-with-0","title":"Fill the null values of the Clay_Content column with 0","text":"<p>image_bands.fillna(0, inplace=True) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 443} id=\"T9sSI3iiM4sA\" outputId=\"0ff7670b-612a-4c8e-9eac-cf9576559c77\"\nimage_bands\n</code></pre></p> <p>```python id=\"WRv5SDqiM-jY\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#rename-the-values-of-the-categorical-variables","title":"Rename the values of the categorical variables","text":""},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#define-the-values-for-the-geomorphology","title":"Define the values for the geomorphology","text":"<p>geomorpholoy_dict = {1: \"Active_Flood_Plain\",                      2: \"Embankment\",                      3: \"Older_Alluvial_Plain\",                      4: \"Older_Flood_Plain\",                      5: \"Pond\",                      6: \"River\",                      7: \"WatBod_Lake\",                      8: \"Younger_Alluvial_Plain\"}</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#define-the-values-for-the-lithology","title":"Define the values for the lithology","text":"<p>lithology_dict = {1: \"Cl_wi_S_Si_Ir_N\",                   2: \"Fe_Ox_S_Si_Cl\",                   3: \"S_Si_Gr\",                   4: \"S_Si_Cl\",                   5: \"S_Si_Cl_wi_Cal_Co\"}</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#define-the-values-for-the-lulc","title":"Define the values for the LULC","text":"<p>lulc_dict = {1: \"Waterbodies\",              2: \"Natural_Vegetation\",              3: \"Agricultural_Field\",              4: \"Bare_Ground\",              5: \"Built_UP_Area\"} <pre><code>```python id=\"kpXw_cgmNNzj\"\nimage_bands.replace({\"Geomorphology\": geomorpholoy_dict, \"Lithology\": lithology_dict, \"LULC\": lulc_dict},\n                     inplace=True)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"E8T2UnEYOAsz\" outputId=\"2eca1317-38d4-4527-cb5a-4c602a70db38\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#apply-ohe-on-geomorphology-lithology-and-lulc-columns","title":"Apply OHE on 'Geomorphology', 'Lithology' and 'LULC' Columns*","text":"<p>image_bands = pd.get_dummies(image_bands, columns=[\"Geomorphology\", \"Lithology\", \"LULC\"]) image_bands.info() <pre><code>&lt;!-- #region id=\"UuIjndx3OnKL\" --&gt;\n## **Select the Best Features**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"41RNwp4LVxE3\" outputId=\"91c8bf8f-21c5-412d-8182-08ede3ffcb75\"\n# Define the best features in a list\nselected_features = ['Dist_to_River', 'TWI', 'Rainfall', 'Clay_Content', 'TRI', 'NDVI',\n                     'MFI', 'Elevation', 'MNDWI', 'Drainage_Density',\n                     'Geomorphology_Active_Flood_Plain',\n                     'Geomorphology_Older_Alluvial_Plain', 'Geomorphology_Older_Flood_Plain',\n                     'Lithology_Cl_wi_S_Si_Ir_N', 'Lithology_Fe_Ox_S_Si_Cl',\n                     'Lithology_S_Si_Cl', 'Lithology_S_Si_Cl_wi_Cal_Co',\n                     'LULC_Agricultural_Field', 'LULC_Built_UP_Area',\n                     'LULC_Natural_Vegetation']\nlen(selected_features)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 443} id=\"49cHcn7jWA-B\" outputId=\"96062b8b-87af-42b6-8fc5-6f562f3a09c8\" image_bands = image_bands[selected_features] image_bands <pre><code>&lt;!-- #region id=\"h3XnSg-PXRhg\" --&gt;\n## **Export the Data as CSV**\n&lt;!-- #endregion --&gt;\n\n```python id=\"D1bGi7UsXL6B\"\noutput_folder = \"/content/drive/MyDrive/ML &amp; DL/\"\nfile_name = \"Image_CSV.csv\"\n</code></pre></p> <p>```python id=\"qR2z-BYRXnBD\"</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/Notebooks/9_Convert_the_Image_into_CSV/#image_bandsto_csvoutput_folderfile_name","title":"image_bands.to_csv(output_folder+file_name)","text":"<p>```</p>"},{"location":"projects/INDI-Res/","title":"INDI-Res","text":"<p>A Time Series of Reservoir Area, Water Level, and Storage in India Derived from High-Resolution Multi-Satellite Observations</p>"},{"location":"projects/INDI-Res/#overview","title":"Overview","text":"<p>INDI-Res is a geospatial data and analysis project that provides a consistent, high-resolution time series of reservoir surface area, water level, and storage dynamics across India. The dataset is derived from multi-satellite Earth observation data, enabling long-term monitoring of surface water resources at national and sub-national scales.</p> <p>This project is designed to support: - Hydrological and water resource assessments - Climate variability and drought studies - Agricultural water management and irrigation planning - Reservoir operation and policy-relevant analysis  </p>"},{"location":"projects/INDI-Res/#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udce1 Multi-satellite integration (optical and/or radar-based observations)</li> <li>\ud83d\uddfa\ufe0f High spatial resolution reservoir surface area mapping</li> <li>\ud83d\udcc8 Time series of water level and storage estimates</li> <li>\ud83c\uddee\ud83c\uddf3 Nationwide coverage across India</li> <li>\ud83d\udd01 Reproducible research pipeline using Python and Jupyter notebooks</li> </ul>"},{"location":"projects/INDI-Res/#repository-structure","title":"Repository Structure","text":"<p>```text INDI-Res/ \u2502 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 environment.yml \u251c\u2500\u2500 .gitignore \u2502 \u251c\u2500\u2500 data/ \u2502   \u251c\u2500\u2500 raw/              # Original satellite and ancillary datasets \u2502   \u251c\u2500\u2500 interim/          # Preprocessed but non-final data \u2502   \u251c\u2500\u2500 processed/        # Final reservoir area, level, and storage products \u2502   \u2514\u2500\u2500 external/         # Third-party datasets (e.g., DEM, reservoir boundaries) \u2502 \u251c\u2500\u2500 notebooks/ \u2502   \u251c\u2500\u2500 00_exploration/           # Initial data inspection and QA \u2502   \u251c\u2500\u2500 01_preprocessing/         # Satellite data preprocessing \u2502   \u251c\u2500\u2500 02_feature_engineering/   # Area\u2013elevation\u2013storage relationships \u2502   \u251c\u2500\u2500 03_modeling/              # Water level and storage estimation \u2502   \u251c\u2500\u2500 04_evaluation/            # Validation and uncertainty analysis \u2502   \u2514\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 src/ \u2502   \u2514\u2500\u2500 indi_res/ \u2502       \u251c\u2500\u2500 init.py \u2502       \u251c\u2500\u2500 config.py \u2502       \u251c\u2500\u2500 data/ \u2502       \u2502   \u251c\u2500\u2500 load.py \u2502       \u2502   \u251c\u2500\u2500 preprocess.py \u2502       \u2502   \u2514\u2500\u2500 utils.py \u2502       \u251c\u2500\u2500 features/ \u2502       \u2502   \u251c\u2500\u2500 build_features.py \u2502       \u2502   \u2514\u2500\u2500 scaling.py \u2502       \u251c\u2500\u2500 models/ \u2502       \u2502   \u251c\u2500\u2500 train.py \u2502       \u2502   \u251c\u2500\u2500 predict.py \u2502       \u2502   \u2514\u2500\u2500 evaluate.py \u2502       \u251c\u2500\u2500 visualization/ \u2502       \u2502   \u2514\u2500\u2500 plots.py \u2502       \u2514\u2500\u2500 utils/ \u2502           \u2514\u2500\u2500 io.py \u2502 \u251c\u2500\u2500 scripts/ \u2502   \u251c\u2500\u2500 run_preprocessing.py \u2502   \u251c\u2500\u2500 train_model.py \u2502   \u2514\u2500\u2500 evaluate_model.py \u2502 \u251c\u2500\u2500 experiments/ \u2502   \u251c\u2500\u2500 exp_001_baseline/ \u2502   \u2502   \u251c\u2500\u2500 config.yaml \u2502   \u2502   \u2514\u2500\u2500 results.json \u2502   \u2514\u2500\u2500 exp_002_multisatellite/ \u2502 \u251c\u2500\u2500 results/ \u2502   \u251c\u2500\u2500 figures/ \u2502   \u251c\u2500\u2500 tables/ \u2502   \u2514\u2500\u2500 metrics/ \u2502 \u251c\u2500\u2500 docs/ \u2502   \u251c\u2500\u2500 methodology.md \u2502   \u251c\u2500\u2500 data_description.md \u2502   \u2514\u2500\u2500 model_details.md \u2502 \u2514\u2500\u2500 tests/     \u251c\u2500\u2500 test_data.py     \u251c\u2500\u2500 test_models.py     \u2514\u2500\u2500 test_features.py</p>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/","title":"Experimenting With Sam3","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: torch     language: python     name: python3</p>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#import-libraries","title":"Import libraries","text":"<pre><code># %pip install \"segment-geospatial[samgeo3]\"\n</code></pre> <pre><code>import os\nimport numpy as np\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\nfrom samgeo import SamGeo3\n\nroot_dir = \"/beegfs/halder/GITHUB/RESEARCH/INDI-Res/\"\nos.chdir(root_dir)\nload_dotenv()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#define-the-file-path","title":"Define the file path","text":"<pre><code>waterbody_file_path = os.path.join(root_dir, 'data', 'raw', 'waterbody.tif')\nfield_boundary_file_path = os.path.join(root_dir, 'data', 'raw', 'field_boundary.tif')\n\nwith rio.open(waterbody_file_path, 'r') as src:\n    waterbody_image = src.read([1, 2, 3])\n    waterbody_meta = src.meta\n\nwith rio.open(field_boundary_file_path, 'r') as src:\n    field_boundary_image = src.read([1, 2, 3])\n    field_boundary_meta = src.meta\n</code></pre> <pre><code># Plot the images\n# Convert to channel-last\nwaterbody_rgb = np.transpose(waterbody_image, (1, 2, 0))\nfield_boundary_rgb = np.transpose(field_boundary_image, (1, 2, 0))\n\ndef normalize_rgb(img):\n    img = img.astype('float32')\n    img_min, img_max = img.min(), img.max()\n    return (img - img_min) / (img_max - img_min)\n\nwaterbody_rgb = normalize_rgb(waterbody_rgb)\nfield_boundary_rgb = normalize_rgb(field_boundary_rgb)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\naxes[0].imshow(waterbody_rgb)\naxes[0].set_title(\"Waterbody (RGB)\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(field_boundary_rgb)\naxes[1].set_title(\"Field Boundary (RGB)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#request-access-to-sam3","title":"Request access to SAM3","text":"<pre><code>from huggingface_hub import login\nlogin(token=os.getenv('HF_TOKEN'))\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#initiailize-sam3","title":"Initiailize SAM3","text":"<pre><code>sam3 = SamGeo3(backend='transformers', device=None, checkpoint_path=None, load_from_HF=True)\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#set-the-image","title":"Set the image","text":"<pre><code>sam3.set_image(waterbody_file_path)\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/00_experimenting_with_SAM3/#generate-masks-with-text-prompt","title":"Generate masks with text prompt","text":"<pre><code>sam3.generate_masks(prompt=\"waterbody\")\n</code></pre> <pre><code>sam3.show_anns()\n</code></pre> <pre><code>sam3.show_masks()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/","title":"Data Extraction From Stac","text":"<p>jupyter:   jupytext:     text_representation:       extension: .md       format_name: markdown       format_version: '1.3'       jupytext_version: 1.19.1   kernelspec:     display_name: geo     language: python     name: python3</p>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#import-libraries","title":"Import libraries","text":"<pre><code>import os\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport planetary_computer as pc\nimport pystac_client\nimport rioxarray\nfrom odc.stac import load\n\nroot_dir = \"/beegfs/halder/GITHUB/RESEARCH/INDI-Res/\"\nos.chdir(root_dir)\ndata_dir = os.path.join(root_dir, \"data\")\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#read-the-datasets","title":"Read the datasets","text":"<pre><code># Read the shapefile of India\nindia_boundary_path = os.path.join(\n    data_dir, \"external\", \"India_Boundary\", \"India_Country_Boundary.shp\"\n)\nindia_boundary = gpd.read_file(india_boundary_path)\nprint(india_boundary.shape)\nindia_boundary.head()\n</code></pre> <pre><code># Read the Global DAM Watch v1.0 database for India\ngdw_reservoirs_IN = gpd.read_file(\n    os.path.join(data_dir, \"processed\", \"GDW_reservoirs_v1_0_IN.gpkg\")\n)\ngdw_reservoirs_IN.geometry = gdw_reservoirs_IN.geometry.buffer(distance=1000)\ngdw_reservoirs_IN[\"area\"] = gdw_reservoirs_IN.area / 1e6\ngdw_reservoirs_IN.sort_values(by=\"area\", ascending=False, inplace=True)\ngdw_reservoirs_IN.to_crs(crs=\"EPSG:4326\", inplace=True)\nprint(gdw_reservoirs_IN.shape)\ngdw_reservoirs_IN.head()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#search-the-stac-api","title":"Search the STAC API","text":"<pre><code>API_URL = \"https://hda.data.destination-earth.eu/stac/v2\"\nclient = pystac_client.Client.open(API_URL)\n\nbbox = list(gdw_reservoirs_IN.iloc[1].geometry.bounds)\n\nsearch = client.search(\n    collections=[\"EO.ECMWF.DAT.CAMS_GLOBAL_RADIATIVE_FORCING_AUX\"],\n    bbox=bbox,\n    datetime=\"2001-01-01/2002-01-01\",\n)\n\nitems = [pc.sign(i) for i in search.get_all_items()]\nprint(f\"Found {len(items)} scenes matching your criteria.\")\n</code></pre> <pre><code># Get all collections\ncollections = client.get_collections()\n\n# Print basic info about each collection\nfor col in collections:\n    print(f\"ID: {col.id}\")\n    print(f\"Title: {col.title}\")\n    print(f\"Description: {col.description}\\n\")\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#read-the-assets","title":"Read the assets","text":"<pre><code># Display the available assets for the first item\nassets_df = pd.DataFrame.from_dict(items[0].assets, orient=\"index\").reset_index()\nassets_df.columns = [\"asset\", \"href\"]\nprint(assets_df.shape)\nassets_df.head()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#load-the-data","title":"Load the data","text":"<pre><code># Load Data into Memory (Lazy Loading)\ndata = load(\n    items=items,\n    band=[\"green\", \"nir08\"],\n    bbox=bbox,\n    resolution=30,\n    groupby=\"solar_day\",\n    chunks={\"x\": 2048, \"y\": 2048},\n)\n\ndata\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/00_exploration/01_data_extraction_from_STAC/#calculate-ndwi","title":"Calculate NDWI","text":"<pre><code>ndwi = (data.green - data.nir08) / (data.green + data.nir08)\n\nprint(\"Streaming pixels and plotting... this may take 10-20 seconds.\")\nndWi_snapshot = ndwi.isel(time=0).compute()\n\nplt.figure(figsize=(10, 8))\nndWi_snapshot.plot.imshow(cmap=\"Blues_r\", robust=False)\n# plt.title(f\"NDWI: {data.time[0].dt.date.item()}\")\nplt.axis(\"off\")\nplt.show()\n</code></pre> <pre><code>ndwi = (data.green - data.nir08) / (data.green + data.nir08)\n\nprint(\"Streaming pixels and plotting... this may take 10-20 seconds.\")\nndWi_snapshot = ndwi.isel(time=9).compute()\n\nplt.figure(figsize=(10, 8))\nndWi_snapshot.plot.imshow(cmap=\"Blues_r\", robust=False)\n# plt.title(f\"NDWI: {data.time[0].dt.date.item()}\")\nplt.axis(\"off\")\nplt.show()\n</code></pre> <pre><code>ndWi_snapshot\n</code></pre> <pre><code>ndwi = (data.green - data.nir08) / (data.green + data.nir08)\n\nprint(\"Streaming pixels and plotting... this may take 10-20 seconds.\")\nndWi_snapshot = ndwi.isel(time=23).compute()\n\nplt.figure(figsize=(10, 8))\nndWi_snapshot.plot.imshow(cmap=\"Blues_r\", robust=False)\n# plt.title(f\"NDWI: {data.time[0].dt.date.item()}\")\nplt.axis(\"off\")\nplt.show()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/01_preprocessing/00_extract_indian_reservoirs/","title":"Extract Indian Reservoirs","text":""},{"location":"projects/INDI-Res/notebooks/01_preprocessing/00_extract_indian_reservoirs/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport os\n\nroot_dir = \"/beegfs/halder/GITHUB/RESEARCH/INDI-Res/\"\nos.chdir(root_dir)\ndata_dir = os.path.join(root_dir, 'data')\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/01_preprocessing/00_extract_indian_reservoirs/#read-the-datasets","title":"Read the datasets","text":"<pre><code># Read the shapefile of India\nindia_boundary_path = os.path.join(data_dir, 'external', 'India_Boundary', 'India_Country_Boundary.shp')\nindia_boundary = gpd.read_file(india_boundary_path)\nprint(india_boundary.shape)\nindia_boundary.head()\n</code></pre> <pre><code># Read the Global DAM Watch v1.0 database\ngdw_reservoirs_data = gpd.read_file(os.path.join(data_dir, 'external', 'GDW_v1_0', 'GDW_reservoirs_v1_0.shp'))\ngdw_reservoirs_data = gdw_reservoirs_data.to_crs(crs=india_boundary.crs)\n\ngdw_barriers_data = gpd.read_file(os.path.join(data_dir, 'external', 'GDW_v1_0', 'GDW_barriers_v1_0.shp'))\ngdw_barriers_data = gdw_barriers_data.to_crs(crs=india_boundary.crs)\n\n# Subset the data for India\ngdw_reservoirs_subset = gpd.sjoin(left_df=gdw_reservoirs_data, right_df=india_boundary, predicate='intersects')\ngdw_barriers_subset = gpd.sjoin(left_df=gdw_barriers_data, right_df=india_boundary, predicate='intersects')\n\ngdw_reservoirs_subset = gdw_reservoirs_subset.iloc[:, :-2].reset_index(drop=True)\ngdw_barriers_subset = gdw_barriers_subset.iloc[:, :-2].reset_index(drop=True)\ngdw_reservoirs_bounds = gdw_reservoirs_subset.copy()\ngdw_reservoirs_bounds['geometry'] = gdw_reservoirs_bounds.geometry.envelope # bounds for each reservoir\n\nprint(gdw_reservoirs_subset.shape, gdw_reservoirs_bounds.shape, gdw_barriers_subset.shape)\ngdw_reservoirs_subset.head()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/01_preprocessing/00_extract_indian_reservoirs/#plot-the-data","title":"Plot the data","text":"<pre><code># Plot the data\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))\naxes = axes.flatten()\n\nindia_boundary.plot(ax=axes[0], color='none', edgecolor='k', linewidth=1)\ngdw_reservoirs_subset.plot(ax=axes[0], color='blue')\naxes[0].set_title('GDW Reservoirs')\n\nindia_boundary.plot(ax=axes[1], color='none', edgecolor='k', linewidth=1)\ngdw_barriers_subset.plot(ax=axes[1], color='red', markersize=0.5, alpha=0.5)\naxes[1].set_title('GDW Barriers')\n\nindia_boundary.plot(ax=axes[2], color='none', edgecolor='k', linewidth=1)\ngdw_reservoirs_bounds.plot(ax=axes[2], color='blue', markersize=0.5, alpha=0.5)\naxes[2].set_title('GDW Reservoirs Bounds')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/01_preprocessing/00_extract_indian_reservoirs/#save-the-data","title":"Save the data","text":"<pre><code># gdw_reservoirs_subset.to_file(os.path.join(data_dir, 'processed', 'GDW_reservoirs_v1_0_IN.gpkg'))\n# gdw_barriers_subset.to_file(os.path.join(data_dir, 'processed', 'GDW_barriers_v1_0_IN.gpkg'))\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/03_modeling/00_GANFilling/","title":"00 GANFilling","text":""},{"location":"projects/INDI-Res/notebooks/03_modeling/00_GANFilling/#import-libraries","title":"Import libraries","text":"<pre><code>import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/03_modeling/00_GANFilling/#convlstm","title":"ConvLSTM","text":"<pre><code>class ConvLSTMCell(nn.Module):\n\n    def __init__(self,input_dim, hidden_dim, kernel_size, bias):\n        \"\"\"\n        Initialize ConvLSTM cell.\n        Parameters\n        ----------\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding,\n                              bias=self.bias)\n\n    def __initStates(self, size, device):\n        return torch.zeros(size).to(device), torch.zeros(size).to(device)\n\n    def forward(self, input_tensor, cur_state):\n        if cur_state == None:\n            h_cur, c_cur = self.__initStates([input_tensor.shape[0], self.hidden_dim, input_tensor.shape[2], input_tensor.shape[3]], device=input_tensor.device)\n        else:\n            h_cur, c_cur = cur_state\n\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n</code></pre> <pre><code># EXAMPLE USAGE\n# Parameters\nbatch_size = 2\ninput_dim = 3\nhidden_dim = 16\nheight, width = 32, 32\nkernel_size = (3, 3)\n\n# Create dummy input (one time step)\nx = torch.randn(batch_size, input_dim, height, width).to(device)\n\n# Initialize ConvLSTMCell\ncell = ConvLSTMCell(\n    input_dim=input_dim,\n    hidden_dim=hidden_dim,\n    kernel_size=kernel_size,\n    bias=True,\n).to(device)\n\n# Forward pass (first time step)\nh_next, c_next = cell(x, cur_state=None)\n\nprint(\"h_next shape:\", h_next.shape)\nprint(\"c_next shape:\", c_next.shape)\n\n# Second time step\nx2 = torch.randn(batch_size, input_dim, height, width).to(device)\n\nh2, c2 = cell(x2, cur_state=(h_next, c_next))\n\nprint(\"h2 shape:\", h2.shape)\nprint(\"c2 shape:\", c2.shape)\n</code></pre> <pre><code>class ConvLSTM(nn.Module):\n    \"\"\"\n    Parameters:\n        input_dim: Number of channels in input\n        hidden_dim: Number of hidden channels\n        kernel_size: Size of kernel in convolutions\n        num_layers: Number of LSTM layers stacked on each other\n        batch_first: Whether or not dimension 0 is the batch or not\n        bias: Bias or no bias in Convolution\n        return_all_layers: Return the list of computations for all layers\n        Note: Will do same padding.\n    Input:\n        A tensor of size B, T, C, H, W or T, B, C, H, W\n    Output:\n        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n            0 - layer_output_list is the list of lists of length T of each output\n            1 - last_state_list is the list of last states\n                    each element of the list is a tuple (h, c) for hidden state and memory\n    Example:\n        &gt;&gt; x = torch.rand((32, 10, 64, 128, 128))\n        &gt;&gt; convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n        &gt;&gt; _, last_states = convlstm(x)\n        &gt;&gt; h = last_states[0][0]  # 0 for layer index, 0 for h index\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super().__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\n            raise ValueError(\"Inconsistent list length.\")\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bias = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n\n            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        \"\"\"\n        Parameters\n        ----------\n        input_tensor: todo\n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: todo\n            None. todo implement stateful\n        Returns\n        -------\n        last_state_list, layer_output\n        \"\"\"\n        if not self.batch_first:\n            # (t, b, c, h, w) -&gt; (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        b, _, _, h, w = input_tensor.size()\n\n        # Implement stateful ConvLSTM\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n\n        layer_output_list = []\n        last_state_list = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n                                                 cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1:]\n            layer_state_list = last_state_list[-1:]\n\n        return layer_output_list, layer_state_list\n\n    def _init_hidden(self, batch_size, image_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, tuple) or\n                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n            raise ValueError(\"'kernel_size' must be tuple or list of tuples.\")\n\n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if not isinstance(param, list):\n            param = [param] * num_layers\n        return param\n</code></pre> <pre><code># EXAMPLE USAGE\n# Parameters\nbatch_size = 2\ntime_steps = 5\ninput_dim = 3\nhidden_dim = 16\nheight, width = 32, 32\nkernel_size = (3, 3)\nnum_layers = 1\n\n# Dummy input (B, T, C, H, W)\nx = torch.randn(batch_size, time_steps, input_dim, height, width).to(device)\n\n# Model\nmodel = ConvLSTM(\n    input_dim=input_dim,\n    hidden_dim=hidden_dim,\n    kernel_size=kernel_size,\n    num_layers=num_layers,\n    batch_first=True,\n    bias=True,\n    return_all_layers=False\n).to(device)\n\n# Forward pass\nlayer_outputs, last_states = model(x)\n\nprint(\"Layer outputs length:\", len(layer_outputs))\nprint(\"Last states length:\", len(last_states))\nprint(\"Output shape:\", layer_outputs[0].shape)\nprint(\"Hidden state shape:\", last_states[0][0].shape)\nprint(\"Cell state shape:\", last_states[0][1].shape)\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/03_modeling/00_GANFilling/#generator","title":"Generator","text":"<pre><code>class Generator(nn.Module):\n    def __init__(self, in_channels=4, out_channels=3, hidden_dim=64):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n\n        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=3, stride=2, padding=1)    \n        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n        self.conv5 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n        self.conv6 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n        self.conv7 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n\n        self.conv_lstm_e1 = ConvLSTMCell(hidden_dim, hidden_dim, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e2 = ConvLSTMCell(hidden_dim * 2, hidden_dim * 2, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e3 = ConvLSTMCell(hidden_dim * 4, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e4 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e5 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e6 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_e7 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)    \n\n        self.conv_lstm_d1 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d2 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d3 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d4 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d5 = ConvLSTMCell(hidden_dim * 4 * 2, hidden_dim * 2, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d6 = ConvLSTMCell(hidden_dim * 2 * 2, hidden_dim, kernel_size=(3, 3), bias=False)\n        self.conv_lstm_d7 = ConvLSTMCell(hidden_dim * 2, hidden_dim, kernel_size=(3, 3), bias=False)\n\n        self.up = nn.Upsample(scale_factor=2)\n        self.conv_out = nn.Conv2d(hidden_dim, out_channels, kernel_size=3, stride=1, padding=1)\n\n        self.slope = 0.2\n\n    def weight_init(self, mean, std):\n        for m in self.modules():\n            normal_init(m, mean, std)\n\n    def forward_step(self, input, states_encoder, states_decoder):\n        e1 = self.conv1(input)\n        states_e1 = self.conv_lstm_e1(e1, states_encoder[0])\n        e2 = self.conv2(F.leaky_relu(states_e1[0], self.slope))\n        states_e2 = self.conv_lstm_e2(e2, states_encoder[1])\n        e3 = self.conv3(F.leaky_relu(states_e2[0], self.slope))\n        states_e3 = self.conv_lstm_e3(e3, states_encoder[2])\n        e4 = self.conv4(F.leaky_relu(states_e3[0], self.slope))\n        states_e4 = self.conv_lstm_e4(e4, states_encoder[3])\n        e5 = self.conv5(F.leaky_relu(states_e4[0], self.slope))\n        states_e5 = self.conv_lstm_e5(e5, states_encoder[4])\n        e6 = self.conv6(F.leaky_relu(states_e5[0], self.slope))\n        states_e6 = self.conv_lstm_e6(e6, states_encoder[5])\n        e7 = self.conv7(F.leaky_relu(states_e6[0], self.slope))\n        states_e7 = self.conv_lstm_e7(e7, states_encoder[6])\n\n        states_d1 = self.conv_lstm_d1(F.relu(states_e7[0]), states_decoder[0])\n        d1 = self.up(states_d1[0])\n        d1 = torch.cat([d1, e6], 1)\n\n        states_d2 = self.conv_lstm_d2(F.relu(d1), states_decoder[1])\n        d2 = self.up(states_d2[0])\n        d2 = torch.cat([d2, e5], 1)\n\n        states_d3 = self.conv_lstm_d3(F.relu(d2), states_decoder[2])\n        d3 = self.up(states_d3[0])\n        d3 = torch.cat([d3, e4], 1) \n\n        states_d4 = self.conv_lstm_d4(F.relu(d3), states_decoder[3])\n        d4 = self.up(states_d4[0])\n        d4 = torch.cat([d4, e3], 1) \n\n        states_d5 = self.conv_lstm_d5(F.relu(d4), states_decoder[4])\n        d5 = self.up(states_d5[0])\n        d5 = torch.cat([d5, e2], 1)\n\n        states_d6 = self.conv_lstm_d6(F.relu(d5), states_decoder[5])\n        d6 = self.up(states_d6[0])\n        d6 = torch.cat([d6, e1], 1) \n\n        states_d7 = self.conv_lstm_d7(F.relu(d6), states_decoder[6])\n        d7 = self.up(states_d7[0])\n\n        out = torch.tanh(self.conv_out(d7))\n\n        states_e = [states_e1, states_e2, states_e3, states_e4, states_e5, states_e6, states_e7]\n        states_d = [states_d1, states_d2, states_d3, states_d4, states_d5, states_d6, states_d7]\n\n        return out, (states_e, states_d)\n\n    def forward(self, x):\n        states_encoder = [None] * 7\n        states_decoder = [None] * 7\n        batch, _, h, w, t = x.shape\n        output = torch.zeros((batch, self.conv_out.out_channels, h, w, t), device=x.device)\n\n        for t in range(x.shape[4]):\n            output[..., t], states = self.forward_step(x[..., t], states_encoder, states_decoder)\n            states_encoder, states_decoder = states[0], states[1]\n        return output, states\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n</code></pre> <pre><code># EXAMPLE USAGE\nstates_encoder = [None] * 7\nstates_decoder = [None] * 7\nx = torch.randn((2, 4, 128, 128, 10), dtype=torch.float32) \nmodel = Generator(in_channels=4, out_channels=4)\ny, states = model(x)\nprint(y.size())\n</code></pre>"},{"location":"projects/INDI-Res/notebooks/03_modeling/00_GANFilling/#discriminator","title":"Discriminator","text":"<pre><code>class Discriminator(nn.Module):\n    def __init__(self, in_channels=4, hidden_dim=64):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=4, stride=2, padding=1)\n        self.conv2_bn = nn.BatchNorm2d(hidden_dim * 2, track_running_stats=False)\n        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=4, stride=2, padding=1)\n        self.conv3_bn = nn.BatchNorm2d(hidden_dim * 4, track_running_stats=False)\n        self.conv4_lstm = ConvLSTMCell(hidden_dim * 4, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=4, stride=1, padding=1)\n        self.conv4_bn = nn.BatchNorm2d(hidden_dim * 8, track_running_stats=False)\n        self.conv5_lstm = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n        self.conv5 = nn.Conv2d(hidden_dim * 8, 1, kernel_size=4, stride=1, padding=1)\n\n        self.slope = 0.2\n\n        torch.backends.cudnn.deterministic = True\n\n    def weight_init(self, mean, std):\n        for m in self.modules():\n            normal_init(m, mean, std)\n\n    def forward_step(self, input, states):\n        x = F.leaky_relu(self.conv1(input), self.slope)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), self.slope)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), self.slope)\n        states1 = self.conv4_lstm(x, states[0])\n        x = F.leaky_relu(self.conv4_bn(self.conv4(states1[0])), self.slope)\n        states2 = self.conv5_lstm(x, states[1])\n        x = F.leaky_relu(self.conv5(states2[0]), self.slope)\n\n        return x.squeeze(dim=1), (states1, states2) \n\n    def forward(self, x):\n        batch, _, h, w, t = x.shape\n        output = torch.zeros((batch, (h//8)-2, (w//8)-2, t), device=x.device)\n        states = (None, None)\n        for timestep in range(t):\n            output[..., timestep], states = self.forward_step(x[..., timestep], states)\n        return F.sigmoid(output)\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n</code></pre> <pre><code># EXAMPLE USAGE\nx = torch.randn((16, 3, 32, 32, 4), dtype=torch.float32) \nmodel = Discriminator(in_channels=3)\ny = model(x)\nprint(y.size())\n</code></pre>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/","title":"PyImgProc-Image-Processing-using-Python","text":"<p>The \"PyImgProc-Image-Processing-using-Python\" GitHub repository is a comprehensive resource for image processing tasks implemented in Python. </p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/","title":"018 Image Processing using Pillow in Python","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#image-processing-using-pillow-in-python","title":"Image Processing using Pillow in Python","text":"<p>PIL stands for Python Imaging Library. It's a library in Python that adds support for opening, manipulating, and saving many different image file formats. PIL provides functionalities such as image resizing, cropping, filtering, and basic image processing operations.</p> <p>PIL has been a popular choice for working with images in Python for many years. However, its development was discontinued after version 1.1.7. Fortunately, the Pillow library was created as a fork of PIL to continue its development and maintenance. Pillow is essentially a drop-in replacement for PIL, providing all the functionalities of PIL and more, while also being actively maintained and updated. Therefore, when people refer to PIL today, they often mean Pillow.</p> <p>Pillow (or PIL) is widely used in various Python projects for tasks such as image manipulation, computer vision, web development, and more.</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#import-necessary-libraries","title":"Import Necessary Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"IwZFMtReh0h5\" outputId=\"894e71d8-f7ab-4565-a548-99d7c7c6e3a2\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"-lcnxX7sAY-B\"\nfrom PIL import Image\nimport glob\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#load-an-image","title":"Load an Image","text":"<p>\ud83e\udd14 Note: The mode of an image defines the type and depth of a pixel in the image. The current release supports the following standard modes:</p> <ul> <li> <p>1 (1-bit pixels, black and white, stored with one pixel per byte)</p> </li> <li> <p>L (8-bit pixels, black and white)</p> </li> <li> <p>P (8-bit pixels, mapped to any other mode using a colour palette)</p> </li> <li> <p>RGB (3x8-bit pixels, true colour)</p> </li> <li> <p>RGBA (4x8-bit pixels, true colour with transparency mask)</p> </li> <li> <p>CMYK (4x8-bit pixels, colour separation)</p> </li> <li> <p>YCbCr (3x8-bit pixels, colour video format)</p> </li> <li> <p>I (32-bit signed integer pixels)</p> </li> <li> <p>F (32-bit floating point pixels)</p> </li> </ul> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"q0Wf66r3icS-\" outputId=\"8841ce98-efac-4ad4-c8ad-7a2e3b70b669\" img_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/PyImgProc-Image-Processing-using-Python/Datasets/High_Res_RGB_Google_Image.tif\" img = Image.open(img_path)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#print-the-type-of-the-image","title":"Print the type of the image","text":"<p>print(\"Image Type:\", type(img))</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#print-the-format-of-the-image","title":"Print the format of the image","text":"<p>print(\"Image Format:\", img.format)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#print-the-mode-of-the-image","title":"Print the mode of the image","text":"<p>print(\"Image Mode:\", img.mode)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#print-the-size-of-the-image","title":"Print the size of the image","text":"<p>print(\"Image Size:\", img.size) <pre><code>&lt;!-- #region id=\"gDJwQs4qrbUk\" --&gt;\n## **Resizing an Image**\nIn the Python Imaging Library (PIL), which is now maintained as the Pillow library, you can resize an image using the resize() method. Additionally, the thumbnail() method provides a way to resize an image while preserving its aspect ratio.\n\n- In the `resize(` method, you specify the new dimensions for the image. If the aspect ratio of the new dimensions doesn't match the aspect ratio of the original image, the resized image will be distorted.\n\n- In contrast, the `thumbnail()` method resizes the image to fit within the specified dimensions while preserving the aspect ratio. The image is resized so that the longer side fits within the given dimensions, and the other side is adjusted accordingly. This means the image is resized proportionally and not distorted.\n\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"5N4m44pyIRBZ\" --&gt;\n### **Resize**\n&lt;!-- #endregion --&gt;\n\n```python id=\"jA5JY6Vtrd78\"\nimg_resized = img.resize((200, 300))\nimg_resized.save(\"test_image_resize.jpg\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 334} id=\"6TqFJU2bGK5k\" outputId=\"8a979af2-744d-469a-e443-1efe2450f314\" print(img_resized.size) display(img_resized) <pre><code>```python id=\"Ml4nziNTH72H\"\nimg_resized = img.resize((4048, 4600))\nimg_resized.save(\"test_image_resize.jpg\")\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"X2VvfNHgIP-g\" outputId=\"a242c636-d67a-49b7-c22f-4ce024a23396\" print(img_resized.size)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#displayimg_resized","title":"display(img_resized)","text":"<pre><code>&lt;!-- #region id=\"R0Llxkl8IUFZ\" --&gt;\n### **Thumbnail**\n&lt;!-- #endregion --&gt;\n\n```python id=\"6pIFKBDBGfVW\"\nimg.thumbnail((200, 300))\nimg.save(\"test_image_thumbnail.jpg\")\n</code></pre> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"ewZsX7YlHDDi\" outputId=\"27f42c77-15b6-49a5-a6dd-292184a5966b\" print(img.size) display(img) <pre><code>```python id=\"GBJqCmhwHt-F\"\n# img.thumbnail((1200, 1200))\n# img.save(\"test_image_thumbnail.jpg\")\n</code></pre></p> <p>```python id=\"JCYCyXzRH0zm\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#printimgsize","title":"print(img.size)","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#displayimg","title":"display(img)","text":"<pre><code>&lt;!-- #region id=\"yT2SRvQuIkyr\" --&gt;\n## **Cropping an Image**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 184} id=\"xGUujgavIo2K\" outputId=\"1b413426-da2c-4ac7-b2e0-4b7f4a2d83e7\"\ncropped_image = img.crop((0, 0, 150, 150))\ncropped_image.save(f\"cropped_image.jpg\")\n\nprint(cropped_image.size)\ndisplay(cropped_image)\n</code></pre> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"PwQJhElnJ5Wy\" outputId=\"62c2ba58-6945-4ed7-fa15-fec9ae7a71b9\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#copy-and-paste-images","title":"Copy and Paste images","text":"<p>img_google = Image.open(\"/content/google.png\")</p> <p>img_copy = img.copy()</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#paste-the-google-image-on-top-of-the-copied-image","title":"Paste the google image on top of the copied image","text":"<p>img_copy.paste(img_google, (10, 10)) img_copy.save(\"Pasted_Image.jpg\")</p> <p>print(img_copy.size) display(img_copy) <pre><code>&lt;!-- #region id=\"MTTJpkPVLdH9\" --&gt;\n## **Image Rotation**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"OINqbiRWLf94\" outputId=\"4e8b8bfb-f727-4e3e-8c5e-494a7333e487\"\n# Rotate the image about 90 degrees\nimg_90 = img.rotate(90)\nimg_90.save(\"rotated_image_90.jpg\")\n\nprint(img_90.size)\ndisplay(img_90)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"jT3PPLpDLzt5\" outputId=\"491edf17-34dd-45a8-e35c-20379d058609\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#rotate-the-image-about-45-degrees","title":"Rotate the image about 45 degrees","text":"<p>img_45 = img.rotate(45) img_45.save(\"rotated_image_45.jpg\")</p> <p>print(img_45.size) display(img_45) <pre><code>&lt;!-- #region id=\"ge0LhiSiMChT\" --&gt;\n## **Image Flip**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"2p1YTqVmMFVL\" outputId=\"bfbace2f-e79d-47f6-f276-d62a1f79abd3\"\n# Flip the image Left to Right\nimage_flipLR = img.transpose(Image.FLIP_LEFT_RIGHT)\nimage_flipLR.save(\"flipped_image_LR.jpg\")\n\nprint(image_flipLR.size)\ndisplay(image_flipLR)\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"PoBhIHohMQ85\" outputId=\"eb8f6b59-007e-427f-9bab-bff11fe0c0e3\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#flip-the-image-top-to-bottom","title":"Flip the image Top to Bottom","text":"<p>image_flipTB = img.transpose(Image.FLIP_TOP_BOTTOM) image_flipTB.save(\"flipped_image_TB.jpg\")</p> <p>print(image_flipTB.size) display(image_flipTB) <pre><code>&lt;!-- #region id=\"weLRYD1BMtgn\" --&gt;\n## **Grayscale**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 234} id=\"6WyATkOKMyFw\" outputId=\"ee270efc-2d9e-47cc-abe8-49eb4f8d3e25\"\n# Convert the image into grayscale\ngray_img = img.convert(\"L\")\ngray_img.save(\"grayscale_image.jpg\")\n\nprint(gray_img.size)\ndisplay(gray_img)\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#automate-image-processing-task-using-glob","title":"Automate Image Processing Task using glob","text":"<p>```python id=\"2Sv3-OYLO5o1\" folder_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/PyImgProc-Image-Processing-using-Python/Datasets/*.tif\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/018_Image_Processing_using_Pillow_in_Python/#read-all-the-tif-files-and-save-a-45-degree-rotated-version-in-jpg-format","title":"Read all the tif files and save a 45 degree rotated version in jpg format","text":"<p>for f in glob.glob(folder_path):     temp_img = Image.open(f)     temp_img_rotated = temp_img.rotate(45)     # Convert into Grayscale     temp_img_rotated = temp_img_rotated.convert(\"L\")     file_name = f.split(\"/\")[-1].split(\".\")[0] + \"_Rotated.jpg\"     temp_img_rotated.save(file_name, \"PNG\") ```</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/","title":"019 Image Processing using SciPy","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#image-processing-using-scipy","title":"Image Processing using SciPy","text":"<p>Image processing using SciPy involves leveraging the powerful capabilities of the SciPy library, which is a collection of mathematical algorithms and functions built on top of NumPy. With SciPy, users can perform various image processing tasks such as filtering, segmentation, feature detection, and more. By utilizing modules like <code>scipy.ndimage</code> and <code>scipy.signal</code>, one can apply convolution filters, morphological operations, and other advanced techniques to manipulate and analyze images. Additionally, SciPy integrates seamlessly with other Python libraries like Matplotlib for visualization and OpenCV for further processing or computer vision tasks. Its extensive functionality, combined with the flexibility of Python, makes SciPy a valuable tool for image processing tasks in research, academia, and industry.</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#import-necessary-libraries","title":"Import Necessary Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"JOK9skdHaZl0\" outputId=\"0a07fe68-5118-46bf-e218-6fa9e1d93fe6\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"unDA5SznYwfY\"\nfrom scipy import ndimage\nfrom skimage import io\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#load-an-image","title":"Load an Image","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"jwW2pQLlaVXq\" outputId=\"2f9da6e3-a349-4104-b7f3-673c8671aec0\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#read-the-image-with-skimage","title":"Read the image with skimage","text":"<p>img_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/PyImgProc-Image-Processing-using-Python/Datasets/High_Res_RGB_Google_Image.tif\" img = io.imread(img_path)</p> <p>print(\"Image Shape:\", img.shape) print(\"Image Type:\", type(img)) print(\"Image dtype:\", img.dtype) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"LRLk2DYbb972\" outputId=\"fcf407df-b97f-4676-cb0c-194e6aacaeb1\"\n# Show the RGB image\nplt.imshow(img);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"bN_bshRkbt4E\" outputId=\"7d274885-e90d-4a2e-d33c-be01c032319f\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#read-the-image-in-grayscale","title":"Read the image in grayscale","text":"<p>img_grayscale = io.imread(img_path, as_gray=True)</p> <p>print(\"Image Shape:\", img_grayscale.shape) print(\"Image Type:\", type(img_grayscale)) print(\"Image dtype:\", img_grayscale.dtype) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"VBAnpdZGc7HV\" outputId=\"7e27c03f-0da0-4b41-a3ed-a47b7687ae71\"\n# Show the Grayscale image\nplt.imshow(img_grayscale)\nplt.colorbar();\n</code></pre></p> <p>\ud83e\udd14 Note:  When using <code>io.imread()</code> from scikit-image with the <code>as_gray=True</code> parameter, it performs automatic conversion of the image to grayscale. This function will read the image from the specified path and convert it to a grayscale representation using a suitable method.</p> <p>The method employed by scikit-image when <code>as_gray=True</code> is typically based on luminance, which takes into account the relative contributions of each color channel to human perception of brightness. The specific formula used for the conversion might not always be explicitly documented, but it generally follows similar principles to the luminance method. This function typically utilizes the following formula to convert an RGB image to grayscale:</p> \\[ I = 0.2125 \\times R + 0.7154 \\times G + 0.0721 \\times B \\] <p>These coefficients are slightly different from the standard luminance coefficients but are based on similar principles. They approximate the human eye's sensitivity to different colors and aim to produce a perceptually accurate grayscale representation of the original image.</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#extracting-pixel-values","title":"Extracting Pixel Values","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ZRCwyAayfWU1\" outputId=\"781cb1d5-4457-425b-ff38-2b02474cb5e9\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#check-the-pixel-values-at-0-0-of-rgb-and-grayscale-image","title":"Check the pixel values at (0, 0) of RGB and Grayscale image","text":"<p>print(img[0, 0]) print(img_grayscale[0, 0]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"iqWO5J0Af2NH\" outputId=\"d9ab3758-1f56-4cf4-8fe2-bbf6389739df\"\n# Slice the RGB image\nimg_slice = img[10:500, 10:500]\n\nplt.imshow(img_slice);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"L3WpQ9aFgN9b\" outputId=\"8dc3cba8-784b-485a-c258-0cc95a7dc0c8\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#extract-the-image-statistics-from-the-grayscale-image","title":"Extract the image statistics from the grayscale image","text":"<p>mean_gray = img_grayscale.mean() max_gray = img_grayscale.max() min_gray = img_grayscale.min()</p> <p>print(\"Grayscale Mean:\", mean_gray) print(\"Grayscale Max:\", max_gray) print(\"Grayscale Min:\", min_gray) <pre><code>&lt;!-- #region id=\"ha_kgJahhDZQ\" --&gt;\n## **Flipping Image**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 347} id=\"6GdnNkGFhGsQ\" outputId=\"3a165f08-70cf-4b9c-e8a6-abd9dfb7b766\"\n# Flip the image Left to Right and Up to Down\nflippedLR = np.fliplr(img)\nfilppedUD = np.flipud(img)\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\naxes = axes.flatten()\n\naxes[0].imshow(flippedLR)\naxes[1].imshow(filppedUD);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"qq2UOUXdkJce\" outputId=\"7d048252-8271-4a8d-f22c-c82f051fef2b\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#rotate-the-image-by-45-degrees","title":"Rotate the image by 45 degrees","text":"<p>rotated45 = ndimage.rotate(img, angle=45, reshape=False)</p> <p>plt.imshow(rotated45); <pre><code>&lt;!-- #region id=\"H71uDj6_lzjo\" --&gt;\n## **Filter Images**\n&lt;!-- #endregion --&gt;\n\n&lt;!-- #region id=\"1OxxJpeiwFko\" --&gt;\n### **Uniform Filter**\nA uniform filter, also known as a box filter or a mean filter, is a type of image filter commonly used in image processing for smoothing or blurring an image.\n\nThe uniform filter operates by replacing each pixel value in the image with the average of the pixel values in its neighborhood. The size of the neighborhood, or kernel, is usually specified by a square or rectangular window of a certain width and height. The pixel value at the center of the window is replaced with the mean value of all the pixel values within that window.\n\nMathematically, if $ I(x, y) $ represents the pixel value at coordinates $ (x, y) $ in the original image, and $ k $ is the size of the kernel, then the uniform filter operation can be defined as:\n\n$$ I_{\\text{smoothed}}(x, y) = \\frac{1}{k^2} \\sum_{i=x-\\frac{k}{2}}^{x+\\frac{k}{2}} \\sum_{j=y-\\frac{k}{2}}^{y+\\frac{k}{2}} I(i, j) $$\n\nwhere $ I_{\\text{smoothed}}(x, y) $ represents the smoothed pixel value at coordinates $ (x, y) $.\n\nThe uniform filter is effective in reducing noise in an image and can also be used for blurring effects. However, it may also result in loss of image details since it averages pixel values within the kernel, leading to a loss of sharpness in the image.\n\nUniform filters are widely used in various image processing applications, including image denoising, image smoothing, and pre-processing steps for feature extraction and object recognition. They are simple to implement and computationally efficient, making them popular choices for basic image filtering tasks.\n\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQXWraGTc3SsHJP31hcIwH-bhy90yXhKtfVdTueDIKG&amp;s\" width=50%&gt;\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"d0Zz3uIwyCPE\" outputId=\"cb5e4919-79ba-4864-c57a-9a7b4f9b884b\"\n# Apply uniform filter on grayscale image\nuniform_filtered = ndimage.uniform_filter(img_grayscale, size=3)\nplt.imshow(uniform_filtered);\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#gaussian-filter","title":"Gaussian Filter","text":"<p>A Gaussian filter, also known as a Gaussian blur, is a type of image filter used for smoothing or blurring an image. It is named after the Gaussian function, which describes its shape.</p> <p>The Gaussian filter operates by applying a convolution operation with a Gaussian kernel to the input image. The Gaussian kernel is a two-dimensional matrix with values computed using the Gaussian function. The size of the kernel and the standard deviation \\((\\sigma)\\) parameter control the amount of blurring applied to the image.</p> <p>Mathematically, the Gaussian function is defined as:</p> \\[ G(x, y) = \\frac{1}{{2\\pi \\sigma^2}} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} \\] <p>In the context of image processing, the Gaussian kernel is typically created by discretizing this function into a square matrix. The size of the matrix (kernel size) and the standard deviation determine the shape and spread of the Gaussian distribution.</p> <p>The Gaussian filter operation involves convolving the input image with the Gaussian kernel. For each pixel in the image, the filter computes a weighted average of the pixel values in its neighborhood, where the weights are determined by the values in the Gaussian kernel.</p> <p>Gaussian filtering is commonly used for various image processing tasks, including noise reduction, edge detection, and feature extraction. It effectively removes high-frequency noise from the image while preserving important details and edges. The amount of blurring can be adjusted by varying the standard deviation parameter and the size of the kernel.</p> <p></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"6fdavCPhzEEj\" outputId=\"af83a478-043b-4add-f3eb-ec84fa3b6f69\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#apply-gaussian-filter-on-grayscale-image","title":"Apply gaussian filter on grayscale image","text":"<p>gaussian_filtered = ndimage.gaussian_filter(img_grayscale, sigma=5) plt.imshow(gaussian_filtered); <pre><code>&lt;!-- #region id=\"aNpaPe1W1WiM\" --&gt;\n### **Median Filter**\nA median filter is a type of non-linear image filter used for removing noise from an image. Unlike linear filters such as the Gaussian filter or the mean filter, which compute weighted averages of pixel values in the neighborhood, the median filter replaces each pixel's value with the median value of the pixels within its neighborhood.\n\nHere's how the median filter works:\n\n1. **Window**: Similar to other filters, the median filter operates on a sliding window or kernel that moves across the image. The size of the window is usually specified by the width and height of a square or rectangular region.\n\n2. **Median Computation**: For each pixel in the image, the median filter collects the pixel values within the window centered around that pixel.\n\n3. **Median Value**: The filter then sorts the collected pixel values and selects the middle value, which is the median. If the number of pixels within the window is even, the median is calculated as the average of the two middle values.\n\n4. **Replacement**: Finally, the original pixel value at the center of the window is replaced with the computed median value.\n\nThe median filter is particularly effective in removing salt-and-pepper noise, which manifests as isolated bright and dark pixels scattered throughout the image. Since the median filter selects the middle value, it is robust to outliers and preserves edges and sharp features better than linear filters.\n\nHowever, the median filter is computationally more expensive compared to linear filters, especially for larger window sizes, because it involves sorting the pixel values within the neighborhood for each pixel. Despite this drawback, the median filter is widely used in various image processing applications where noise reduction is crucial while preserving image details.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"UyD1YgWE1uDG\" outputId=\"29e7096d-803d-4c34-976f-0546c2a7c9d7\"\n# Apply median filter on grayscale image\nmedian_filtered = ndimage.median_filter(img_grayscale, size=3)\nplt.imshow(gaussian_filtered);\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#sobel-filter","title":"Sobel Filter","text":"<p>The Sobel filter, named after its inventor Irwin Sobel, is a widely used edge detection filter in image processing. It is specifically designed to highlight edges in an image by computing the gradient magnitude.</p> <p>The Sobel filter is based on convolution with a pair of 3x3 kernels: one for detecting horizontal changes in intensity (the Sobel X kernel) and another for detecting vertical changes in intensity (the Sobel Y kernel).</p> <p>The Sobel X kernel: <pre><code>-1  0  1\n-2  0  2\n-1  0  1\n</code></pre></p> <p>The Sobel Y kernel: <pre><code>-1 -2 -1\n 0  0  0\n 1  2  1\n</code></pre></p> <p>To apply the Sobel filter:</p> <ol> <li>Convolve the input image with the Sobel X kernel to compute the horizontal gradient.</li> <li>Convolve the input image with the Sobel Y kernel to compute the vertical gradient.</li> <li>Compute the gradient magnitude at each pixel as the square root of the sum of the squared horizontal and vertical gradients.</li> </ol> <p>The magnitude of the gradient represents the rate of change of intensity in the image, which tends to be higher at edges. By highlighting areas of high gradient magnitude, the Sobel filter effectively identifies edges in the image.</p> <p>The orientation of the gradient can also be computed from the Sobel X and Y gradients, allowing for edge detection in specific directions.</p> <p>The Sobel filter is widely used in various image processing tasks, including edge detection, feature extraction, and image segmentation. It is computationally efficient and provides good results for detecting edges in both grayscale and color images.</p> <p></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 347} id=\"q2rMweKt2HNY\" outputId=\"0db131b1-5e29-4991-c4a6-07620f8777e8\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/019_Image_Processing_using_SciPy/#apply-sobel-filter-on-grayscale-image","title":"Apply sobel filter on grayscale image","text":"<p>sobel_filtered_horizontal = ndimage.sobel(img_grayscale, axis=1) sobel_filtered_vertical = ndimage.sobel(img_grayscale, axis=0)</p> <p>fig, axes = plt.subplots(ncols=2, figsize=(8, 4)) axes[0].imshow(sobel_filtered_horizontal, vmin=0, vmax=0.2) axes[1].imshow(sobel_filtered_vertical, vmin=0, vmax=0.2); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 1000} id=\"aX2dSj_S5Swu\" outputId=\"f77011f1-e9d5-46d4-9a23-04667c0f4e91\"\n# Plot all the images\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(12, 12))\naxes = axes.flatten()\n\nimages = [img, img_grayscale, uniform_filtered, gaussian_filtered,\n          median_filtered, sobel_filtered_horizontal, sobel_filtered_vertical]\n\nimage_names = [\"RGB Image\", \"Grayscale Image\", \"Uniform Filter\", \"Gaussian Filter (\u03c3 = 5)\",\n               \"Median Filter\", \"Sobel Filter (Horizontal)\", \"Sobel Filter (Vertical)\"]\n\nfor i in range(len(images)):\n\n    if image_names[i] in [\"Sobel Filter (Horizontal)\", \"Sobel Filter (Vertical)\"]:\n        vmin, vmax = 0, 0.2\n    else:\n        vmin, vmax = None, None\n\n    axes[i].imshow(images[i], vmin=vmin, vmax=vmax)\n    axes[i].set_title(image_names[i], fontsize=12)\n    axes[i].set_xticklabels(\"\")\n    axes[i].set_yticklabels(\"\")\n    axes[i].tick_params(left = False, bottom = False)\n\nplt.tight_layout(pad=1);\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/","title":"025 introduction to opencv in python","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#introduction-to-opencv-in-python","title":"Introduction to OpenCV in Python","text":"<p>OpenCV (Open Source Computer Vision Library) is an open-source software library for computer vision, image processing, and machine learning. It was initially developed by Intel and later supported by Willow Garage and Itseez. OpenCV is designed to be highly efficient, providing tools for real-time computer vision applications.</p> <p></p> <p>Key Features of OpenCV: 1. Image Processing: OpenCV includes numerous functions for tasks such as filtering, image transformations, color space conversion, edge detection, and morphological operations.</p> <ol> <li> <p>Computer Vision: It supports object detection, face recognition, motion analysis, camera calibration, and 3D reconstruction.</p> </li> <li> <p>Machine Learning: OpenCV has modules for implementing machine learning algorithms, like support vector machines, k-nearest neighbors, and neural networks, which can be used for tasks like object classification and recognition.</p> </li> <li> <p>Cross-Platform Support: OpenCV is cross-platform, meaning it works on various operating systems, including Windows, macOS, Linux, Android, and iOS.</p> </li> <li> <p>Language Bindings: It provides interfaces for several programming languages, including C++, Python, Java, and MATLAB, making it accessible to a wide range of developers.</p> </li> <li> <p>Real-Time Operations: OpenCV is optimized for performance, making it suitable for real-time image processing and computer vision applications.</p> </li> </ol> <p>Due to its versatility and comprehensive set of tools, OpenCV is widely used in fields such as robotics, automotive industry, security, augmented reality, and even in research and academia.</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python id=\"IE47MY0vCsU9\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"32d28b83-a8cb-4b5a-f661-1381e61131f1\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"A7YsiMOICem5\"\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom google.colab.patches import cv2_imshow\n\nplt.rcParams[\"font.family\"] = \"DeJavu Serif\"\nplt.rcParams[\"font.serif\"] = \"Times New Roman\"\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#read-an-image-using-opencv","title":"Read an Image using OpenCV","text":"<p>In OpenCV, the <code>cv2.imread()</code> function is used to read an image from a file and load it into a variable as a NumPy array. This function allows you to specify how the image should be read, whether in color, grayscale, or unchanged.</p> <p>Parameters - <code>filename</code>:   - This is the path to the image file you want to read.</p> <ul> <li><code>flags</code> (optional):</li> <li>This parameter specifies how the image should be read. It can take one of the following values:<ul> <li><code>cv2.IMREAD_COLOR</code> (or <code>1</code>): Reads the image in color mode (default). Any transparency of the image will be neglected.</li> <li><code>cv2.IMREAD_GRAYSCALE</code> (or <code>0</code>): Reads the image in grayscale mode (ignoring color information).</li> <li><code>cv2.IMREAD_UNCHANGED</code> (or <code>-1</code>): Reads the image as is, including the alpha channel if it exists.</li> </ul> </li> </ul> <p>\ud83e\udd14 Note: The conversion to grayscale is typically done using a weighted sum of the original color channels. The standard formula used by OpenCV for this conversion is: $$ \\text{Gray} = 0.299 \\times R + 0.587 \\times G + 0.114 \\times B $$    - Here, \\(R\\) , \\(G\\), and \\(B\\) are the red, green, and blue components of the pixel, respectively. The weights are applied to account for human perception, as the human eye is more sensitive to green light and less to blue.</p> <p>```python id=\"lR8ral1BCly0\" colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} outputId=\"325b6ccb-430e-48a9-c1c2-fd08b730993c\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#define-the-image-path","title":"Define the image path","text":"<p>img_path = r\"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/PyImgProc-Image-Processing-using-Python/Datasets/High_Res_RGB_Google_Image.tif\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#read-the-image-in-bgr-default-format-using-opencv","title":"Read the image in BGR (Default) format using OpenCV","text":"<p>img = cv2.imread(img_path, 1)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#plot-the-rgb-image-using-matplotlib","title":"Plot the RGB image using matplotlib","text":"<p>plt.imshow(img[:, :, ::-1]); <pre><code>&lt;!-- #region id=\"lDBcCbdkWr21\" --&gt;\n## **Splitting Channels**\nIn image processing with OpenCV, splitting channels involves separating an image's color channels into individual components. This is typically done with color images that use the RGB or BGR color models, where each pixel consists of multiple color channels.\n\nFor a color image, the `cv2.split()` function is used to split the image into its constituent channels. In the BGR color model (which is used by default in OpenCV), the image has three channels: Blue, Green, and Red.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"ORs3HvP3aMme\" outputId=\"e0c1314c-e8be-4998-adf2-d2ccf82d7613\"\n# Check the shape of the image\nimg.shape\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"R-1vsTa8aBt0\" outputId=\"2bc32205-72a2-4918-ff2b-b7e2c5def10c\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#extract-all-the-four-corner-pixels","title":"Extract all the four corner pixels","text":"<p>print(\"Top left pixel:\", img[0, 0]) print(\"Top right:\", img[0, 2047]) print(\"Bottom left:\", img[2047, 0]) print(\"Bottom right:\", img[0, 2047]) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 407} id=\"wdBu3lsjbnYp\" outputId=\"5e8c0add-626f-43e6-cd3a-0e21a4a5f491\"\n# Extract all the bands seperately\nred = img[:, :, 2]\ngreen = img[:, :, 1]\nblue = img[:, :, 0]\n\n# Plot the bands\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\naxes = axes.flatten()\n\naxes[0].imshow(red, cmap=\"Reds\")\naxes[0].set_title(\"Red Channel\")\n\naxes[1].imshow(green, cmap=\"Greens\")\naxes[1].set_title(\"Green Channel\")\n\naxes[2].imshow(blue, cmap=\"Blues\")\naxes[2].set_title(\"Blue Channel\");\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 407} id=\"xpJkA2tEeiE6\" outputId=\"9f24148c-c13d-467e-ace9-8db7442cde85\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#extract-all-the-bands-seperately-using-opencvs-builtin-function","title":"Extract all the bands seperately using OpenCV's builtin function","text":"<p>blue, green, red = cv2.split(img)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#plot-the-bands","title":"Plot the bands","text":"<p>fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5)) axes = axes.flatten()</p> <p>axes[0].imshow(red, cmap=\"Reds\") axes[0].set_title(\"Red Channel\")</p> <p>axes[1].imshow(green, cmap=\"Greens\") axes[1].set_title(\"Green Channel\")</p> <p>axes[2].imshow(blue, cmap=\"Blues\") axes[2].set_title(\"Blue Channel\"); <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"-wmkDgaofAPA\" outputId=\"74341352-0afe-4431-ecf7-8f2c6da45b58\"\n# Combine all the seperate bands to form an RGB image\nimg_merged = cv2.merge((blue, green, red))\n\nplt.imshow(img_merged[:, :, ::-1]);\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#resizing-image","title":"Resizing Image","text":"<p>To resize an image using OpenCV, you can use the <code>cv2.resize()</code> function.</p> <p>Parameters:</p> <ol> <li> <p><code>src</code>: The input image you want to resize. (Type: <code>numpy.ndarray</code>)</p> </li> <li> <p><code>dsize</code>: Desired output size as a tuple <code>(width, height)</code>. This specifies the new dimensions of the image. If you use <code>dsize</code>, <code>fx</code> and <code>fy</code> are ignored.</p> </li> <li> <p><code>fx</code> and <code>fy</code> (Optional): Scaling factors for width and height. They adjust the size by a factor relative to the original dimensions.</p> </li> <li> <p><code>interpolation</code>: Method for resampling the image:</p> </li> <li><code>cv2.INTER_NEAREST</code>: Fast but low quality.</li> <li><code>cv2.INTER_LINEAR</code>: Good balance of quality and speed; default for enlarging.</li> <li><code>cv2.INTER_CUBIC</code>: High quality for enlarging, slower.</li> <li><code>cv2.INTER_LANCZOS4</code>: Very high quality, especially for reducing size, but slow.</li> </ol> <p>Note: Use <code>dsize</code> for exact dimensions, and <code>fx</code>/<code>fy</code> for scaling by factors. Choose the interpolation method based on the quality and speed you need.</p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 453} id=\"nj6k1faLf4M9\" outputId=\"bf030862-4014-41fc-a597-efef65df55c6\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#increase-the-size-of-the-pixels-by-two-times","title":"Increase the size of the pixels by two times","text":"<p>resized_img = cv2.resize(img, dsize=None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/025_introduction_to_opencv_in_python/#plot-the-original-image-and-the-resized-image","title":"Plot the original image and the resized image","text":"<p>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) axes = axes.flatten()</p> <p>axes[0].imshow(img[:, :, ::-1]) axes[0].set_title(f\"Original Image\\nShape: {img.shape}\")</p> <p>axes[1].imshow(resized_img[:, :, ::-1]) axes[1].set_title(f\"Resized Image\\nShape: {resized_img.shape}\"); ```</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/","title":"21 waterbodies extraction using entropy and otsu's threshold","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#water-bodies-extraction-using-entropy-and-otsus-threshold","title":"Water Bodies Extraction using Entropy and Otsu's Threshold","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#import-required-libraries","title":"Import Required Libraries","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"hJlqQPvjGkT2\" outputId=\"90152821-2da3-493a-94a4-d7dffd2eb138\" from google.colab import drive drive.mount(\"/content/drive\") <pre><code>```python id=\"rgrFS-LQGxtY\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom skimage import io\nfrom skimage.filters.rank import entropy\nfrom skimage.morphology import disk\nfrom skimage.filters import threshold_otsu\n\nplt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#load-the-image","title":"Load the Image","text":"<p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"HfyXF7h7HRm4\" outputId=\"5e493ec5-d240-4592-db90-d2dd59b48667\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#read-the-image-with-skimage","title":"Read the image with skimage","text":"<p>img_path = \"/content/drive/MyDrive/Colab Notebooks/GitHub Repo/PyImgProc-Image-Processing-using-Python/Datasets/High_Res_RGB_Google_Image.tif\" img = io.imread(img_path) print(img.shape) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"nu1lBHenHM7u\" outputId=\"20a5bcd7-759f-4753-9dfc-4eb38b93e6f6\"\n# Plot the image\nplt.imshow(img);\n</code></pre></p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#applying-entropy-filter","title":"Applying Entropy Filter","text":"<p>In image processing, an entropy filter is a type of spatial filter used to enhance or detect features in an image based on the concept of entropy. Entropy, in the context of image processing, measures the amount of randomness or uncertainty in the distribution of pixel values within a neighborhood of the image.</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#how-entropy-filter-works","title":"How Entropy Filter Works","text":"<ol> <li> <p>Neighborhood Definition: Similar to other spatial filters, an entropy filter operates by sliding a window or kernel across the image. This window defines a local neighborhood around each pixel.</p> </li> <li> <p>Entropy Calculation: Within each neighborhood, the entropy filter computes the entropy of the pixel values. Entropy is calculated using the histogram of pixel intensities within the neighborhood. The formula for entropy calculation is often based on Shannon's entropy formula:</p> </li> </ol> <p>$\\(H = -\\sum_{i} P(i) \\log_2 P(i)\\)$</p> <p>where \\(P(i)\\) represents the probability of occurrence of pixel value \\(i\\) within the neighborhood. The sum is taken over all possible pixel values.</p> <ol> <li> <p>Filter Response: The calculated entropy value for each neighborhood is used to determine the response of the filter. Typically, high entropy values indicate regions with high variability or complexity, while low entropy values indicate regions with more uniform intensity distributions.</p> </li> <li> <p>Enhancement or Detection: Depending on the application, the entropy filter may enhance regions with high entropy (e.g., edges, textures) or suppress regions with low entropy (e.g., uniform regions, noise).</p> </li> </ol>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#applications-of-entropy-filter","title":"Applications of Entropy Filter","text":"<ol> <li> <p>Texture Analysis: Entropy filters are often used for texture analysis in images. Regions with high entropy correspond to complex textures, while regions with low entropy correspond to smoother textures.</p> </li> <li> <p>Edge Detection: Entropy filters can be used for edge detection since edges often correspond to regions with high variability in pixel values.</p> </li> <li> <p>Image Segmentation: Entropy-based segmentation methods use entropy filters to identify regions with distinct texture or intensity characteristics.</p> </li> <li> <p>Noise Reduction: By suppressing regions with low entropy, entropy filters can help reduce noise in an image.</p> </li> </ol> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"p1FnftbzOJxr\" outputId=\"b10b44f2-6ee0-4ad5-a298-1b1f4e65cd50\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#apply-the-entropy-filter-on-red-channel-of-the-rgb-image","title":"Apply the entropy filter on red channel of the RGB image","text":"<p>entropy_img = entropy(img[:, :, 0], footprint=disk(3))</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#plot-the-entropy-image","title":"Plot the entropy image","text":"<p>plt.imshow(entropy_img) plt.colorbar(); <pre><code>&lt;!-- #region id=\"odzIdXACVYfm\" --&gt;\n## **Applying Otsu's Thresholding**\nOtsu's thresholding is a global thresholding technique used in image processing to automatically perform clustering-based image thresholding. Named after Nobuyuki Otsu, who introduced it in 1979, the method is used to convert a grayscale image into a binary image by finding an optimal threshold that minimizes the intra-class variance or equivalently maximizes the inter-class variance.\n\n#### **How Otsu's Thresholding Works**\n\n1. **Histogram Calculation**: Compute the histogram of the grayscale image, which represents the frequency of each gray level (intensity).\n\n2. **Probability Distribution**: Normalize the histogram to obtain the probability distribution of each gray level.\n\n3. **Class Probabilities and Means**:\n   - For each possible threshold $( t )$:\n     - Compute the class probabilities:\n       $$\n       \\omega_0(t) = \\sum_{i=0}^{t-1} P(i) \\quad \\text{(probability of class 1)}\n       $$\n       $$\n       \\omega_1(t) = \\sum_{i=t}^{L-1} P(i) \\quad \\text{(probability of class 2)}\n       $$\n     - Compute the class means:\n       $$\n       \\mu_0(t) = \\frac{\\sum_{i=0}^{t-1} i \\cdot P(i)}{\\omega_0(t)}\n       $$\n       $$\n       \\mu_1(t) = \\frac{\\sum_{i=t}^{L-1} i \\cdot P(i)}{\\omega_1(t)}\n       $$\n\n\n4. **Intra-Class Variance Calculation**: Calculate the intra-class variance for each threshold $( t )$:\n   $$\n   \\sigma^2_w(t) = \\omega_0(t) \\sigma^2_0(t) + \\omega_1(t) \\sigma^2_1(t)\n   $$\n\n   where $ \\sigma^2_0(t) $ and $ \\sigma^2_1(t) $ are the variances of the two classes, computed as:\n   $$\n   \\sigma^2_0(t) = \\sum_{i=0}^{t-1} (\\mu_0(t) - i)^2 P(i)\n   $$\n\n   $$\n   \\sigma^2_1(t) = \\sum_{i=t}^{L-1} (\\mu_1(t) - i)^2 P(i)\n   $$\n\n5. **Optimal Threshold Selection**: The optimal threshold $( t^* )$ is the one that minimizes the intra-class variance (or equivalently, maximizes the inter-class variance):\n   $$\n   t^* = \\arg \\min_{t} \\sigma^2_w(t)\n   $$\n\n#### **Applications**\n\n- **Image Binarization**: Converting grayscale images to binary images for applications such as document scanning, medical imaging, and industrial inspection.\n- **Object Segmentation**: Separating objects from the background in an image.\n- **Preprocessing**: Preparing images for further analysis by reducing complexity.\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"mW63KNUlVlH3\" outputId=\"13a081db-227a-4f56-8163-5914adf6e247\"\n# Plot the histogtam of the entropy image\nsns.histplot(entropy_img.flatten());\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\"} id=\"DbYuhXhUcS0G\" outputId=\"7dc4903d-9134-42fa-aa9c-549b2a68cf88\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#apply-otsus-thresholding-on-entropy-image","title":"Apply Otsu's thresholding on entropy image","text":""},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#get-the-otsus-threshold-value","title":"Get the Otsu's threshold value","text":"<p>thresh = threshold_otsu(entropy_img) print(\"Otsu's threshold value:\", thresh) <pre><code>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 435} id=\"W2b4L0WGczyC\" outputId=\"2c9b7412-be83-4e75-a764-3b4fb7acd74e\"\n# Create a binary image\nbinary_img = entropy_img &lt;= thresh\n\n# Plot the binary image\nplt.imshow(binary_img);\n</code></pre></p> <p>```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 430} id=\"TPixfl4jeYAr\" outputId=\"255e3b47-d3d3-48cf-e266-edf2c43508f5\"</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/21_waterbodies_extraction_using_entropy_and_otsu%27s_threshold/#plot-the-otsus-threshold-on-the-histogram","title":"Plot the Otsu's threshold on the histogram","text":"<p>plt.figure()</p> <p>ax = sns.histplot(entropy_img.flatten()) plt.axvline(x=thresh, c=\"red\", linestyle=\"--\", label=f\"Otsu's Threshold\\n({round(thresh, 4)})\") plt.legend(); <pre><code>&lt;!-- #region id=\"s5z-weNNf4sS\" --&gt;\n## **Plot all the Images in a Single Layout**\n&lt;!-- #endregion --&gt;\n\n```python colab={\"base_uri\": \"https://localhost:8080/\", \"height\": 397} id=\"m42vgLsJgEwT\" outputId=\"6dc9d568-a976-4a8f-afe9-945bc0fffd0c\"\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\naxes = axes.flatten()\n\naxes[0].imshow(img)\naxes[0].set_title(\"RGB Image\")\n\naxes[1].imshow(entropy_img)\naxes[1].set_title(\"Entropy Image\")\n\naxes[2].imshow(binary_img)\naxes[2].set_title(\"Binary Image (Otsu's thresholded)\");\n</code></pre></p>"}]}