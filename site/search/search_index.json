{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Python Documentation","text":"<p>This site hosts various documentation related to Python, Data Science, Machine Learning, and Geospatial Analysis.</p> <p>Browse the sections on the left or use the search bar to find topics.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/","title":"Data-Science-with-Python","text":"<p>This comprehensive course is designed to take you from a beginner level to a proficient level in data science with Python. The course covers the essentials of data science, including data exploration and visualization, data cleaning and wrangling, statistical analysis, and machine learning.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/","title":"Jupyter Notebook Tutorial","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#01-working-with-codes","title":"01. Working with Codes","text":"<pre><code># Import the pandas library and give it an alias 'pd'\nimport pandas as pd\n\n# Define the URL where the Iris dataset is hosted as a csv\nurl = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n\n# Use pandas to read the CSV file located at the specified URL into a DataFrame\ndf = pd.read_csv(url)\n\n# Print the first 5 rows of the DataFrame to verify that the data was loaded correctly\ndf.head()\n</code></pre> sepal.length sepal.width petal.length petal.width variety 0 5.1 3.5 1.4 0.2 Setosa 1 4.9 3.0 1.4 0.2 Setosa 2 4.7 3.2 1.3 0.2 Setosa 3 4.6 3.1 1.5 0.2 Setosa 4 5.0 3.6 1.4 0.2 Setosa"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#02-working-with-plots","title":"02. Working with Plots","text":"<pre><code>%matplotlib inline\ndf.plot(y=\"petal.length\", color=\"green\")\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>df.plot.bar(x=\"variety\", y=\"petal.length\")\n</code></pre> <pre><code>&lt;Axes: xlabel='variety'&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#03-working-with-magic-commands","title":"03. Working with Magic Commands","text":"<pre><code>%lsmagic\n</code></pre> <pre><code>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n</code></pre> <pre><code>%time for i in range(1, 10000): i*i\n</code></pre> <pre><code>CPU times: total: 0 ns\nWall time: 1.05 ms\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#04-adding-youtube-videos","title":"04. Adding YouTube Videos","text":"<pre><code>%%HTML\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6_jEgQjwfok\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#05-adding-markdown","title":"05. Adding Markdown","text":"<p>This is an example of a Markdown document. Markdown is a lightweight markup language that is used to format plain text documents. It's easy to learn and use, and it's supported by many tools and platforms.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#headers","title":"Headers","text":"<p>You can use hash symbols (<code>#</code>) to create headers of different levels. For example:</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-1","title":"Heading 1","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-2","title":"Heading 2","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-3","title":"Heading 3","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-4","title":"Heading 4","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-5","title":"Heading 5","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#heading-6","title":"Heading 6","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#text-formatting","title":"Text Formatting","text":"<p>You can use asterisks (<code>*</code>) or underscores (<code>_</code>) to add emphasis to text. For example:</p> <p>This text will be italicized. This text will also be italicized.</p> <p>This text will be bold. This text will also be bold.</p> <p>You can combine these characters to create different effects, like so:</p> <p>This text will be bold and italicized.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#lists","title":"Lists","text":"<p>You can create ordered and unordered lists using hyphens (<code>-</code>) or numbers (<code>1.</code>). For example:</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#unordered-list","title":"Unordered List","text":"<ul> <li>Item 1</li> <li>Item 2</li> <li>Item 3</li> </ul>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#ordered-list","title":"Ordered List","text":"<ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#links","title":"Links","text":"<p>You can create links to other websites or pages using square brackets (<code>[]</code>) and parentheses (<code>()</code>). For example:</p> <p>Click here to visit GeoNext</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#images","title":"Images","text":"<p>You can include images in your Markdown document using the same syntax as links, but with an exclamation mark (<code>!</code>) at the beginning. For example:</p> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/02_Introduction_to_Jupyter_Notebook/01_Jupyter_Notebook_Tutorial/#code","title":"Code","text":"<p>You can include code snippets in your Markdown document by wrapping the code in backticks ( `) or by using a code block. For example:</p> <pre><code>print(\"Hello World!\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/","title":"Introduction to NumPy","text":"<p>NumPy is a library for the Python programming language that provides support for arrays, matrices, and other numerical operations. It is an essential library for scientific computing in Python. In this module, you will learn about the basics of NumPy and how to use it for numerical computations.</p> <p>Prerequisites: Before starting with NumPy, it is recommended to have a basic understanding of Python programming language, especially control statements, functions, and data types.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install NumPy, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install numpy\n</code></pre> <pre><code>import numpy as np\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#02-creation-of-one-dimensional-arrays","title":"02. Creation of One-Dimensional Arrays","text":"<p>In NumPy, you can create a one-dimensional array using the numpy.array() method.</p> <pre><code>myArr = np.array([1, 2, 3, 4, 5], dtype=\"int8\")\nmyArr\n</code></pre> <pre><code>array([1, 2, 3, 4, 5], dtype=int8)\n</code></pre> <pre><code># Accessing elements of an one-dimensional array\nprint(myArr[0], myArr[3])\n</code></pre> <pre><code>1 4\n</code></pre> <pre><code># Print the data type\nmyArr.dtype\n</code></pre> <pre><code>dtype('int8')\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#03-creation-of-two-dimensional-arrays","title":"03. Creation of Two-Dimensional Arrays","text":"<p>In NumPy, you can create a two-dimensional array using the numpy.array() method and passing a list of lists as an argument.</p> <pre><code>myArr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"int8\")\nmyArr2\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]], dtype=int8)\n</code></pre> <pre><code># Accessing elements of two-dimensional array\nprint(myArr2[0])\nprint(myArr2[2][0])\nprint(myArr2[0, 2])\n</code></pre> <pre><code>[1 2 3]\n7\n3\n</code></pre> <pre><code># Print the data type\nmyArr2.dtype\n</code></pre> <pre><code>dtype('int8')\n</code></pre> <pre><code># Print the shape of the array\nmyArr2.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code># Changing the element of a two-dimensional array\nmyArr2[0, 2] = 10\nmyArr2\n</code></pre> <pre><code>array([[ 1,  2, 10],\n       [ 4,  5,  6],\n       [ 7,  8,  9]], dtype=int8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#04-other-ways-of-array-creation","title":"04. Other Ways of Array Creation","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#01-array-creation-from-other-python-structures","title":"01. Array Creation from Other Python Structures","text":"<pre><code># Array creation from List\nmyList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlistArray = np.array(myList)\nlistArray\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>listArray.dtype\n</code></pre> <pre><code>dtype('int32')\n</code></pre> <pre><code>listArray.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code># Array creation from tuple\nmyTuple = ((1, 2, 3), (4, 5, 6), (7, 8, 9))\ntupleArray = np.array(myTuple)\ntupleArray\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>tupleArray.dtype\n</code></pre> <pre><code>dtype('int32')\n</code></pre> <pre><code># Array cration from set\nmySet = {1, 2, 3, 4, 5, 6}\nsetArray = np.array(mySet)\nsetArray\n</code></pre> <pre><code>array({1, 2, 3, 4, 5, 6}, dtype=object)\n</code></pre> <pre><code># dtype object is not efficient for numeric calculations\nsetArray.dtype\n</code></pre> <pre><code>dtype('O')\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#02-intrinsic-numpy-array-creation-objects","title":"02. Intrinsic NumPy Array Creation Objects","text":"<pre><code># Array creation using zeros function\nzeros = np.zeros((3, 3))\nzeros\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code># Array creation using range function\nrngArray = np.arange(0, 11, 2)\nrngArray\n</code></pre> <pre><code>array([ 0,  2,  4,  6,  8, 10])\n</code></pre> <pre><code># Array creation using linspace function\n# linspace function is used to create a linearly spaced array\nlspace = np.linspace(start=0, stop=50, num=5)\nlspace\n</code></pre> <pre><code>array([ 0. , 12.5, 25. , 37.5, 50. ])\n</code></pre> <pre><code>lspace2 = np.linspace(1, 2, 5)\nlspace2\n</code></pre> <pre><code>array([1.  , 1.25, 1.5 , 1.75, 2.  ])\n</code></pre> <pre><code># Array creation using empty function\n# empty function is used to create an empty array\nempArray = np.empty((4, 6)) # Elements will be random in this case\nempArray\n</code></pre> <pre><code>array([[6.23042070e-307, 4.67296746e-307, 1.69121096e-306,\n        2.78148153e-307, 1.29060531e-306, 8.45599366e-307],\n       [7.56593017e-307, 1.33511290e-306, 1.02359645e-306,\n        1.24610383e-306, 1.69118108e-306, 8.06632139e-308],\n       [1.20160711e-306, 1.69119330e-306, 1.29062229e-306,\n        1.60217812e-306, 1.37961370e-306, 1.69118515e-306],\n       [1.11258277e-307, 1.05700515e-307, 1.11261774e-306,\n        1.29060871e-306, 8.34424766e-308, 2.12203497e-312]])\n</code></pre> <pre><code># Array creation using empty_like function\n# empty_like function is used to generate a copy of previously created array\nempArray2 = np.empty_like(lspace)\nempArray2\n</code></pre> <pre><code>array([1.  , 1.25, 1.5 , 1.75, 2.  ])\n</code></pre> <pre><code># Array creation using identity function\n# identity function is used to create an identity matrix\nidentityMatrix = np.identity(4)\nidentityMatrix\n</code></pre> <pre><code>array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n</code></pre> <pre><code>identityMatrix.shape\n</code></pre> <pre><code>(4, 4)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#05-reshaping-numpy-1-d-array-to-2-d-array","title":"05. Reshaping NumPy 1-D Array to 2-D Array","text":"<pre><code># Creating NumPy one-dimensional array using range function\narr = np.arange(50)\narr\n</code></pre> <pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n</code></pre> <pre><code># Reshaping 1-D array to 2-D array using range function\narr = arr.reshape((5, 10))\narr\n</code></pre> <pre><code>array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n</code></pre> <pre><code>arr.shape\n</code></pre> <pre><code>(5, 10)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/01_Introduction_to_Numpy/#06-converting-numpy-2-d-array-to-1-d-array","title":"06. Converting NumPy 2-D Array to 1-D Array","text":"<pre><code># Two-dimensional array\narr\n</code></pre> <pre><code>array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n</code></pre> <pre><code># Converting 2-D array into 1-D array\narr = arr.ravel()\narr\narr\n</code></pre> <pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/","title":"Numpy Axis","text":"<p>In NumPy, the axis parameter is used to specify the dimension of an array along which a particular operation should be performed. It is an important parameter in many NumPy functions that deal with multi-dimensional arrays. Understanding how to use the axis parameter is crucial in performing complex operations on multi-dimensional arrays.</p> <p>The axis parameter can take values of 0, 1, 2, and so on, where 0 represents the first dimension (rows), 1 represents the second dimension (columns), and so on.</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating a 2-D list\nmyList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n</code></pre> <pre><code># Creating a 2-D NumPy array from the list\nmyArr = np.array(myList)\nmyArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/#01-summing-elements-along-a-specific-axis","title":"01. Summing Elements along a Specific Axis","text":"<pre><code># Summing elements along axis 0 (Row)\nrow_sum = myArr.sum(axis=0)\nrow_sum\n</code></pre> <pre><code>array([12, 15, 18])\n</code></pre> <pre><code># Summing elements along axis 1 (Column)\ncolumn_sum = myArr.sum(axis=1)\ncolumn_sum\n</code></pre> <pre><code>array([ 6, 15, 24])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/02_NumPy_Axis/#02-finding-the-maximum-element-along-a-specific-axis","title":"02. Finding the Maximum Element along a Specific Axis","text":"<pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code># Finding the maximum element along axis 0 (Row)\nmax_axis_0 = myArr.max(axis=0)\nmax_axis_0\n</code></pre> <pre><code>array([7, 8, 9])\n</code></pre> <pre><code># Finding the maximum element along axis 1 (Column)\nmax_axis_1 = myArr.max(axis=1)\nmax_axis_1\n</code></pre> <pre><code>array([3, 6, 9])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/","title":"Numpy Attributes and Methods","text":"<p>NumPy is a powerful library for working with arrays and matrices in Python. It provides various attributes and methods that can be used to manipulate and analyze arrays. In this section, we will discuss some of the important NumPy attributes and methods.</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating a NumPy 2-D array\nmyArr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-numpy-attributes","title":"01. NumPy Attributes","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-t-attribute","title":"01. 'T' Attribute","text":"<p>In NumPy, the T attribute is used to get the transpose of a matrix. The transpose of a matrix is obtained by interchanging the rows and columns of the matrix.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code># Creating transpose of the matrix\nmyArr_transpose = myArr.T\nmyArr_transpose\n</code></pre> <pre><code>array([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-flat-attribute","title":"02. 'flat' Attribute","text":"<p>In NumPy, the flat attribute is used to get a 1-dimensional iterator over a multi-dimensional array. The flat attribute returns a flat iterator that traverses the array in row-major (C-style) order, which means that it first traverses the rows of the array, and then the columns.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.flat\n</code></pre> <pre><code>&lt;numpy.flatiter at 0x160f8c39a20&gt;\n</code></pre> <pre><code>for item in myArr.flat:\n    print(item)\n</code></pre> <pre><code>1\n2\n3\n4\n5\n6\n7\n8\n9\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#03-ndim-attribute","title":"03. 'ndim' Attribute","text":"<p>This attribute returns the number of dimensions of the array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.ndim\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#04-size-attribute","title":"04. 'size' Attribute","text":"<p>This attribute returns the total number of elements in the array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.size\n</code></pre> <pre><code>9\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#05-nbytes-attribute","title":"05. 'nbytes' Attribute","text":"<p>In NumPy, the nbytes attribute is used to get the total number of bytes occupied by the array data in memory. </p> <pre><code>myArr.nbytes\n</code></pre> <pre><code>36\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-numpy-methods","title":"02. NumPy Methods","text":"<pre><code># Creating a NumPy 1-D array\nmyArr2 = np.array([45, 48, 25, 87, 16])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#01-argmax-method","title":"01. 'argmax' Method","text":"<p>np.argmax(arr, axis=None, out=None) returns the indices of the maximum values along an axis.</p> <pre><code># argmax in 1-D array\nmyArr2.argmax()\n</code></pre> <pre><code>3\n</code></pre> <pre><code># argmax in 2-D array\nmyArr.argmax()\n</code></pre> <pre><code>8\n</code></pre> <pre><code># Finding maximum values along axis\nmyArr.argmax(axis=0)\n</code></pre> <pre><code>array([2, 2, 2], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#02-argmin-method","title":"02. 'argmin' Method","text":"<p>np.argmin(arr, axis=None, out=None) returns the indices of the minimum values along an axis.</p> <pre><code># argmin in 1-D array\nmyArr2.argmin()\n</code></pre> <pre><code>4\n</code></pre> <pre><code># argmin in 2-D array\nmyArr.argmin()\n</code></pre> <pre><code>0\n</code></pre> <pre><code># Finding minimum values along axis\nmyArr.argmin(axis=0)\n</code></pre> <pre><code>array([0, 0, 0], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#03-argsort-method","title":"03. 'argsort' Method","text":"<p>argsort() is a method provided by NumPy that returns the indices that would sort an array in ascending or descending order.</p> <pre><code># argsort in 1-D array\nmyArr2.argsort()\n</code></pre> <pre><code>array([4, 2, 0, 1, 3], dtype=int64)\n</code></pre> <pre><code># argsort in 2-D array\nmyArr.argsort()\n</code></pre> <pre><code>array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int64)\n</code></pre> <pre><code># Sorting values along axis\nmyArr.argsort(axis=0)\n</code></pre> <pre><code>array([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2]], dtype=int64)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#04-ravel-method","title":"04. 'ravel' Method","text":"<p>ravel() is used to flatten an array into a 1D array. </p> <pre><code>myArr.ravel()\n</code></pre> <pre><code>array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/03_NumPy_Attributes_and_Methods/#05-reshape-method","title":"05. 'reshape' Method","text":"<p>reshape() is used to change the shape of an array.</p> <pre><code>myArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.reshape((9, 1))\n</code></pre> <pre><code>array([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8],\n       [9]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/","title":"Mathematical Operations Using NumPy","text":"<p>NumPy provides a wide range of mathematical operations that can be performed on arrays. These operations include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more advanced mathematical functions (trigonometric functions, logarithmic functions, etc.).</p> <pre><code>import numpy as np\n</code></pre> <pre><code># Creating our first array\narr1 = np.array([[1, 3, 5], [4, 7, 4], [3, 6, 1]])\narr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 4],\n       [3, 6, 1]])\n</code></pre> <pre><code># Creating our second array\narr2 = np.array([[2, 7, 6], [3, 4, 8], [1, 7, 3]])\narr2\n</code></pre> <pre><code>array([[2, 7, 6],\n       [3, 4, 8],\n       [1, 7, 3]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-basic-matrix-operations","title":"01. Basic Matrix Operations","text":"<p>A wide range of matrix operations can be performed on arrays by using NumPy. These operations include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more advanced matrix operations (determinants, inverses, eigenvalues, etc.).</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-matrix-addition","title":"01. Matrix Addition","text":"<pre><code># Addition\narr1 + arr2\n</code></pre> <pre><code>array([[ 3, 10, 11],\n       [ 7, 11, 12],\n       [ 4, 13,  4]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-matrix-subtraction","title":"02. Matrix Subtraction","text":"<pre><code># Subtraction\narr1 - arr2\n</code></pre> <pre><code>array([[-1, -4, -1],\n       [ 1,  3, -4],\n       [ 2, -1, -2]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-matrix-multiplication","title":"03. Matrix Multiplication","text":"<pre><code># Multiplication\narr1 * arr2\n</code></pre> <pre><code>array([[ 2, 21, 30],\n       [12, 28, 32],\n       [ 3, 42,  3]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#04-matrix-division","title":"04. Matrix Division","text":"<pre><code># Division\narr1 / arr2\n</code></pre> <pre><code>array([[0.5       , 0.42857143, 0.83333333],\n       [1.33333333, 1.75      , 0.5       ],\n       [3.        , 0.85714286, 0.33333333]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-basic-statistical-operations","title":"02. Basic Statistical Operations","text":"<p>NumPy provides several methods to perform basic statistical operations on arrays such as sqrt(), sum(), min(), and max().</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-sqrt-method","title":"01. 'sqrt' Method","text":"<p>The numpy.sqrt() method is used to calculate the square root of each element in a NumPy array.</p> <pre><code>np.sqrt(arr1)\n</code></pre> <pre><code>array([[1.        , 1.73205081, 2.23606798],\n       [2.        , 2.64575131, 2.        ],\n       [1.73205081, 2.44948974, 1.        ]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-sum-method","title":"02. 'sum' Method","text":"<p>The sum() method returns the sum of all elements in the array or along a specified axis.</p> <pre><code>np.sum(arr1)\n</code></pre> <pre><code>34\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-min-method","title":"03. 'min' Method","text":"<p>The min() method returns the minimum value in the array or along a specified axis.</p> <pre><code>np.min(arr1, axis=0)\n</code></pre> <pre><code>array([1, 3, 1])\n</code></pre> <pre><code># Return minimum value of an array\narr1.min()\n</code></pre> <pre><code>1\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#04-max-method","title":"04. 'max' Method","text":"<p>The max() method returns the maximum value in the array or along a specified axis.</p> <pre><code>np.max(arr1, axis=0)\n</code></pre> <pre><code>array([4, 7, 5])\n</code></pre> <pre><code># Return maximum value of an array\narr1.max()\n</code></pre> <pre><code>7\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-other-useful-methods","title":"03. Other Useful Methods","text":"<p>NumPy provides several methods to work with Boolean arrays, including where(), count_nonzero(), and nonzero(). </p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#01-where-method","title":"01. 'where' Method","text":"<p>The where() method returns an array of the same shape as the input array, where each element is replaced by either the value x if the corresponding element in the Boolean mask is True, or the value y if the corresponding element in the Boolean mask is False. You can also use the where() method to extract the indices of the elements that meet a certain condition.</p> <pre><code>arr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 4],\n       [3, 6, 1]])\n</code></pre> <pre><code>np.where(arr1&gt;5, 1, 0)\n</code></pre> <pre><code>array([[0, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#02-count_nonzero-method","title":"02. 'count_nonzero' Method","text":"<p>The count_nonzero() method returns the number of non-zero elements in the input array or along a specified axis.</p> <pre><code>np.count_nonzero(arr1)\n</code></pre> <pre><code>9\n</code></pre> <pre><code># Changing the element of arr1 to 0 at the index position off [1, 2]\narr1[1, 2] = 0\narr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 0],\n       [3, 6, 1]])\n</code></pre> <pre><code>np.count_nonzero(arr1)\n</code></pre> <pre><code>8\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/04_Mathematical_Operations_Using_NumPy/#03-nonzero-method","title":"03. 'nonzero' Method","text":"<p>The nonzero() method returns a tuple of arrays, one for each dimension of the input array, containing the indices of the non-zero elements in that dimension. You can also use the nonzero() method to extract the non-zero elements of an array.</p> <pre><code>arr1\n</code></pre> <pre><code>array([[1, 3, 5],\n       [4, 7, 0],\n       [3, 6, 1]])\n</code></pre> <pre><code>arr1[1, 2]\n</code></pre> <pre><code>0\n</code></pre> <pre><code>np.nonzero(arr1)\n</code></pre> <pre><code>(array([0, 0, 0, 1, 1, 2, 2, 2], dtype=int64),\n array([0, 1, 2, 0, 1, 0, 1, 2], dtype=int64))\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/05_Advantages_of_Using_NumPy/","title":"Advantages of Using NumPy","text":"<p>NumPy arrays are more memory-efficient than Python lists because they are homogeneous, which means that all elements in the array are of the same data type. This allows NumPy to store the data more efficiently in memory.</p> <p>When you create a Python list, Python has to allocate memory for each element of the list, as well as for the list object itself. This means that lists can use more memory than necessary if the elements have different data types.</p> <p>In contrast, NumPy arrays are designed to be memory-efficient. When you create a NumPy array, NumPy allocates a block of memory for the entire array, based on the data type and size of the array. This means that NumPy arrays can use less memory than equivalent lists, especially for large datasets.</p> <pre><code>import sys\nimport numpy as np\n</code></pre> <pre><code># Creating a list\nmyList = [1, 2, 3, 4, 5]\nmyList\n</code></pre> <pre><code>[1, 2, 3, 4, 5]\n</code></pre> <pre><code># Creating a NumPy array from the list\nmyArr = np.array(myList)\nmyArr\n</code></pre> <pre><code>array([1, 2, 3, 4, 5])\n</code></pre> <pre><code># Print the size of the python list\n# Memory in Bytes\nsys.getsizeof(myList) * len(myList)\n</code></pre> <pre><code>600\n</code></pre> <pre><code># Print the size of the NumPy array\n# Memory in Bytes\nmyArr.itemsize * myArr.size\n</code></pre> <pre><code>20\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/03_Introduction_to_NumPy/05_Advantages_of_Using_NumPy/#convert-numpy-array-to-list","title":"Convert NumPy Array to List","text":"<pre><code>arr_to_lst = myArr.tolist()\narr_to_lst\n</code></pre> <pre><code>[1, 2, 3, 4, 5]\n</code></pre> <pre><code>sys.getsizeof(arr_to_lst)\n</code></pre> <pre><code>96\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/","title":"Introduction to Pandas","text":"<p>Pandas is a Python library that provides powerful data manipulation capabilities. It is built on top of NumPy and provides easy-to-use data structures and data analysis tools for data processing and analysis.</p> <p>In this module, we will cover the basics of using Pandas for data analysis. We will start with an introduction to the Pandas library and then move on to topics such as data structures, data cleaning, data visualization, and statistical analysis.</p> <p>Prerequisites: Before starting with Pandas, you should have a basic understanding of Python programming and NumPy. If you are new to Python, we recommend taking an introductory Python course before starting with this course.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install Pandas, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install pandas\n</code></pre> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#02-import-a-csv-data","title":"02. Import a CSV Data","text":"<pre><code># Creating a dataframe\npath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\ndf = pd.read_csv(path)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/01_Introduction_to_Pandas/#03-basic-operations","title":"03. Basic Operations","text":"<pre><code># Print the first five row of csv\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code># Print the shape of the csv\ndf.shape\n</code></pre> <pre><code>(1206, 9)\n</code></pre> <pre><code># Fill th NaN values with 0\ndf.fillna(0, inplace=True)\n</code></pre> <pre><code># Print the maximum earthquake magnitude\ndf[\"Mag\"].max()\n</code></pre> <pre><code>9.1\n</code></pre> <pre><code># Print the Location Name of the earthquakes where total deaths were greater than 50000\ndf[\"Location Name\"][df[\"Total Deaths\"] &gt; 50000]\n</code></pre> <pre><code>272         INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n320    PAKISTAN:  MUZAFFARABAD, URI, ANANTNAG, BARAMULA\n490                            CHINA:  SICHUAN PROVINCE\n607                              HAITI:  PORT-AU-PRINCE\nName: Location Name, dtype: object\n</code></pre> <pre><code># Print the Location Name where the magnitude of the earthquake crossed 8.5 \ndf[\"Location Name\"][df[\"Mag\"] &gt;= 8.5]\n</code></pre> <pre><code>272    INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n294                      INDONESIA:  SUMATERA:  SW\n614          CHILE:  MAULE, CONCEPCION, TALCAHUANO\n674                                 JAPAN:  HONSHU\n736         INDONESIA:  N SUMATRA:  OFF WEST COAST\nName: Location Name, dtype: object\n</code></pre> <pre><code># Print the average focal depth (km) of the earthquakes\ndf[\"Focal Depth (km)\"].mean()\n</code></pre> <pre><code>30.892205638474294\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/","title":"Pandas Dataframe","text":"<p>Pandas DataFrame is a two-dimensional labeled data structure, where the columns can have different data types, such as integers, floats, and strings. It is one of the most popular data structures used in data analysis and machine learning tasks.</p> <p>In this course, we will cover the basics of using Pandas DataFrame for data analysis. We will start with an introduction to Pandas DataFrame and then move on to topics such as data manipulation, data cleaning, and data visualization.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-creating-a-pandas-dataframe","title":"01. Creating a Pandas DataFrame","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-creating-dataframe-from-dictionary","title":"01. Creating DataFrame from Dictionary","text":"<pre><code>weather_dict = {\n    \"day\": [\"1/1/2020\", \"1/2/2020\", \"1/3/2020\", \"1/4/2020\", \"1/5/2020\", \"1/6/2020\"],\n    \"temperature\": [32, 35, 28, 24, 32, 31],\n    \"windspeed\": [6, 7, 2, 7, 4, 2],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\", \"Snow\", \"Rain\", \"Sunny\"]\n}\n</code></pre> <pre><code># Creating dataframe from dictionary\ndf1 = pd.DataFrame(weather_dict)\ndf1\n</code></pre> day temperature windspeed event 0 1/1/2020 32 6 Rain 1 1/2/2020 35 7 Sunny 2 1/3/2020 28 2 Snow 3 1/4/2020 24 7 Snow 4 1/5/2020 32 4 Rain 5 1/6/2020 31 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-creating-dataframe-from-csv","title":"02. Creating DataFrame from CSV","text":"<pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.csv\"\ndf2 = pd.read_csv(path)\ndf2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the type of the df2 variable\ntype(df2)\n</code></pre> <pre><code>pandas.core.frame.DataFrame\n</code></pre> <pre><code># Printing the shape of the dataframe\nrows, colums = df2.shape\nrows\n</code></pre> <pre><code>6\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-head-method","title":"02. head() Method","text":"<p>In Pandas, the head() method is used to view the first few rows of a DataFrame. By default, it displays the first 5 rows of the DataFrame. This method is useful to get a quick overview of the data in the DataFrame.</p> <pre><code># Print first five rows of the dataframe\ndf2.head()\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain <pre><code># Print the first two rows of the dataframe\ndf2.head(2)\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#03-tail-method","title":"03. tail() Method","text":"<p>In Pandas, the tail() method is used to view the last few rows of a DataFrame. By default, it displays the last 5 rows of the DataFrame. This method is useful to get a quick overview of the data in the DataFrame.</p> <pre><code># Print last five rows of dataframe\ndf2.tail()\n</code></pre> day temperature windspeed event 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the last two rows of the dataframe\ndf2.tail(2)\n</code></pre> day temperature windspeed event 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#04-indexing-and-slicing-in-dataframe","title":"04. Indexing and Slicing in DataFrame","text":"<pre><code># Print row number 2 to 4\ndf2[2:5]\n</code></pre> day temperature windspeed event 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain <pre><code># Print the names of the columns\ndf2.columns\n</code></pre> <pre><code>Index(['day', 'temperature', 'windspeed', 'event'], dtype='object')\n</code></pre> <pre><code># Print individual column of the dataframe\ndf2.day\n</code></pre> <pre><code>0    01-01-2020\n1    01-02-2020\n2    01-03-2020\n3    01-04-2020\n4    01-05-2020\n5    01-06-2020\nName: day, dtype: object\n</code></pre> <pre><code># Print the type of a column\ntype(df2[\"event\"])\n</code></pre> <pre><code>pandas.core.series.Series\n</code></pre> <pre><code># Print specific columns from dataframe\ndf2[[\"day\", \"temperature\", \"event\"]]\n</code></pre> day temperature event 0 01-01-2020 32 Rain 1 01-02-2020 35 Sunny 2 01-03-2020 28 Snow 3 01-04-2020 24 Snow 4 01-05-2020 32 Rain 5 01-06-2020 32 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#05-operations-with-dataframe","title":"05. Operations with DataFrame","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#01-max-method","title":"01. max() Method","text":"<pre><code># Print the maximum temperature\ndf2[\"temperature\"].max()\n</code></pre> <pre><code>35\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#02-min-method","title":"02. min() Method","text":"<pre><code># Print the minimum temperature\ndf2[\"temperature\"].min()\n</code></pre> <pre><code>24\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#03-mean-method","title":"03. mean() Method","text":"<pre><code># Print the mean (average) of the temperature\ndf2[\"temperature\"].mean()\n</code></pre> <pre><code>30.5\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#04-std-method","title":"04. std() Method","text":"<pre><code># Print the standard deviation of the temperature\ndf2[\"temperature\"].std()\n</code></pre> <pre><code>3.8858718455450894\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#05-describe-method","title":"05. describe() Method","text":"<pre><code># Print the statistics of the whole dataframe\ndf2.describe()\n</code></pre> temperature windspeed count 6.000000 6.000000 mean 30.500000 4.666667 std 3.885872 2.338090 min 24.000000 2.000000 25% 29.000000 2.500000 50% 32.000000 5.000000 75% 32.000000 6.750000 max 35.000000 7.000000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#06-conditional-selection-in-dataframe","title":"06. Conditional Selection in DataFrame","text":"<pre><code># Print all the rows where temperature greater than or equal to 30\ndf2[df2.temperature &gt;= 30]\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code># Print the row where temperature is maximum\ndf2[df2.temperature == df2[\"temperature\"].max()]\n</code></pre> day temperature windspeed event 1 01-02-2020 35 7 Sunny <pre><code># Print only the day and temperature column where the temperature is maximum\ndf2[[\"day\", \"temperature\"]][df2.temperature == df2[\"temperature\"].max()]\n</code></pre> day temperature 1 01-02-2020 35"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#07-set_index-method","title":"07. set_index() Method","text":"<p>In Pandas, the set_index() method is used to set one or more columns as the index of a DataFrame. This method returns a new DataFrame with the specified column(s) set as the index.</p> <pre><code>df2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny <pre><code>df2.index\n</code></pre> <pre><code>RangeIndex(start=0, stop=6, step=1)\n</code></pre> <pre><code># Set the 'day' column as the index of the dataframe\ndf2.set_index(\"day\", inplace=True)\n</code></pre> <pre><code>df2\n</code></pre> temperature windspeed event day 01-01-2020 32 6 Rain 01-02-2020 35 7 Sunny 01-03-2020 28 2 Snow 01-04-2020 24 7 Snow 01-05-2020 32 4 Rain 01-06-2020 32 2 Sunny <pre><code># The 'loc' function is used to access a group of rows and columns by label(s) or a boolean array.\ndf2.loc[\"01-01-2020\"]\n</code></pre> <pre><code>temperature      32\nwindspeed         6\nevent          Rain\nName: 01-01-2020, dtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/02_Pandas_Dataframe/#08-reset_index-method","title":"08. reset_index() Method","text":"<p>In Pandas, the reset_index() method is used to reset the index of a DataFrame to a default numbered index. It is often used to reset the index after setting it to a column or multiple columns using the set_index() method.</p> <pre><code>df2.reset_index(inplace=True)\n</code></pre> <pre><code>df2\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/","title":"Different Ways of Creating DataFrame","text":"<p>Pandas is a powerful library for data manipulation and analysis in Python. It provides the DataFrame object, which is a two-dimensional table-like data structure with rows and columns. In this tutorial, we will cover different ways of creating a DataFrame in Pandas.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#01-creating-dataframe-using-read_csv-method","title":"01. Creating DataFrame Using read_csv() Method","text":"<p>Pandas provides a read_csv() method which allows us to create a DataFrame by reading a CSV file. This is one of the most common ways of creating a DataFrame in Pandas, especially when working with large datasets.</p> <pre><code>csv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.csv\"\ndf1 = pd.read_csv(csv_path)\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2020 32 6 Rain 1 01-02-2020 35 7 Sunny 2 01-03-2020 28 2 Snow 3 01-04-2020 24 7 Snow 4 01-05-2020 32 4 Rain 5 01-06-2020 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#02-creating-dataframe-using-read_excel-method","title":"02. Creating DataFrame Using read_excel() Method","text":"<p>In addition to read_csv(), Pandas also provides a read_excel() method which allows us to create a DataFrame by reading an Excel file.</p> <pre><code>xls_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\sample_weather_data.xlsx\"\ndf2 = pd.read_excel(xls_path)\ndf2\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#03-creating-dataframe-from-dictionary","title":"03. Creating DataFrame from Dictionary","text":"<p>Another way to create a DataFrame in Pandas is by using a Python dictionary. The keys of the dictionary represent the column names of the DataFrame, while the values represent the data for each column. The values can be of any data type that can be represented in a Pandas DataFrame (such as lists, NumPy arrays, or Pandas Series).</p> <pre><code>weather_dict = {\n    \"day\": [\"2020-01-01\", \"2020-02-01\", \"2020-03-01\", \"2020-04-01\", \"2020-05-01\", \"2020-06-01\"],\n    \"temperature\": [32, 35, 28, 24, 32, 32],\n    \"windspeed\": [6, 7, 2, 7, 4, 2],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\", \"Snow\", \"Rain\", \"Sunny\"]\n}\n</code></pre> <pre><code>df3 = pd.DataFrame(weather_dict)\ndf3\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#04-creating-dataframe-from-a-list-of-tuples","title":"04. Creating DataFrame from a List of Tuples","text":"<p>Another way to create a DataFrame in Pandas is by using a list of tuples. In this case, you need to provide the column names. The column names are optional, but if you don't specify them, Pandas will assign default column names (0, 1, 2, etc.) to the DataFrame.</p> <pre><code>weather_data = [\n    (\"2020-01-01\", 32, 6, \"Rain\"),\n    (\"2020-02-01\", 35, 7, \"Sunny\"),\n    (\"2020-03-01\", 28, 2, \"Snow\"),\n    (\"2020-04-01\", 24, 7, \"Snow\"),\n    (\"2020-05-01\", 32, 4, \"Rain\"),\n    (\"2020-06-01\", 32, 2, \"Sunny\")\n]\n</code></pre> <pre><code>df4 = pd.DataFrame(weather_data, columns=[\"day\", \"temperature\", \"windspeed\", \"event\"])\ndf4\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/03_Different_Ways_of_Creating_Dataframe/#05-creating-dataframe-using-list-of-dictionaries","title":"05. Creating DataFrame Using List of Dictionaries","text":"<p>Another way to create a DataFrame in Pandas is by using a list of dictionaries. In this case, the keys of each dictionary are used as column names in the resulting DataFrame. The order of the keys in the first dictionary determines the order of the columns in the DataFrame.</p> <pre><code>weather_dict_list = [\n    {\"day\": \"2020-01-01\", \"temperature\": 32, \"windspeed\": 6, \"event\": \"Rain\"},\n    {\"day\": \"2020-02-01\", \"temperature\": 35, \"windspeed\": 7, \"event\": \"Sunny\"},\n    {\"day\": \"2020-03-01\", \"temperature\": 28, \"windspeed\": 2, \"event\": \"Snow\"},\n    {\"day\": \"2020-04-01\", \"temperature\": 24, \"windspeed\": 7, \"event\": \"Snow\"},\n    {\"day\": \"2020-05-01\", \"temperature\": 32, \"windspeed\": 4, \"event\": \"Rain\"},\n    {\"day\": \"2020-06-01\", \"temperature\": 32, \"windspeed\": 2, \"event\": \"Sunny\"},\n]\n</code></pre> <pre><code>df5 = pd.DataFrame(weather_dict_list)\ndf5\n</code></pre> day temperature windspeed event 0 2020-01-01 32 6 Rain 1 2020-02-01 35 7 Sunny 2 2020-03-01 28 2 Snow 3 2020-04-01 24 7 Snow 4 2020-05-01 32 4 Rain 5 2020-06-01 32 2 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/","title":"Read and Write Excel CSV","text":"<p>Pandas is a powerful tool for reading and writing data in various formats including Excel and CSV. In this module, we will explore how to read and write Excel and CSV files using Pandas.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-read-csv-file-using-read_csv-method","title":"01. Read CSV File Using read_csv() Method","text":"<p>To read a CSV file using Pandas, you can use the read_csv() function. This function takes the filename as an argument and returns a Pandas DataFrame object.</p> <pre><code>filepath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\stock_data.csv\"\ndf = pd.read_csv(filepath)\ndf\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-skip-rows-in-dataframe-using-skiprows-method","title":"02. Skip Rows in DataFrame Using skiprows() Method","text":"<p>In Pandas, you can use the skiprows() method to skip rows in a DataFrame while reading a CSV or Excel file. This can be useful when you have header rows, comment lines, or other non-data rows that you want to exclude from the DataFrame.</p> <pre><code>df1 = pd.read_csv(filepath, skiprows=1)\ndf1\n</code></pre> GOOGL 27.82 87 845 larry page 0 WMT 4.61 484 65 n.a. 1 MSFT -1 85 64 bill gates 2 RIL not available 50 1023 mukesh ambani 3 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#03-import-data-from-csv-with-null-header","title":"03. Import Data from CSV with \"null header\"","text":"<p>Sometimes you may encounter CSV files that do not have a header row or have a header row with blank or null values. In Pandas, you can still import such CSV files and specify column names later using the header parameter in the read_csv() function.</p> <pre><code>df2 = pd.read_csv(filepath, skiprows=1, header=None)\ndf2\n</code></pre> 0 1 2 3 4 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata <pre><code>df3 = pd.read_csv(filepath, skiprows=1, header=None, names=[\"tickets\", \"eps\", \"revenue\", \"price\", \"people\"])\ndf3\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#04-reading-limited-data-from-csv","title":"04. Reading Limited Data from CSV","text":"<p>In Pandas, you can read a limited number of rows from a CSV file using the nrows parameter in the read_csv() function.</p> <pre><code>df4 = pd.read_csv(filepath, nrows=4)\ndf4\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#05-clean-up-messy-data-from-csv-using-na_values-method","title":"05. Clean Up Messy Data from CSV using na_values() Method","text":"<p>When working with CSV files, you may encounter missing or null values that can make your data messy and difficult to work with. In Pandas, you can use the na_values() method to clean up messy data by specifying which values should be treated as null values.</p> <pre><code>df5 = pd.read_csv(filepath, na_values=[\"not available\", \"n.a.\"])\ndf5\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845.0 larry page 1 WMT 4.61 484 65.0 NaN 2 MSFT -1.00 85 64.0 bill gates 3 RIL NaN 50 1023.0 mukesh ambani 4 TATA 5.60 -1 NaN ratan tata <p>In addition to using a list to specify which values should be treated as null values when reading a CSV file in Pandas, you can also use a dictionary to map specific null values to specific columns. This can be helpful when you need to treat different columns differently based on the null values they contain.</p> <pre><code>df6 = pd.read_csv(filepath, na_values={\n    \"revenue\": [-1, \"n.a.\", \"not applicable\"],\n    \"eps\": [\"n.a.\", 'not available'],\n    \"people\": [\"n.a.\"],\n    \"price\": [\"n.a.\", \"not available\"]\n})\ndf6\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87.0 845.0 larry page 1 WMT 4.61 484.0 65.0 NaN 2 MSFT -1.00 85.0 64.0 bill gates 3 RIL NaN 50.0 1023.0 mukesh ambani 4 TATA 5.60 NaN NaN ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#06-write-dataframe-into-csv-using-to_csv-method","title":"06. Write DataFrame into CSV Using to_csv() Method","text":"<p>Once you have cleaned up and processed your data in a Pandas DataFrame, you may want to save it to a CSV file for further analysis or sharing with others. You can easily do this using the to_csv() method in Pandas.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-index-parameter","title":"01. index Parameter","text":"<p>In the above code, we use the to_csv() method to write the DataFrame to a CSV file called 'new_stock_data.csv'. We can pass index=False to exclude the DataFrame index from being written to the file.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-columns-parameter","title":"02. columns Parameter","text":"<p>The to_csv() method in Pandas allows you to customize the output of your DataFrame to a CSV file. One of the options you can specify is the columns parameter, which allows you to write only specific columns from your DataFrame to the CSV file.</p> <pre><code>df6.columns\n</code></pre> <pre><code>Index(['tickets', 'eps', 'revenue', 'price', 'people'], dtype='object')\n</code></pre> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False, columns=[\"tickets\", \"eps\"])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#03-header-parameter","title":"03. header Parameter","text":"<p>The header parameter allows you to include or exclude the column names as the first row in the CSV file.</p> <pre><code>df6.to_csv(\"new_stock_data.csv\", index=False, header=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#07-read-excel-file-using-read_excel-method","title":"07. Read Excel File Using read_excel() Method","text":"<p>To read an excel file using Pandas, you can use the read_excel() function. This function takes the filename as an argument and returns a Pandas DataFrame object.</p> <pre><code>filepath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\stock_data.xlsx\"\ndf7 = pd.read_excel(filepath, sheet_name=\"stock_data\")\ndf7\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 n.a. 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#08-converters-argument-in-read_excel","title":"08. Converters Argument in read_excel()","text":"<p>The read_excel() function in Pandas allows you to read data from an Excel file into a Pandas DataFrame. One of the arguments you can use to customize the import process is the converters parameter.</p> <p>The converters parameter is used to specify a dictionary of functions that should be applied to specific columns during the import process. The keys of the dictionary represent the column names or indices, and the values are the functions to apply to the corresponding columns.</p> <p>In this example, we define a custom function 'convert_people_cell()' that converts any 'n.a.' input to a string which is 'bill gates'. We then read an Excel file called data.xlsx using the read_excel() function and pass a dictionary to the converters parameter. The dictionary has one key-value pair, where the key is the name of the column to apply the function to (people), and the value is the function to apply (convert_people_cell).</p> <pre><code>def convert_people_cell(cell):\n    if cell == \"n.a.\":\n        return \"jeff bezos\"\n    else:\n        return cell\n</code></pre> <pre><code>df8 = pd.read_excel(filepath, converters={\n    \"people\": convert_people_cell\n})\ndf8\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 jeff bezos 2 MSFT -1 85 64 bill gates 3 RIL not available 50 1023 mukesh ambani 4 TATA 5.6 -1 n.a. ratan tata <pre><code>def convert_eps_cell(cell):\n    if cell == \"not available\":\n        return None\n    else:\n        return cell\n</code></pre> <pre><code>df9 = pd.read_excel(filepath, converters={\n    \"eps\": convert_eps_cell,\n    \"people\": convert_people_cell\n})\ndf9\n</code></pre> tickets eps revenue price people 0 GOOGL 27.82 87 845 larry page 1 WMT 4.61 484 65 jeff bezos 2 MSFT -1.00 85 64 bill gates 3 RIL NaN 50 1023 mukesh ambani 4 TATA 5.60 -1 n.a. ratan tata"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#09-write-dataframe-into-excel-file-using-to_excel-method","title":"09. Write DataFrame into 'excel' File using to_excel() Method","text":"<p>To write a Pandas DataFrame to an Excel file, you can use the to_excel() method.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#01-index-parameter_1","title":"01. index Parameter","text":"<p>The to_excel() method in Pandas allows you to write a DataFrame to an Excel file with various options to customize the output. One of these options is the index parameter, which controls whether or not to include the DataFrame's index in the Excel file.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#02-startrow-and-startcol-parameter","title":"02. startrow and startcol Parameter","text":"<p>The startrow and startcol parameters in the to_excel() method of Pandas allow you to specify the starting row and column for writing data to an Excel file.</p> <pre><code>df9.to_excel(\"new_stocks.xlsx\", sheet_name=\"Stocks\", index=False, startrow=1, startcol=1)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/04_Read_Write_Excel_CSV/#10-use-excelwritter-class","title":"10. Use ExcelWritter() Class","text":"<p>The ExcelWriter class in Pandas is a powerful tool for writing data frames to one or more sheets in an Excel file. This class provides a lot of flexibility and options for formatting the output, such as specifying the sheet name, adding headers and footers, setting column widths and row heights, and so on.</p> <pre><code># Creating two separate dataframe\ndf_stocs = pd.DataFrame({\n    \"tickets\": [\"GOOGLE\", \"WMT\", \"MSFT\"],\n    \"price\": [845, 65, 64],\n    \"Pe\": [30.37, 14.26, 30.97],\n    \"eps\": [27.82, 4.61, 2.12]\n})\n\ndf_weather = pd.DataFrame({\n    \"day\": [\"1/1/2020\", \"1/2/2020\", \"1/3/2020\"],\n    \"temperature\": [32, 35, 28],\n    \"event\": [\"Rain\", \"Sunny\", \"Snow\"]\n})\n</code></pre> <pre><code>with pd.ExcelWriter(\"stocks_and_weather.xlsx\") as writer:\n    df_stocs.to_excel(writer, sheet_name=\"Stock\", index=False)\n    df_weather.to_excel(writer, sheet_name=\"Weather\", index=False)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/","title":"Handle Missing Data","text":"<p>Handling missing data is an important part of data analysis, and Pandas provides a number of methods for dealing with missing values. In this notebook, we will cover some common techniques for handling missing data using Pandas.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Creating a dataframe from CSV\ndf1 = pd.read_csv(\"weather_data.csv\")\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2020 32.0 6.0 Rain 1 01-04-2020 NaN 7.0 Sunny 2 01-05-2020 NaN NaN NaN 3 01-06-2020 24.0 NaN Snow 4 01-07-2020 NaN 4.0 Rain 5 01-08-2020 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-convert-string-column-into-date-type","title":"01. Convert String Column into Date Type","text":"<p>In Pandas, it is common to work with data that includes dates. However, sometimes the dates are stored as strings, which makes it difficult to perform any operations on them. In this case, it is necessary to convert the string column into a date type.</p> <p>Pandas provides the to_datetime() method for converting a string column into a date type. This method is very powerful and flexible, allowing you to convert many different string formats into dates.</p> <pre><code># Print the datatype of values in 'day' column\ntype(df1.day[0])\n</code></pre> <pre><code>str\n</code></pre> <pre><code>df2 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf2\n</code></pre> day temperature windspeed event 0 2020-01-01 32.0 6.0 Rain 1 2020-01-04 NaN 7.0 Sunny 2 2020-01-05 NaN NaN NaN 3 2020-01-06 24.0 NaN Snow 4 2020-01-07 NaN 4.0 Rain 5 2020-01-08 32.0 NaN Sunny <pre><code># Print the datatype of values in 'day' column\ntype(df2.day[0])\n</code></pre> <pre><code>pandas._libs.tslibs.timestamps.Timestamp\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-use-date-as-index-of-dataframe","title":"02. Use Date as Index of DataFrame","text":"<p>To use a date column as the index of a DataFrame, we can use the set_index() method of the DataFrame object, and pass the name of the date column as an argument. The set_index() method will return a new DataFrame with the specified column as the index.</p> <p>The inplace=True argument is used to modify the DataFrame in place, rather than creating a new one.</p> <pre><code>df3 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf3.set_index(\"day\", inplace=True)\ndf3\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#03-use-fillna-method","title":"03. Use fillna() Method","text":"<p>In Pandas, fillna() is a method used to fill missing or null values in a DataFrame with a specified value or technique. This method can be used to clean up the data before further processing.</p> <pre><code>df4 = df3.fillna(0)\ndf4\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 0.0 7.0 Sunny 2020-01-05 0.0 0.0 0 2020-01-06 24.0 0.0 Snow 2020-01-07 0.0 4.0 Rain 2020-01-08 32.0 0.0 Sunny <pre><code>df5 = df3.fillna({\n    \"temperature\": 0,\n    \"windspeed\": 0,\n    \"event\": \"no event\"\n})\ndf5\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 0.0 7.0 Sunny 2020-01-05 0.0 0.0 no event 2020-01-06 24.0 0.0 Snow 2020-01-07 0.0 4.0 Rain 2020-01-08 32.0 0.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#04-use-fillnamethodffillbfill-method","title":"04. Use fillna(method=\"ffill\"/\"bfill\") Method","text":"<p>The fillna() method in pandas is used to fill the missing or NaN values in a DataFrame with a specified value or method. The method parameter can be used to fill the missing values using forward or backward filling method. When method='ffill', it fills the missing values with the previous non-missing value along each column. When method='bfill', it fills the missing values with the next non-missing value along each column.</p> <pre><code># Using fillna(method=\"ffill\") Method\ndf6 = df3.fillna(method=\"ffill\")\ndf6\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 32.0 7.0 Sunny 2020-01-05 32.0 7.0 Sunny 2020-01-06 24.0 7.0 Snow 2020-01-07 24.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny <pre><code># Using fillna(method=\"bfill\") Method\ndf7 = df3.fillna(method=\"bfill\")\ndf7\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 24.0 7.0 Sunny 2020-01-05 24.0 4.0 Snow 2020-01-06 24.0 4.0 Snow 2020-01-07 32.0 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-axis-parameter-in-fillna-method","title":"01. 'axis' Parameter in fillna() Method","text":"<p>The fillna() method in pandas is used to fill the missing or NaN values in a DataFrame with a specified value or method. The axis parameter is used to specify the direction in which the fill operation should be applied.</p> <p>The axis parameter can be set to 0 or 'index' to apply the fill operation along the rows, and to 1 or 'columns' to apply the fill operation along the columns.</p> <pre><code>df8 = df3.fillna(method=\"ffill\", axis=\"columns\")\ndf8\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 24.0 Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 32.0 Sunny <pre><code>df9 = df3.fillna(method=\"bfill\", axis=1)\ndf9\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 7.0 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 Snow Snow 2020-01-07 4.0 4.0 Rain 2020-01-08 32.0 Sunny Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-limit-parameter-in-fillna-method","title":"02. 'limit' Parameter in fillna() Method","text":"<p>The limit parameter in the fillna() method is used to specify the maximum number of consecutive NaN values to be filled. This parameter takes an integer value.</p> <pre><code>df10 = df3.fillna(method=\"ffill\", limit=1)\ndf10\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 32.0 7.0 Sunny 2020-01-05 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 24.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#05-use-interpolate-method-to-do-interpolation","title":"05. Use interpolate() Method to do Interpolation","text":"<p>The interpolate() method in Pandas is used to fill the missing values (NaN) in a DataFrame or a Series by using various interpolation techniques. The interpolate() method supports several interpolation techniques, including linear, quadratic, cubic, and more. You can specify the interpolation technique by passing a value for the method parameter. For example, to use quadratic interpolation, you can set method='quadratic'</p> <pre><code>df3\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code>df11 = df3.interpolate()\ndf11\n</code></pre> temperature windspeed event day 2020-01-01 32.000000 6.0 Rain 2020-01-04 29.333333 7.0 Sunny 2020-01-05 26.666667 6.0 NaN 2020-01-06 24.000000 5.0 Snow 2020-01-07 28.000000 4.0 Rain 2020-01-08 32.000000 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-interpolate-method-time","title":"01. interpolate() Method 'time'","text":"<p>The interpolate() method in Pandas can perform different types of interpolation, including linear, polynomial, and time-based interpolation. Time-based interpolation is used when dealing with time series data, where missing values are filled based on the time difference between observations.</p> <p>To perform time-based interpolation, the interpolate() method needs to be called with the method parameter set to 'time'. This method uses the time stamps of the available data points to estimate the values of missing data points.</p> <pre><code>df12 = df3.interpolate(method=\"time\")\ndf12\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 27.2 7.0 Sunny 2020-01-05 25.6 6.0 NaN 2020-01-06 24.0 5.0 Snow 2020-01-07 28.0 4.0 Rain 2020-01-08 32.0 4.0 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#06-use-dropna-method-to-drop-all-the-rows-with-nan-values","title":"06. Use dropna() Method to Drop all the rows With 'NaN' Values","text":"<p>In Pandas, the dropna() method is used to remove the rows or columns containing NaN or missing data. This method is useful when we have missing values in our dataset and want to remove them.</p> <pre><code>df13 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf13.set_index(\"day\", inplace=True)\ndf13\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Dropping all the rows With 'NaN' Values\ndf14 = df13.dropna()\ndf14\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#01-how-parameter-in-dropna-method","title":"01. 'how' Parameter in dropna() Method","text":"<p>The how parameter in the dropna() method of pandas is used to determine the condition for dropping the rows or columns containing the missing values (NaN values). * how='any': This is the default option. If any missing value is present in a row or column, the entire row or column will be dropped. * how='all': Only the rows or columns containing all missing values will be dropped. * how='thresh': Only the rows or columns containing a minimum number of non-missing values, specified by the thresh parameter, will be kept.</p> <pre><code># Using dropna(how=\"all\")\ndf15 = df13.dropna(how=\"all\")\ndf15\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#02-thresh-parameter-in-dropna-method","title":"02. 'thresh' Parameter in dropna() Method","text":"<p>The thresh parameter in the dropna() method of pandas is used in combination with the how='thresh' option to specify the minimum number of non-missing values required to keep a row or column.</p> <p>For example, if thresh=2, only the rows or columns containing at least two non-missing values will be kept, and all others will be dropped.</p> <pre><code>df13\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code>df16 = df13.dropna(thresh=2)\ndf16\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/05_Handle_Missing_Data/#07-inserting-missing-dates","title":"07. Inserting Missing Dates","text":"<p>In Pandas, missing dates can be inserted into a DataFrame using the reindex() method. This method returns a new DataFrame with the specified index.</p> <p>To insert missing dates, we first need to create a range of dates that includes all the missing dates we want to add. We can do this using the pd.date_range() function. This function takes a start and end date, and returns a range of dates in between.</p> <pre><code>df17 = pd.read_csv(\"weather_data.csv\", parse_dates=[\"day\"])\ndf17.set_index(\"day\", inplace=True)\ndf17\n</code></pre> temperature windspeed event day 2020-01-01 32.0 6.0 Rain 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Inserting missing dates\ndate_range = pd.date_range(\"01-01-2020\", \"01-08-2020\")\ndate_id = pd.DatetimeIndex(date_range)\ndf18 = df17.reindex(date_id)\ndf18\n</code></pre> temperature windspeed event 2020-01-01 32.0 6.0 Rain 2020-01-02 NaN NaN NaN 2020-01-03 NaN NaN NaN 2020-01-04 NaN 7.0 Sunny 2020-01-05 NaN NaN NaN 2020-01-06 24.0 NaN Snow 2020-01-07 NaN 4.0 Rain 2020-01-08 32.0 NaN Sunny <pre><code># Filling missing values\n# Filling numeric values using interpolate method\ndf18.interpolate(inplace=True)\n# Filling non-numeric values using fillna method\ndf18.fillna(method=\"ffill\", inplace=True)\ndf18\n</code></pre> temperature windspeed event 2020-01-01 32.0 6.000000 Rain 2020-01-02 30.4 6.333333 Rain 2020-01-03 28.8 6.666667 Rain 2020-01-04 27.2 7.000000 Sunny 2020-01-05 25.6 6.000000 Sunny 2020-01-06 24.0 5.000000 Snow 2020-01-07 28.0 4.000000 Rain 2020-01-08 32.0 4.000000 Sunny"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/","title":"Handle Missing Data","text":"<p>In data analysis, missing data can be a common occurrence, and handling it properly is important to ensure accurate analysis. Pandas is a powerful library in Python that provides several methods for handling missing data. One of the methods to handle missing data is by using the replace() method.</p> <p>The replace() method in Pandas can be used to replace a specific value with another value, including replacing missing or null values.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>df1 = pd.read_csv(\"weather_data2.csv\")\ndf1\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#01-use-replace-method-to-replace-values","title":"01. Use replace() Method to Replace Values","text":"<p>Pandas provides the replace() method to replace values in a DataFrame. The replace() method can be used to replace a single value or multiple values at once.</p> <pre><code># Replacing single value\ndf2 = df1.replace(-99999, \"NaN\")\ndf2\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0 <pre><code># Replacing multiple values\ndf3 = df1.replace([-99999, -88888], \"NaN\")\ndf3\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0 <pre><code># Replacing values based on specific columns\ndf4 = df1.replace({\n    \"temperature\": -99999,\n    \"windspeed\": [-99999, -88888],\n    \"event\": \"0\"\n}, \"NaN\")\ndf4\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 NaN <pre><code># Mapping specific values\ndf5 = df1.replace({\n    -99999: \"NaN\",\n    \"No Event\": \"Sunny\"\n})\ndf5\n</code></pre> day temperature windspeed event 0 01-01-2017 32F 6 mph Rain 1 01-02-2017 -99999 7 mph Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 Sunny 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#02-regex-regular-expression-parameter","title":"02. regex() (Regular Expression) Parameter","text":"<p>The regex parameter is a boolean parameter used in Pandas to determine whether to interpret the to_replace parameter in the replace() method as a regular expression. When regex is set to True, to_replace is treated as a regular expression pattern, and any matches are replaced with the value parameter.</p> <p>Regular expressions are a powerful tool for pattern matching in text data. They allow us to search for patterns in text, and can be used to extract specific parts of a string or replace certain parts of a string with another value.</p> <p>In the context of Pandas, using regular expressions can be very useful for cleaning and transforming data. For example, we could use regular expressions to extract email addresses from a column of text data or to replace certain characters with others.</p> <pre><code># Using regex\ndf6 = df1.replace(\"[A-Za-z]\", \"\", regex=True)\ndf6\n</code></pre> day temperature windspeed event 0 01-01-2017 32 6 1 01-02-2017 -99999 7 2 01-03-2017 28 -99999 3 01-04-2017 -99999 7 4 01-05-2017 32 -88888 5 01-06-2017 31 2 6 01-06-2017 34 5 0 <pre><code># Using regex based on specific columns\ndf7 = df1.replace({\n    \"temperature\": \"[A-Za-z]\",\n    \"windspeed\": \"[A-Za-z]\"\n}, \"\", regex=True)\ndf7\n</code></pre> day temperature windspeed event 0 01-01-2017 32 6 Rain 1 01-02-2017 -99999 7 Sunny 2 01-03-2017 28 -99999 Snow 3 01-04-2017 -99999 7 No Event 4 01-05-2017 32 -88888 Rain 5 01-06-2017 31 2 Sunny 6 01-06-2017 34 5 0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/06_Handle_Missing_Data/#03-replace-the-list-of-values-with-another-list-of-values","title":"03. Replace the List of Values with Another List of Values","text":"<p>In Pandas, we can use the replace() method to replace a list of values in a DataFrame column with another list of values. This can be useful when we want to replace multiple values with a single value or when we want to replace values with different values depending on their original value.</p> <pre><code># Creatin a new dataframe\ndf = pd.DataFrame({\n    \"score\": [\"exceptional\", \"average\", \"good\", \"poor\", \"average\", \"exceptional\"],\n    \"student\": [\"rob\", \"maya\", \"parthiv\", \"tom\", \"julian\", \"erica\"]\n})\ndf\n</code></pre> score student 0 exceptional rob 1 average maya 2 good parthiv 3 poor tom 4 average julian 5 exceptional erica <pre><code>new_df = df.replace([\"poor\", \"average\", \"good\", \"exceptional\"], [1, 2, 3, 4])\nnew_df\n</code></pre> score student 0 4 rob 1 2 maya 2 3 parthiv 3 1 tom 4 2 julian 5 4 erica"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/","title":"Group By (Split, Apply, Combine)","text":"<p>One of the most powerful features of Pandas is its ability to group data and perform aggregations on the grouped data. The groupby() method is used to group data in Pandas based on one or more columns.</p> <p>Split-Apply-Combine is a common pattern in data analysis where the data is first split into groups based on one or more criteria, then a function is applied to each group individually, and finally the results are combined into a single data structure.</p> <p>Here's a breakdown of each step in the Split-Apply-Combine process:</p> <ul> <li> <p>Split: The data is split into smaller groups based on one or more criteria. For example, we might split data based on the values in a particular column or based on a time period.</p> </li> <li> <p>Apply: A function is applied to each group individually. This function could be an aggregation function like sum() or mean(), or it could be a transformation function that modifies the data within each group.</p> </li> <li> <p>Combine: The results from each group are combined back together into a single data structure. This could be a new DataFrame or Series, or it could be a summary statistic like a mean or a standard deviation.</p> </li> </ul> <pre><code>import pandas as pd\n</code></pre> <pre><code># Creating a dataframe\nfilepath = r\"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\04_Introduction_to_Pandas\\Datasets\\weather_by_cities.csv\"\ndf1 = pd.read_csv(filepath)\ndf1\n</code></pre> day city temperature windspeed event 0 01-01-2017 new york 32 6 Rain 1 01-02-2017 new york 36 7 Sunny 2 01-03-2017 new york 28 12 Snow 3 01-04-2017 new york 33 7 Sunny 4 01-01-2017 mumbai 90 5 Sunny 5 01-02-2017 mumbai 85 12 Fog 6 01-03-2017 mumbai 87 15 Fog 7 01-04-2017 mumbai 92 5 Rain 8 01-01-2017 paris 45 20 Sunny 9 01-02-2017 paris 50 13 Cloudy 10 01-03-2017 paris 54 8 Cloudy 11 01-04-2017 paris 42 10 Cloudy"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#01-use-groupby-method","title":"01. Use groupby() Method","text":"<p>The groupby() method is used to group data in Pandas. It takes one or more column names as arguments and returns a GroupBy object.</p> <pre><code>groups = df1.groupby(\"city\")\ngroups\n</code></pre> <pre><code>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000021627575EE0&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#02-groupby-representation-internally","title":"02. groupby() Representation Internally","text":"<pre><code>for city, city_df in groups:\n    print(city)\n    print(city_df)\n</code></pre> <pre><code>mumbai\n          day    city  temperature  windspeed  event\n4  01-01-2017  mumbai           90          5  Sunny\n5  01-02-2017  mumbai           85         12    Fog\n6  01-03-2017  mumbai           87         15    Fog\n7  01-04-2017  mumbai           92          5   Rain\nnew york\n          day      city  temperature  windspeed  event\n0  01-01-2017  new york           32          6   Rain\n1  01-02-2017  new york           36          7  Sunny\n2  01-03-2017  new york           28         12   Snow\n3  01-04-2017  new york           33          7  Sunny\nparis\n           day   city  temperature  windspeed   event\n8   01-01-2017  paris           45         20   Sunny\n9   01-02-2017  paris           50         13  Cloudy\n10  01-03-2017  paris           54          8  Cloudy\n11  01-04-2017  paris           42         10  Cloudy\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#03-aggregating-data-using-groupby","title":"03. Aggregating Data using groupby()","text":"<p>Once the data is grouped, we can perform various aggregation functions on the data. Some of the commonly used aggregation functions are:</p> <ul> <li>sum(): returns the sum of values in each group</li> <li>mean(): returns the mean of values in each group</li> <li>median(): returns the median of values in each group</li> <li>min(): returns the minimum value in each group</li> <li>max(): returns the maximum value in each group</li> <li>count(): returns the number of values in each group</li> </ul> <pre><code>groups.max()\n</code></pre> day temperature windspeed event city mumbai 01-04-2017 92 15 Sunny new york 01-04-2017 36 12 Sunny paris 01-04-2017 54 20 Sunny <pre><code>groups.mean()\n</code></pre> <pre><code>C:\\Users\\KRISH\\AppData\\Local\\Temp\\ipykernel_3440\\642604181.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  groups.mean()\n</code></pre> temperature windspeed city mumbai 88.50 9.25 new york 32.25 8.00 paris 47.75 12.75 <pre><code>groups.describe()\n</code></pre> temperature windspeed count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max city mumbai 4.0 88.50 3.109126 85.0 86.50 88.5 90.50 92.0 4.0 9.25 5.057997 5.0 5.00 8.5 12.75 15.0 new york 4.0 32.25 3.304038 28.0 31.00 32.5 33.75 36.0 4.0 8.00 2.708013 6.0 6.75 7.0 8.25 12.0 paris 4.0 47.75 5.315073 42.0 44.25 47.5 51.00 54.0 4.0 12.75 5.251984 8.0 9.50 11.5 14.75 20.0"},{"location":"data-science/Data-Science-Bootcamp-with-Python/04_Introduction_to_Pandas/07_Group_By_Method/#04-plotting-groupby-data","title":"04. Plotting groupby() Data","text":"<pre><code>%matplotlib inline\ngroups.plot()\n</code></pre> <pre><code>city\nmumbai      Axes(0.125,0.11;0.775x0.77)\nnew york    Axes(0.125,0.11;0.775x0.77)\nparis       Axes(0.125,0.11;0.775x0.77)\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/","title":"Introduction to Matplotlib","text":"<p>Matplotlib is a widely used Python library for data visualization. It provides a variety of functions for creating different types of graphs and charts. Matplotlib can be used to create simple line plots, scatter plots, histograms, bar charts, 3D plots, and more. In this module, we will cover the basics of Matplotlib and how to use it for data visualization.</p> <p>Prerequisites: * Basic understanding of Python * Familiarity with NumPy library is recommended but not required.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#01-installation-and-importing","title":"01. Installation and Importing","text":"<p>To install Matplotlib, use the pip package manager in the terminal by typing the following command:</p> <pre><code># !pip install matplotlib\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#02-plotting-simple-data","title":"02. Plotting Simple Data","text":"<p>Let's start by creating a simple line plot.</p> <pre><code># Creating two variable\nx = [1, 2, 3, 4, 5, 6, 7]\ny = [40, 38, 43, 45, 42, 40, 39]\n</code></pre> <pre><code>plt.plot(x, y)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x20719040370&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#03-api-for-plot","title":"03. API for Plot","text":"<p>In Matplotlib, you can customize the appearance of lines in a plot using the color, linewidth, and linestyle arguments in the plot() function.</p> <ul> <li> <p>The color argument sets the color of the line. It can be specified using a string such as \"red\" or \"blue\", a hex code such as \"#FF5733\", or an RGB tuple such as (0.5, 0.5, 0.5).</p> </li> <li> <p>The linewidth argument sets the width of the line. It can be specified using a floating-point number.</p> </li> <li> <p>The linestyle argument sets the style of the line. It can be specified using a string such as \"solid\", \"dashed\", or \"dotted\", or a combination of those using a \":\" for dots and a \"--\" for dashes.</p> </li> </ul> <pre><code># Creating a new plot\nplt.plot(x, y, color=\"red\", linewidth=4, linestyle=\"dotted\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x2071908e8e0&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/01_Introduction_to_Matplotlib/#04-adding-title-and-labels-to-the-chart","title":"04. Adding Title and Labels to the Chart","text":"<p>In Matplotlib, you can add titles and labels to your plot to provide context and improve its readability.</p> <ul> <li> <p>To add a title to your plot, you can use the title() method of the axes object. This method takes a string argument that specifies the title of the plot.</p> </li> <li> <p>To add labels to the x-axis and y-axis, you can use the xlabel() and ylabel() methods of the axes object, respectively. These methods take a string argument that specifies the label of the axis. </p> </li> </ul> <pre><code># Creating another plot\nplt.plot(x, y, color=\"green\", linewidth=4, linestyle=\"dashed\")\nplt.title(\"Weather\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Temperature\")\n</code></pre> <pre><code>Text(0, 0.5, 'Temperature')\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/","title":"Format Strings in plot() Function","text":"<p>In Matplotlib, you can customize the appearance of data points in a plot using format strings in the plot() function. A format string is a shorthand way of specifying the color, marker, and line style of the plot.</p> <p>A format string consists of one or more characters that represent the color, marker, and line style of the plot. The characters are specified in a specific order: first the color, then the marker, and finally the line style. Each component is represented by a single character code.</p> <p>Click Here to Check the Documentation of plot() Function </p> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code># Creating two variables\ndays = [1, 2, 3, 4, 5, 6, 7]\ntemperature = [50, 51, 52, 48, 47, 49, 46]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/#01-plotting-using-format-string","title":"01. Plotting Using Format String","text":"<p>Here are the most commonly used codes: * Color codes: b (blue), g (green), r (red), c (cyan), m (magenta), y (yellow), k (black), w (white) * Marker codes: . (point), o (circle), v (downward-pointing triangle), ^ (upward-pointing triangle), s (square), + (plus), x (cross), * (star), p (pentagon), h (hexagon) * Line style codes: - (solid), -- (dashed), : (dotted), -. (dash-dot)</p> <pre><code>plt.plot(days, temperature, \"r.--\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2648070&gt;]\n</code></pre> <p></p> <pre><code># Plotting with keyword arguments\nplt.plot(days, temperature, color=\"r\", marker=\".\", linestyle=\"--\", markersize=10)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c268da00&gt;]\n</code></pre> <p></p> <pre><code>plt.plot(days, temperature, color=\"#58508d\", marker=\"D\", linestyle=\"\", markersize=5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2712370&gt;]\n</code></pre> <p></p> <pre><code>plt.plot(days, temperature, color=\"#58508d\", marker=\"D\", linestyle=\"\", markersize=5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2796220&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/05_Introduction_to_Matplotlib/02_Format_Strings_in_plot_Function/#02-alpha-property-for-plot-api","title":"02. alpha Property for plot API","text":"<p>In Matplotlib, the alpha property of the plot() function can be used to adjust the transparency of data points, lines, and other plot elements.</p> <p>The alpha property takes a value between 0 and 1, where 0 is completely transparent and 1 is completely opaque. By default, the value of alpha is 1, which means that plot elements are fully opaque.</p> <pre><code>plt.plot(days, temperature, color=\"#367635\",marker=\".\", markersize=10, linestyle=\"--\", alpha=0.5)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x220c2890400&gt;]\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/","title":"Simple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#description","title":"Description:","text":"<p>Simple linear regression is a statistical technique used to establish a relationship between two variables - one independent and one dependent. The purpose of this technique is to determine whether there is a linear relationship between the two variables, and if so, to develop a mathematical model that can be used to predict the value of the dependent variable based on the value of the independent variable.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#assumptions","title":"Assumptions:","text":"<p>Simple linear regression is a parametric test, meaning that it makes certain assumptions about the data. These assumptions are: 1. Homogeneity of variance: the size of the error in our prediction doesn\u2019t change significantly across the values of the independent variable. 2. Independence of observations: the observations in the dataset were collected using statistically valid sampling methods, and there are no hidden relationships among observations. 3. Normality: The data follows a normal distribution.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#how-to-perform-a-simple-linear-regression","title":"How to perform a Simple Linear Regression:","text":"<p>Simple linear regression Formula: $$ y = {\\beta_0} + {\\beta_1{X}} + {\\epsilon} $$ * y is the predicted value of the dependent variable (y) for any given value of the independent variable (x). * B0 is the intercept, the predicted value of y when the x is 0. * B1 is the regression coefficient or slope \u2013 how much we expect y to change as x increases. * x is the independent variable ( the variable we expect is influencing y). * e is the error of the estimate, or how much variation there is in our estimate of the regression coefficient.</p> <p>Linear regression finds the line of best fit line through your data by searching for the regression coefficient (B1) that minimizes the total error (e) of the model.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#simple-linear-regression-project","title":"Simple Linear Regression Project:","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#predicting-average-temperature-in-northern-hemisphere-based-on-latitude-using-simple-linear-regression","title":"Predicting Average Temperature in Northern Hemisphere based on Latitude using Simple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#project-description","title":"Project Description:","text":"<p>The relationship between latitude and temperature is a well-established phenomenon in climatology. In general, temperature decreases as we move away from the equator towards the poles. This is because the Earth's surface receives more direct sunlight near the equator than at the poles, and therefore the equator is warmer.</p> <p>The project of predicting average temperature based on latitude using simple linear regression is an exciting machine learning endeavor that has many real-world applications. The goal of this project is to build a model that can accurately estimate the average temperature of a particular location based on its latitude information.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#02-reading-the-csv-file-with-pandas","title":"02. Reading the CSV File with Pandas","text":"<pre><code># Defining the path of the csv\ncsv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\GlobalLandTemperaturesByCity.csv\"\n</code></pre> <pre><code># Reading the csv file with pandas library\ndf = pd.read_csv(csv_path)\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13N 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania 0.80S 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45N 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil 7.23S 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48N 91.27W <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(20000, 8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#03-data-cleaning","title":"03. Data Cleaning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#301-removing-the-rows-with-missing-values","title":"3.01 Removing the Rows with Missing Values","text":"<pre><code># Counting the number of missing values (i.e., NaN values) in each column of the pandas DataFrame\ndf.isnull().sum()\n</code></pre> <pre><code>Unnamed: 0                       0\ndt                               0\nAverageTemperature               0\nAverageTemperatureUncertainty    0\nCity                             0\nCountry                          0\nLatitude                         0\nLongitude                        0\ndtype: int64\n</code></pre> <pre><code># Dropping the rows with missing values\ndf.dropna(inplace=True)\n# Checking the shape of the dataframe after dropping rows with missing values\ndf.shape\n</code></pre> <pre><code>(20000, 8)\n</code></pre> <pre><code># Checking the dataframe after dropping the rows with missing values\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13N 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania 0.80S 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45N 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil 7.23S 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48N 91.27W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#302-manipulating-the-latitude-column","title":"3.02 Manipulating the 'Latitude' Column","text":"<pre><code># Adding a \"-\" (minus) before the latitudes of the sothern hemisphere\ndf[\"Latitude\"] = df[\"Latitude\"].apply(lambda x: \"-\"+x if x.endswith(\"S\") else x)\n</code></pre> <pre><code># Removing the 'N' and 'S' from 'Latitude' column\ndf[\"Latitude\"] = df[\"Latitude\"].str.replace(\"N\", \"\")\ndf[\"Latitude\"] = df[\"Latitude\"].str.replace(\"S\", \"\")\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 0 0 1881-01-01 7.858 0.962 Nuevo Laredo United States 28.13 99.09W 1 1 1962-09-01 21.115 0.309 Musoma Tanzania -0.80 34.55E 2 2 1841-09-01 11.590 1.466 Lyubertsy Russia 55.45 36.85E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil -7.23 34.86W 4 4 1915-04-01 26.726 0.935 Carmen Mexico 18.48 91.27W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#303-changing-the-datatype-of-the-latitude-column","title":"3.03 Changing the Datatype of the 'Latitude' Column","text":"<pre><code># Changing the datatype of the 'Latitude' column from 'str' to 'float'\nconvert_dict = {\"Latitude\": float}\ndf = df.astype(convert_dict)\ndf.dtypes\n</code></pre> <pre><code>Unnamed: 0                         int64\ndt                                object\nAverageTemperature               float64\nAverageTemperatureUncertainty    float64\nCity                              object\nCountry                           object\nLatitude                         float64\nLongitude                         object\ndtype: object\n</code></pre> <pre><code># Checking the latitudes of the southern hemisphere\ndf[df[\"Latitude\"] &lt; 0].head()\n</code></pre> Unnamed: 0 dt AverageTemperature AverageTemperatureUncertainty City Country Latitude Longitude 1 1 1962-09-01 21.115 0.309 Musoma Tanzania -0.80 34.55E 3 3 1972-06-01 24.751 0.386 Jo\u00e3o Pessoa Brazil -7.23 34.86W 8 8 1965-07-01 20.737 0.402 Bukavu Congo (Democratic Republic Of The) -2.41 28.13E 14 14 1943-06-01 22.852 0.513 Toliary Madagascar -23.31 42.82E 19 19 1985-06-01 15.466 0.427 Ferraz De Vasconcelos Brazil -23.31 46.31W"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#304-selecting-the-northern-latitudes-only","title":"3.04 Selecting the Northern Latitudes Only","text":"<pre><code># Selecting the latitudes of the Northern Hemisphere only\ndf = df[df[\"Latitude\"] &gt;= 0]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#305-selecting-a-random-sample-from-the-dataframe","title":"3.05 Selecting a Random Sample from the DataFrame","text":"<pre><code># Selecting a random sample from the dataframe\ndf = df.sample(10000, random_state=0)\n</code></pre> <pre><code># Selecting only two columns 'Latitude' and 'AverageTemperature' from the dataframe\ndf = df[[\"Latitude\", \"AverageTemperature\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#306-removing-the-outliers","title":"3.06 Removing the Outliers","text":"<pre><code># Visualizing the boxplot of the dataframe\nsns.boxplot(df)\nplt.title(\"Boxplot before Outliers Removal\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Boxplot before Outliers Removal')\n</code></pre> <pre><code># Removing the outliers from the 'AverageTemperature' column\n# Getting the value of First and Third Quartile (Q1 &amp; Q3) of the 'AverageTemperature'\nQ1 = df[\"AverageTemperature\"].quantile(0.25)\nQ3 = df[\"AverageTemperature\"].quantile(0.75)\nprint(\"Quartile1:\", Q1)\nprint(\"Quartile2:\", Q3)\n</code></pre> <pre><code>Quartile1: 8.628\nQuartile2: 25.2815\n</code></pre> <pre><code># Calulating the Inter Quartile Range (IQR)\nIQR = Q3 - Q1\nprint(\"IQR:\", IQR)\n</code></pre> <pre><code>IQR: 16.6535\n</code></pre> <pre><code># Calculating the Higher Fence and Lower Fence\nlower_fence =  Q1 - (1.5 * IQR)\nhigher_fence = Q3 + (1.5 * IQR)\nprint(\"Lower Fence:\", lower_fence)\nprint(\"Higher Fence:\", higher_fence)\n</code></pre> <pre><code>Lower Fence: -16.35225\nHigher Fence: 50.261750000000006\n</code></pre> <pre><code># Removing the Outliers\ndf = df[~((df[\"AverageTemperature\"] &lt; lower_fence) | (df[\"AverageTemperature\"] &gt; higher_fence))]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#307-checking-the-final-dataframe","title":"3.07 Checking the Final DataFrame","text":"<pre><code>df.shape\n</code></pre> <pre><code>(9927, 2)\n</code></pre> <pre><code># Resetting the index of the dataframe\ndf.reset_index(drop=True, inplace=True)\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Latitude AverageTemperature 0 32.95 20.134 1 36.17 12.346 2 47.42 -0.779 3 36.17 15.944 4 40.99 0.793 <pre><code># Describing the univariate statistics of the dataframe\ndf.describe()\n</code></pre> Latitude AverageTemperature count 9927.000000 9927.000000 mean 33.014089 16.345360 std 14.580933 10.448532 min 0.800000 -16.335000 25% 23.310000 8.856500 50% 34.560000 18.094000 75% 44.200000 25.317500 max 69.920000 37.283000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#04-data-visualization","title":"04. Data Visualization","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#401-histogram-of-average-temperature-with-kernel-density-estimation-kde","title":"4.01 Histogram of Average Temperature with Kernel Density Estimation (KDE))","text":"<pre><code># Visualizing Histogram of 'AverageTemperature' with Probability Density Function\nsns.histplot(df[\"AverageTemperature\"], kde=True)\nplt.title(\"Historam of Average Temperature\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#402-boxplot-of-average-temperature","title":"4.02 Boxplot of Average Temperature","text":"<pre><code># Visualizing 5 number summary of the 'AverageTemperature'\nsns.boxplot(x=df[\"AverageTemperature\"], width=0.5)\nplt.title(\"Boxplot of Average Temperature after Outliers Removal\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#403-scatterplot-between-latitude-and-average-temperature","title":"4.03 Scatterplot between Latitude and Average Temperature","text":"<pre><code>sns.scatterplot(x=df[\"Latitude\"], y=df[\"AverageTemperature\"], marker=\".\")\nplt.title(\"Scatterplot between North Latitude and Average Temperature\")\nplt.xlabel(\"North Latitude\")\nplt.ylabel(\"Average Temperature (in \u00b0C)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#05-dividing-the-data-into-trainining-and-testing-set","title":"05. Dividing the Data into Trainining and Testing Set","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#501-defining-the-dependent-and-independent-variable","title":"5.01 Defining the Dependent and Independent Variable","text":"<pre><code># Dependent Variable (y) = 'AverageTemperature'\n# Independent Variable (x) = 'Latitude'\nx = df[[\"Latitude\"]]\ny = df[[\"AverageTemperature\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#502-splitting-the-data-into-training-and-testing-set","title":"5.02 Splitting the Data into Training and Testing Set","text":"<pre><code># Importing the tarin_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Training Data = 70% and Testing Data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#06-instantiating-the-simple-linear-regression-model","title":"06. Instantiating the Simple Linear Regression Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#601-importing-linearregression-model-from-sklearn-library","title":"6.01 Importing LinearRegression Model from sklearn Library","text":"<pre><code># Importing the Linear Regression Model from sklearn library\nfrom sklearn.linear_model import LinearRegression\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#602-generating-a-linearregression-object","title":"6.02 Generating a LinearRegression Object","text":"<pre><code># Creating a linear regression object\nlin_reg = LinearRegression()\n# Feeding the training data to the model\nlin_reg.fit(x_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#603-getting-the-coefficients-of-the-linear-regression-model","title":"6.03 Getting the Coefficients of the Linear Regression Model","text":"<pre><code># Getting the slope of the model\nlin_reg.coef_\n</code></pre> <pre><code>array([[-0.48602283]])\n</code></pre> <pre><code># Getting the y-intercept of the model\nlin_reg.intercept_\n</code></pre> <pre><code>array([32.37125607])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#07-validation-of-the-model","title":"07. Validation of the Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#701-importing-some-validation-metrics","title":"7.01 Importing Some Validation Metrics","text":"<pre><code># Importing some validation metrics from sklearn library\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#702-validating-the-linear-regression-model","title":"7.02 Validating the Linear Regression Model","text":"<pre><code># Predicting the AverageTemperature of the x_test (Latitude) data\ny_predicted = lin_reg.predict(x_test)\n</code></pre> <pre><code># Defining the actual 'Average Temperature' data\ny_actual = y_test\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#703-calculating-the-mean-absolute-error-mean-squared-error-of-the-model","title":"7.03 Calculating the Mean Absolute Error, Mean Squared Error of the Model","text":"<pre><code># Calculating the Mean Absolute Error (MAE)\nMAE = mean_absolute_error(y_actual, y_predicted)\nprint(\"Mean Absolute Error (MAE):\", MAE.round(4))\n</code></pre> <pre><code>Mean Absolute Error (MAE): 6.0816\n</code></pre> <pre><code># Calulating the Mean Squared Error (MSE)\nMSE = mean_squared_error(y_actual, y_predicted)\nprint(\"Mean Squared Error (MSE):\", MSE.round(4))\n</code></pre> <pre><code>Mean Squared Error (MSE): 57.4893\n</code></pre> <pre><code># Calulating the Root Mean Squared Error (MSE)\nRMSE = np.sqrt(MSE)\nprint(\"Root Mean Squared Error (RMSE):\", RMSE.round(4))\n</code></pre> <pre><code>Root Mean Squared Error (RMSE): 7.5822\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/01_Simple_Linear_Regression/#704-plotting-the-linear-regression-line","title":"7.04 Plotting the Linear Regression Line","text":"<pre><code>sns.scatterplot(x=df[\"Latitude\"], y=df[\"AverageTemperature\"], marker=\".\")\nplt.plot(x, lin_reg.predict(x), color=\"red\", label=\"Regression Line\")\nplt.title(\"Scatterplot between North Latitude and Average Temperature\")\nplt.xlabel(\"North Latitude\")\nplt.ylabel(\"Average Temperature (in \u00b0C)\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/","title":"Multiple Linear Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#01-import-the-essential-libraries","title":"01. Import the Essential Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn import linear_model\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#02-creating-a-dataframe","title":"02. Creating a DataFrame","text":"<pre><code>df = pd.read_csv(\"Housing.csv\")\ndf.head()\n</code></pre> price area bedrooms bathrooms stories mainroad guestroom basement hotwaterheating airconditioning parking prefarea furnishingstatus 0 13300000 7420 4 2 3 yes no no no yes 2 yes furnished 1 12250000 8960 4 4 4 yes no no no yes 3 no furnished 2 12250000 9960 3 2 2 yes no yes no no 2 yes semi-furnished 3 12215000 7500 4 2 2 yes no yes no yes 3 yes furnished 4 11410000 7420 4 1 2 yes yes yes no yes 2 no furnished <pre><code># Slecting only the fields which have numerical values\nnew_df = df[[\"price\", \"area\", \"bedrooms\", \"bathrooms\"]]\nnew_df.head()\n</code></pre> price area bedrooms bathrooms 0 13300000 7420 4 2 1 12250000 8960 4 4 2 12250000 9960 3 2 3 12215000 7500 4 2 4 11410000 7420 4 1"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#03-creating-linear-regression-object","title":"03. Creating Linear Regression Object","text":"<pre><code># Defining independent(x) and dependent(y) variables\n# In this case, independent variable(x) will be 'area', 'bedrooms', 'bathrooms', 'stories', 'parking'\n# and dependent variable will be 'price'\nx = new_df[[\"area\", \"bedrooms\", \"bathrooms\"]]\ny = new_df[[\"price\"]]\n</code></pre> <pre><code># Creating a Linear Regression model\nreg = linear_model.LinearRegression()\n# Training the model using the x and y\nreg.fit(x, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#04-getting-the-coefficients","title":"04. Getting the Coefficients","text":"<p>Linear Equation: y = m1x1 + m2x2 + m3x3 + ... + mnxn + c where, y = dependent variable m = slope x = independent variables c = y-intercept</p> <pre><code># Getting the slopes for all the independent variables\nreg.coef_\n</code></pre> <pre><code>array([[3.78762754e+02, 4.06820034e+05, 1.38604950e+06]])\n</code></pre> <pre><code># Getting the y-intercept\nreg.intercept_\n</code></pre> <pre><code>array([-173171.60763263])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/02_Multiple_Linear_Regression/#05-predicting-price-of-houses","title":"05. Predicting Price of Houses","text":"<pre><code>reg.predict([[0, 4, 3]])\n</code></pre> <pre><code>C:\\Users\\KRISH\\miniconda3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\narray([[43488532.37932754]])\n</code></pre> <pre><code>\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/03_Cost_Function/","title":"Cost Function","text":"<pre><code>import numpy as np\n</code></pre> <p>y = 2x + 3</p> <pre><code>x = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 7, 9, 11, 13])\n</code></pre> <pre><code>def gradient_descent(x, y):\n    m_curr = b_curr = 0\n    iterations = 1000\n    learning_rate = 0.05\n    n = len(x)\n\n    for i in range(iterations):\n        y_predicted = m_curr * x + b_curr\n        cost = (1 / n) * sum([val**2 for val in (y - y_predicted)])\n        print(f\"m {m_curr}, b {b_curr}, cost {cost}, iteration {i}\")\n        md = -(2 / n) * sum(x * (y - y_predicted))\n        bd = -(2 / n) * sum(y - y_predicted)\n        m_curr = m_curr - learning_rate * md\n        b_curr = b_curr - learning_rate * bd\n</code></pre> <pre><code>gradient_descent(x, y)\n</code></pre> <pre><code>m 0, b 0, cost 89.0, iteration 0\nm 3.1, b 0.9, cost 3.8599999999999994, iteration 1\nm 2.52, b 0.78, cost 0.9763999999999999, iteration 2\nm 2.6140000000000003, b 0.8460000000000001, cost 0.8513360000000004, iteration 3\nm 2.5848, b 0.8772, cost 0.81970064, iteration 4\nm 2.57836, b 0.91404, cost 0.7921173536, iteration 5\nm 2.567952, b 0.949128, cost 0.7655590528640001, iteration 6\nm 2.5584664, b 0.9838296, cost 0.7398944506073604, iteration 7\nm 2.54900448, b 1.01790672, cost 0.7150903372945663, iteration 8\nm 2.539727536, b 1.0514147040000001, cost 0.6911177570948888, iteration 9\nm 2.5306028352, b 1.0843549728000001, cost 0.667948830166544, iteration 10\nm 2.52163322464, b 1.1167386249600002, cost 0.6455566148366696, iteration 11\nm 2.512815090048, b 1.1485747950720002, cost 0.6239150727392739, iteration 12\nm 2.5041460524735997, b 1.1798727885504001, cost 0.602999038418565, iteration 13\nm 2.49562355818752, b 1.2106416939532803, cost 0.5827841900618126, iteration 14\nm 2.487245135995264, b 1.2408904571016963, cost 0.5632470212170514, iteration 15\nm 2.479008349269965, b 1.2706278705929475, cost 0.5443648134590486, iteration 16\nm 2.4709108038951193, b 1.2998625787526632, cost 0.5261156099716162, iteration 17\nm 2.4629501459846894, b 1.328603079708861, cost 0.508478190015543, iteration 18\nm 2.4551240614888727, b 1.3568577279425682, cost 0.4914320442524625, iteration 19\nm 2.447430275468342, b 1.3846347367016496, cost 0.4749573508959591, iteration 20\nm 2.439866551442671, b 1.411942180390982, cost 0.4590349526621794, iteration 21\nm 2.432430690738438, b 1.4387879969190824, cost 0.4436463344931504, iteration 22\nm 2.4251205318504314, b 1.4651799900056428, cost 0.42877360202689574, iteration 23\nm 2.417933949813264, b 1.4911258314499491, cost 0.41439946078931755, iteration 24\nm 2.410868855583689, b 1.516633063360975, cost 0.40050719608364665, iteration 25\nm 2.403923195433338, b 1.5417091003497707, cost 0.38708065355407184, iteration 26\nm 2.397094950351735, b 1.5663612316847921, cost 0.3741042204009627, iteration 27\nm 2.390382135459389, b 1.5905966234107924, cost 0.36156280722581025, iteration 28\nm 2.3837827994308234, b 1.6144223204318966, cost 0.3494418304848192, iteration 29\nm 2.3772950239273487, b 1.63784524855946, cost 0.33772719553070274, iteration 30\nm 2.3709169230394274, b 1.6608722165253094, cost 0.32640528022299464, iteration 31\nm 2.3646466427384647, b 1.6835099179609503, cost 0.31546291908779983, iteration 32\nm 2.3584823603378684, b 1.7057649333433158, cost 0.30488738800857484, iteration 33\nm 2.3524222839632185, b 1.7276437319076237, cost 0.29466638943012996, iteration 34\nm 2.346464652031391, b 1.7491526735278957, cost 0.2847880380586522, iteration 35\nm 2.3406077327384924, b 1.7702980105656887, cost 0.275240847041116, iteration 36\nm 2.334849823556444, b 1.791085889687572, cost 0.26601371460801604, iteration 37\nm 2.329189250738084, b 1.8115223536518816, cost 0.2570959111638838, iteration 38\nm 2.323624368830627, b 1.8316133430652684, cost 0.2484770668105837, iteration 39\nm 2.318153560197357, b 1.8513646981095535, cost 0.2401471592888743, iteration 40\nm 2.312775234547398, b 1.870782160239391, cost 0.23209650232421133, iteration 41\nm 2.307487828473443, b 1.8898713738512325, cost 0.2243157343632518, iteration 42\nm 2.302289804997286, b 1.9086378879240764, cost 0.21679580768794726, iteration 43\nm 2.2971796531230484, b 1.9270871576324828, cost 0.20952797789457814, iteration 44\nm 2.2921558873979504, b 1.94522454593232, cost 0.2025037937254882, iteration 45\nm 2.287217047480509, b 1.963055325119703, cost 0.19571508724170383, iteration 46\nm 2.282361697716038, b 1.9805846783635799, cost 0.1891539643249979, iteration 47\nm 2.2775884267193223, b 1.9978177012124105, cost 0.18281279549836707, iteration 48\nm 2.2728958469643445, b 2.014759403075373, cost 0.1766842070542369, iteration 49\nm 2.2682825943809535, b 2.031414708678532, cost 0.17076107248009, iteration 50\nm 2.263747327958345, b 2.047788459496393, cost 0.16503650417153282, iteration 51\nm 2.2592887293552475, b 2.0638854151592505, cost 0.1595038454231776, iteration 52\nm 2.2549055025167, b 2.079710254836751, cost 0.1541566626880198, iteration 53\nm 2.2505963732973044, b 2.095267578598066, cost 0.14898873809630875, iteration 54\nm 2.24636008909085, b 2.110561908749068, cost 0.14399406222521696, iteration 55\nm 2.2421954184661947, b 2.1255976911469063, cost 0.13916682711089676, iteration 56\nm 2.2381011508093085, b 2.140379296492357, cost 0.13450141949479977, iteration 57\nm 2.234076095971362, b 2.154911021600329, cost 0.12999241429640668, iteration 58\nm 2.230119083922765, b 2.1691970906488875, cost 0.12563456830477537, iteration 59\nm 2.2262289644130573, b 2.1832416564071693, cost 0.12142281408157163, iteration 60\nm 2.2224046066365437, b 2.197048801442535, cost 0.11735225406849686, iteration 61\nm 2.218644898903585, b 2.2106225393073187, cost 0.11341815489225414, iteration 62\nm 2.214948748317446, b 2.223966815705511, cost 0.10961594186043476, iteration 63\nm 2.2113150804566017, b 2.237085509639726, cost 0.10594119364192585, iteration 64\nm 2.207742839062422, b 2.2499824345387727, cost 0.1023896371256476, iteration 65\nm 2.204230985732126, b 2.262661339366169, cost 0.09895714245164931, iteration 66\nm 2.200778499616937, b 2.275125909709914, cost 0.09563971820877826, iteration 67\nm 2.197384377125332, b 2.2873797688538415, cost 0.09243350679334461, iteration 68\nm 2.1940476316313147, b 2.299426478830858, cost 0.08933477992338035, iteration 69\nm 2.190767293187611, b 2.311269541458378, cost 0.08633993430327605, iteration 70\nm 2.187542408243725, b 2.3229123993562566, cost 0.08344548743375868, iteration 71\nm 2.1843720393687502, b 2.334358436947513, cost 0.08064807356233224, iteration 72\nm 2.181255264978871, b 2.3456109814421366, cost 0.07794443976947857, iteration 73\nm 2.1781911790694717, b 2.3566733038042615, cost 0.0753314421860593, iteration 74\nm 2.175178890951774, b 2.367548619702994, cost 0.07280604233752795, iteration 75\nm 2.1722175249939246, b 2.3782400904471626, cost 0.07036530361069966, iteration 76\nm 2.1693062203664586, b 2.3887508239042687, cost 0.06800638783896373, iteration 77\nm 2.1664441307920734, b 2.399083875403904, cost 0.0657265520019769, iteration 78\nm 2.1636304242996216, b 2.4092422486258918, cost 0.06352314503599484, iteration 79\nm 2.16086428298227, b 2.419228896473416, cost 0.0613936047511307, iteration 80\nm 2.1581449027597484, b 2.4290467219313934, cost 0.05933545485196432, iteration 81\nm 2.155471493144607, b 2.4386985789103295, cost 0.057346302058027765, iteration 82\nm 2.1528432770124404, b 2.4481872730759147, cost 0.05542383332082406, iteration 83\nm 2.1502594903759817, b 2.4575155626645913, cost 0.053565813134143936, iteration 84\nm 2.1477193821630243, b 2.4666861592853375, cost 0.05177008093454941, iteration 85\nm 2.145222213998096, b 2.4757017287078966, cost 0.05003454858900295, iteration 86\nm 2.1427672599878216, b 2.484564891637678, cost 0.04835719796672347, iteration 87\nm 2.1403538065099146, b 2.4932782244775638, cost 0.04673607859243991, iteration 88\nm 2.1379811520057395, b 2.501844260076833, cost 0.04516930537831803, iteration 89\nm 2.135648606776376, b 2.510265488467428, cost 0.043655056431923134, iteration 90\nm 2.133355492782134, b 2.518544357587772, cost 0.04219157093766536, iteration 91\nm 2.131101143445455, b 2.526683273994355, cost 0.040777147109271154, iteration 92\nm 2.128884903457148, b 2.534684603561283, cost 0.039410140210890766, iteration 93\nm 2.1267061285859, b 2.5425506721680105, cost 0.0380889606445504, iteration 94\nm 2.124564185491007, b 2.5502837663754394, cost 0.03681207210171745, iteration 95\nm 2.122458451538267, b 2.557886134090593, cost 0.035577989776834094, iteration 96\nm 2.1203883146189955, b 2.5653599852200535, cost 0.03438527864073859, iteration 97\nm 2.118353172972084, b 2.5727074923123494, cost 0.03323255177196899, iteration 98\nm 2.1163524350090865, b 2.579930791189489, cost 0.03211846874400929, iteration 99\nm 2.1143855191422443, b 2.587031981567814, cost 0.031041734066597326, iteration 100\nm 2.1124518536154313, b 2.5940131276683593, cost 0.030001095679291486, iteration 101\nm 2.110550876337949, b 2.600876258816894, cost 0.028995343495533815, iteration 102\nm 2.1086820347211366, b 2.60762337003382, cost 0.028023307995524594, iteration 103\nm 2.1068447855177403, b 2.6142564226140967, cost 0.02708385886626944, iteration 104\nm 2.1050385946639967, b 2.620777344697365, cost 0.026175903687214466, iteration 105\nm 2.103262937124391, b 2.6271880318284295, cost 0.02529838665994734, iteration 106\nm 2.101517296739032, b 2.6334903475082694, cost 0.024450287380481515, iteration 107\nm 2.099801166073616, b 2.639686123735733, cost 0.023630619652699258, iteration 108\nm 2.0981140462719186, b 2.645777161540075, cost 0.02283843034157164, iteration 109\nm 2.096455446910786, b 2.6517652315044917, cost 0.02207279826482414, iteration 110\nm 2.094824885857574, b 2.657652074280807, cost 0.021332833121757225, iteration 111\nm 2.0932218891300005, b 2.663439401095454, cost 0.020617674457976888, iteration 112\nm 2.091645990758364, b 2.6691288942469087, cost 0.019926490664832017, iteration 113\nm 2.090096732650091, b 2.6747222075947086, cost 0.01925847801239354, iteration 114\nm 2.0885736644565784, b 2.6802209670402104, cost 0.01861285971485296, iteration 115\nm 2.087076343442279, b 2.6856267709992157, cost 0.017988885027251458, iteration 116\nm 2.0856043343560073, b 2.6909411908666105, cost 0.01738582837249017, iteration 117\nm 2.0841572093044163, b 2.6961657714731473, cost 0.01680298849760723, iteration 118\nm 2.0827345476276142, b 2.7013020315345075, cost 0.016239687658338633, iteration 119\nm 2.0813359357768864, b 2.7063514640927724, cost 0.01569527083101607, iteration 120\nm 2.0799609671944794, b 2.7113155369504294, cost 0.015169104950885935, iteration 121\nm 2.078609242195423, b 2.716195693097043, cost 0.014660578175960946, iteration 122\nm 2.077280367851345, b 2.720993351128712, cost 0.014169099175552197, iteration 123\nm 2.075973957876252, b 2.725709905660437, cost 0.0136940964426512, iteration 124\nm 2.0746896325142434, b 2.7303467277315177, cost 0.01323501762936315, iteration 125\nm 2.0734270184291206, b 2.734905165204093, cost 0.01279132890462135, iteration 126\nm 2.07218574859586, b 2.7393865431549473, cost 0.012362514333429959, iteration 127\nm 2.07096546219393, b 2.743792164260695, cost 0.011948075276920229, iteration 128\nm 2.0697658045023983, b 2.7481233091764463, cost 0.011547529812517096, iteration 129\nm 2.068586426796826, b 2.7523812369080822, cost 0.011160412173544932, iteration 130\nm 2.067426986247893, b 2.756567185178226, cost 0.010786272207619406, iteration 131\nm 2.0662871458217427, b 2.7606823707860357, cost 0.010424674853196461, iteration 132\nm 2.065166574182015, b 2.7647279899609094, cost 0.010075199633669624, iteration 133\nm 2.064064945593526, b 2.768705218710214, cost 0.009737440168425999, iteration 134\nm 2.062981939827583, b 2.772615213161135, cost 0.009411003700295115, iteration 135\nm 2.061917242068901, b 2.7764591098967464, cost 0.009095510638838102, iteration 136\nm 2.0608705428240857, b 2.7802380262864013, cost 0.008790594118948577, iteration 137\nm 2.0598415378316712, b 2.7839530608105356, cost 0.008495899574250211, iteration 138\nm 2.0588299279736724, b 2.787605293379981, cost 0.008211084324796268, iteration 139\nm 2.0578354191886383, b 2.7911957856498812, cost 0.007935817178590546, iteration 140\nm 2.0568577223861717, b 2.7947255813283016, cost 0.007669778046467093, iteration 141\nm 2.0558965533628926, b 2.79819570647962, cost 0.007412657569880772, iteration 142\nm 2.054951632719825, b 2.8016071698227902, cost 0.007164156761175289, iteration 143\nm 2.0540226857811805, b 2.8049609630245635, cost 0.006923986655911223, iteration 144\nm 2.0531094425145127, b 2.808258060987753, cost 0.006691867976849126, iteration 145\nm 2.052211637452223, b 2.8114994221346237, cost 0.0064675308091975735, iteration 146\nm 2.0513290096143906, b 2.8146859886854947, cost 0.00625071428674765, iteration 147\nm 2.0504613024329124, b 2.817818686932628, cost 0.006041166288529687, iteration 148\nm 2.0496082636769204, b 2.8208984275094915, cost 0.005838643145638964, iteration 149\nm 2.0487696453794606, b 2.8239261056554663, cost 0.0056429093578906605, iteration 150\nm 2.047945203765414, b 2.8269026014760814, cost 0.005453737319971924, iteration 151\nm 2.047134699180634, b 2.829828780198849, cost 0.005270907056776289, iteration 152\nm 2.046337896022282, b 2.832705492424774, cost 0.005094205967609158, iteration 153\nm 2.0455545626703397, b 2.8355335743756123, cost 0.0049234285789696635, iteration 154\nm 2.0447844714202823, b 2.838313848136949, cost 0.004758376305619132, iteration 155\nm 2.0440273984168873, b 2.8410471218971693, cost 0.004598857219660502, iteration 156\nm 2.0432831235891604, b 2.8437341901823863, cost 0.004444685827358471, iteration 157\nm 2.042551430586368, b 2.8463758340873997, cost 0.0042956828534413995, iteration 158\nm 2.0418321067151433, b 2.848972821502749, cost 0.004151675032634976, iteration 159\nm 2.0411249428776608, b 2.8515259073379315, cost 0.004012494908183505, iteration 160\nm 2.0404297335108543, b 2.85403583374084, cost 0.0038779806371261267, iteration 161\nm 2.0397462765266625, b 2.8565033303134997, cost 0.003747975802100954, iteration 162\nm 2.039074373253284, b 2.858929114324151, cost 0.0036223292294580824, iteration 163\nm 2.0384138283774265, b 2.8613138909157505, cost 0.003500894813470044, iteration 164\nm 2.0377644498875322, b 2.8636583533109476, cost 0.003383531346435587, iteration 165\nm 2.0371260490179623, b 2.865963183013593, cost 0.003270102354479139, iteration 166\nm 2.036498440194126, b 2.868229050006845, cost 0.003160475938854661, iteration 167\nm 2.035881440978534, b 2.8704566129479225, cost 0.0030545246225695453, iteration 168\nm 2.03527487201777, b 2.87264651935957, cost 0.002952125202150655, iteration 169\nm 2.034678556990352, b 2.874799405818282, cost 0.002853158604379423, iteration 170\nm 2.0340923225554803, b 2.876915898139348, cost 0.002757509747829726, iteration 171\nm 2.0335159983026476, b 2.8789966115587693, cost 0.0026650674090478023, iteration 172\nm 2.0329494167021047, b 2.8810421509120983, cost 0.0025757240932181157, iteration 173\nm 2.03239241305616, b 2.883053110810257, cost 0.002489375909165006, iteration 174\nm 2.031844825451307, b 2.885030075812383, cost 0.0024059224485447534, iteration 175\nm 2.0313064947111545, b 2.886973620595753, cost 0.0023252666690877825, iteration 176\nm 2.0307772643501587, b 2.888884310122831, cost 0.0022473147817549484, iteration 177\nm 2.0302569805281347, b 2.8907626998055003, cost 0.0021719761416765025, iteration 178\nm 2.0297454920055364, b 2.89260933566651, cost 0.002099163142747632, iteration 179\nm 2.0292426500994933, b 2.894424754498198, cost 0.002028791115757323, iteration 180\nm 2.028748308640591, b 2.8962094840185304, cost 0.0019607782299322874, iteration 181\nm 2.028262323930382, b 2.8979640430245, cost 0.0018950453977817196, iteration 182\nm 2.027784554699612, b 2.8996889415429354, cost 0.0018315161831319372, iteration 183\nm 2.027314862067158, b 2.9013846809787585, cost 0.0017701167122437873, iteration 184\nm 2.026853109499657, b 2.9030517542607353, cost 0.001710775587910338, iteration 185\nm 2.026399162771814, b 2.904690645984765, cost 0.0016534238064336356, iteration 186\nm 2.0259528899273893, b 2.9063018325547443, cost 0.0015979946773853376, iteration 187\nm 2.025514161240838, b 2.907885782321053, cost 0.0015444237460568813, iteration 188\nm 2.0250828491796002, b 2.9094429557166963, cost 0.0014926487185096338, iteration 189\nm 2.024658828367031, b 2.9109738053911465, cost 0.0014426093891373427, iteration 190\nm 2.024241975545953, b 2.9124787763419224, cost 0.0013942475706576515, iteration 191\nm 2.023832169542828, b 2.9139583060439445, cost 0.0013475070264496187, iteration 192\nm 2.023429291232534, b 2.9154128245767015, cost 0.0013023334051602985, iteration 193\nm 2.0230332235037363, b 2.916842754749271, cost 0.0012586741775033304, iteration 194\nm 2.0226438512248452, b 2.918248512223223, cost 0.0012164785751761667, iteration 195\nm 2.0222610612105485, b 2.9196305056334473, cost 0.0011756975318250324, iteration 196\nm 2.021884742188911, b 2.920989136706938, cost 0.0011362836259893234, iteration 197\nm 2.0215147847690274, b 2.9223248003795708, cost 0.001098191025958192, iteration 198\nm 2.021151081409226, b 2.9236378849109053, cost 0.001061375436476129, iteration 199\nm 2.020793526385806, b 2.9249287719970467, cost 0.001025794047235109, iteration 200\nm 2.0204420157623053, b 2.9261978368816, cost 0.000991405483093303, iteration 201\nm 2.0200964473592893, b 2.9274454484647485, cost 0.0009581697559628608, iteration 202\nm 2.0197567207246463, b 2.928671969410487, cost 0.0009260482183106579, iteration 203\nm 2.019422737104389, b 2.9298777562520444, cost 0.000895003518217432, iteration 204\nm 2.0190943994139476, b 2.931063159495523, cost 0.0008649995559441768, iteration 205\nm 2.0187716122099486, b 2.932228523721786, cost 0.0008360014419539533, iteration 206\nm 2.018454281662469, b 2.933374187686623, cost 0.000807975456341349, iteration 207\nm 2.0181423155277662, b 2.9345004844192197, cost 0.0007808890096220431, iteration 208\nm 2.0178356231214574, b 2.935607741318968, cost 0.0007547106048365685, iteration 209\nm 2.0175341152921638, b 2.9366962802506342, cost 0.0007294098009248421, iteration 210\nm 2.017237704395593, b 2.9377664176379215, cost 0.0007049571773281187, iteration 211\nm 2.0169463042690645, b 2.9388184645554514, cost 0.0006813242997781602, iteration 212\nm 2.016659830206458, b 2.9398527268191867, cost 0.0006584836872327109, iteration 213\nm 2.0163781989335985, b 2.9408695050753306, cost 0.0006364087799199053, iteration 214\nm 2.0161013285840412, b 2.941869094887718, cost 0.0006150739084535631, iteration 215\nm 2.0158291386752807, b 2.942851786823734, cost 0.0005944542639841898, iteration 216\nm 2.015561550085352, b 2.9438178665387764, cost 0.0005745258693502642, iteration 217\nm 2.015298485029832, b 2.9447676148592934, cost 0.0005552655511971699, iteration 218\nm 2.015039867039229, b 2.9457013078644145, cost 0.000536650913030237, iteration 219\nm 2.0147856209367525, b 2.946619216966204, cost 0.0005186603091714572, iteration 220\nm 2.0145356728164634, b 2.947521608988558, cost 0.0005012728195892724, iteration 221\nm 2.0142899500217863, b 2.948408746244763, cost 0.00048446822557211844, iteration 222\nm 2.0140483811243923, b 2.9492808866137508, cost 0.00046822698621744297, iteration 223\nm 2.0138108959034353, b 2.950138283615058, cost 0.0004525302157088875, iteration 224\nm 2.013577425325139, b 2.9509811864825215, cost 0.000437359661355429, iteration 225\nm 2.01334790152273, b 2.9518098402367277, cost 0.00042269768236648616, iteration 226\nm 2.013122257776709, b 2.952624485756236, cost 0.0004085272293385638, iteration 227\nm 2.0129004284954584, b 2.9534253598475995, cost 0.0003948318244298947, iteration 228\nm 2.012682349196174, b 2.954212695314202, cost 0.00038159554219932676, iteration 229\nm 2.012467956486122, b 2.9549867210239293, cost 0.0003688029910877851, iteration 230\nm 2.012257188044209, b 2.9557476619756997, cost 0.00035643929552053974, iteration 231\nm 2.012049982602869, b 2.956495739364867, cost 0.0003444900786092948, iteration 232\nm 2.011846279930253, b 2.9572311706475194, cost 0.0003329414454344322, iteration 233\nm 2.011646020812719, b 2.9579541696036915, cost 0.0003217799668875029, iteration 234\nm 2.0114491470376206, b 2.9586649463995065, cost 0.00031099266405542106, iteration 235\nm 2.011255601376386, b 2.9593637076482695, cost 0.0003005669931282556, iteration 236\nm 2.0110653275678807, b 2.9600506564705267, cost 0.00029049083081287305, iteration 237\nm 2.010878270302054, b 2.96072599255311, cost 0.0002807524602355139, iteration 238\nm 2.0106943752038617, b 2.9613899122071827, cost 0.0002713405573171734, iteration 239\nm 2.010513588817459, b 2.962042608425306, cost 0.00026224417760553994, iteration 240\nm 2.010335858590662, b 2.9626842709375376, cost 0.00025345274354845977, iteration 241\nm 2.0101611328596727, b 2.963315086266585, cost 0.00024495603219404225, iteration 242\nm 2.0099893608340573, b 2.963935237782025, cost 0.00023674416330307318, iteration 243\nm 2.0098204925819867, b 2.964544905753605, cost 0.00022880758786000452, iteration 244\nm 2.00965447901572, b 2.9651442674036486, cost 0.0002211370769689657, iteration 245\nm 2.0094912718773337, b 2.965733496958568, cost 0.0002137237111223087, iteration 246\nm 2.0093308237246963, b 2.966312765699511, cost 0.00020655886982851574, iteration 247\nm 2.0091730879176772, b 2.9668822420121512, cost 0.00019963422158815705, iteration 248\nm 2.009018018604587, b 2.967442091435633, cost 0.0001929417142057066, iteration 249\nm 2.0088655707088514, b 2.9679924767106938, cost 0.0001864735654262438, iteration 250\nm 2.0087156999159066, b 2.968533557826969, cost 0.00018022225388599245, iteration 251\nm 2.0085683626603186, b 2.9690654920695, cost 0.00017418051036617734, iteration 252\nm 2.008423516113118, b 2.9695884340644545, cost 0.00016834130934027756, iteration 253\nm 2.0082811181693514, b 2.9701025358240734, cost 0.0001626978608044125, iteration 254\nm 2.008141127435843, b 2.9706079467908606, cost 0.00015724360238177987, iteration 255\nm 2.0080035032191574, b 2.9711048138810217, cost 0.00015197219169171806, iteration 256\nm 2.0078682055137778, b 2.971593281527172, cost 0.00014687749897454057, iteration 257\nm 2.0077351949904707, b 2.9720734917203218, cost 0.00014195359996372998, iteration 258\nm 2.0076044329848566, b 2.9725455840511485, cost 0.00013719476899696085, iteration 259\nm 2.0074758814861697, b 2.973009695750577, cost 0.0001325954723581247, iteration 260\nm 2.00734950312621, b 2.9734659617296684, cost 0.00012815036184263433, iteration 261\nm 2.007225261168479, b 2.9739145146188384, cost 0.0001238542685382261, iteration 262\nm 2.0071031194975006, b 2.9743554848064107, cost 0.00011970219681452683, iteration 263\nm 2.0069830426083266, b 2.974789000476519, cost 0.00011568931851389433, iteration 264\nm 2.0068649955962115, b 2.9752151876463695, cost 0.00011181096733710637, iteration 265\nm 2.006748944146468, b 2.975634170202869, cost 0.0001080626334172468, iteration 266\nm 2.0066348545244925, b 2.9760460699386417, cost 0.0001044399580755298, iteration 267\nm 2.0065226935659584, b 2.9764510065874297, cost 0.00010093872875281702, iteration 268\nm 2.006412428667175, b 2.9768490978588993, cost 9.75548741111627e-05, iteration 269\nm 2.0063040277756126, b 2.9772404594728568, cost 9.428445929957153e-05, iteration 270\nm 2.006197459380582, b 2.9776252051928873, cost 9.112368137837175e-05, iteration 271\nm 2.0060926925040756, b 2.978003446859424, cost 8.806886489706522e-05, iteration 272\nm 2.005989696691765, b 2.978375294422259, cost 8.511645762040662e-05, iteration 273\nm 2.005888442004146, b 2.9787408559725033, cost 8.226302639773913e-05, iteration 274\nm 2.0057888990078343, b 2.9791002377740092, cost 7.950525317084055e-05, iteration 275\nm 2.0056910387670137, b 2.979453544294258, cost 7.683993111554967e-05, iteration 276\nm 2.0055948328350213, b 2.979800878234728, cost 7.426396091281949e-05, iteration 277\nm 2.0055002532460793, b 2.980142340560749, cost 7.177434714467426e-05, iteration 278\nm 2.0054072725071674, b 2.98047803053085, cost 6.936819481110102e-05, iteration 279\nm 2.0053158635900283, b 2.980808045725615, cost 6.704270596361031e-05, iteration 280\nm 2.005225999923313, b 2.981132482076045, cost 6.479517645173692e-05, iteration 281\nm 2.005137655384855, b 2.9814514338914466, cost 6.262299277850866e-05, iteration 282\nm 2.0050508042940804, b 2.9817649938868453, cost 6.052362906147483e-05, iteration 283\nm 2.0049654214045383, b 2.9820732532099368, cost 5.849464409545375e-05, iteration 284\nm 2.0048814818965655, b 2.9823763014675815, cost 5.653367851386111e-05, iteration 285\nm 2.004798961370069, b 2.9826742267518536, cost 5.463845204516603e-05, iteration 286\nm 2.004717835837437, b 2.9829671156656477, cost 5.280676086131905e-05, iteration 287\nm 2.004638081716562, b 2.9832550533478517, cost 5.10364750150596e-05, iteration 288\nm 2.0045596758239883, b 2.9835381234980978, cost 4.93255359631504e-05, iteration 289\nm 2.004482595368172, b 2.9838164084010916, cost 4.767195417266154e-05, iteration 290\nm 2.004406817942855, b 2.984089988950531, cost 4.6073806807456196e-05, iteration 291\nm 2.0043323215205553, b 2.9843589446726213, cost 4.452923549226037e-05, iteration 292\nm 2.004259084446158, b 2.9846233537491926, cost 4.303644415169875e-05, iteration 293\nm 2.0041870854306265, b 2.984883293040426, cost 4.1593696921745634e-05, iteration 294\nm 2.0041163035448095, b 2.9851388381071957, cost 4.019931613122875e-05, iteration 295\nm 2.0040467182133606, b 2.9853900632330332, cost 3.8851680350969584e-05, iteration 296\nm 2.003978309208754, b 2.9856370414457216, cost 3.754922250832148e-05, iteration 297\nm 2.003911056645408, b 2.9858798445385233, cost 3.6290428064943247e-05, iteration 298\nm 2.0038449409739023, b 2.9861185430910484, cost 3.5073833255675404e-05, iteration 299\nm 2.003779942975295, b 2.9863532064897726, cost 3.389802338637342e-05, iteration 300\nm 2.0037160437555386, b 2.986583902948207, cost 3.276163118889191e-05, iteration 301\nm 2.0036532247399843, b 2.9868106995267247, cost 3.166333523117067e-05, iteration 302\nm 2.003591467667984, b 2.987033662152057, cost 3.060185838065213e-05, iteration 303\nm 2.003530754587585, b 2.987252855636456, cost 2.957596631916295e-05, iteration 304\nm 2.0034710678503047, b 2.987468343696535, cost 2.858446610762486e-05, iteration 305\nm 2.0034123901060092, b 2.9876801889717903, cost 2.7626204798887063e-05, iteration 306\nm 2.003354704297862, b 2.9878884530428085, cost 2.6700068096999528e-05, iteration 307\nm 2.0032979936573714, b 2.988093196449169, cost 2.5804979061515924e-05, iteration 308\nm 2.003242241699512, b 2.988294478707041, cost 2.493989685517287e-05, iteration 309\nm 2.0031874322179366, b 2.988492358326483, cost 2.4103815533580215e-05, iteration 310\nm 2.0031335492802613, b 2.988686892828454, cost 2.3295762875472415e-05, iteration 311\nm 2.0030805772234377, b 2.98887813876153, cost 2.2514799252184216e-05, iteration 312\nm 2.0030285006491972, b 2.9890661517183457, cost 2.1760016535018917e-05, iteration 313\nm 2.0029773044195767, b 2.989250986351752, cost 2.1030537039250442e-05, iteration 314\nm 2.002926973652517, b 2.9894326963907036, cost 2.032551250351652e-05, iteration 315\nm 2.0028774937175373, b 2.989611334655878, cost 1.9644123103442462e-05, iteration 316\nm 2.0028288502314826, b 2.989786953075029, cost 1.898557649832854e-05, iteration 317\nm 2.002781029054343, b 2.9899596026980815, cost 1.8349106909778214e-05, iteration 318\nm 2.0027340162851415, b 2.9901293337119705, cost 1.7733974231230625e-05, iteration 319\nm 2.002687798257895, b 2.990296195455231, cost 1.7139463167356884e-05, iteration 320\nm 2.002642361537641, b 2.9904602364323396, cost 1.6564882402259655e-05, iteration 321\nm 2.002597692916534, b 2.9906215043278133, cost 1.60095637956432e-05, iteration 322\nm 2.0025537794100026, b 2.9907800460200717, cost 1.5472861605806873e-05, iteration 323\nm 2.002510608252978, b 2.990935907595064, cost 1.4954151738827869e-05, iteration 324\nm 2.002468166896183, b 2.991089134359664, cost 1.445283102279757e-05, iteration 325\nm 2.0024264430024825, b 2.991239770854843, cost 1.3968316506459802e-05, iteration 326\nm 2.0023854244432986, b 2.9913878608686137, cost 1.3500044781323599e-05, iteration 327\nm 2.002345099295086, b 2.991533447448763, cost 1.3047471326512552e-05, iteration 328\nm 2.0023054558358626, b 2.991676572915361, cost 1.26100698755995e-05, iteration 329\nm 2.0022664825418053, b 2.991817278873066, cost 1.2187331804618788e-05, iteration 330\nm 2.0022281680838994, b 2.9919556062232178, cost 1.1778765540647888e-05, iteration 331\nm 2.0021905013246446, b 2.9920915951757263, cost 1.1383895990180191e-05, iteration 332\nm 2.0021534713148177, b 2.9922252852607603, cost 1.1002263986661902e-05, iteration 333\nm 2.00211706729029, b 2.992356715340239, cost 1.0633425756580058e-05, iteration 334\nm 2.0020812786688995, b 2.992485923619128, cost 1.0276952403412037e-05, iteration 335\nm 2.0020460950473717, b 2.9926129476565455, cost 9.932429408901351e-06, iteration 336\nm 2.002011506198299, b 2.9927378243766793, cost 9.599456151029557e-06, iteration 337\nm 2.0019775020671666, b 2.992860590079522, cost 9.277645438180883e-06, iteration 338\nm 2.001944072769427, b 2.9929812804514198, cost 8.966623058886961e-06, iteration 339\nm 2.0019112085876314, b 2.9930999305754495, cost 8.666027346692225e-06, iteration 340\nm 2.001878899968602, b 2.993216574941615, cost 8.375508759582018e-06, iteration 341\nm 2.0018471375206555, b 2.993331247456873, cost 8.094729473551503e-06, iteration 342\nm 2.0018159120108723, b 2.9934439814549894, cost 7.823362989738142e-06, iteration 343\nm 2.0017852143624157, b 2.993554809706229, cost 7.561093754795127e-06, iteration 344\nm 2.0017550356518896, b 2.993663764426881, cost 7.307616793928649e-06, iteration 345\nm 2.0017253671067468, b 2.9937708772886262, cost 7.062637356274208e-06, iteration 346\nm 2.0016962001027374, b 2.9938761794277395, cost 6.82587057214774e-06, iteration 347\nm 2.0016675261614045, b 2.9939797014541445, cost 6.597041121805301e-06, iteration 348\nm 2.001639336947616, b 2.9940814734603087, cost 6.3758829152678295e-06, iteration 349\nm 2.001611624267146, b 2.994181525029993, cost 6.162138782921593e-06, iteration 350\nm 2.0015843800642874, b 2.9942798852468497, cost 5.955560176465032e-06, iteration 351\nm 2.0015575964195165, b 2.9943765827028788, cost 5.755906879899972e-06, iteration 352\nm 2.0015312655471846, b 2.994471645506736, cost 5.562946730180943e-06, iteration 353\nm 2.0015053797932607, b 2.994565101291907, cost 5.37645534727524e-06, iteration 354\nm 2.0014799316331016, b 2.994656977224738, cost 5.196215873214411e-06, iteration 355\nm 2.0014549136692685, b 2.994747300012334, cost 5.022018719963531e-06, iteration 356\nm 2.001430318629373, b 2.99483609591032, cost 4.85366132567027e-06, iteration 357\nm 2.001406139363967, b 2.994923390730476, cost 4.690947919145713e-06, iteration 358\nm 2.0013823688444607, b 2.9950092098482384, cost 4.533689292192039e-06, iteration 359\nm 2.0013590001610826, b 2.9950935782100765, cost 4.381702579613439e-06, iteration 360\nm 2.001336026520869, b 2.995176520340744, cost 4.234811046545413e-06, iteration 361\nm 2.00131344124569, b 2.995258060350409, cost 4.09284388296629e-06, iteration 362\nm 2.0012912377703085, b 2.995338221941661, cost 3.955636005058435e-06, iteration 363\nm 2.001269409640471, b 2.9954170284164023, cost 3.823027863251998e-06, iteration 364\nm 2.001247950511032, b 2.9954945026826207, cost 3.6948652566878756e-06, iteration 365\nm 2.0012268541441105, b 2.995570667261049, cost 3.5709991539172554e-06, iteration 366\nm 2.001206114407274, b 2.995645544291711, cost 3.451285519601106e-06, iteration 367\nm 2.0011857252717595, b 2.9957191555403577, cost 3.3355851470134313e-06, iteration 368\nm 2.001165680810717, b 2.995791522404794, cost 3.223763496177528e-06, iteration 369\nm 2.00114597519749, b 2.9958626659211, cost 3.1156905374144083e-06, iteration 370\nm 2.0011266027039207, b 2.9959326067697427, cost 3.0112406001376504e-06, iteration 371\nm 2.001107557698685, b 2.996001365281592, cost 2.9102922267253358e-06, iteration 372\nm 2.001088834645654, b 2.9960689614438274, cost 2.8127280312811814e-06, iteration 373\nm 2.0010704281022864, b 2.9961354149057486, cost 2.7184345631362976e-06, iteration 374\nm 2.0010523327180465, b 2.996200744984488, cost 2.6273021749219444e-06, iteration 375\nm 2.0010345432328487, b 2.9962649706706252, cost 2.5392248950754956e-06, iteration 376\nm 2.0010170544755277, b 2.996328110633708, cost 2.4541003046093913e-06, iteration 377\nm 2.0009998613623345, b 2.996390183227679, cost 2.371829418008042e-06, iteration 378\nm 2.000982958895463, b 2.9964512064962108, cost 2.292316568139323e-06, iteration 379\nm 2.0009663421615906, b 2.996511198177951, cost 2.215469294994737e-06, iteration 380\nm 2.0009500063304557, b 2.9965701757116787, cost 2.141198238188458e-06, iteration 381\nm 2.000933946653451, b 2.9966281562413744, cost 2.0694170330300297e-06, iteration 382\nm 2.0009181584622424, b 2.9966851566212016, cost 2.0000422101131872e-06, iteration 383\nm 2.0009026371674152, b 2.9967411934204087, cost 1.9329930982423333e-06, iteration 384\nm 2.000887378257136, b 2.9967962829281434, cost 1.8681917306345287e-06, iteration 385\nm 2.0008723772958437, b 2.9968504411581884, cost 1.8055627542516115e-06, iteration 386\nm 2.000857629922959, b 2.9969036838536165, cost 1.7450333421789989e-06, iteration 387\nm 2.000843131851619, b 2.9969560264913673, cost 1.686533108940892e-06, iteration 388\nm 2.000828878867428, b 2.997007484286745, cost 1.6299940286534194e-06, iteration 389\nm 2.000814866827234, b 2.997058072197842, cost 1.5753503559252941e-06, iteration 390\nm 2.000801091657924, b 2.9971078049298874, cost 1.5225385493966499e-06, iteration 391\nm 2.0007875493552416, b 2.9971566969395216, cost 1.4714971978646103e-06, iteration 392\nm 2.0007742359826195, b 2.997204762438997, cost 1.4221669488648965e-06, iteration 393\nm 2.000761147670039, b 2.9972520154003113, cost 1.3744904396535516e-06, iteration 394\nm 2.0007482806129024, b 2.9972984695592686, cost 1.328412230510546e-06, iteration 395\nm 2.000735631070929, b 2.997344138419471, cost 1.2838787402658308e-06, iteration 396\nm 2.000723195367066, b 2.997389035256245, cost 1.2408381839964914e-06, iteration 397\nm 2.00071096988642, b 2.9974331731205006, cost 1.1992405128127549e-06, iteration 398\nm 2.000698951075208, b 2.9974765648425246, cost 1.1590373556511853e-06, iteration 399\nm 2.0006871354397218, b 2.9975192230357095, cost 1.1201819630364643e-06, iteration 400\nm 2.000675519545315, b 2.997561160100222, cost 1.0826291527128802e-06, iteration 401\nm 2.0006641000154017, b 2.9976023882266056, cost 1.0463352571101619e-06, iteration 402\nm 2.0006528735304783, b 2.9976429193993246, cost 1.0112580725629653e-06, iteration 403\nm 2.0006418368271546, b 2.9976827654002487, cost 9.773568102324307e-07, iteration 404\nm 2.00063098669721, b 2.9977219378120776, cost 9.445920486806256e-07, iteration 405\nm 2.000620319986656, b 2.997760448021707, cost 9.129256880287831e-07, iteration 406\nm 2.0006098335948224, b 2.9977983072235395, cost 8.823209056522382e-07, iteration 407\nm 2.0005995244734556, b 2.9978355264227385, cost 8.527421133598955e-07, iteration 408\nm 2.0005893896258327, b 2.997872116438428, cost 8.241549160170468e-07, iteration 409\nm 2.000579426105888, b 2.9979080879068354, cost 7.965260715444415e-07, iteration 410\nm 2.0005696310173606, b 2.997943451284385, cost 7.698234522647023e-07, iteration 411\nm 2.0005600015129485, b 2.9979782168507385, cost 7.440160075466379e-07, iteration 412\nm 2.0005505347934838, b 2.9980123947117803, cost 7.19073727693696e-07, iteration 413\nm 2.0005412281071178, b 2.998045994802557, cost 6.949676090489643e-07, iteration 414\nm 2.000532078748521, b 2.998079026890166, cost 6.716696202715433e-07, iteration 415\nm 2.000523084058098, b 2.998111500576593, cost 6.491526697393401e-07, iteration 416\nm 2.0005142414212123, b 2.998143425301504, cost 6.273905740424706e-07, iteration 417\nm 2.0005055482674274, b 2.99817481034499, cost 6.063580275424435e-07, iteration 418\nm 2.0004970020697606, b 2.998205664830263, cost 5.860305729438251e-07, iteration 419\nm 2.000488600343945, b 2.9982359977263084, cost 5.663845728510913e-07, iteration 420\nm 2.0004803406477127, b 2.998265817850494, cost 5.473971822872235e-07, iteration 421\nm 2.0004722205800807, b 2.998295133871131, cost 5.290463221273682e-07, iteration 422\nm 2.000464237780653, b 2.9983239543099933, cost 5.113106534211992e-07, iteration 423\nm 2.0004563899289365, b 2.998352287544798, cost 4.941695525845435e-07, iteration 424\nm 2.000448674743667, b 2.998380141811637, cost 4.77603087413727e-07, iteration 425\nm 2.000441089982142, b 2.9984075252073734, cost 4.615919939099072e-07, iteration 426\nm 2.000433633439574, b 2.9984344456919936, cost 4.461176538779225e-07, iteration 427\nm 2.0004263029484446, b 2.998460911090922, cost 4.3116207327472085e-07, iteration 428\nm 2.000419096377879, b 2.998486929097296, cost 4.167078612893068e-07, iteration 429\nm 2.0004120116330233, b 2.998512507274203, cost 4.0273821011569484e-07, iteration 430\nm 2.000405046654437, b 2.9985376530568755, cost 3.892368754096778e-07, iteration 431\nm 2.0003981994174937, b 2.998562373754857, cost 3.7618815740197184e-07, iteration 432\nm 2.0003914679317933, b 2.9985866765541234, cost 3.635768826385896e-07, iteration 433\nm 2.0003848502405837, b 2.998610568519173, cost 3.513883863387471e-07, iteration 434\nm 2.0003783444201897, b 2.9986340565950806, cost 3.396084953412681e-07, iteration 435\nm 2.0003719485794567, b 2.998657147609516, cost 3.2822351162342296e-07, iteration 436\nm 2.0003656608591998, b 2.9986798482747274, cost 3.1722019637466914e-07, iteration 437\nm 2.000359479431662, b 2.9987021651894947, cost 3.065857545982072e-07, iteration 438\nm 2.0003534024999854, b 2.9987241048410467, cost 2.9630782023581875e-07, iteration 439\nm 2.000347428297687, b 2.998745673606946, cost 2.863744417862908e-07, iteration 440\nm 2.0003415550881476, b 2.9987668777569456, cost 2.7677406841127414e-07, iteration 441\nm 2.0003357811641016, b 2.998787723454807, cost 2.674955364972882e-07, iteration 442\nm 2.000330104847148, b 2.9988082167600956, cost 2.5852805668066325e-07, iteration 443\nm 2.0003245244872563, b 2.9988283636299418, cost 2.4986120129795766e-07, iteration 444\nm 2.0003190384622918, b 2.9988481699207705, cost 2.4148489226178747e-07, iteration 445\nm 2.0003136451775396, b 2.998867641390006, cost 2.3338938933969108e-07, iteration 446\nm 2.000308343065244, b 2.9988867836977438, cost 2.2556527883011955e-07, iteration 447\nm 2.0003031305841525, b 2.998905602408396, cost 2.180034626152422e-07, iteration 448\nm 2.000298006219066, b 2.998924102992311, cost 2.1069514758109576e-07, iteration 449\nm 2.0002929684804003, b 2.99894229082736, cost 2.036318353927086e-07, iteration 450\nm 2.000288015903752, b 2.998960171200504, cost 1.9680531261116642e-07, iteration 451\nm 2.000283147049474, b 2.998977749309328, cost 1.9020764114430228e-07, iteration 452\nm 2.0002783605022545, b 2.998995030263553, cost 1.8383114901549317e-07, iteration 453\nm 2.0002736548707087, b 2.9990120190865213, cost 1.77668421442709e-07, iteration 454\nm 2.0002690287869727, b 2.9990287207166566, cost 1.717122922145975e-07, iteration 455\nm 2.0002644809063055, b 2.9990451400088993, cost 1.6595583535992449e-07, iteration 456\nm 2.0002600099067, b 2.9990612817361177, cost 1.6039235709222388e-07, iteration 457\nm 2.0002556144884944, b 2.999077150590496, cost 1.5501538802659654e-07, iteration 458\nm 2.0002512933740015, b 2.999092751184898, cost 1.4981867565660968e-07, iteration 459\nm 2.0002470453071304, b 2.9991080880542076, cost 1.4479617708467426e-07, iteration 460\nm 2.0002428690530247, b 2.999123165656648, cost 1.3994205199341067e-07, iteration 461\nm 2.0002387633977032, b 2.9991379883750757, cost 1.3525065585564157e-07, iteration 462\nm 2.0002347271477072, b 2.999152560518257, cost 1.3071653337114823e-07, iteration 463\nm 2.000230759129752, b 2.9991668863221195, cost 1.2633441212130913e-07, iteration 464\nm 2.000226858190389, b 2.9991809699509817, cost 1.22099196440329e-07, iteration 465\nm 2.000223023195667, b 2.999194815498767, cost 1.1800596148720155e-07, iteration 466\nm 2.0002192530308034, b 2.99920842699019, cost 1.140499475228059e-07, iteration 467\nm 2.0002155465998626, b 2.99922180838193, cost 1.1022655437075701e-07, iteration 468\nm 2.0002119028254346, b 2.9992349635637785, cost 1.065313360705455e-07, iteration 469\nm 2.000208320648323, b 2.9992478963597704, cost 1.0295999570921204e-07, iteration 470\nm 2.0002047990272365, b 2.9992606105292965, cost 9.950838042043909e-08, iteration 471\nm 2.0002013369384875, b 2.999273109768196, cost 9.61724765597035e-08, iteration 472\nm 2.0001979333756923, b 2.99928539770983, cost 9.294840503374454e-08, iteration 473\nm 2.000194587349482, b 2.9992974779261394, cost 8.983241679335354e-08, iteration 474\nm 2.00019129788721, b 2.999309353928681, cost 8.682088847004657e-08, iteration 475\nm 2.000188064032675, b 2.99932102916965, cost 8.391031816596976e-08, iteration 476\nm 2.0001848848458375, b 2.9993325070428827, cost 8.109732137964312e-08, iteration 477\nm 2.0001817594025515, b 2.999343790884843, cost 7.83786270711003e-08, iteration 478\nm 2.0001786867942917, b 2.9993548839755935, cost 7.57510738583689e-08, iteration 479\nm 2.0001756661278924, b 2.9993657895397465, cost 7.321160634119732e-08, iteration 480\nm 2.000172696525287, b 2.999376510747404, cost 7.075727154813042e-08, iteration 481\nm 2.00016977712325, b 2.9993870507150775, cost 6.83852155027598e-08, iteration 482\nm 2.000166907073152, b 2.9993974125065948, cost 6.609267990444405e-08, iteration 483\nm 2.0001640855407063, b 2.9994075991339897, cost 6.38769989224823e-08, iteration 484\nm 2.000161311705732, b 2.999417613558379, cost 6.17355960932464e-08, iteration 485\nm 2.000158584761913, b 2.9994274586908216, cost 5.966598132812731e-08, iteration 486\nm 2.0001559039165624, b 2.9994371373931656, cost 5.7665748014830105e-08, iteration 487\nm 2.000153268390394, b 2.9994466524788805, cost 5.573257021977345e-08, iteration 488\nm 2.0001506774172966, b 2.9994560067138742, cost 5.3864199984076227e-08, iteration 489\nm 2.000148130244108, b 2.999465202817298, cost 5.2058464708970585e-08, iteration 490\nm 2.0001456261304, b 2.999474243462336, cost 5.0313264629494776e-08, iteration 491\nm 2.000143164348259, b 2.9994831312769823, cost 4.862657037294957e-08, iteration 492\nm 2.0001407441820795, b 2.9994918688448062, cost 4.699642059883101e-08, iteration 493\nm 2.00013836492835, b 2.9995004587057017, cost 4.542091971893986e-08, iteration 494\nm 2.0001360258954546, b 2.9995089033566265, cost 4.389823569126882e-08, iteration 495\nm 2.0001337264034666, b 2.9995172052523276, cost 4.242659789264004e-08, iteration 496\nm 2.000131465783955, b 2.9995253668060546, cost 4.100429505622438e-08, iteration 497\nm 2.000129243379788, b 2.999533390390263, cost 3.962967328457475e-08, iteration 498\nm 2.000127058544942, b 2.9995412783373, cost 3.830113412451502e-08, iteration 499\nm 2.0001249106443155, b 2.9995490329400876, cost 3.7017132710156866e-08, iteration 500\nm 2.000122799053542, b 2.9995566564527842, cost 3.5776175964473736e-08, iteration 501\nm 2.0001207231588105, b 2.9995641510914433, cost 3.4576820864567087e-08, iteration 502\nm 2.000118682356686, b 2.9995715190346557, cost 3.3417672763283335e-08, iteration 503\nm 2.000116676053935, b 2.9995787624241843, cost 3.229738376748904e-08, iteration 504\nm 2.0001147036673514, b 2.9995858833655853, cost 3.1214651169901664e-08, iteration 505\nm 2.0001127646235894, b 2.9995928839288215, cost 3.016821593584658e-08, iteration 506\nm 2.000110858358995, b 2.9995997661488625, cost 2.9156861237819137e-08, iteration 507\nm 2.000108984319442, b 2.9996065320262777, cost 2.817941104162898e-08, iteration 508\nm 2.0001071419601724, b 2.9996131835278175, cost 2.7234728737391134e-08, iteration 509\nm 2.0001053307456376, b 2.999619722586984, cost 2.6321715819770587e-08, iteration 510\nm 2.000103550149341, b 2.9996261511045943, cost 2.5439310608683315e-08, iteration 511\nm 2.0001017996536876, b 2.9996324709493325, cost 2.4586487016206082e-08, iteration 512\nm 2.0001000787498313, b 2.999638683958293, cost 2.3762253352316448e-08, iteration 513\nm 2.0000983869375286, b 2.999644791937514, cost 2.2965651173012342e-08, iteration 514\nm 2.000096723724993, b 2.9996507966625043, cost 2.219575416442355e-08, iteration 515\nm 2.000095088628749, b 2.999656699878756, cost 2.145166706632386e-08, iteration 516\nm 2.0000934811734985, b 2.9996625033022557, cost 2.0732524631486106e-08, iteration 517\nm 2.0000919008919733, b 2.9996682086199806, cost 2.003749061864842e-08, iteration 518\nm 2.000090347324808, b 2.9996738174903905, cost 1.93657568203685e-08, iteration 519\nm 2.000088820020402, b 2.999679331543909, cost 1.8716542124422874e-08, iteration 520\nm 2.000087318534787, b 2.9996847523833976, cost 1.808909160337981e-08, iteration 521\nm 2.000085842431502, b 2.9996900815846215, cost 1.748267563861547e-08, iteration 522\nm 2.0000843912814634, b 2.9996953206967087, cost 1.6896589070632873e-08, iteration 523\nm 2.000082964662841, b 2.999700471242599, cost 1.6330150379929762e-08, iteration 524\nm 2.0000815621609362, b 2.9997055347194865, cost 1.5782700894064276e-08, iteration 525\nm 2.0000801833680604, b 2.999710512599257, cost 1.5253604021850355e-08, iteration 526\nm 2.0000788278834167, b 2.9997154063289133, cost 1.4742244512878204e-08, iteration 527\nm 2.000077495312984, b 2.999720217330997, cost 1.4248027742547621e-08, iteration 528\nm 2.0000761852694025, b 2.9997249470040024, cost 1.3770379020334507e-08, iteration 529\nm 2.0000748973718587, b 2.9997295967227813, cost 1.330874292140984e-08, iteration 530\nm 2.0000736312459795, b 2.9997341678389455, cost 1.286258264105366e-08, iteration 531\nm 2.0000723865237187, b 2.999738661681257, cost 1.2431379370579304e-08, iteration 532\nm 2.0000711628432506, b 2.9997430795560156, cost 1.2014631692934305e-08, iteration 533\nm 2.0000699598488705, b 2.999747422747439, cost 1.161185500130658e-08, iteration 534\nm 2.000068777190881, b 2.999751692518034, cost 1.1222580934471042e-08, iteration 535\nm 2.0000676145255016, b 2.999755890108966, cost 1.0846356832490916e-08, iteration 536\nm 2.00006647151476, b 2.999760016740419, cost 1.0482745210149268e-08, iteration 537\nm 2.000065347826398, b 2.9997640736119493, cost 1.0131323248760569e-08, iteration 538\nm 2.0000642431337754, b 2.999768061902835, cost 9.791682303888903e-09, iteration 539\nm 2.0000631571157723, b 2.999771982772419, cost 9.463427430695598e-09, iteration 540\nm 2.000062089456697, b 2.9997758373604455, cost 9.146176924072041e-09, iteration 541\nm 2.0000610398461967, b 2.999779626787392, cost 8.83956187527404e-09, iteration 542\nm 2.0000600079791626, b 2.9997833521547936, cost 8.543225742870437e-09, iteration 543\nm 2.000058993555646, b 2.9997870145455656, cost 8.256823938140152e-09, iteration 544\nm 2.0000579962807654, b 2.9997906150243154, cost 7.980023424079358e-09, iteration 545\nm 2.0000570158646287, b 2.9997941546376543, cost 7.712502328475834e-09, iteration 546\nm 2.000056052022241, b 2.9997976344145, cost 7.45394956960665e-09, iteration 547\nm 2.0000551044734256, b 2.999801055366378, cost 7.204064494110706e-09, iteration 548\nm 2.000054172942744, b 2.9998044184877126, cost 6.962556528043388e-09, iteration 549\nm 2.000053257159412, b 2.9998077247561183, cost 6.729144838422725e-09, iteration 550\nm 2.000052356857223, b 2.999810975132683, cost 6.503558006898574e-09, iteration 551\nm 2.000051471774473, b 2.999814170562248, cost 6.285533714145493e-09, iteration 552\nm 2.0000506016538786, b 2.9998173119736813, cost 6.074818434735612e-09, iteration 553\nm 2.000049746242508, b 2.9998204002801496, cost 5.8711671424135896e-09, iteration 554\nm 2.0000489052917043, b 2.999823436379382, cost 5.67434302512389e-09, iteration 555\nm 2.000048078557015, b 2.9998264211539327, cost 5.484117209620641e-09, iteration 556\nm 2.000047265798119, b 2.999829355471435, cost 5.300268495716046e-09, iteration 557\nm 2.0000464667787576, b 2.9998322401848556, cost 5.122583098039516e-09, iteration 558\nm 2.0000456812666676, b 2.999835076132743, cost 4.95085439886598e-09, iteration 559\nm 2.00004490903351, b 2.9998378641394683, cost 4.7848827064767044e-09, iteration 560\nm 2.0000441498548085, b 2.9998406050154682, cost 4.624475023950436e-09, iteration 561\nm 2.0000434035098786, b 2.999843299557479, cost 4.4694448242298635e-09, iteration 562\nm 2.0000426697817684, b 2.9998459485487676, cost 4.31961183347053e-09, iteration 563\nm 2.000041948457193, b 2.9998485527593606, cost 4.174801821158383e-09, iteration 564\nm 2.000041239326473, b 2.9998511129462666, cost 4.034846397711926e-09, iteration 565\nm 2.0000405421834726, b 2.999853629853698, cost 3.899582818745409e-09, iteration 566\nm 2.000039856825543, b 2.9998561042132863, cost 3.768853795448162e-09, iteration 567\nm 2.0000391830534596, b 2.999858536744295, cost 3.6425073121103014e-09, iteration 568\nm 2.0000385206713656, b 2.9998609281538275, cost 3.520396449138145e-09, iteration 569\nm 2.0000378694867154, b 2.999863279137035, cost 3.4023792121824403e-09, iteration 570\nm 2.000037229310218, b 2.999865590377317, cost 3.2883183672462488e-09, iteration 571\nm 2.000036599955783, b 2.99986786254652, cost 3.178081280761494e-09, iteration 572\nm 2.0000359812404658, b 2.9998700963051332, cost 3.0715397656458358e-09, iteration 573\nm 2.0000353729844136, b 2.9998722923024803, cost 2.968569932169328e-09, iteration 574\nm 2.0000347750108145, b 2.999874451176908, cost 2.869052043759658e-09, iteration 575\nm 2.0000341871458462, b 2.999876573555973, cost 2.7728703779795896e-09, iteration 576\nm 2.0000336092186233, b 2.9998786600566216, cost 2.6799130917582522e-09, iteration 577\nm 2.0000330410611515, b 2.9998807112853725, cost 2.590072091529344e-09, iteration 578\nm 2.000032482508273, b 2.99988272783849, cost 2.503242907318318e-09, iteration 579\nm 2.0000319333976257, b 2.999884710302159, cost 2.419324571563951e-09, iteration 580\nm 2.0000313935695897, b 2.9998866592526556, cost 2.3382195013768625e-09, iteration 581\nm 2.000030862867244, b 2.999888575256513, cost 2.2598333852794838e-09, iteration 582\nm 2.0000303411363216, b 2.9998904588706883, cost 2.1840750734350684e-09, iteration 583\nm 2.0000298282251614, b 2.999892310642723, cost 2.1108564717349175e-09, iteration 584\nm 2.000029323984667, b 2.9998941311109024, cost 2.040092439356859e-09, iteration 585\nm 2.000028828268263, b 2.9998959208044123, cost 1.971700689688378e-09, iteration 586\nm 2.00002834093185, b 2.999897680243492, cost 1.905601694656687e-09, iteration 587\nm 2.0000278618337672, b 2.9998994099395877, cost 1.8417185922887424e-09, iteration 588\nm 2.000027390834747, b 2.999901110395499, cost 1.7799770973963241e-09, iteration 589\nm 2.000026927797876, b 2.999902782105525, cost 1.720305415006832e-09, iteration 590\nm 2.0000264725885546, b 2.9999044255556093, cost 1.662634157042905e-09, iteration 591\nm 2.0000260250744617, b 2.999906041223482, cost 1.6068962615810037e-09, iteration 592\nm 2.000025585125509, b 2.999907629578795, cost 1.5530269149084542e-09, iteration 593\nm 2.0000251526138104, b 2.999909191083263, cost 1.5009634760165467e-09, iteration 594\nm 2.00002472741364, b 2.9999107261907936, cost 1.4506454040130714e-09, iteration 595\nm 2.000024309401398, b 2.9999122353476224, cost 1.402014187360453e-09, iteration 596\nm 2.0000238984555736, b 2.9999137189924405, cost 1.3550132762944366e-09, iteration 597\nm 2.000023494456711, b 2.9999151775565243, cost 1.30958801662969e-09, iteration 598\nm 2.0000230972873716, b 2.9999166114638585, cost 1.265685586481153e-09, iteration 599\nm 2.0000227068321053, b 2.9999180211312613, cost 1.2232549347260957e-09, iteration 600\nm 2.000022322977411, b 2.9999194069685036, cost 1.1822467217215566e-09, iteration 601\nm 2.000021945611708, b 2.9999207693784298, cost 1.1426132618637828e-09, iteration 602\nm 2.0000215746253, b 2.999922108757074, cost 1.1043084680606733e-09, iteration 603\nm 2.0000212099103476, b 2.9999234254937766, cost 1.067287798365951e-09, iteration 604\nm 2.000020851360832, b 2.9999247199712946, cost 1.0315082040208165e-09, iteration 605\nm 2.0000204988725283, b 2.9999259925659154, cost 9.969280793630289e-10, iteration 606\nm 2.0000201523429726, b 2.9999272436475652, cost 9.635072135623713e-10, iteration 607\nm 2.000019811671433, b 2.9999284735799168, cost 9.312067437805838e-10, iteration 608\nm 2.0000194767588817, b 2.999929682720495, cost 8.999891100628852e-10, iteration 609\nm 2.000019147507963, b 2.9999308714207813, cost 8.698180115458756e-10, iteration 610\nm 2.0000188238229692, b 2.999932040026314, cost 8.406583643912264e-10, iteration 611\nm 2.000018505609809, b 2.999933188876792, cost 8.124762608185818e-10, iteration 612\nm 2.0000181927759817, b 2.99993431830617, cost 7.85238929835247e-10, iteration 613\nm 2.000017885230551, b 2.9999354286427584, cost 7.589146989629108e-10, iteration 614\nm 2.0000175828841176, b 2.9999365202093173, cost 7.334729576404032e-10, iteration 615\nm 2.000017285648793, b 2.9999375933231502, cost 7.088841214061714e-10, iteration 616\nm 2.0000169934381757, b 2.9999386482961974, cost 6.851195975792313e-10, iteration 617\nm 2.0000167061673233, b 2.999939685435125, cost 6.621517520280322e-10, iteration 618\nm 2.0000164237527303, b 2.9999407050414155, cost 6.399538770737996e-10, iteration 619\nm 2.0000161461123023, b 2.999941707411455, cost 6.185001603067773e-10, iteration 620\nm 2.0000158731653332, b 2.9999426928366186, cost 5.977656546816693e-10, iteration 621\nm 2.000015604832481, b 2.9999436616033566, cost 5.777262495015307e-10, iteration 622\nm 2.000015341035745, b 2.999944613993277, cost 5.583586422835158e-10, iteration 623\nm 2.0000150816984426, b 2.9999455502832255, cost 5.396403118019512e-10, iteration 624\nm 2.000014826745188, b 2.9999464707453702, cost 5.215494917903966e-10, iteration 625\nm 2.0000145761018704, b 2.999947375647277, cost 5.040651456802989e-10, iteration 626\nm 2.0000143296956296, b 2.999948265251988, cost 4.871669421386148e-10, iteration 627\nm 2.0000140874548404, b 2.9999491398181006, cost 4.708352314311233e-10, iteration 628\nm 2.0000138493090858, b 2.9999499995998384, cost 4.5505102250203393e-10, iteration 629\nm 2.0000136151891397, b 2.999950844847129, cost 4.397959609975658e-10, iteration 630\nm 2.0000133850269473, b 2.999951675805674, cost 4.250523078737088e-10, iteration 631\nm 2.0000131587556034, b 2.9999524927170222, cost 4.1080291873794877e-10, iteration 632\nm 2.000012936309333, b 2.999953295818639, cost 3.9703122395431053e-10, iteration 633\nm 2.000012717623475, b 2.9999540853439752, cost 3.8372120939969167e-10, iteration 634\nm 2.00001250263446, b 2.9999548615225353, cost 3.7085739773650256e-10, iteration 635\nm 2.000012291279793, b 2.9999556245799437, cost 3.5842483056476314e-10, iteration 636\nm 2.0000120834980377, b 2.9999563747380114, cost 3.4640905088604284e-10, iteration 637\nm 2.0000118792287926, b 2.999957112214799, cost 3.347960863733322e-10, iteration 638\nm 2.000011678412681, b 2.9999578372246813, cost 3.2357243310933865e-10, iteration 639\nm 2.0000114809913274, b 2.999958549978409, cost 3.1272503991257533e-10, iteration 640\nm 2.0000112869073448, b 2.99995925068317, cost 3.02241293075782e-10, iteration 641\nm 2.000011096104315, b 2.9999599395426495, cost 2.921090017767861e-10, iteration 642\nm 2.000010908526774, b 2.99996061675709, cost 2.8231638388374695e-10, iteration 643\nm 2.000010724120196, b 2.999961282523349, cost 2.728520522427942e-10, iteration 644\nm 2.000010542830976, b 2.9999619370349553, cost 2.637050013959183e-10, iteration 645\nm 2.000010364606416, b 2.999962580482167, cost 2.54864594901676e-10, iteration 646\nm 2.0000101893947084, b 2.9999632130520255, cost 2.4632055284927057e-10, iteration 647\nm 2.0000100171449215, b 2.9999638349284106, cost 2.380629399894421e-10, iteration 648\nm 2.000009847806985, b 2.999964446292093, cost 2.3008215408481164e-10, iteration 649\nm 2.0000096813316737, b 2.999965047320788, cost 2.2236891482628946e-10, iteration 650\nm 2.000009517670596, b 2.9999656381892073, cost 2.149142530338792e-10, iteration 651\nm 2.0000093567761783, b 2.999966219069108, cost 2.0770950019189887e-10, iteration 652\nm 2.0000091986016497, b 2.9999667901293434, cost 2.0074627838896963e-10, iteration 653\nm 2.000009043101032, b 2.999967351535914, cost 1.9401649059229706e-10, iteration 654\nm 2.0000088902291226, b 2.999967903452013, cost 1.875123111644275e-10, iteration 655\nm 2.000008739941484, b 2.999968446038075, cost 1.8122617687011795e-10, iteration 656\nm 2.0000085921944293, b 2.9999689794518223, cost 1.7515077798618563e-10, iteration 657\nm 2.0000084469450106, b 2.999969503848311, cost 1.6927904985106842e-10, iteration 658\nm 2.0000083041510055, b 2.999970019379977, cost 1.6360416466050117e-10, iteration 659\nm 2.000008163770906, b 2.9999705261966776, cost 1.581195234264064e-10, iteration 660\nm 2.0000080257639063, b 2.9999710244457383, cost 1.5281874849887374e-10, iteration 661\nm 2.000007890089888, b 2.9999715142719925, cost 1.4769567593980757e-10, iteration 662\nm 2.0000077567094134, b 2.999971995817827, cost 1.4274434848843807e-10, iteration 663\nm 2.0000076255837107, b 2.99997246922322, cost 1.3795900858904403e-10, iteration 664\nm 2.000007496674663, b 2.999972934625785, cost 1.3333409169086087e-10, iteration 665\nm 2.0000073699447984, b 2.9999733921608076, cost 1.288642198186094e-10, iteration 666\nm 2.0000072453572777, b 2.999973841961287, cost 1.2454419524876714e-10, iteration 667\nm 2.000007122875886, b 2.9999742841579753, cost 1.2036899452562e-10, iteration 668\nm 2.000007002465019, b 2.999974718879412, cost 1.1633376259671527e-10, iteration 669\nm 2.0000068840896743, b 2.999975146251965, cost 1.1243380717751598e-10, iteration 670\nm 2.000006767715443, b 2.999975566399866, cost 1.0866459327640935e-10, iteration 671\nm 2.0000066533084957, b 2.9999759794452467, cost 1.0502173792328122e-10, iteration 672\nm 2.0000065408355763, b 2.999976385508173, cost 1.0150100509467826e-10, iteration 673\nm 2.0000064302639906, b 2.999976784706683, cost 9.809830078389597e-11, iteration 674\nm 2.0000063215615964, b 2.9999771771568176, cost 9.480966821375748e-11, iteration 675\nm 2.0000062146967954, b 2.999977562972657, cost 9.163128327638914e-11, iteration 676\nm 2.0000061096385235, b 2.9999779422663524, cost 8.85594500254539e-11, iteration 677\nm 2.0000060063562417, b 2.99997831514816, cost 8.55905964502264e-11, iteration 678\nm 2.0000059048199277, b 2.9999786817264718, cost 8.272127026179532e-11, iteration 679\nm 2.000005805000066, b 2.9999790421078463, cost 7.994813494162406e-11, iteration 680\nm 2.00000570686764, b 2.999979396397042, cost 7.726796578267207e-11, iteration 681\nm 2.0000056103941235, b 2.999979744697046, cost 7.467764621326884e-11, iteration 682\nm 2.0000055155514738, b 2.9999800871091042, cost 7.217416412226742e-11, iteration 683\nm 2.000005422312121, b 2.999980423732752, cost 6.975460838028835e-11, iteration 684\nm 2.0000053306489627, b 2.9999807546658404, cost 6.741616546117413e-11, iteration 685\nm 2.000005240535352, b 2.9999810800045674, cost 6.515611615300902e-11, iteration 686\nm 2.0000051519450945, b 2.999981399843505, cost 6.297183239404791e-11, iteration 687\nm 2.0000050648524392, b 2.999981714275626, cost 6.086077423591382e-11, iteration 688\nm 2.000004979232068, b 2.9999820233923318, cost 5.882048686476721e-11, iteration 689\nm 2.0000048950590936, b 2.9999823272834782, cost 5.684859777527923e-11, iteration 690\nm 2.0000048123090473, b 2.9999826260374025, cost 5.494281400076067e-11, iteration 691\nm 2.0000047309578743, b 2.999982919740948, cost 5.310091943564308e-11, iteration 692\nm 2.0000046509819285, b 2.999983208479491, cost 5.132077226211889e-11, iteration 693\nm 2.00000457235796, b 2.999983492336963, cost 4.960030246637414e-11, iteration 694\nm 2.000004495063115, b 2.999983771395879, cost 4.7937509449327554e-11, iteration 695\nm 2.0000044190749247, b 2.9999840457373566, cost 4.633045964885095e-11, iteration 696\nm 2.0000043443713005, b 2.9999843154411434, cost 4.477728433935939e-11, iteration 697\nm 2.000004270930527, b 2.999984580585639, cost 4.3276177450206996e-11, iteration 698\nm 2.0000041987312556, b 2.999984841247917, cost 4.182539343647251e-11, iteration 699\nm 2.0000041277524994, b 2.9999850975037488, cost 4.042324528458939e-11, iteration 700\nm 2.0000040579736256, b 2.999985349427624, cost 3.9068102540890046e-11, iteration 701\nm 2.0000039893743504, b 2.9999855970927736, cost 3.7758389398623166e-11, iteration 702\nm 2.000003921934733, b 2.999985840571191, cost 3.6492582870728905e-11, iteration 703\nm 2.0000038556351694, b 2.999986079933652, cost 3.5269211055035074e-11, iteration 704\nm 2.0000037904563874, b 2.999986315249736, cost 3.4086851371645735e-11, iteration 705\nm 2.0000037263794406, b 2.999986546587846, cost 3.2944128930639126e-11, iteration 706\nm 2.0000036633857023, b 2.999986774015229, cost 3.183971494694807e-11, iteration 707\nm 2.000003601456861, b 2.9999869975979956, cost 3.07723251699722e-11, iteration 708\nm 2.0000035405749155, b 2.9999872174011375, cost 2.974071841272534e-11, iteration 709\nm 2.000003480722167, b 2.999987433488549, cost 2.874369507376329e-11, iteration 710\nm 2.0000034218812184, b 2.999987645923044, cost 2.7780095797015704e-11, iteration 711\nm 2.0000033640349653, b 2.999987854766374, cost 2.684880007748933e-11, iteration 712\nm 2.0000033071665912, b 2.999988060079247, cost 2.5948724970371792e-11, iteration 713\nm 2.000003251259567, b 2.999988261921345, cost 2.507882385038602e-11, iteration 714\nm 2.00000319629764, b 2.9999884603513407, cost 2.4238085164732487e-11, iteration 715\nm 2.0000031422648337, b 2.9999886554269146, cost 2.3425531268166648e-11, iteration 716\nm 2.000003089145442, b 2.999988847204773, cost 2.2640217311078108e-11, iteration 717\nm 2.000003036924024, b 2.999989035740663, cost 2.1881230095676326e-11, iteration 718\nm 2.0000029855853985, b 2.9999892210893897, cost 2.1147687054804785e-11, iteration 719\nm 2.0000029351146433, b 2.999989403304831, cost 2.0438735199707142e-11, iteration 720\nm 2.0000028854970866, b 2.999989582439955, cost 1.9753550137108215e-11, iteration 721\nm 2.0000028367183047, b 2.9999897585468336, cost 1.9091335119694867e-11, iteration 722\nm 2.000002788764119, b 2.9999899316766587, cost 1.8451320094444343e-11, iteration 723\nm 2.0000027416205906, b 2.999990101879757, cost 1.7832760836589962e-11, iteration 724\nm 2.0000026952740138, b 2.999990269205604, cost 1.7234938064485164e-11, iteration 725\nm 2.0000026497109173, b 2.9999904337028394, cost 1.665715661620695e-11, iteration 726\nm 2.0000026049180564, b 2.99999059541928, cost 1.6098744622883955e-11, iteration 727\nm 2.0000025608824106, b 2.999990754401935, cost 1.5559052748297336e-11, iteration 728\nm 2.000002517591178, b 2.999990910697018, cost 1.5037453423557878e-11, iteration 729\nm 2.000002475031777, b 2.9999910643499628, cost 1.4533340117440791e-11, iteration 730\nm 2.0000024331918334, b 2.9999912154054336, cost 1.4046126623555387e-11, iteration 731\nm 2.0000023920591867, b 2.9999913639073403, cost 1.3575246404275578e-11, iteration 732\nm 2.0000023516218794, b 2.9999915098988503, cost 1.3120151900518975e-11, iteration 733\nm 2.000002311868157, b 2.9999916534224016, cost 1.2680313917214425e-11, iteration 734\nm 2.000002272786464, b 2.9999917945197145, cost 1.2255220995842453e-11, iteration 735\nm 2.000002234365439, b 2.999991933231804, cost 1.1844378825633916e-11, iteration 736\nm 2.0000021965939148, b 2.9999920695989917, cost 1.144730966266491e-11, iteration 737\nm 2.000002159460911, b 2.999992203660918, cost 1.1063551790039631e-11, iteration 738\nm 2.0000021229556335, b 2.999992335456553, cost 1.069265895655465e-11, iteration 739\nm 2.000002087067471, b 2.999992465024208, cost 1.0334199880751169e-11, iteration 740\nm 2.0000020517859904, b 2.999992592401546, cost 9.98775772778755e-12, iteration 741\nm 2.0000020171009374, b 2.9999927176255943, cost 9.652929653631148e-12, iteration 742\nm 2.0000019830022278, b 2.9999928407327534, cost 9.329326301494666e-12, iteration 743\nm 2.0000019494799512, b 2.9999929617588097, cost 9.016571382771572e-12, iteration 744\nm 2.000001916524362, b 2.9999930807389434, cost 8.714301207897683e-12, iteration 745\nm 2.0000018841258806, b 2.9999931977077403, cost 8.422164296096917e-12, iteration 746\nm 2.0000018522750898, b 2.999993312699202, cost 8.139820937126918e-12, iteration 747\nm 2.0000018209627304, b 2.9999934257467546, cost 7.866942814839176e-12, iteration 748\nm 2.0000017901797005, b 2.99999353688326, cost 7.603212617312757e-12, iteration 749\nm 2.000001759917052, b 2.9999936461410237, cost 7.348323667581906e-12, iteration 750\nm 2.000001730165988, b 2.9999937535518058, cost 7.101979580900475e-12, iteration 751\nm 2.0000017009178594, b 2.9999938591468287, cost 6.863893893836879e-12, iteration 752\nm 2.0000016721641654, b 2.999993962956788, cost 6.633789755598632e-12, iteration 753\nm 2.000001643896547, b 2.9999940650118595, cost 6.4113995936900834e-12, iteration 754\nm 2.0000016161067875, b 2.9999941653417097, cost 6.196464804790306e-12, iteration 755\nm 2.0000015887868083, b 2.9999942639755024, cost 5.988735458150423e-12, iteration 756\nm 2.0000015619286686, b 2.9999943609419097, cost 5.787969998822859e-12, iteration 757\nm 2.00000153552456, b 2.999994456269118, cost 5.593934969619621e-12, iteration 758\nm 2.0000015095668084, b 2.999994549984838, cost 5.406404740308207e-12, iteration 759\nm 2.000001484047868, b 2.9999946421163117, cost 5.225161248104546e-12, iteration 760\nm 2.0000014589603197, b 2.99999473269032, cost 5.049993735868737e-12, iteration 761\nm 2.000001434296872, b 2.999994821733192, cost 4.880698511384026e-12, iteration 762\nm 2.000001410050355, b 2.9999949092708116, cost 4.717078715584633e-12, iteration 763\nm 2.0000013862137207, b 2.999994995328624, cost 4.558944082705912e-12, iteration 764\nm 2.000001362780041, b 2.9999950799316455, cost 4.406110730920679e-12, iteration 765\nm 2.0000013397425023, b 2.999995163104469, cost 4.258400943053837e-12, iteration 766\nm 2.000001317094409, b 2.9999952448712714, cost 4.115642955024992e-12, iteration 767\nm 2.0000012948291777, b 2.9999953252558216, cost 3.977670763065372e-12, iteration 768\nm 2.000001272940336, b 2.9999954042814863, cost 3.844323933460952e-12, iteration 769\nm 2.0000012514215206, b 2.999995481971237, cost 3.715447399487389e-12, iteration 770\nm 2.000001230266477, b 2.999995558347657, cost 3.5908913035833672e-12, iteration 771\nm 2.000001209469055, b 2.9999956334329485, cost 3.470510808255752e-12, iteration 772\nm 2.0000011890232097, b 2.999995707248937, cost 3.354165929665918e-12, iteration 773\nm 2.000001168922998, b 2.9999957798170804, cost 3.2417213798080293e-12, iteration 774\nm 2.000001149162576, b 2.9999958511584732, cost 3.133046404269629e-12, iteration 775\nm 2.0000011297362, b 2.999995921293853, cost 3.028014629540134e-12, iteration 776\nm 2.000001110638224, b 2.9999959902436077, cost 2.9265039270774435e-12, iteration 777\nm 2.0000010918630955, b 2.99999605802778, cost 2.8283962528031247e-12, iteration 778\nm 2.0000010734053566, b 2.9999961246660733, cost 2.7335775253709973e-12, iteration 779\nm 2.0000010552596423, b 2.999996190177859, cost 2.641937485659802e-12, iteration 780\nm 2.0000010374206783, b 2.9999962545821806, cost 2.5533695747275993e-12, iteration 781\nm 2.0000010198832783, b 2.999996317897759, cost 2.467770800422585e-12, iteration 782\nm 2.0000010026423443, b 2.9999963801429996, cost 2.385041626784717e-12, iteration 783\nm 2.000000985692866, b 2.9999964413359965, cost 2.3050858522492e-12, iteration 784\nm 2.0000009690299145, b 2.999996501494537, cost 2.2278105028723486e-12, iteration 785\nm 2.0000009526486475, b 2.999996560636109, cost 2.1531257228473724e-12, iteration 786\nm 2.0000009365443026, b 2.999996618777904, cost 2.080944661488985e-12, iteration 787\nm 2.0000009207121985, b 2.999996675936823, cost 2.011183387318859e-12, iteration 788\nm 2.000000905147733, b 2.999996732129481, cost 1.9437607800220985e-12, iteration 789\nm 2.000000889846383, b 2.999996787372213, cost 1.8785984391450676e-12, iteration 790\nm 2.000000874803698, b 2.9999968416810767, cost 1.8156205906873925e-12, iteration 791\nm 2.0000008600153074, b 2.9999968950718596, cost 1.7547540009769119e-12, iteration 792\nm 2.0000008454769116, b 2.9999969475600814, cost 1.6959278938663423e-12, iteration 793\nm 2.0000008311842845, b 2.9999969991609996, cost 1.639073864103742e-12, iteration 794\nm 2.0000008171332713, b 2.9999970498896142, cost 1.584125800162014e-12, iteration 795\nm 2.0000008033197885, b 2.9999970997606713, cost 1.531019807716224e-12, iteration 796\nm 2.0000007897398198, b 2.9999971487886676, cost 1.479694131441695e-12, iteration 797\nm 2.0000007763894176, b 2.999997196987855, cost 1.4300890898724016e-12, iteration 798\nm 2.0000007632647017, b 2.9999972443722442, cost 1.3821469991497743e-12, iteration 799\nm 2.0000007503618566, b 2.9999972909556094, cost 1.3358121129312504e-12, iteration 800\nm 2.0000007376771314, b 2.9999973367514916, cost 1.2910305501873252e-12, iteration 801\nm 2.0000007252068395, b 2.999997381773203, cost 1.2477502371883342e-12, iteration 802\nm 2.0000007129473554, b 2.999997426033831, cost 1.2059208478602702e-12, iteration 803\nm 2.0000007008951153, b 2.999997469546241, cost 1.165493739755016e-12, iteration 804\nm 2.000000689046616, b 2.999997512323082, cost 1.12642190497183e-12, iteration 805\nm 2.000000677398414, b 2.999997554376789, cost 1.0886599080593472e-12, iteration 806\nm 2.000000665947122, b 2.9999975957195857, cost 1.0521638386626042e-12, iteration 807\nm 2.000000654689412, b 2.9999976363634904, cost 1.016891257663486e-12, iteration 808\nm 2.0000006436220117, b 2.9999976763203176, cost 9.828011496074702e-13, iteration 809\nm 2.0000006327417035, b 2.9999977156016824, cost 9.49853872676285e-13, iteration 810\nm 2.000000622045325, b 2.9999977542190033, cost 9.180111156572275e-13, iteration 811\nm 2.0000006115297664, b 2.9999977921835055, cost 8.872358496389461e-13, iteration 812\nm 2.0000006011919718, b 2.999997829506225, cost 8.574922888732727e-13, iteration 813\nm 2.0000005910289356, b 2.999997866198011, cost 8.287458485114212e-13, iteration 814\nm 2.000000581037703, b 2.9999979022695293, cost 8.009630976958001e-13, iteration 815\nm 2.000000571215371, b 2.9999979377312656, cost 7.741117319137631e-13, iteration 816\nm 2.0000005615590832, b 2.9999979725935275, cost 7.481605285496285e-13, iteration 817\nm 2.0000005520660333, b 2.99999800686645, cost 7.230793083937528e-13, iteration 818\nm 2.0000005427334617, b 2.9999980405599946, cost 6.988389072279313e-13, iteration 819\nm 2.0000005335586555, b 2.999998073683957, cost 6.754111367634285e-13, iteration 820\nm 2.0000005245389474, b 2.9999981062479644, cost 6.527687564657364e-13, iteration 821\nm 2.000000515671716, b 2.9999981382614838, cost 6.308854357374561e-13, iteration 822\nm 2.0000005069543834, b 2.9999981697338205, cost 6.097357283025824e-13, iteration 823\nm 2.0000004983844155, b 2.9999982006741233, cost 5.892950407767063e-13, iteration 824\nm 2.000000489959321, b 2.999998231091386, cost 5.695396028770961e-13, iteration 825\nm 2.0000004816766523, b 2.999998260994451, cost 5.50446443698769e-13, iteration 826\nm 2.0000004735339996, b 2.99999829039201, cost 5.319933601503779e-13, iteration 827\nm 2.0000004655289967, b 2.9999983192926094, cost 5.141588952594075e-13, iteration 828\nm 2.0000004576593176, b 2.9999983477046492, cost 4.969223107200429e-13, iteration 829\nm 2.0000004499226733, b 2.999998375636389, cost 4.80263562408479e-13, iteration 830\nm 2.000000442316816, b 2.999998403095948, cost 4.641632800504082e-13, iteration 831\nm 2.000000434839534, b 2.9999984300913085, cost 4.48602740234265e-13, iteration 832\nm 2.000000427488654, b 2.9999984566303173, cost 4.335638500099177e-13, iteration 833\nm 2.0000004202620394, b 2.9999984827206894, cost 4.190291217469036e-13, iteration 834\nm 2.000000413157589, b 2.9999985083700085, cost 4.049816530329145e-13, iteration 835\nm 2.0000004061732386, b 2.999998533585731, cost 3.914051092413266e-13, iteration 836\nm 2.0000003993069573, b 2.9999985583751863, cost 3.7828370378161e-13, iteration 837\nm 2.0000003925567484, b 2.9999985827455804, cost 3.656021776813065e-13, iteration 838\nm 2.000000385920651, b 2.999998606703998, cost 3.53345785599507e-13, iteration 839\nm 2.000000379396736, b 2.999998630257403, cost 3.415002757809976e-13, iteration 840\nm 2.0000003729831053, b 2.999998653412642, cost 3.300518720681352e-13, iteration 841\nm 2.000000366677897, b 2.999998676176446, cost 3.1898726302058486e-13, iteration 842\nm 2.0000003604792767, b 2.9999986985554323, cost 3.0829358262574004e-13, iteration 843\nm 2.0000003543854428, b 2.999998720556106, cost 2.9795839519066697e-13, iteration 844\nm 2.000000348394624, b 2.9999987421848626, cost 2.8796968309699177e-13, iteration 845\nm 2.000000342505079, b 2.9999987634479894, cost 2.7831583068879833e-13, iteration 846\nm 2.0000003367150954, b 2.999998784351667, cost 2.6898561299552364e-13, iteration 847\nm 2.0000003310229904, b 2.9999988049019715, cost 2.5996818039649836e-13, iteration 848\nm 2.0000003254271093, b 2.999998825104877, cost 2.5125304669820297e-13, iteration 849\nm 2.0000003199258263, b 2.9999988449662567, cost 2.4283007745865203e-13, iteration 850\nm 2.0000003145175405, b 2.999998864491883, cost 2.346894789754408e-13, iteration 851\nm 2.000000309200681, b 2.9999988836874327, cost 2.2682178440692053e-13, iteration 852\nm 2.000000303973702, b 2.999998902558485, cost 2.1921784515801816e-13, iteration 853\nm 2.0000002988350842, b 2.999998921110526, cost 2.118688196449222e-13, iteration 854\nm 2.0000002937833337, b 2.999998939348948, cost 2.047661609660394e-13, iteration 855\nm 2.000000288816982, b 2.999998957279053, cost 1.9790161143633973e-13, iteration 856\nm 2.0000002839345856, b 2.9999989749060534, cost 1.9126718803865134e-13, iteration 857\nm 2.000000279134725, b 2.9999989922350725, cost 1.8485517532985508e-13, iteration 858\nm 2.000000274416006, b 2.9999990092711477, cost 1.7865811877800756e-13, iteration 859\nm 2.000000269777055, b 2.999999026019231, cost 1.7266881091168247e-13, iteration 860\nm 2.000000265216525, b 2.9999990424841916, cost 1.668802875136484e-13, iteration 861\nm 2.0000002607330902, b 2.9999990586708147, cost 1.612858183948942e-13, iteration 862\nm 2.0000002563254466, b 2.999999074583806, cost 1.5587889713893073e-13, iteration 863\nm 2.0000002519923132, b 2.9999990902277913, cost 1.5065323658331129e-13, iteration 864\nm 2.0000002477324315, b 2.9999991056073183, cost 1.4560276057617523e-13, iteration 865\nm 2.0000002435445614, b 2.999999120726857, cost 1.4072159561006822e-13, iteration 866\nm 2.0000002394274867, b 2.999999135590803, cost 1.3600406607216806e-13, iteration 867\nm 2.0000002353800106, b 2.9999991502034766, cost 1.3144468622286128e-13, iteration 868\nm 2.0000002314009557, b 2.9999991645691257, cost 1.2703815440934454e-13, iteration 869\nm 2.0000002274891666, b 2.9999991786919264, cost 1.2277934672988224e-13, iteration 870\nm 2.0000002236435055, b 2.999999192575984, cost 1.1866331041361673e-13, iteration 871\nm 2.000000219862854, b 2.999999206225334, cost 1.146852598956438e-13, iteration 872\nm 2.0000002161461143, b 2.9999992196439442, cost 1.1084056830964835e-13, iteration 873\nm 2.000000212492205, b 2.9999992328357155, cost 1.0712476589969412e-13, iteration 874\nm 2.000000208900065, b 2.9999992458044824, cost 1.035335318411679e-13, iteration 875\nm 2.0000002053686488, b 2.9999992585540145, cost 1.000626892397716e-13, iteration 876\nm 2.0000002018969307, b 2.9999992710880186, cost 9.670820273469178e-14, iteration 877\nm 2.0000001984839013, b 2.9999992834101374, cost 9.346617174250673e-14, iteration 878\nm 2.000000195128569, b 2.999999295523953, cost 9.033282588470707e-14, iteration 879\nm 2.0000001918299573, b 2.9999993074329874, cost 8.730452188680395e-14, iteration 880\nm 2.000000188587108, b 2.9999993191407013, cost 8.437773847769604e-14, iteration 881\nm 2.000000185399079, b 2.999999330650499, cost 8.15490718592595e-14, iteration 882\nm 2.0000001822649427, b 2.9999993419657254, cost 7.881523303898552e-14, iteration 883\nm 2.000000179183788, b 2.99999935308967, cost 7.617304314209119e-14, iteration 884\nm 2.00000017615472, b 2.9999993640255664, cost 7.361942966707699e-14, iteration 885\nm 2.000000173176858, b 2.9999993747765936, cost 7.115142305123352e-14, iteration 886\nm 2.000000170249336, b 2.999999385345877, cost 6.876615356274003e-14, iteration 887\nm 2.0000001673713035, b 2.9999993957364883, cost 6.64608474711055e-14, iteration 888\nm 2.000000164541923, b 2.9999994059514483, cost 6.423282421023085e-14, iteration 889\nm 2.000000161760373, b 2.9999994159937264, cost 6.20794927173344e-14, iteration 890\nm 2.0000001590258445, b 2.9999994258662417, cost 5.999834927665978e-14, iteration 891\nm 2.0000001563375434, b 2.999999435571864, cost 5.798697363479369e-14, iteration 892\nm 2.0000001536946863, b 2.999999445113415, cost 5.60430271370714e-14, iteration 893\nm 2.000000151096507, b 2.9999994544936675, cost 5.416424935069066e-14, iteration 894\nm 2.0000001485422487, b 2.9999994637153486, cost 5.234845502213933e-14, iteration 895\nm 2.0000001460311707, b 2.9999994727811394, cost 5.0593533356423743e-14, iteration 896\nm 2.000000143562541, b 2.999999481693674, cost 4.889744343015315e-14, iteration 897\nm 2.0000001411356436, b 2.9999994904555445, cost 4.7258212895503015e-14, iteration 898\nm 2.0000001387497726, b 2.999999499069297, cost 4.5673935814349034e-14, iteration 899\nm 2.0000001364042337, b 2.9999995075374355, cost 4.414276977286478e-14, iteration 900\nm 2.000000134098346, b 2.999999515862422, cost 4.2662934152756603e-14, iteration 901\nm 2.0000001318314387, b 2.9999995240466757, cost 4.1232708347313e-14, iteration 902\nm 2.0000001296028533, b 2.9999995320925765, cost 3.985042940526768e-14, iteration 903\nm 2.0000001274119414, b 2.999999540002463, cost 3.851448950374e-14, iteration 904\nm 2.000000125258067, b 2.9999995477786343, cost 3.722333575509332e-14, iteration 905\nm 2.000000123140603, b 2.9999995554233507, cost 3.597546621938791e-14, iteration 906\nm 2.0000001210589344, b 2.9999995629388345, cost 3.476943017405634e-14, iteration 907\nm 2.0000001190124563, b 2.9999995703272706, cost 3.360382509201315e-14, iteration 908\nm 2.0000001170005732, b 2.9999995775908066, cost 3.247729547478972e-14, iteration 909\nm 2.0000001150227007, b 2.999999584731554, cost 3.138853155316017e-14, iteration 910\nm 2.0000001130782636, b 2.9999995917515885, cost 3.033626725983189e-14, iteration 911\nm 2.000000111166697, b 2.9999995986529506, cost 2.931927887123225e-14, iteration 912\nm 2.0000001092874453, b 2.9999996054376465, cost 2.8336383608092867e-14, iteration 913\nm 2.000000107439962, b 2.9999996121076484, cost 2.7386439221387535e-14, iteration 914\nm 2.0000001056237093, b 2.999999618664895, cost 2.6468340212361254e-14, iteration 915\nm 2.0000001038381603, b 2.999999625111293, cost 2.5581019740181996e-14, iteration 916\nm 2.000000102082796, b 2.9999996314487154, cost 2.4723445333669195e-14, iteration 917\nm 2.0000001003571057, b 2.999999637679005, cost 2.38946203167441e-14, iteration 918\nm 2.0000000986605877, b 2.9999996438039727, cost 2.3093580759285032e-14, iteration 919\nm 2.0000000969927494, b 2.999999649825399, cost 2.2319395123905658e-14, iteration 920\nm 2.0000000953531054, b 2.9999996557450346, cost 2.157116292676566e-14, iteration 921\nm 2.0000000937411793, b 2.9999996615645994, cost 2.08480145988068e-14, iteration 922\nm 2.0000000921565024, b 2.9999996672857856, cost 2.0149109021022472e-14, iteration 923\nm 2.000000090598614, b 2.9999996729102563, cost 1.9473633349150317e-14, iteration 924\nm 2.0000000890670617, b 2.9999996784396465, cost 1.8820802154499093e-14, iteration 925\nm 2.0000000875614, b 2.9999996838755636, cost 1.8189856451786847e-14, iteration 926\nm 2.000000086081191, b 2.9999996892195875, cost 1.7580062301795868e-14, iteration 927\nm 2.000000084626005, b 2.9999996944732716, cost 1.6990711020980018e-14, iteration 928\nm 2.000000083195418, b 2.999999699638143, cost 1.6421117088770343e-14, iteration 929\nm 2.0000000817890156, b 2.999999704715703, cost 1.587061802231469e-14, iteration 930\nm 2.0000000804063878, b 2.9999997097074282, cost 1.533857373693351e-14, iteration 931\nm 2.000000079047133, b 2.999999714614769, cost 1.482436582493905e-14, iteration 932\nm 2.0000000777108564, b 2.999999719439152, cost 1.4327395982391942e-14, iteration 933\nm 2.0000000763971686, b 2.99999972418198, cost 1.3847086478450843e-14, iteration 934\nm 2.0000000751056892, b 2.9999997288446316, cost 1.3382878927636959e-14, iteration 935\nm 2.0000000738360417, b 2.999999733428462, cost 1.2934233314599523e-14, iteration 936\nm 2.0000000725878575, b 2.9999997379348033, cost 1.25006279763218e-14, iteration 937\nm 2.0000000713607733, b 2.999999742364966, cost 1.2081558883854148e-14, iteration 938\nm 2.000000070154433, b 2.999999746720237, cost 1.1676538512101057e-14, iteration 939\nm 2.0000000689684856, b 2.9999997510018837, cost 1.1285095991995437e-14, iteration 940\nm 2.000000067802586, b 2.9999997552111495, cost 1.0906776152488483e-14, iteration 941\nm 2.0000000666563964, b 2.9999997593492584, cost 1.054113900766254e-14, iteration 942\nm 2.000000065529583, b 2.9999997634174136, cost 1.0187759522072448e-14, iteration 943\nm 2.0000000644218177, b 2.999999767416797, cost 9.846226650405525e-15, iteration 944\nm 2.000000063332779, b 2.999999771348572, cost 9.51614327141886e-15, iteration 945\nm 2.0000000622621505, b 2.999999775213881, cost 9.197125557657086e-15, iteration 946\nm 2.0000000612096205, b 2.9999997790138475, cost 8.888802439960888e-15, iteration 947\nm 2.0000000601748837, b 2.9999997827495766, cost 8.59081564324814e-15, iteration 948\nm 2.000000059157639, b 2.999999786422154, cost 8.302818427973982e-15, iteration 949\nm 2.00000005815759, b 2.9999997900326467, cost 8.02447592498508e-15, iteration 950\nm 2.000000057174447, b 2.9999997935821052, cost 7.755464669841367e-15, iteration 951\nm 2.0000000562079236, b 2.9999997970715606, cost 7.495471615199351e-15, iteration 952\nm 2.0000000552577397, b 2.9999998005020276, cost 7.244194546597296e-15, iteration 953\nm 2.0000000543236176, b 2.999999803874503, cost 7.001341279521903e-15, iteration 954\nm 2.0000000534052877, b 2.999999807189967, cost 6.76662941558432e-15, iteration 955\nm 2.000000052502481, b 2.9999998104493844, cost 6.539785943448039e-15, iteration 956\nm 2.0000000516149363, b 2.9999998136537016, cost 6.320547139155734e-15, iteration 957\nm 2.000000050742396, b 2.9999998168038506, cost 6.1086580670245e-15, iteration 958\nm 2.0000000498846053, b 2.9999998199007467, cost 5.903872357156558e-15, iteration 959\nm 2.0000000490413155, b 2.9999998229452904, cost 5.705951768003548e-15, iteration 960\nm 2.0000000482122817, b 2.999999825938367, cost 5.5146664039926065e-15, iteration 961\nm 2.0000000473972617, b 2.9999998288808456, cost 5.329793532714747e-15, iteration 962\nm 2.0000000465960204, b 2.9999998317735828, cost 5.151118323105729e-15, iteration 963\nm 2.000000045808323, b 2.999999834617418, cost 4.9784329882561506e-15, iteration 964\nm 2.0000000450339424, b 2.9999998374131795, cost 4.811536784601224e-15, iteration 965\nm 2.0000000442726518, b 2.9999998401616788, cost 4.650235522341167e-15, iteration 966\nm 2.0000000435242313, b 2.9999998428637156, cost 4.494341762737905e-15, iteration 967\nm 2.000000042788462, b 2.9999998455200747, cost 4.343674118922934e-15, iteration 968\nm 2.0000000420651314, b 2.9999998481315284, cost 4.198057456493035e-15, iteration 969\nm 2.000000041354028, b 2.999999850698836, cost 4.057322425206821e-15, iteration 970\nm 2.000000040654946, b 2.999999853222744, cost 3.9213053459982765e-15, iteration 971\nm 2.0000000399676825, b 2.9999998557039858, cost 3.7898481100537784e-15, iteration 972\nm 2.0000000392920363, b 2.9999998581432825, cost 3.662797837453182e-15, iteration 973\nm 2.0000000386278116, b 2.9999998605413434, cost 3.5400067750076325e-15, iteration 974\nm 2.0000000379748157, b 2.9999998628988656, cost 3.4213321239081604e-15, iteration 975\nm 2.0000000373328586, b 2.9999998652165343, cost 3.3066358912535145e-15, iteration 976\nm 2.000000036701754, b 2.9999998674950232, cost 3.19578475120463e-15, iteration 977\nm 2.0000000360813175, b 2.9999998697349946, cost 3.0886497374873576e-15, iteration 978\nm 2.00000003547137, b 2.9999998719370997, cost 2.9851063174497633e-15, iteration 979\nm 2.000000034871733, b 2.9999998741019787, cost 2.8850340561132495e-15, iteration 980\nm 2.000000034282233, b 2.999999876230261, cost 2.7883165945746132e-15, iteration 981\nm 2.0000000337026984, b 2.999999878322565, cost 2.694841494003567e-15, iteration 982\nm 2.000000033132961, b 2.9999998803794994, cost 2.604500054368438e-15, iteration 983\nm 2.000000032572854, b 2.9999998824016614, cost 2.5171871837386626e-15, iteration 984\nm 2.000000032022216, b 2.999999884389639, cost 2.4328013292137906e-15, iteration 985\nm 2.0000000314808863, b 2.9999998863440105, cost 2.3512445003655312e-15, iteration 986\nm 2.000000030948708, b 2.9999998882653434, cost 2.2724217151034114e-15, iteration 987\nm 2.000000030425526, b 2.9999998901541964, cost 2.1962413822031206e-15, iteration 988\nm 2.0000000299111886, b 2.999999892011119, cost 2.122614926403684e-15, iteration 989\nm 2.000000029405545, b 2.9999998938366503, cost 2.051456696955813e-15, iteration 990\nm 2.0000000289084503, b 2.999999895631322, cost 1.9826840214252576e-15, iteration 991\nm 2.0000000284197585, b 2.9999998973956545, cost 1.9162168339910374e-15, iteration 992\nm 2.0000000279393277, b 2.9999998991301613, cost 1.8519778476029844e-15, iteration 993\nm 2.000000027467019, b 2.999999900835347, cost 1.7898924279063252e-15, iteration 994\nm 2.0000000270026943, b 2.9999999025117066, cost 1.729888367908757e-15, iteration 995\nm 2.0000000265462186, b 2.9999999041597274, cost 1.671895823340033e-15, iteration 996\nm 2.00000002609746, b 2.999999905779889, cost 1.6158474478226761e-15, iteration 997\nm 2.0000000256562873, b 2.999999907372662, cost 1.5616780553619768e-15, iteration 998\nm 2.0000000252225725, b 2.9999999089385097, cost 1.509324582997523e-15, iteration 999\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nplt.scatter(x, y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x19ec5009c60&gt;\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/","title":"Simple Logistic Regression (Binary Classification)","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#description","title":"Description:","text":"<p>Logistic regression is a statistical technique used to analyze the relationship between a dependent variable and one or more independent variables. In binary classification problems, the dependent variable is dichotomous, meaning it has only two possible outcomes. For example, whether a customer will purchase a product or not, whether a person has a certain disease or not, or whether an email is spam or not.</p> <p>The goal of logistic regression is to find the best model that predicts the probability of an event occurring (in binary classification, the probability of the positive outcome). The logistic regression model uses a function called the sigmoid function, which maps any input value to a value between 0 and 1. This allows the model to output a probability score that can be interpreted as the likelihood of the positive outcome.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#02-reading-the-csv-file-with-pandas","title":"02. Reading the CSV File with Pandas","text":"<pre><code># Defining the path of the csv\ncsv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\HR_comma_sep.csv\"\n</code></pre> <pre><code># Reading the csv file with pandas library\ndf = pd.read_csv(csv_path)\ndf.head()\n</code></pre> satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years Department salary 0 0.38 0.53 2 157 3 0 1 0 sales low 1 0.80 0.86 5 262 6 0 1 0 sales medium 2 0.11 0.88 7 272 4 0 1 0 sales medium 3 0.72 0.87 5 223 5 0 1 0 sales low 4 0.37 0.52 2 159 3 0 1 0 sales low <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(14999, 10)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#03-data-cleaning","title":"03. Data Cleaning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#301-checking-the-dataframe-for-null-values","title":"3.01 Checking the DataFrame for Null Values","text":"<pre><code># Checking the dataframe if there is any null values\ndf.isnull().sum()\n</code></pre> <pre><code>satisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nDepartment               0\nsalary                   0\ndtype: int64\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#302-checking-the-datatypes-for-all-the-columns","title":"3.02 Checking the datatypes for all the Columns","text":"<pre><code>df.dtypes\n</code></pre> <pre><code>satisfaction_level       float64\nlast_evaluation          float64\nnumber_project             int64\naverage_montly_hours       int64\ntime_spend_company         int64\nWork_accident              int64\nleft                       int64\npromotion_last_5years      int64\nDepartment                object\nsalary                    object\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#04-data-exploration-and-visualization","title":"04. Data Exploration and Visualization","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#401-plotting-the-correlation-matrix-of-the-dataframe","title":"4.01 Plotting the Correlation Matrix of the DataFrame","text":"<p>Plotting the Correlation Matrix to check which variables have direct and clear impact on employee retention (i.e. whether they leave the company or continue to work) </p> <pre><code># Dropping the columns with qualitative data\ndata = df.loc[:, \"satisfaction_level\":\"promotion_last_5years\"]\n# Checking the shape of the data\ndata.shape\n</code></pre> <pre><code>(14999, 8)\n</code></pre> <pre><code>data.head()\n</code></pre> satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years 0 0.38 0.53 2 157 3 0 1 0 1 0.80 0.86 5 262 6 0 1 0 2 0.11 0.88 7 272 4 0 1 0 3 0.72 0.87 5 223 5 0 1 0 4 0.37 0.52 2 159 3 0 1 0 <pre><code># Plotting the correlation matrix\nsns.heatmap(data.corr(), annot=True)\nplt.title(\"Correlation Matrix\")\nplt.show()\n</code></pre> <p></p> <p>Interpretaion of Correlation Matrix:  A correlation matrix is a table that displays the pairwise correlations between all the variables in a dataset. Correlation coefficients range from -1 to 1, where a value of -1 indicates a perfect negative correlation, a value of 1 indicates a perfect positive correlation, and a value of 0 indicates no correlation.</p> <p>In the given dataset, the correlation matrix shows that the 'satisfaction_level' column has the maximum negative correlation coefficient of -0.39 with the \"left' column. This means that the satisfaction level of employees has a larger impact on their retention.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#402-plotting-bar-charts-between-employee-salaries-and-retention","title":"4.02 Plotting Bar Charts between Employee Salaries and Retention","text":"<p>The pandas crosstab() function is used to compute a cross-tabulation table of two or more factors. It is a useful tool in data analysis for examining the relationship between two or more variables, especially when one or both of the variables are categorical.</p> <p>Here, we are calculating the total number of employees in each salary bracket who stayed or left by summing across the rows using pandas crosstab() function.</p> <pre><code># Calculating the total number of employees who stayed or left in each salary bracket\nsalary_left_data = pd.crosstab(df.salary, df.left)\nsalary_left_data\n</code></pre> left 0 1 salary high 1155 82 low 5144 2172 medium 5129 1317 <pre><code># Renaming the column names\n# 0 = Retained\n# 1 = Left\ncolumnNames = {\n    0: \"Retained\",\n    1: \"Left\"\n}\nsalary_left_data.rename(columns=columnNames, inplace=True)\nsalary_left_data\n</code></pre> left Retained Left salary high 1155 82 low 5144 2172 medium 5129 1317 <pre><code># Plotting the bar chart\nsalary_left_data.plot(kind=\"bar\", color=[\"green\", \"red\"])\nplt.title(\"Bar Chart Showing Impact of Salaries on Retention\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Salary\")\n</code></pre> <pre><code>Text(0.5, 0, 'Salary')\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#403-plotting-bar-charts-between-departments-and-retention","title":"4.03 Plotting Bar Charts between Departments and Retention","text":"<p>Here, we are calculating the total number of employees in each department who stayed or left by summing across the rows using pandas crosstab() function.</p> <pre><code># Calculating the total number of employees who stayed or left in each department\ndept_left_data = pd.crosstab(df.Department, df.left)\ndept_left_data.head()\n</code></pre> left 0 1 Department IT 954 273 RandD 666 121 accounting 563 204 hr 524 215 management 539 91 <pre><code># Renaming the column names\n# 0 = Retained\n# 1 = Left\ndept_left_data.rename(columns=columnNames, inplace=True)\ndept_left_data.head()\n</code></pre> left Retained Left Department IT 954 273 RandD 666 121 accounting 563 204 hr 524 215 management 539 91 <pre><code># Plotting the bar chart\ndept_left_data.plot(kind=\"bar\", color=[\"green\", \"red\"])\nplt.title(\"Bar Chart Showing Correlaton between Departments and Employee Retention\")\nplt.xlabel(\"Department\")\nplt.ylabel(\"Count\")\nplt.show()\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#404-plotting-the-scatterplot-between-satisfaction_level-and-left","title":"4.04 Plotting the Scatterplot between 'satisfaction_level' and 'left'","text":"<pre><code># Extracting a small sample from the dataframe to represent the scatterplot\ntest_df = df.sample(50, random_state=75)\n</code></pre> <pre><code># Plotting the Scatterplot between 'satisfaction_level' and 'left'\nsns.scatterplot(x=test_df[\"satisfaction_level\"], y=test_df[\"left\"], color=\"red\")\nplt.title(\"Scatterplot between 'satisfaction_level' and 'left'\")\nplt.ylabel(\"Employee Retention (0=Retained, 1=Left)\")\nplt.xlabel(\"Satisfaction Level\")\nplt.grid()\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#05-dividing-the-data-into-training-and-testing-set","title":"05. Dividing the Data into Training and Testing Set","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#501-defifining-the-dependent-and-independent-variable","title":"5.01 Defifining the Dependent and Independent Variable","text":"<pre><code># Dependent Variable (y) = \"left\"\n# Independent Variable (x) = \"satisfaction_level\"\nx = df[[\"satisfaction_level\"]]\ny = df[[\"left\"]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#502-splitting-the-data-into-training-and-testing-set","title":"5.02 Splitting the Data into Training and Testing Set","text":"<pre><code># Importing the train_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Training data = 70% and Testing data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#06-instantiating-the-simple-logistic-regression-model","title":"06. Instantiating the Simple Logistic Regression Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#601-importing-logistic-regression-model-from-sklearn-library","title":"6.01 Importing Logistic Regression Model from sklearn Library","text":"<pre><code># Importing the Logistic Regression Model from sklearn library\nfrom sklearn.linear_model import LogisticRegression\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#602-generating-a-logistic-regression-object","title":"6.02 Generating a Logistic Regression Object","text":"<pre><code># Creating a linear regression object\nlog_reg = LogisticRegression()\n# Feeding the training data to the model\nlog_reg.fit(x_train, y_train)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#603-getting-the-coefficients-of-the-linear-regression-model","title":"6.03 Getting the Coefficients of the Linear Regression Model","text":"<pre><code># Getting the slope of the model\nlog_reg.coef_\n</code></pre> <pre><code>array([[-3.84563398]])\n</code></pre> <pre><code># Getting the y-intercept of the model\nlog_reg.intercept_\n</code></pre> <pre><code>array([0.97025215])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#07-validation-of-the-model","title":"07. Validation of the Model","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Simple_Logistic_Regression/#701-validating-the-logistic-regression-model","title":"7.01 Validating the Logistic Regression Model","text":"<pre><code># Predicting the left status of the x_test (satisfaction_level) data\ny_predict = log_reg.predict(x_test)\ny_predict\n</code></pre> <pre><code>array([0, 0, 1, ..., 0, 0, 0], dtype=int64)\n</code></pre> <pre><code># Getting the accuracy score of the model\nlog_reg.score(x_test, y_test)\n</code></pre> <pre><code>0.7624444444444445\n</code></pre> <pre><code># Getting the prediction probability of the x_test data\nlog_reg.predict_proba(x_test)\n</code></pre> <pre><code>array([[0.9118948 , 0.0881052 ],\n       [0.62036288, 0.37963712],\n       [0.35762467, 0.64237533],\n       ...,\n       [0.61126493, 0.38873507],\n       [0.67300916, 0.32699084],\n       [0.93368009, 0.06631991]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Untitled/","title":"Untitled","text":"<pre><code>import geopandas as gpd\n</code></pre> <pre><code>\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/","title":"Simple Linear Regression Exercise","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#01-importing-required-libraries","title":"01. Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#02-reading-the-csv-with-pandas","title":"02. Reading the CSV with Pandas","text":"<pre><code>df = pd.read_csv(\"GDP_per_capita_usa.csv\")\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 $23,315.08B $70,249 5.95% 1 2020 $21,060.47B $63,531 -2.77% 2 2019 $21,380.98B $65,120 2.29% 3 2018 $20,533.06B $62,823 2.95% 4 2017 $19,477.34B $59,908 2.24%"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#03-data-preprocessing","title":"03. Data Preprocessing","text":"<pre><code># Removing the dollar, comma and percentage symbol from the dataframe\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\"$\", \"\")\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\",\", \"\")\ndf[\"GDP\"] = df[\"GDP\"].str.replace(\"B\", \"\")\ndf[\"Per Capita\"] = df[\"Per Capita\"].str.replace(\"$\", \"\")\ndf[\"Per Capita\"] = df[\"Per Capita\"].str.replace(\",\", \"\")\ndf[\"Growth\"] = df[\"Growth\"].str.replace(\"%\", \"\")\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 23315.08 70249 5.95 1 2020 21060.47 63531 -2.77 2 2019 21380.98 65120 2.29 3 2018 20533.06 62823 2.95 4 2017 19477.34 59908 2.24 <pre><code># Checking the datatypes of the columns\ndf.dtypes\n</code></pre> <pre><code>Year           int64\nGDP           object\nPer Capita    object\nGrowth        object\ndtype: object\n</code></pre> <pre><code># Using a dictionary to change the datatype of columns from object to int and float\ndt_convert = {\n    \"GDP\": float,\n    \"Per Capita\": float,\n}\n</code></pre> <pre><code># Changing the datatypes of the columns\ndf = df.astype(dt_convert)\ndf.dtypes\n</code></pre> <pre><code>Year            int64\nGDP           float64\nPer Capita    float64\nGrowth         object\ndtype: object\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> Year GDP Per Capita Growth 0 2021 23315.08 70249.0 5.95 1 2020 21060.47 63531.0 -2.77 2 2019 21380.98 65120.0 2.29 3 2018 20533.06 62823.0 2.95 4 2017 19477.34 59908.0 2.24 <pre><code># Evaluating the general statistics of the dataframe\ndf.describe()\n</code></pre> Year GDP Per Capita count 62.000000 62.000000 62.000000 mean 1990.500000 7935.587903 27417.145161 std 18.041619 6738.805659 20172.070442 min 1960.000000 543.300000 3007.000000 25% 1975.250000 1732.027500 7998.750000 50% 1990.500000 6060.635000 24115.500000 75% 2005.750000 13621.492500 45757.250000 max 2021.000000 23315.080000 70249.000000"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#04-create-a-scatterplot-between-year-and-gdp-per-capita","title":"04. Create a Scatterplot between Year and GDP Per Capita","text":"<pre><code># Selecting independent(x) and dependent variable(y)\n# independent variable = Year\n# dependent variable = GDP Per Capita (US $)\nx = df[[\"Year\"]].values\ny = df[[\"Per Capita\"]].values\n</code></pre> <pre><code>plt.plot(x, y)\nplt.title(\"USA GDP Per Capita (US$)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"GDP Per Capita (US$)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#05-creating-a-linear-regression-object","title":"05. Creating a Linear Regression Object","text":"<pre><code>lin_reg = linear_model.LinearRegression()\n</code></pre> <pre><code># Training the Linear Regression model\nlin_reg.fit(x, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#06-accessing-the-slope-and-intercept","title":"06. Accessing the Slope and Intercept","text":"<p>Linear Equation:  y = mx + c where,     y = independent variable     m = slope     x = dependent variable     c = intercept</p> <pre><code>slope = lin_reg.coef_\nprint(\"Slope(m):\", slope)\n</code></pre> <pre><code>Slope(m): [[1099.76930825]]\n</code></pre> <pre><code>intercept = lin_reg.intercept_\nprint(\"intercept(c):\", intercept)\n</code></pre> <pre><code>intercept(c): [-2161673.66291456]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#07-predicting-the-gdp-of-year-2030-2040-and-2050","title":"07. Predicting the GDP of Year 2030, 2040 and 2050","text":"<pre><code>lin_reg.predict([[2030], [2040], [2050]])\n</code></pre> <pre><code>array([[70858.03283725],\n       [81855.72591977],\n       [92853.41900229]])\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/06_Linear_Regression/Exercises/01_Simple_Linear_Regression_Exercise/#assessing-the-accuracy-of-the-linear-regression-model","title":"Assessing the Accuracy of the Linear Regression Model","text":"<pre><code># Predicting the GDP of all the years in the dataframe\npredicted_gdp = lin_reg.predict(x)\n</code></pre> <pre><code># Plotting the Regression Line\nplt.plot(x, y)\nplt.plot(x, predicted_gdp, linestyle=\"--\", linewidth=1)\nplt.title(\"USA GDP Per Capita (US$)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"GDP Per Capita (US$)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/","title":"Decision Tree Classifier","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#setting-up-the-current-working-directory","title":"Setting Up the Current Working Directory","text":"<pre><code># Checking the current working directory\nprint(os.getcwd())\n</code></pre> <pre><code>D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\07_Decision_Tree\n</code></pre> <pre><code># Changing the location of the current working directory\nfile_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\"\nos.chdir(file_path)\n</code></pre> <pre><code># Checking the working directory location\nprint(os.getcwd())\n</code></pre> <pre><code>D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#reading-the-dataset-using-pandas","title":"Reading the Dataset using Pandas","text":"<pre><code># Reading the DailyDelhiClimateTest.csv data\ndataset = \"DailyDelhiClimateTest.csv\"\ndata_path = file_path + \"\\\\\" + dataset\ndf = pd.read_csv(data_path)\n</code></pre> <pre><code># Checking the dataframe\ndf.head()\n</code></pre> date meantemp humidity wind_speed meanpressure 0 2017-01-01 15.913043 85.869565 2.743478 59.000000 1 2017-01-02 18.500000 77.222222 2.894444 1018.277778 2 2017-01-03 17.111111 81.888889 4.016667 1018.333333 3 2017-01-04 18.700000 70.050000 4.545000 1015.700000 4 2017-01-05 18.388889 74.944444 3.300000 1014.333333 <pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(114, 5)\n</code></pre> <pre><code># Printing the column names\ndf.columns\n</code></pre> <pre><code>Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object')\n</code></pre> <pre><code># Checking the datatype of all the columns\ndf.dtypes\n</code></pre> <pre><code>date             object\nmeantemp        float64\nhumidity        float64\nwind_speed      float64\nmeanpressure    float64\ndtype: object\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing","text":"<pre><code># Checking the dataframe whether there is any null value or not\ndf.isnull().sum()\n</code></pre> <pre><code>date            0\nmeantemp        0\nhumidity        0\nwind_speed      0\nmeanpressure    0\ndtype: int64\n</code></pre> <pre><code># Removing the data column from the dataset\ndf.drop([\"date\"], axis=1, inplace=True)\n</code></pre> <pre><code># Checking the shape of the data\ndf.shape\n</code></pre> <pre><code>(114, 4)\n</code></pre> <pre><code># Creating a blank array\narray = []\n\n# Generating a for loop to categorize the humidity into 3 levels\nfor i in df[\"humidity\"]:\n    if i &lt; 40:\n        array.append(1)\n    elif i &gt;= 40 and i &lt; 60:\n        array.append(2)\n    else:\n        array.append(3)\n</code></pre> <pre><code># Converting the array into numpy array\narray = np.array(array)\narray\n</code></pre> <pre><code>array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 1])\n</code></pre> <pre><code># Adding the humidity level data into the main dataframe\ndf[\"humidity_level\"] = array\n</code></pre> <pre><code># Checking the dataframe after adding 'humidity_level' column\ndf.head()\n</code></pre> meantemp humidity wind_speed meanpressure humidity_level 0 15.913043 85.869565 2.743478 59.000000 3 1 18.500000 77.222222 2.894444 1018.277778 3 2 17.111111 81.888889 4.016667 1018.333333 3 3 18.700000 70.050000 4.545000 1015.700000 3 4 18.388889 74.944444 3.300000 1014.333333 3"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#data-exploration-and-visualization","title":"Data Exploration and Visualization","text":"<pre><code># Visualizing the correlations between variables\nplt.figure(figsize=(8, 6))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")\nplt.title(\"Correlation between Variables\")\nplt.show()\n</code></pre> <pre><code># Visualizing the Scatterplot between mean temperature and humidity\nplt.figure(figsize=(8, 6))\nplt.scatter(x=df[\"meantemp\"], y=df[\"humidity\"], cmap=\"rainbow\", c=df[\"meantemp\"], edgecolor=\"gray\")\nplt.grid()\nplt.colorbar(orientation=\"horizontal\")\nplt.title(\"Scatterplot between Mean Temperature and Humidity\")\nplt.xlabel(\"Mean Temperature (\u00b0C)\")\nplt.xlim(10, 35)\nplt.ylabel(\"Humidity (g/m^3)\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#preparing-the-training-and-testing-dataset","title":"Preparing the Training and Testing Dataset","text":"<pre><code># Selecting the independent and dependent variable\nx = df[[\"meantemp\", \"wind_speed\"]]\ny = df[[\"humidity_level\"]]\n</code></pre> <pre><code># Importing train_test_split from sklearn library\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Preparing training and testing dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=20)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#instantiating-the-decision-tree-classifier-model","title":"Instantiating the Decision Tree Classifier Model","text":"<pre><code># Importing the decision tree classifier model\nfrom sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code># Instantiating the model\ntreeModel = DecisionTreeClassifier()\n# Fitting the training data to the model\ntreeModel.fit(x_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier()</pre> <pre><code># Visualizing the tree split of the model\nfrom sklearn import tree\nplt.figure(figsize=(14, 10))\ntree.plot_tree(treeModel, filled=True, rounded=True)\nplt.show()\n</code></pre> <pre><code># Applying post prunning technique\ntreeModel2 = DecisionTreeClassifier(max_depth=2)\n# Fitting the training data to the model\ntreeModel2.fit(x_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier(max_depth=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(max_depth=2)</pre> <pre><code># Visualizing the tree split after post prunning\nplt.figure(figsize=(12, 6))\ntree.plot_tree(treeModel2, filled=True, rounded=True)\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/07_Decision_Tree/Decision_Tree_Classifier/#validation-of-the-model","title":"Validation of the Model","text":"<pre><code># Predict the test data using the decision tree model\ny_pred = treeModel2.predict(x_test)\n</code></pre> <pre><code># Import the model validation metrics from the sklearn library\nfrom sklearn.metrics import accuracy_score, classification_report\n</code></pre> <pre><code>score = accuracy_score(y_test, y_pred)\nprint(\"Model Score: \", score)\n</code></pre> <pre><code>Model Score:  0.7142857142857143\n</code></pre> <pre><code>print(classification_report(y_test, y_pred))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n           1       0.70      1.00      0.82         7\n           2       0.40      0.50      0.44         8\n           3       0.93      0.70      0.80        20\n\n    accuracy                           0.71        35\n   macro avg       0.68      0.73      0.69        35\nweighted avg       0.76      0.71      0.72        35\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/","title":"Predicting Life Expectancy using Machine Learning","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#author-krishnagopal-halder","title":"Author: Krishnagopal Halder","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#project-description","title":"Project Description:","text":"<p>Numerous studies have been conducted in the past on factors that impact life expectancy, taking into account demographic variables, income composition, and mortality rates. However, these studies have neglected to consider the influence of immunization and the human development index. Additionally, some past research relied on a single-year dataset for all countries and utilized multiple linear regression. To address these issues, this study aims to develop a regression model based on mixed effects model and multiple linear regression, utilizing data spanning the period from 2000 to 2015 for all countries. The study will consider key immunization factors such as Hepatitis B, Polio, and Diphtheria, as well as other factors such as mortality, economics, and social factors, in order to gain a comprehensive understanding of the factors that contribute to life expectancy. By examining data from different countries, this study will help countries identify which factors are most strongly correlated with lower life expectancy values, and offer guidance for prioritizing areas for improvement to efficiently improve the life expectancy of their populations.</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#01-importing-necessary-libraries","title":"01. Importing Necessary Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n%matplotlib inline\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#02-importing-dataset","title":"02. Importing Dataset","text":"<p>Metadata Information: * Country: Country * Year: Year * Status: Developed or Developing status * Life expectancy: Life Expectancy in age * Adult Mortality: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population) * infant deaths: Number of Infant Deaths per 1000 population * Alcohol: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol) * percentage expenditure: Expenditure on health as a percentage of Gross Domestic Product per capita(%) * Hepatitis B: Hepatitis B (HepB) immunization coverage among 1-year-olds (%) * Measles: Measles - number of reported cases per 1000 population * BMI: Average Body Mass Index of entire population * under-five deaths: Number of under-five deaths per 1000 population * Polio: Polio (Pol3) immunization coverage among 1-year-olds (%) * Total expenditure: General government expenditure on health as a percentage of total government expenditure (%) * Diphtheria: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%) * HIV/AIDS: Deaths per 1 000 live births HIV/AIDS (0-4 years) * GDP: Gross Domestic Product per capita (in USD) * Population: Population of the country * thinness 1-19 years: Prevalence of thinness among children and adolescents for Age 10 to 19 (% ) * thinness 5-9 years: Prevalence of thinness among children for Age 5 to 9(%) * Income composition of resources: Human Development Index in terms of income composition of resources (index ranging from 0 to 1) * Schooling: Number of years of Schooling(years)</p> <pre><code># Reading the Life Expectancy Data.csv using Pandas\npath = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\Life Expectancy Data.csv\"\ndf = pd.read_csv(path)\ndf.head()\n</code></pre> Country Year Status Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles ... Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 Afghanistan 2015 Developing 65.0 263.0 62 0.01 71.279624 65.0 1154 ... 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 Afghanistan 2014 Developing 59.9 271.0 64 0.01 73.523582 62.0 492 ... 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 Afghanistan 2013 Developing 59.9 268.0 66 0.01 73.219243 64.0 430 ... 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 Afghanistan 2012 Developing 59.5 272.0 69 0.01 78.184215 67.0 2787 ... 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 Afghanistan 2011 Developing 59.2 275.0 71 0.01 7.097109 68.0 3013 ... 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <p>5 rows \u00d7 22 columns</p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#03-data-visualization","title":"03. Data Visualization","text":"<pre><code># Dropping the columns which contain qualitative values\ndf.drop([\"Country\", \"Status\"], axis=1, inplace=True)\ndf.head()\n</code></pre> Year Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles BMI under-five deaths Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 2015 65.0 263.0 62 0.01 71.279624 65.0 1154 19.1 83 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 2014 59.9 271.0 64 0.01 73.523582 62.0 492 18.6 86 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 2013 59.9 268.0 66 0.01 73.219243 64.0 430 18.1 89 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 2012 59.5 272.0 69 0.01 78.184215 67.0 2787 17.6 93 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 2011 59.2 275.0 71 0.01 7.097109 68.0 3013 17.2 97 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <pre><code># Plotting correlation matrix\nplt.figure(figsize=(14, 12))\nplt.title(\"Correlation Matrix\")\nsns.heatmap(df.corr(), annot=True)\n</code></pre> <pre><code>&lt;Axes: title={'center': 'Correlation Matrix'}&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#04-data-preprocessing","title":"04. Data Preprocessing","text":"<pre><code># Checking the shape of the dataframe\ndf.shape\n</code></pre> <pre><code>(2938, 20)\n</code></pre> <pre><code># Dropping the rows with Null values\ndf.dropna(inplace=True)\ndf.head()\n</code></pre> Year Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles BMI under-five deaths Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling 0 2015 65.0 263.0 62 0.01 71.279624 65.0 1154 19.1 83 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 1 2014 59.9 271.0 64 0.01 73.523582 62.0 492 18.6 86 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 2 2013 59.9 268.0 66 0.01 73.219243 64.0 430 18.1 89 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 3 2012 59.5 272.0 69 0.01 78.184215 67.0 2787 17.6 93 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 4 2011 59.2 275.0 71 0.01 7.097109 68.0 3013 17.2 97 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <pre><code># Checking the shape of the data after dropping rows with null values\ndf.shape\n</code></pre> <pre><code>(1649, 20)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#05-preparing-data-for-training-and-testing","title":"05. Preparing Data for Training and Testing","text":"<pre><code># Selecting independent variables\nx = df.drop(\"Life expectancy \", axis=1).values\n# Selecting dependent variable\ny = df[\"Life expectancy \"].values\n</code></pre> <pre><code># Splitting the data into training and testing set\n# Training data = 70% and Testing data = 30%\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=75)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#06-implementing-linear-regression-model","title":"06. Implementing Linear Regression Model","text":"<pre><code># Creating Linear Regression object\nlin_reg = linear_model.LinearRegression()\n# Training the model with training data\nlin_reg.fit(x_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#07-finding-the-multiple-linear-equation","title":"07. Finding the Multiple Linear Equation","text":"<p>In multiple linear regression, the equation takes the form of:</p> <p>y = b0 + b1x1 + b2x2 + ... + bnxn</p> <p>where y is the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the intercept, and b1, b2, ..., bn are the coefficients that represent the impact of each independent variable on the dependent variable.</p> <pre><code># Getting the intercept (b0)\nintercept = lin_reg.intercept_\nprint(\"Intercept:\", intercept.round(4))\n</code></pre> <pre><code>Intercept: 294.6671\n</code></pre> <pre><code># Getting the coefficients (b1, b2, ....., bn)\ncoefficients = lin_reg.coef_\n# Printing all the coefficients\nfor i in range(len(coefficients)):\n    print(f\"b{i+1} = {coefficients[i].round(4)}\", end=\", \")\n</code></pre> <pre><code>b1 = -0.1209, b2 = -0.0155, b3 = 0.0738, b4 = -0.1065, b5 = 0.0004, b6 = -0.0033, b7 = -0.0, b8 = 0.031, b9 = -0.0557, b10 = 0.0066, b11 = 0.0994, b12 = 0.0159, b13 = -0.4637, b14 = 0.0, b15 = -0.0, b16 = -0.092, b17 = 0.0386, b18 = 11.7172, b19 = 0.8605,\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#08-predicting-the-life-expectancyy-of-x_test-data","title":"08. Predicting the Life Expectancy(y) of x_test Data","text":"<pre><code># Predicting the y value based on x_test data\ny_predicted = lin_reg.predict(x_test)\n</code></pre> <pre><code># Plotting scatter diagram between y_test(actual) and y_predicted data\nsns.scatterplot(x=y_test, y=y_predicted)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_predicted\")\nplt.title(\"Scatter Plot between y_true(actual) and y_predicted\")\nplt.show()\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/01_Predicting_Life_Expectancy_Using_Linear_Regression/#09-validation-of-the-model","title":"09. Validation of the Model","text":"<pre><code># Calculating the Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_predicted)\nprint(\"Mean Absolute Error(MAE) =\", mae.round(4))\n\n# Calculating Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_predicted)\nprint(\"Mean Squared Error(MSE) =\", mse.round(4))\n\n# Calculating Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error(RMSE) =\", rmse.round(4))\n</code></pre> <pre><code>Mean Absolute Error(MAE) = 2.7465\nMean Squared Error(MSE) = 12.9831\nRoot Mean Squared Error(RMSE) = 3.6032\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/","title":"Predicting Aqi Using Regression","text":""},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Projects/02_Predicting_AQI_using_Regression/#reading-aqi-data-csv-file-using-pandas","title":"Reading AQI Data CSV File using Pandas","text":"<pre><code>csv_path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\AQI_data.csv\"\ndf = pd.read_csv(csv_path)\n</code></pre> <pre><code>df.head()\n</code></pre> City Date PM2.5 PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI AQI_Bucket 0 Ahmedabad 01-01-2015 NaN NaN 0.92 18.22 17.15 NaN 0.92 27.64 133.36 0.00 0.02 0.00 NaN NaN 1 Ahmedabad 02-01-2015 NaN NaN 0.97 15.69 16.46 NaN 0.97 24.55 34.06 3.68 5.50 3.77 NaN NaN 2 Ahmedabad 03-01-2015 NaN NaN 17.40 19.30 29.70 NaN 17.40 29.07 30.70 6.80 16.40 2.25 NaN NaN 3 Ahmedabad 04-01-2015 NaN NaN 1.70 18.48 17.97 NaN 1.70 18.59 36.08 4.43 10.14 1.00 NaN NaN 4 Ahmedabad 05-01-2015 NaN NaN 22.10 21.42 37.76 NaN 22.10 39.33 39.31 7.01 18.89 2.78 NaN NaN <pre><code>df.shape\n</code></pre> <pre><code>(29531, 16)\n</code></pre> <pre><code>df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 29531 entries, 0 to 29530\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   City        29531 non-null  object \n 1   Date        29531 non-null  object \n 2   PM2.5       24933 non-null  float64\n 3   PM10        18391 non-null  float64\n 4   NO          25949 non-null  float64\n 5   NO2         25946 non-null  float64\n 6   NOx         25346 non-null  float64\n 7   NH3         19203 non-null  float64\n 8   CO          27472 non-null  float64\n 9   SO2         25677 non-null  float64\n 10  O3          25509 non-null  float64\n 11  Benzene     23908 non-null  float64\n 12  Toluene     21490 non-null  float64\n 13  Xylene      11422 non-null  float64\n 14  AQI         24850 non-null  float64\n 15  AQI_Bucket  24850 non-null  object \ndtypes: float64(13), object(3)\nmemory usage: 3.6+ MB\n</code></pre> <pre><code>df.isnull().sum()\n</code></pre> <pre><code>City              0\nDate              0\nPM2.5          4598\nPM10          11140\nNO             3582\nNO2            3585\nNOx            4185\nNH3           10328\nCO             2059\nSO2            3854\nO3             4022\nBenzene        5623\nToluene        8041\nXylene        18109\nAQI            4681\nAQI_Bucket     4681\ndtype: int64\n</code></pre> <pre><code>df.describe()\n</code></pre> PM2.5 PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI count 24933.000000 18391.000000 25949.000000 25946.000000 25346.000000 19203.000000 27472.000000 25677.000000 25509.000000 23908.000000 21490.000000 11422.000000 24850.000000 mean 67.450578 118.127103 17.574730 28.560659 32.309123 23.483476 2.248598 14.531977 34.491430 3.280840 8.700972 3.070128 166.463581 std 64.661449 90.605110 22.785846 24.474746 31.646011 25.684275 6.962884 18.133775 21.694928 15.811136 19.969164 6.323247 140.696585 min 0.040000 0.010000 0.020000 0.010000 0.000000 0.010000 0.000000 0.010000 0.010000 0.000000 0.000000 0.000000 13.000000 25% 28.820000 56.255000 5.630000 11.750000 12.820000 8.580000 0.510000 5.670000 18.860000 0.120000 0.600000 0.140000 81.000000 50% 48.570000 95.680000 9.890000 21.690000 23.520000 15.850000 0.890000 9.160000 30.840000 1.070000 2.970000 0.980000 118.000000 75% 80.590000 149.745000 19.950000 37.620000 40.127500 30.020000 1.450000 15.220000 45.570000 3.080000 9.150000 3.350000 208.000000 max 949.990000 1000.000000 390.680000 362.210000 467.630000 352.890000 175.810000 193.860000 257.730000 455.030000 454.850000 170.370000 2049.000000 <pre><code>df.nunique()\n</code></pre> <pre><code>City             26\nDate           2009\nPM2.5         11716\nPM10          12571\nNO             5776\nNO2            7404\nNOx            8156\nNH3            5922\nCO             1779\nSO2            4761\nO3             7699\nBenzene        1873\nToluene        3608\nXylene         1561\nAQI             829\nAQI_Bucket        6\ndtype: int64\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>Index(['City', 'Date', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2',\n       'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n      dtype='object')\n</code></pre> <pre><code>sns.barplot(x=\"City\", y=\"AQI\", data=df)\n</code></pre> <pre><code>&lt;Axes: xlabel='City', ylabel='AQI'&gt;\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Practice/","title":"Practice","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>myList = [1, 2, 3, 4]\nmyList\n</code></pre> <pre><code>[1, 2, 3, 4]\n</code></pre> <pre><code>myList2 = [[1,2], [3,4]]\nmyList2\n</code></pre> <pre><code>[[1, 2], [3, 4]]\n</code></pre> <pre><code>myArr1 = np.array([1, 2, 3, 4])\nmyArr1\n</code></pre> <pre><code>array([1, 2, 3, 4])\n</code></pre> <pre><code>myArr1[3]\n</code></pre> <pre><code>4\n</code></pre> <pre><code>type(myArr1)\n</code></pre> <pre><code>numpy.ndarray\n</code></pre> <pre><code>myArr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=\"int64\")\nmyArr2\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]], dtype=int64)\n</code></pre> <pre><code>myArr2[2, 1]\n</code></pre> <pre><code>8\n</code></pre> <pre><code>myArr2.dtype\n</code></pre> <pre><code>dtype('int64')\n</code></pre> <pre><code>myArr2.nbytes\n</code></pre> <pre><code>72\n</code></pre> <pre><code>myArr2.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <pre><code>tup = ((1,2), (3, 4))\nmyArr3 = np.array(tup)\nmyArr3\n</code></pre> <pre><code>array([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>arr4 = np.ones((3, 3))\narr4\n</code></pre> <pre><code>array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n</code></pre> <pre><code>arr5 = np.zeros((3, 3))\narr5\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code>arr6 = np.arange(1, 101, 2)\narr6\n</code></pre> <pre><code>array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33,\n       35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67,\n       69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])\n</code></pre> <pre><code>arr7 = np.linspace(1, 10, 5)\narr7\n</code></pre> <pre><code>array([ 1.  ,  3.25,  5.5 ,  7.75, 10.  ])\n</code></pre> <pre><code>arr8 = np.empty((3, 3))\narr8\n</code></pre> <pre><code>array([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre> <pre><code>identityMatrix = np.identity(6, dtype=\"int8\")\nidentityMatrix\n</code></pre> <pre><code>array([[1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1]], dtype=int8)\n</code></pre> <pre><code>identityMatrix.reshape((4, 9))\n</code></pre> <pre><code>array([[1, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 1]], dtype=int8)\n</code></pre> <pre><code>identityMatrix.ravel()\n</code></pre> <pre><code>array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], dtype=int8)\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled/","title":"Untitled","text":"<pre><code>import numpy as np\n</code></pre> <pre><code>myArr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nmyArr\n</code></pre> <pre><code>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <pre><code>myArr.sum(axis=0)\n</code></pre> <pre><code>array([12, 15, 18])\n</code></pre> <pre><code>myArr.max(axis=0)\n</code></pre> <pre><code>array([7, 8, 9])\n</code></pre> <pre><code>myArr.T\n</code></pre> <pre><code>array([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n</code></pre> <pre><code>myArr.flat\n</code></pre> <pre><code>&lt;numpy.flatiter at 0x1fda9706e40&gt;\n</code></pre> <pre><code>print(myArr)\n</code></pre> <pre><code>[[1 2 3]\n [4 5 6]\n [7 8 9]]\n</code></pre>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled1/","title":"Untitled1","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\n</code></pre> <pre><code>df = pd.read_csv(path)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code>df.shape\n</code></pre> <pre><code>(1206, 9)\n</code></pre> <pre><code>df[\"Mag\"].max()\n</code></pre> <pre><code>9.1\n</code></pre> <pre><code>df[\"Location Name\"][df[\"Total Deaths\"] &gt; 50000]\n</code></pre> <pre><code>272         INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n320    PAKISTAN:  MUZAFFARABAD, URI, ANANTNAG, BARAMULA\n490                            CHINA:  SICHUAN PROVINCE\n607                              HAITI:  PORT-AU-PRINCE\nName: Location Name, dtype: object\n</code></pre> <pre><code>df[\"Location Name\"][df[\"Mag\"] &gt; 8.5]\n</code></pre> <pre><code>272    INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST\n294                      INDONESIA:  SUMATERA:  SW\n614          CHILE:  MAULE, CONCEPCION, TALCAHUANO\n674                                 JAPAN:  HONSHU\n736         INDONESIA:  N SUMATRA:  OFF WEST COAST\nName: Location Name, dtype: object\n</code></pre> <pre><code>df[\"Mag\"].mean()\n</code></pre> <pre><code>5.945054031587698\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>Index(['Year', 'Mo', 'Dy', 'Location Name', 'Latitude', 'Longitude',\n       'Focal Depth (km)', 'Mag', 'Total Deaths'],\n      dtype='object')\n</code></pre> <pre><code>df.fillna(0, inplace=True)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 0.0 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 0.0 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 0.0 <pre><code>new_df = df[[\"Location Name\", \"Mag\", \"Total Deaths\"]]\nnew_df\n</code></pre> Location Name Mag Total Deaths 0 INDIA-BANGLADESH BORDER:  MAHESHKHALI 4.6 0.0 1 CHINA:  LIAONING PROVINCE 5.1 0.0 2 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 5.9 7.0 3 IRAN:  BARDASKAN, KASHMAR 5.3 1.0 4 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI 4.5 0.0 ... ... ... ... 1201 PHILIPPINES:  MASBATE 6.6 2.0 1202 ALASKA 7.6 0.0 1203 GREECE:  SAMOS; TURKEY:  IZMIR 7.0 118.0 1204 CHILE:  OFF COAST CENTRAL 6.7 0.0 1205 BALKANS NW:  CROATIA:  PETRINJA 6.4 8.0 <p>1206 rows \u00d7 3 columns</p> <pre><code>new_df.describe()\n</code></pre> Mag Total Deaths count 1206.000000 1206.000000 mean 5.930265 681.509121 std 1.099304 11757.576367 min 0.000000 0.000000 25% 5.200000 0.000000 50% 5.900000 0.000000 75% 6.700000 2.000000 max 9.100000 316000.000000 <pre><code>plt.boxplot(new_df[\"Mag\"])\n</code></pre> <pre><code>{'whiskers': [&lt;matplotlib.lines.Line2D at 0x28fd619e1c0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x28fd619e460&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x28fd619e700&gt;,\n  &lt;matplotlib.lines.Line2D at 0x28fd619e9a0&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x28fd617eee0&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x28fd619ec40&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x28fd619eee0&gt;],\n 'means': []}\n</code></pre> <p></p>"},{"location":"data-science/Data-Science-Bootcamp-with-Python/Random_Practices/Untitled2/","title":"Untitled2","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>path = \"D:\\Coding\\Git Repository\\Data-Science-Bootcamp-with-Python\\Datasets\\significant_earthquakes_2000_2020.csv\"\n</code></pre> <pre><code>df = pd.read_csv(path)\ndf.head()\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 0 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 1 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 3 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 4 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN <pre><code>df[df[\"Mag\"] &gt; 8.5]\n</code></pre> Year Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths 272 2004 12 26 INDONESIA:  SUMATRA:  ACEH:  OFF WEST COAST 3.316 95.854 30.0 9.1 227899.0 294 2005 3 28 INDONESIA:  SUMATERA:  SW 2.085 97.108 30.0 8.6 1313.0 614 2010 2 27 CHILE:  MAULE, CONCEPCION, TALCAHUANO -36.122 -72.898 23.0 8.8 558.0 674 2011 3 11 JAPAN:  HONSHU 38.297 142.372 30.0 9.1 18428.0 736 2012 4 11 INDONESIA:  N SUMATRA:  OFF WEST COAST 2.327 93.063 20.0 8.6 10.0 <pre><code>df.set_index(\"Year\", inplace=True)\n</code></pre> <pre><code>df.loc[2000]\n</code></pre> Mo Dy Location Name Latitude Longitude Focal Depth (km) Mag Total Deaths Year 2000 1 3 INDIA-BANGLADESH BORDER:  MAHESHKHALI 22.132 92.771 33.0 4.6 NaN 2000 1 11 CHINA:  LIAONING PROVINCE 40.498 122.994 10.0 5.1 NaN 2000 1 14 CHINA:  YUNNAN PROVINCE:  YAOAN COUNTY 25.607 101.063 33.0 5.9 7.0 2000 2 2 IRAN:  BARDASKAN, KASHMAR 35.288 58.218 33.0 5.3 1.0 2000 2 7 SOUTH AFRICA; SWAZILAND:  MBABANE-MANZINI -26.288 30.888 5.0 4.5 NaN 2000 3 28 JAPAN:  VOLCANO ISLANDS 22.338 143.730 127.0 7.6 NaN 2000 4 5 GREECE:  CRETE 34.220 25.690 38.0 5.5 NaN 2000 5 4 INDONESIA:  SULAWESI:  LUWUK, BANGGAI, PELENG, -1.105 123.573 26.0 7.6 46.0 2000 5 7 TURKEY:  DOGANYOL, PUTURGE 38.164 38.777 5.0 4.1 NaN 2000 5 17 TAIWAN:  TAI-CHUNG COUNTY 24.223 121.058 10.0 5.4 3.0 2000 6 4 INDONESIA:  SUMATRA:  BENGKULU, ENGGANO -4.721 102.087 33.0 7.9 103.0 2000 6 6 TURKEY:  CERKES, CUBUK, ORTA 40.693 32.992 10.0 6.0 2.0 2000 6 7 CHINA:  YUNNAN PROVINCE:  LIUKU; MYANMAR 26.856 97.238 33.0 6.3 NaN 2000 6 7 INDONESIA:  SOUTHERN SUMATERA:  LAHAT -4.612 101.905 33.0 6.7 1.0 2000 6 10 TAIWAN:  NAN-TOU 23.843 121.225 33.0 6.4 2.0 2000 6 17 ICELAND:  VESTMANNAEYJAR, HELLA 63.966 -20.487 10.0 6.5 NaN 2000 6 18 AUSTRALIA:  S, COCOS ISLANDS -13.802 97.453 10.0 7.9 NaN 2000 6 21 ICELAND:  GRIMSNES, SELFOSS, EYRARBAKKI, STOKK... 63.980 -20.758 10.0 6.5 NaN 2000 7 1 JAPAN:  NEAR S COAST HONSHU:  KOZU-SHIMA 34.221 139.131 10.0 6.1 1.0 2000 7 6 NICARAGUA:  MASAYA 11.884 -85.988 33.0 5.4 7.0 2000 7 12 INDONESIA: JAWA:BANDUNG,CIBADAK,CIMANDIRI,KADU... -6.675 106.845 33.0 5.4 NaN 2000 7 15 JAPAN:  NEAR S COAST HONSHU:  NII-JIMA 34.319 139.260 10.0 6.1 NaN 2000 7 16 PHILIPPINES:  BASCO, MOUNT IRADA, BATAN ISLANDS 20.253 122.043 33.0 6.4 NaN 2000 7 30 JAPAN:  HONSHU:  S 33.901 139.376 10.0 6.5 NaN 2000 8 4 RUSSIA:  SAKHALIN ISLAND, UGLEGORSK, MAKAROV 48.786 142.246 10.0 6.8 NaN 2000 8 21 CHINA:  YUNNAN PROVINCE:  WUDING 25.826 102.194 33.0 4.2 1.0 2000 9 3 CALIFORNIA:  NAPA 38.379 -122.413 10.0 5.0 NaN 2000 10 2 TANZANIA:  NKANSI, RUKWA -7.977 30.709 34.0 6.5 NaN 2000 10 6 JAPAN:  HONSHU:  W:  OKAYAMA, TOTTORI 35.456 133.134 10.0 6.7 NaN 2000 10 30 AFGHANISTAN-TAJIKISTAN:  RAKHOR 37.542 69.582 33.0 5.1 NaN 2000 11 8 PANAMA-COLOMBIA:  JURADO 7.042 -77.829 17.0 6.5 NaN 2000 11 16 PAPUA NEW GUINEA:  NEW IRELAND, DUKE OF YORK -4.001 152.327 17.0 8.0 2.0 2000 11 16 PAPUA NEW GUINEA:  NEW IRELAND, NEW BRITAIN -5.233 153.102 30.0 7.8 NaN 2000 11 17 PAPUA NEW GUINEA:  NEW BRITAIN -5.496 151.781 33.0 7.8 NaN 2000 11 25 AZERBAIJAN:  BAKU 40.245 49.946 50.0 6.8 31.0 2000 12 6 TURKMENISTAN:  NEBITDAG-TURKMENBASHI 39.566 54.799 30.0 7.0 11.0 2000 12 15 TURKEY:  AFYON-BOLVADIN 38.457 31.351 10.0 6.0 6.0"},{"location":"data-science/Data-Wrangling-with-Xarray/","title":"Data Wrangling with Xarray: A Hands on Course for Multidimensional Data Analysis","text":"<p>This comprehensive and practical course empowers learners to efficiently manipulate and analyze multidimensional data using the powerful Xarray library in Python.</p>"},{"location":"data-science/End-to-End-Machine-Learning/","title":"End to End Machine Learning","text":"<p>Welcome to the \"End to End Machine Learning\" repository! This curated collection offers an extensive array of original Jupyter Notebooks, meticulously crafted to delve into diverse facets of machine learning. Whether you're a novice eager to grasp the fundamentals or a seasoned practitioner in search of advanced techniques, this repository serves as your comprehensive guide to immersive, hands-on learning experiences. Explore and enhance your understanding of machine learning concepts, algorithms, and applications through interactive and engaging tutorials.</p>"},{"location":"data-science/End-to-End-Machine-Learning/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Gathering</li> <li>Exploratory Data Analysis</li> <li>Feature Engineering</li> <li>Machine Learning Algorithms</li> </ol>"},{"location":"data-science/End-to-End-Machine-Learning/#introduction","title":"Introduction","text":"<p>This repository is designed to be a comprehensive tutorial collection, guiding you through the intricacies of machine learning. Each notebook is crafted to explain concepts, provide practical examples, and encourage interactive learning. Whether you are interested in classical machine learning algorithms, deep learning architectures, or deployment strategies, you'll find a notebook tailored to your needs.</p>"},{"location":"data-science/End-to-End-Machine-Learning/#features","title":"Features","text":"<ul> <li>Diverse Topics: Covering a wide range of machine learning concepts, algorithms, and applications.</li> <li>Interactive Learning: Jupyter Notebooks provide an interactive environment for experimenting with code.</li> <li>Original Content: Authored with care to ensure clarity and originality in explanations and examples.</li> <li>Documentation: Clear explanations, step-by-step instructions, and visualizations to enhance understanding.</li> </ul>"},{"location":"data-science/End-to-End-Machine-Learning/#license","title":"License","text":"<p>This project is licensed under the MIT License. Share, learn, and enjoy the journey of machine learning! \ud83d\udcca\ud83d\ude80</p>"},{"location":"deep-learning/End-to-End-Deep-Learning/","title":"End to End Deep Learning","text":""},{"location":"deep-learning/deep-learning-from-scratch/","title":"\ud83c\udf0d DL4EO: Deep Learning from Scratch for Earth Observation","text":"<p>Welcome to dl4eo \u2014 a hands-on roadmap for learning and applying Deep Learning in Geospatial and Earth Observation (EO) applications. This repository is inspired by the idea of \u201clearning from scratch\u201d but tailored to the unique challenges of satellite imagery, multispectral/hyperspectral data, and geospatial analysis.  </p>"},{"location":"deep-learning/deep-learning-from-scratch/#goals","title":"\ud83d\udccc Goals","text":"<ul> <li>Build deep learning models from the ground up, starting with the basics.  </li> <li>Apply models to geospatial datasets: crop health, land cover, object detection, segmentation, change detection, etc.  </li> <li>Provide clean, reproducible code with step-by-step progression.  </li> <li>Serve as both a learning guide and a research playground.  </li> </ul>"},{"location":"deep-learning/deep-learning-from-scratch/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Open an issue or submit a PR if you\u2019d like to add new models, datasets, or tutorials.</p>"},{"location":"deep-learning/deep-learning-from-scratch/#license","title":"\ud83d\udcdc License","text":"<p>MIT License \u2013 feel free to use this repo for learning and research.</p>"},{"location":"deep-learning/pytorch-for-deep-learning-and-machine-learning/","title":"PyTorch for Deep Learning and Machine Learning","text":""},{"location":"fundamentals/Python-Basics-Learn-to-Code-from-Scratch/","title":"Python Basics: Learn to Code from Scratch","text":"<p>Welcome to \"Python Basics: Learn to Code from Scratch\"! This is a comprehensive course designed for beginners who want to learn Python from scratch. In this repository, you will find all the course materials, including notebooks, assignments, and solutions.</p> <p>Throughout the course, you will learn the basics of Python programming, including data types, operators, conditionals, functions, object-oriented programming, and more. You will also learn how to use Python to solve real-world problems and build simple applications.</p> <p>All the code examples and projects in the course are provided in this repository, along with detailed explanations and instructions. You will have the opportunity to practice what you learn by completing assignments and projects, which will be reviewed by the course instructors.</p> <p>Whether you are a complete beginner or have some programming experience, this course will provide you with a solid foundation in Python programming. So, let's get started!</p>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/","title":"Python for Beginners to Pros","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#1-basics-of-python-programming","title":"1. Basics of Python Programming","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-basics","title":"Python Basics","text":"<ul> <li>Short info about DSMP</li> <li>About Python</li> <li>Python Output/print function</li> <li>Python Data Types</li> <li>Python Variables</li> <li>Python comments</li> <li>Python Keywords and Identifiers</li> <li>Python User Input</li> <li>Python Type conversion</li> <li>Python Literals</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-operators-if-else-loops","title":"Python Operators + if-else + Loops","text":"<ul> <li>Start of the session</li> <li>Python Operators</li> <li>Python if-else</li> <li>Python Modules</li> <li>Python While Loop</li> <li>Python for loop</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-strings","title":"Python Strings","text":"<ul> <li>Introduction</li> <li>Solving Loop problems</li> <li>Break, continue, pass statement in loops</li> <li>Strings</li> <li>String indexing</li> <li>String slicing</li> <li>Edit and delete a string</li> <li>Operations on String</li> <li>Common String functions</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#time-complexity","title":"Time complexity","text":"<ul> <li>Start of the Session</li> <li>PPT presentation on Time Complexity (Efficiency in Programming and Orders of Growth)</li> <li>Examples</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#interview-questions","title":"Interview Questions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#2-python-data-types","title":"2. Python Data Types","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-lists","title":"Python Lists","text":"<ul> <li>Introduction</li> <li>Array vs List</li> <li>How lists are stored in a memory</li> <li>Characteristics of Python List</li> <li>Code Example of Lists</li> <li>Create and access a list</li> <li>append(), extend(), insert()</li> <li>Edit items in a list</li> <li>Deleting items from a list</li> <li>Arithmetic, membership and loop operations on a List</li> <li>Various List functions</li> <li>List comprehension</li> <li>2 Ways to traverse a list</li> <li>Zip() function</li> <li>Python List can store any kind of objects</li> <li>Disadvantages of Python list</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#tuples-set-dictionary","title":"Tuples + Set + Dictionary","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#tuple","title":"Tuple","text":"<ul> <li>Create and access a tuple</li> <li>Can we edit and add items to a tuple?</li> <li>Deletion</li> <li>Operations on tuple</li> <li>Tuple functions</li> <li>List vs tuple</li> <li>Tuple unpacking</li> <li>Zip() on tuple</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#set","title":"Set","text":"<ul> <li>Create and access a set</li> <li>Can we edit and add items to a set?</li> <li>Deletion</li> <li>Operations on set</li> <li>set functions</li> <li>Frozen set (immutable set)</li> <li>Set comprehension</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#dictionary","title":"Dictionary","text":"<ul> <li>Create dictionary</li> <li>Accessing items</li> <li>Add, remove, edit key-value pairs</li> <li>Operations on dictionary</li> <li>Dictionary functions</li> <li>Dictionary comprehension</li> <li>Zip() on dictionary</li> <li>Nested comprehension</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#python-functions","title":"Python Functions","text":"<ul> <li>Create function</li> <li>Arguments and parameters</li> <li>args and kwargs</li> <li>How to access documentation of a function</li> <li>How functions are executed in a memory</li> <li>Variable scope</li> <li>Nested functions with examples</li> <li>Functions are first class citizens</li> <li>Deletion of function</li> <li>Returning of function</li> <li>Advantages of functions</li> <li>Lambda functions</li> <li>Higher order functions</li> <li>map(), filter(), reduce()</li> </ul>"},{"location":"fundamentals/Python-for-Beginners-to-Pros/#array-interview-questions","title":"Array Interview Questions","text":""},{"location":"fundamentals/Python-for-Beginners-to-Pros/#week-2-interview-questions","title":"Week 2 Interview Questions","text":""},{"location":"geospatial/Geospatial_Data_Science_with_Python/","title":"Geospatial Data Science with Python","text":"<p>This project contains several notebooks covering geospatial data analysis with Python.</p>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#01-working-with-projections","title":"01. Working with Projections","text":"<ul> <li>Working with GCS and PCS</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#02-exploring-geospatial-packages","title":"02. Exploring Geospatial Packages","text":"<ul> <li>GeoPandas</li> <li>Spatial Data Structures</li> <li>Spatial Data Manipulation</li> <li>Geocoding</li> <li>Shapely</li> <li>Shapely Properties and Methods</li> <li>Rasterio</li> <li>ipyLeaflet</li> </ul>"},{"location":"geospatial/Geospatial_Data_Science_with_Python/#03-exploratory-spatial-data-analysis","title":"03. Exploratory Spatial Data Analysis","text":"<ul> <li>Exploratory Data Visualization</li> <li>Creating Choropleth Map from Points</li> </ul>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/","title":"Image Analysis in Remote Sensing with Python","text":"<p>This project focuses on analyzing remote sensing imagery using Python, NumPy, and Scikit-Learn.</p>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/#contents","title":"Contents","text":"<ul> <li>Reading and Displaying Image Band</li> <li>Integration of GEE with NumPy</li> </ul>"},{"location":"geospatial/Image-Analysis-in-Remote-Sensing-with-Python/#modules","title":"Modules","text":"<ul> <li> <ol> <li>Images, Arrays, and Matrices</li> </ol> </li> <li>Machine Learning with Scikit Learn</li> </ul>"},{"location":"geospatial/Mastering-Machine-Learning-and-GEE-for-Earth-Science/","title":"Mastering Machine Learning and GEE for Earth Science","text":"<p>This repository is designed to empower individuals with advanced expertise in integrating Machine Learning techniques with Google Earth Engine (GEE).</p>"},{"location":"geospatial/climate-data-downscaling/","title":"Climate Data Downscaling","text":"<p>This project implements climate data downscaling techniques using deep learning.</p>"},{"location":"geospatial/climate-data-downscaling/#files","title":"Files","text":"<ul> <li>Training Notebook</li> <li><code>model.py</code>: Model architecture</li> <li><code>train.py</code>: Training script</li> <li><code>dataset.py</code>: Data loading utilities</li> <li><code>config.py</code>: Configuration settings</li> </ul>"},{"location":"geospatial/climate-data-downscaling/#automated-notebooks-list","title":"\ud83d\udcd4 Automated Notebooks List","text":"<ul> <li>Train</li> </ul>"},{"location":"geospatial/climate-data-downscaling/#data-assets","title":"\ud83d\udcca Data &amp; Assets","text":"<ul> <li>config.py</li> <li>dataset.py</li> <li>loss.py</li> <li>model.py</li> <li>train.py</li> <li>utils.py</li> </ul>"},{"location":"geospatial/climate-data-downscaling/train/","title":"Train","text":""},{"location":"geospatial/climate-data-downscaling/train/#import-libraries","title":"Import libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport logging\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom utils import compute_mean_std, EarlyStopping, setup_logger\nimport config as cfg\nfrom dataset import ClimateDataset\nfrom model import QuantileDownscaler\nfrom loss import QuantileLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n</code></pre>"},{"location":"geospatial/climate-data-downscaling/train/#dataset-and-dataloader","title":"Dataset and dataloader","text":"<pre><code># Collect and sort file paths\nlr_paths = sorted(glob(os.path.join(cfg.DATA_DIR, 'lr_images', '*.nc')))\nhr_paths = sorted(glob(os.path.join(cfg.DATA_DIR, 'hr_images', '*.nc')))\nassert len(lr_paths) == len(hr_paths), \"LR and HR directories must contain same number of files\"\n\n# Random shuffle of indices\nrng = np.random.default_rng(42)\nindices = np.arange(len(lr_paths))\nrng.shuffle(indices)\n\n# 80:20 split index\nsplit_idx = int(0.8 * len(indices))\n\ntrain_idx = indices[:split_idx]\ntest_idx  = indices[split_idx:]\n\n# Gather file lists\ntrain_lr_paths = [lr_paths[i] for i in train_idx]\ntrain_hr_paths = [hr_paths[i] for i in train_idx]\n\ntest_lr_paths = [lr_paths[i] for i in test_idx]\ntest_hr_paths = [hr_paths[i] for i in test_idx]\n\nprint(f\"Train: {len(train_lr_paths)}, Test: {len(test_lr_paths)}\")\n</code></pre> <pre><code># Precompute the normalization stats for all the channels\nmean, std = compute_mean_std(train_lr_paths, num_samples=int(len(train_lr_paths) * 0.5), num_workers=30)\n\ntrain_dataset = ClimateDataset(\n    lr_paths=train_lr_paths, \n    hr_paths=train_hr_paths, \n    mean=mean, std=std, size=128\n)\ntest_dataset = ClimateDataset(\n    lr_paths=test_lr_paths, \n    hr_paths=test_hr_paths, \n    mean=mean, std=std, size=128\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n</code></pre>"},{"location":"geospatial/climate-data-downscaling/train/#model-loss-optimizer","title":"Model, Loss, Optimizer","text":"<pre><code>model = QuantileDownscaler(\n    in_channels=cfg.IMG_CHANNELS, \n    num_channels=64, \n    num_blocks=16, \n    quantiles=[0.05, 0.5, 0.95]\n)\nmodel = nn.DataParallel(model)\n\ncriterion = QuantileLoss(quantiles=[0.05, 0.5, 0.95])\n\noptimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n</code></pre> <pre><code>def train_model(\n    model,\n    train_loader,\n    val_loader,\n    criterion,\n    optimizer,\n    num_epochs=50,\n    device=\"cuda\",\n    checkpoint_path=\"best_model.pth\",\n    patience=10,\n    log_file=\"train.log\",\n):\n    \"\"\"\n    Train model with early stopping, checkpointing, and logging.\n\n    Args:\n        model: PyTorch nn.Module\n        train_loader: DataLoader\n        val_loader: DataLoader\n        criterion: loss function\n        optimizer: optimizer\n        num_epochs: max epochs\n        device: \"cuda\" or \"cpu\"\n        checkpoint_path: file to save best model\n        patience: early stopping patience\n        log_file: log file path\n    \"\"\"\n\n    logger = setup_logger(log_file)\n    early_stopping = EarlyStopping(patience=patience, path=checkpoint_path)\n\n    model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        # -------------------\n        # Training\n        # -------------------\n        model.train()\n        train_loss = 0.0\n        for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n            X, y = X.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            preds = model(X)\n            print(preds.shape)\n            loss = criterion(preds, y)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * X.size(0)\n\n        train_loss /= len(train_loader.dataset)\n\n        # -------------------\n        # Validation\n        # -------------------\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X, y in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n                X, y = X.to(device), y.to(device)\n                preds = model(X)\n                loss = criterion(preds, y)\n                val_loss += loss.item() * X.size(0)\n\n        val_loss /= len(val_loader.dataset)\n\n        logger.info(\n            f\"Epoch [{epoch}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n        )\n\n        # -------------------\n        # Checkpoint &amp; Early Stopping\n        # -------------------\n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            logger.info(\"Early stopping triggered.\")\n            break\n</code></pre> <pre><code>if __name__ == '__main__':\n    train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=test_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=cfg.NUM_EPOCHS,\n        device=cfg.DEVICE,\n        checkpoint_path=os.path.join(cfg.CHECKPOINT_DIR, \"best_model.pth\"),\n        patience=10,\n        log_file=os.path.join(cfg.LOG_DIR, \"train.log\"),\n    )\n</code></pre> <pre><code>X, y = train_dataset[0]\n</code></pre> <pre><code>X_hat = model(X.unsqueeze(dim=0))\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(X.cpu().detach().numpy()[12])\nplt.colorbar(shrink=0.8);\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(X_hat[0].mean(axis=0).cpu().detach().numpy())\nplt.colorbar(shrink=0.8);\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(y.cpu().detach().numpy()[0])\nplt.colorbar(shrink=0.8);\n</code></pre>"},{"location":"geospatial/geonext-handbook/","title":"GeoNext Handbook","text":"<p>Welcome to the GeoNext Handbook, a comprehensive guide for geospatial analysis and machine learning.</p>"},{"location":"geospatial/geonext-handbook/#contents","title":"Contents","text":"<ul> <li>Markdown Guide</li> <li>Jupyter Notebooks Example</li> <li>Maize Leaf Disease Classification using ViT</li> <li>Bibliography</li> </ul>"},{"location":"geospatial/geonext-handbook/CONDUCT/","title":"Code of Conduct","text":""},{"location":"geospatial/geonext-handbook/CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"geospatial/geonext-handbook/CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"geospatial/geonext-handbook/CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in the ways listed below.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs using GitHub issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>GeoNext Handbook could always use more documentation, whether as part of the official GeoNext Handbook docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue on GitHub.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here's how to set up <code>GeoNext Handbook</code> for local development.</p> <ol> <li>Fork the repo on GitHub.</li> <li>Clone your fork locally.</li> <li>Install your local copy into a virtualenv, e.g., using <code>conda</code>.</li> <li>Create a branch for local development and make changes locally.</li> <li>Commit your changes and push your branch to GitHub.</li> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"geospatial/geonext-handbook/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that the GeoNext Handbook project is released with a Contributor Code of Conduct. By contributing to this project you agree to abide by its terms.</p>"},{"location":"geospatial/geonext-handbook/book/bibliography/","title":"Bibliography","text":""},{"location":"geospatial/geonext-handbook/book/markdown/","title":"Markdown Files","text":"<p>Whether you write your book's content in Jupyter Notebooks (<code>.ipynb</code>) or in regular markdown files (<code>.md</code>), you'll write in the same flavor of markdown called MyST Markdown.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#what-is-myst","title":"What is MyST?","text":"<p>MyST stands for \"Markedly Structured Text\". It is a slight variation on a flavor of markdown called \"CommonMark\" markdown, with small syntax extensions to allow you to write roles and directives in the Sphinx ecosystem.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#what-are-roles-and-directives","title":"What are roles and directives?","text":"<p>Roles and directives are two of the most powerful tools in Jupyter Book. They are kind of like functions, but written in a markup language. They both serve a similar purpose, but roles are written in one line, whereas directives span many lines. They both accept different kinds of inputs, and what they do with those inputs depends on the specific role or directive that is being called.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#using-a-directive","title":"Using a directive","text":"<p>At its simplest, you can insert a directive into your book's content like so:</p> <pre><code>```{mydirectivename}\nMy directive content\n```\n</code></pre> <p>This will only work if a directive with name <code>mydirectivename</code> already exists (which it doesn't). There are many pre-defined directives associated with Jupyter Book. For example, to insert a note box into your content, you can use the following directive:</p> <pre><code>```{note}\nHere is a note\n```\n</code></pre> <p>This results in:</p> <pre><code>Here is a note\n</code></pre> <p>In your built book.</p> <p>For more information on writing directives, see the MyST documentation.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#using-a-role","title":"Using a role","text":"<p>Roles are very similar to directives, but they are less-complex and written entirely on one line. You can insert a role into your book's content with this pattern:</p> <pre><code>Some content {rolename}`and here is my role's content!`\n</code></pre> <p>Again, roles will only work if <code>rolename</code> is a valid role's name. For example, the <code>doc</code> role can be used to refer to another page in your book. You can refer directly to another page by its relative path. For example, the role syntax <code>{doc}`index`</code> will result in: {doc}<code>index</code>.</p> <p>For more information on writing roles, see the MyST documentation.</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#adding-a-citation","title":"Adding a citation","text":"<p>You can also cite references that are stored in a <code>bibtex</code> file. For example, the following syntax: <code>{cite}`holdgraf_evidence_2014`</code> will render like this: {cite}<code>holdgraf_evidence_2014</code>.</p> <p>Multiple citations can be used like this:<code>holdgraf_rapid_2016, holdgraf_encoding_2017</code></p> <p>Moreover, you can insert a bibliography into your page with this syntax: The <code>{bibliography}</code> directive must be used for all the <code>{cite}</code> roles to render properly. For example, if the references for your book are stored in <code>references.bib</code>, then the bibliography is inserted with:</p> <pre><code>```{bibliography}\n```\n</code></pre> <p>Resulting in a rendered bibliography that looks like:</p>"},{"location":"geospatial/geonext-handbook/book/markdown/#executing-code-in-your-markdown-files","title":"Executing code in your markdown files","text":"<p>If you'd like to include computational content inside these markdown files, you can use MyST Markdown to define cells that will be executed when your book is built. Jupyter Book uses jupytext to do this.</p> <p>First, add Jupytext metadata to the file. For example, to add Jupytext metadata to this markdown page, run this command:</p> <pre><code>jupyter-book myst init markdown.md\n</code></pre> <p>Once a markdown file has Jupytext metadata in it, you can add the following directive to run the code at build time:</p> <pre><code>```{code-cell}\nprint(\"Here is some code to execute\")\n```\n</code></pre> <p>When your book is built, the contents of any <code>{code-cell}</code> blocks will be executed with your default Jupyter kernel, and their outputs will be displayed in-line with the rest of your content.</p> <p>For more information about executing computational content with Jupyter Book, see The MyST-NB documentation.</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/","title":"Crop-Yield-Forecasting-Germany","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/#automated-notebooks-list","title":"\ud83d\udcd4 Automated Notebooks List","text":"<ul> <li>00 Yield Data Preparation</li> <li>01 Data Preparation For Pbms</li> <li>02 Data Preparation For Pbms</li> </ul>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/","title":"DE Yield Data Preparation","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom difflib import SequenceMatcher\nimport os\nimport geopandas as gpd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the shapefile for DE NUTS3\nde_nuts_gdf = gpd.read_file(r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\shapefiles\\DE_NUTS\\DE_NUTS_3.shp\")\nprint(de_nuts_gdf.shape)\nde_nuts_gdf.head()\n</code></pre> <pre><code>(455, 9)\n</code></pre> NUTS_ID LEVL_CODE CNTR_CODE NAME_LATN NUTS_NAME MOUNT_TYPE URBN_TYPE COAST_TYPE geometry 0 DE11B 3 DE Main-Tauber-Kreis Main-Tauber-Kreis None None None POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 3 DE Heidenheim Heidenheim None None None MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 3 DE Ostalbkreis Ostalbkreis None None None MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 3 DE Baden-Baden, Stadtkreis Baden-Baden, Stadtkreis None None None MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 3 DE Karlsruhe, Stadtkreis Karlsruhe, Stadtkreis None None None POLYGON ((938225.711 6286986.826, 940668.057 6... <pre><code># Read the latest yield data for Germany for the year 2022, and 2023\nde_yield_2023 = pd.read_excel(\n       r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\csvs\\DE_Yield_Latest.xlsx\", \n       skiprows=[i for i in range(7)], nrows=537,\n       header=None,\n       names=[\"district_no\", \"district\", \"ww\", \"rye\",\n              \"wb\", \"sb\", \"oats\", \"triticale\",\n              \"pota_tot\", \"sugarbeet\", \"wrape\", \"silage_maize\"])\n\nde_yield_2023.replace([\"-\", \"\", \"/\", \".\", \"...\"], np.nan, inplace=True) # replace the special characters\nde_yield_2023[\"district_no\"] = de_yield_2023[\"district_no\"].astype(\"int\") # change the datatype of district no into 'int'\nde_yield_2023 = pd.melt(\n       de_yield_2023,\n       id_vars='district_no', \n       value_vars=de_yield_2023.columns[2:], \n       var_name='var', ignore_index=True\n) # melt the dataframe\nde_yield_2023['year'] = 2023 # add the 'year' info\nde_yield_2023['measure'] = 'yield' # add the 'measure' column\nde_yield_2023['outlier'] = np.nan # add the outlier columns\nprint(de_yield_2023.shape)\nde_yield_2023.head()\n</code></pre> <pre><code>(5370, 6)\n</code></pre> district_no var value year measure outlier 0 1 ww 83.2 2023 yield NaN 1 1001 ww NaN 2023 yield NaN 2 1002 ww NaN 2023 yield NaN 3 1003 ww 85.2 2023 yield NaN 4 1004 ww NaN 2023 yield NaN <pre><code>de_yield_2022 = pd.read_excel(\n       r\"D:\\Research Works\\Agriculture\\Germany_Multiple_Crops_Cimate_Change\\Datasets\\Yield_Statistitics_WholeGermany_1999-2022\\Yield_Statistitics_WholeGermany_2022.xlsx\", \n       skiprows=[i for i in range(7)], nrows=537,\n       header=None,\n       names=[\"district_no\", \"district\", \"ww\", \"rye\",\n              \"wb\", \"sb\", \"oats\", \"triticale\",\n              \"pota_tot\", \"sugarbeet\", \"wrape\", \"silage_maize\"])\n\nde_yield_2022.replace([\"-\", \"\", \"/\", \".\", \"...\"], np.nan, inplace=True) # replace the special characters\nde_yield_2022[\"district_no\"] = de_yield_2022[\"district_no\"].astype(\"int\") # change the datatype of district no into 'int'\nde_yield_2022 = pd.melt(\n       de_yield_2022,\n       id_vars='district_no', \n       value_vars=de_yield_2022.columns[2:], \n       var_name='var', ignore_index=True\n) # melt the dataframe\nde_yield_2022['year'] = 2022 # add the 'year' info\nde_yield_2022['measure'] = 'yield' # add the 'measure' column\nde_yield_2022['outlier'] = np.nan # add the outlier columns\nprint(de_yield_2022.shape)\nde_yield_2022.head()\n</code></pre> <pre><code>(5370, 6)\n</code></pre> district_no var value year measure outlier 0 1 ww 95.8 2022 yield NaN 1 1001 ww NaN 2022 yield NaN 2 1002 ww NaN 2022 yield NaN 3 1003 ww 101.7 2022 yield NaN 4 1004 ww NaN 2022 yield NaN <pre><code># Read the yield dataset available for DE from 1979 to 2021\nde_yield_1979_21 = pd.read_csv(r\"D:\\GITHUB\\crop-yield-prediction-germany\\datasets\\csvs\\openagrar_derivate_00056476\\Final_data.csv\")\n\n# Filter the dataframe for yield only\nde_yield_1979_21 = de_yield_1979_21[de_yield_1979_21['measure']=='yield']\n\n# Create a seperate df to store only 'district_no', 'district', and 'nuts_id'\nde_nuts = de_yield_1979_21[['district_no', 'district', 'nuts_id']].drop_duplicates()\n\nprint(de_yield_1979_21.shape)\nde_yield_1979_21.head()\n</code></pre> <pre><code>(179691, 8)\n</code></pre> district_no district nuts_id year var measure value outlier 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 grain_maize yield NaN 0 5 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0 7 1001 Flensburg, kreisfreie Stadt DEF01 1979 potat_tot yield NaN 0 9 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0 11 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#data-processsing","title":"Data Processsing","text":"<pre><code># Merge the district information to the latest dataset\nde_yield_2023 = pd.merge(left=de_yield_2023, right=de_nuts, on='district_no', how='inner')\nde_yield_2022 = pd.merge(left=de_yield_2022, right=de_nuts, on='district_no', how='inner')\n\n# Convert the yield values from dt/ha to t/ha\nde_yield_2023['value'] = de_yield_2023['value'] / 10\nde_yield_2022['value'] = de_yield_2022['value'] / 10\n\n# Reorder the columns based on the data from 1971-2021\nde_yield_2023 = de_yield_2023[de_yield_1979_21.columns]\nde_yield_2022 = de_yield_2022[de_yield_1979_21.columns]\n\nprint('Shape of the data for 2023:', de_yield_2023.shape)\nprint('Shape of the data for 2022:', de_yield_2022.shape)\n</code></pre> <pre><code>Shape of the data for 2023: (3970, 8)\nShape of the data for 2022: (3970, 8)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#merge-all-the-datasets","title":"Merge All the Datasets","text":"<pre><code># Concat all the datasets\nmerged_df = pd.concat((de_yield_1979_21, de_yield_2022, de_yield_2023), ignore_index=True)\n\n# Sort the dataframe based on district number, year, and var\nmerged_df.sort_values(by=['district_no', 'year', 'var'], inplace=True)\nprint(merged_df.shape)\nmerged_df.head()\n</code></pre> <pre><code>(187631, 8)\n</code></pre> district_no district nuts_id year var measure value outlier 0 1001 Flensburg, kreisfreie Stadt DEF01 1979 grain_maize yield NaN 0.0 1 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0.0 2 1001 Flensburg, kreisfreie Stadt DEF01 1979 potat_tot yield NaN 0.0 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0.0 4 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0.0 <pre><code>merged_df[(merged_df['year']==2022) &amp; (merged_df['var']=='ww')]\n</code></pre> district_no district nuts_id year var measure value outlier 179691 1001 Flensburg, kreisfreie Stadt DEF01 2022 ww yield NaN NaN 179692 1002 Kiel, kreisfreie Stadt DEF02 2022 ww yield NaN NaN 179693 1003 L\u00fcbeck, kreisfreie Stadt DEF03 2022 ww yield 10.17 NaN 179694 1004 Neum\u00fcnster, kreisfreie Stadt DEF04 2022 ww yield NaN NaN 179695 1051 Dithmarschen, Landkreis DEF05 2022 ww yield 9.47 NaN ... ... ... ... ... ... ... ... ... 180083 16073 Saalfeld-Rudolstadt, Landkreis DEG0I 2022 ww yield 4.86 NaN 180084 16074 Saale-Holzland-Kreis DEG0J 2022 ww yield 6.16 NaN 180085 16075 Saale-Orla-Kreis DEG0K 2022 ww yield 5.69 NaN 180086 16076 Greiz, Landkreis DEG0L 2022 ww yield 6.64 NaN 180087 16077 Altenburger Land, Landkreis DEG0M 2022 ww yield 8.38 NaN <p>397 rows \u00d7 8 columns</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/00_yield_data_preparation/#plot-the-dataset-in-maps","title":"Plot the Dataset in Maps","text":"<pre><code>def plot_var_map(dataframe, shapefile, var, year):\n\n    # prepare the data \n    dataframe = dataframe[(dataframe['year']==year) &amp; (dataframe['var']==var)]\n    dataframe = dataframe[['nuts_id', 'district_no', 'district', 'year', 'var', 'value']]\n\n    shapefile.rename(columns={'NUTS_ID': 'nuts_id'}, inplace=True)\n\n    # merged the dataframe with the shapefile\n    shapefile_merged = pd.merge(left=shapefile, right=dataframe, on='nuts_id', how='left')\n    shapefile_merged = shapefile_merged[['nuts_id', 'district_no', 'NUTS_NAME', 'district', 'year', 'var', 'value', 'geometry']]\n\n    return shapefile_merged\n</code></pre> <pre><code>test_df = plot_var_map(merged_df, de_nuts_gdf, var='wb', year=2023)\ntest_df\n</code></pre> nuts_id district_no NUTS_NAME district year var value geometry 0 DE11B 8128.0 Main-Tauber-Kreis Main-Tauber-Kreis 2023.0 wb 6.15 POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 8135.0 Heidenheim Heidenheim, Landkreis 2023.0 wb NaN MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 8136.0 Ostalbkreis Ostalbkreis 2023.0 wb 7.19 MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 8211.0 Baden-Baden, Stadtkreis Baden-Baden, kreisfreie Stadt 2023.0 wb NaN MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 8212.0 Karlsruhe, Stadtkreis Karlsruhe, kreisfreie Stadt 2023.0 wb NaN POLYGON ((938225.711 6286986.826, 940668.057 6... ... ... ... ... ... ... ... ... ... 450 DE5 NaN Bremen NaN NaN NaN NaN MULTIPOLYGON (((949166.140 7023798.944, 952179... 451 DE6 NaN Hamburg NaN NaN NaN NaN MULTIPOLYGON (((1134475.694 7117788.896, 11336... 452 DEE NaN Sachsen-Anhalt NaN NaN NaN NaN MULTIPOLYGON (((1294568.737 6984897.844, 12963... 453 DE7 NaN Hessen NaN NaN NaN NaN MULTIPOLYGON (((1057294.287 6737363.291, 10568... 454 DE NaN Deutschland NaN NaN NaN NaN MULTIPOLYGON (((1163782.809 6033270.891, 11632... <p>455 rows \u00d7 8 columns</p> <pre><code>merged_df.shape\n</code></pre> <pre><code>(187631, 8)\n</code></pre> <pre><code>merged_df['value'].isnull().sum()\n</code></pre> <pre><code>43290\n</code></pre> <pre><code>merged_df[merged_df['value']&gt;0]\n</code></pre> district_no district nuts_id year var measure value outlier 1 1001 Flensburg, kreisfreie Stadt DEF01 1979 oats yield 4.95 0.0 3 1001 Flensburg, kreisfreie Stadt DEF01 1979 rye yield 4.35 0.0 4 1001 Flensburg, kreisfreie Stadt DEF01 1979 sb yield 3.63 0.0 5 1001 Flensburg, kreisfreie Stadt DEF01 1979 silage_maize yield 43.60 0.0 7 1001 Flensburg, kreisfreie Stadt DEF01 1979 wb yield 4.52 0.0 ... ... ... ... ... ... ... ... ... 187630 16077 Altenburger Land, Landkreis DEG0M 2023 silage_maize yield 41.94 NaN 186836 16077 Altenburger Land, Landkreis DEG0M 2023 sugarbeet yield 76.63 NaN 184851 16077 Altenburger Land, Landkreis DEG0M 2023 wb yield 10.04 NaN 187233 16077 Altenburger Land, Landkreis DEG0M 2023 wrape yield 4.34 NaN 184057 16077 Altenburger Land, Landkreis DEG0M 2023 ww yield 9.32 NaN <p>144341 rows \u00d7 8 columns</p> <pre><code>144341 + 43290\n</code></pre> <pre><code>187631\n</code></pre> <pre><code>test_df.plot(column='value', edgecolor='k', linewidth=0.3, figsize=(8, 8), legend=True)\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/","title":"Data Preparation for PBMs","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport os\nfrom glob import glob\nimport json\nfrom tqdm.auto import tqdm\nimport ee\nimport geemap\nfrom sklearn.neighbors import BallTree\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nout_master_dir = r'datasets\\master'\nout_temp_dir = r'temp_data'\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#instantiate-a-map-object","title":"Instantiate a Map Object","text":"<pre><code># ee.Authenticate()\n# ee.Initialize()\n</code></pre> <pre><code># Import Germany Shapefile\nde_roi = ee.FeatureCollection('users/geonextgis/Germany_Administrative_Level_2')\npoly_style = {'fillColor': '00000000', 'color': 'black', 'width': 1}\nMap = geemap.Map(basemap='Esri.WorldImagery')\nMap.addLayer(de_roi.style(**poly_style), {}, 'DE ROI')\nMap.centerObject(de_roi, 6)\nMap\n</code></pre> <pre><code>Map(center=[51.055719127031935, 10.373828310619029], controls=(WidgetControl(options=['position', 'transparent\u2026\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the DE NUTS3 Shapefile\nde_nuts3_gdf = gpd.read_file(\"datasets\\shapefiles\\DE_NUTS\\DE_NUTS_3.shp\")\nde_nuts3_gdf = de_nuts3_gdf[de_nuts3_gdf['LEVL_CODE']==3] # Filter for NUTS3\nprint(de_nuts3_gdf.shape)\nde_nuts3_gdf.head()\n</code></pre> <pre><code>(400, 6)\n</code></pre> NUTS_ID LEVL_CODE CNTR_CODE NAME_LATN NUTS_NAME geometry 0 DE11B 3 DE Main-Tauber-Kreis Main-Tauber-Kreis POLYGON ((1074230.536 6408356.046, 1073820.827... 1 DE11C 3 DE Heidenheim Heidenheim MULTIPOLYGON (((1131091.261 6235073.568, 11312... 2 DE11D 3 DE Ostalbkreis Ostalbkreis MULTIPOLYGON (((1141777.678 6284962.486, 11412... 3 DE121 3 DE Baden-Baden, Stadtkreis Baden-Baden, Stadtkreis MULTIPOLYGON (((910859.613 6248068.047, 913127... 4 DE122 3 DE Karlsruhe, Stadtkreis Karlsruhe, Stadtkreis POLYGON ((938225.711 6286986.826, 940668.057 6... <pre><code># Read the soil coordinates\nsoil_coords_df = pd.read_csv(\"datasets\\csvs\\Site_Soil_BZE_WGS84_Coords.csv\")\n\n# Read the soil attributes\nsoil_attributes_df = pd.read_excel(\n    \"datasets\\csvs\\SoilData.xlsx\",\n    sheet_name='SoilData'\n)\nsoil_attributes_df.drop(columns=['Lon', 'Lat'], inplace=True)\nsoil_attributes_df.rename(columns={'Location_id': 'PointID'}, inplace=True)\n\nprint('Soil coordinates data shape:', soil_coords_df.shape)\nprint('Soil coordinates attributes shape:', soil_attributes_df.shape)\n</code></pre> <pre><code>Soil coordinates data shape: (3104, 3)\nSoil coordinates attributes shape: (3087, 31)\n</code></pre> <pre><code># Read the ESA WorldCover dataset\nesa_lulc = ee.ImageCollection(\"ESA/WorldCover/v100\").first()\nvisualization = {\n  'bands': ['Map'],\n}\nMap.addLayer(esa_lulc, visualization, 'ESA LULC', opacity=0.6)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#soil-data-preparation","title":"Soil Data Preparation","text":"<pre><code># Convert the soil coordinates into geodataframe\nsoil_coords_geometry = gpd.points_from_xy(soil_coords_df['Longitude'], soil_coords_df['Latitude'], crs=4326)\nsoil_coords_gdf = gpd.GeoDataFrame(soil_coords_df, geometry=soil_coords_geometry).to_crs(epsg=3857)\nprint(soil_coords_gdf.shape)\nsoil_coords_gdf.head()\n</code></pre> <pre><code>(3104, 4)\n</code></pre> PointID Longitude Latitude geometry 0 2 8.411608 54.859923 POINT (936375.919 7334727.347) 1 3 8.697143 54.864382 POINT (968161.53 7335589.787) 2 4 8.765298 54.866979 POINT (975748.51 7336092.132) 3 5 8.959032 54.863920 POINT (997314.88 7335500.425) 4 6 9.078456 54.870685 POINT (1010609.099 7336809.049) <pre><code># Merge the NUTS3 information in the soil coordinates dataframe\nsoil_coords_gdf = soil_coords_gdf.sjoin(de_nuts3_gdf[['NUTS_ID', 'NUTS_NAME', 'geometry']], how='left', predicate='intersects')\nprint(soil_coords_gdf.shape)\nsoil_coords_gdf.head()\n</code></pre> <pre><code>(3104, 7)\n</code></pre> PointID Longitude Latitude geometry index_right NUTS_ID NUTS_NAME 0 2 8.411608 54.859923 POINT (936375.919 7334727.347) 166.0 DEF07 Nordfriesland 1 3 8.697143 54.864382 POINT (968161.53 7335589.787) 166.0 DEF07 Nordfriesland 2 4 8.765298 54.866979 POINT (975748.51 7336092.132) 166.0 DEF07 Nordfriesland 3 5 8.959032 54.863920 POINT (997314.88 7335500.425) 166.0 DEF07 Nordfriesland 4 6 9.078456 54.870685 POINT (1010609.099 7336809.049) 166.0 DEF07 Nordfriesland <pre><code># Merge the soil coordinate information in the soil attributes dataframe\nsoil_attributes_gdf = pd.merge(\n    left=soil_coords_gdf[['PointID', 'Longitude', 'Latitude', 'NUTS_ID', 'NUTS_NAME', 'geometry']], \n    right=soil_attributes_df, \n    on='PointID', \n    how='inner')\n\nsoil_attributes_gdf.columns = [col.replace('.0', '') for col in soil_attributes_gdf.columns]\nsoil_attributes_gdf.to_crs(crs='epsg:31467', inplace=True) # change the CRS\nsoil_attributes_ee = geemap.gdf_to_ee(soil_attributes_gdf, geodesic=False)\n\nsoil_style = {'color': 'green', 'pointSize': 5}\nMap.addLayer(soil_attributes_ee.style(**soil_style), {}, 'Soil Points')\n\nprint(soil_attributes_gdf.shape)\nsoil_attributes_gdf.head()\n</code></pre> <pre><code>(3087, 36)\n</code></pre> PointID Longitude Latitude NUTS_ID NUTS_NAME geometry SoilLayerDepth_10cm SoilLayerDepth_30cm SoilLayerDepth_50cm SoilLayerDepth_70cm ... BD_50cm BD_70cm BD_1m BD_2m OC_10cm OC_30cm OC_50cm OC_70cm OC_1m OC_2m 0 2 8.411608 54.859923 DEF07 Nordfriesland POINT (3462285.321 6081355.028) 0.1 0.3 0.5 0.7 ... 1.45 1.30 1.54 1.54 2.183 2.075 1.151 0.691 0.157 0.157 1 3 8.697143 54.864382 DEF07 Nordfriesland POINT (3480623.416 6081734.949) 0.1 0.3 0.5 0.7 ... 1.57 1.39 1.47 1.47 2.064 1.589 0.574 0.579 0.581 0.581 2 4 8.765298 54.866979 DEF07 Nordfriesland POINT (3485000.58 6082007.3) 0.1 0.3 0.5 0.7 ... 1.37 1.48 1.49 1.49 1.915 1.517 2.220 1.028 0.652 0.652 3 5 8.959032 54.863920 DEF07 Nordfriesland POINT (3497439.177 6081642.411) 0.1 0.3 0.5 0.7 ... 1.45 1.48 1.60 1.60 3.375 1.701 1.233 1.089 0.674 0.674 4 6 9.078456 54.870685 DEF07 Nordfriesland POINT (3505106.596 6082397.618) 0.1 0.3 0.5 0.7 ... 1.42 0.89 1.35 1.35 2.447 1.166 0.398 3.570 0.241 0.241 <p>5 rows \u00d7 36 columns</p> <pre><code># Save the Soil data\n# soil_attributes_gdf.to_csv(os.path.join(out_csv_dir, 'DE_Soil_BZE_Master.csv'), index=False)\n# soil_attributes_gdf.to_file(os.path.join(out_master_dir, 'DE_Soil_BZE_Master.shp'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#climate-grid-preparation","title":"Climate Grid Preparation","text":"<pre><code># Read the DWD Climate JSON file\ndwd_json_path = \"datasets\\shapefiles\\dwd_ubn_latlon_to_rowcol.json\"\n\nwith open(dwd_json_path, 'r') as file:\n    dwd_json = json.load(file)\n\n# Function to convert the json data into a table\ndef json_to_table(data):\n    coords_info = [i[0] for i in dwd_json]\n    row_col_info = [i[1] for i in dwd_json]\n\n    coords_info = pd.DataFrame(coords_info, columns=['Latitude', 'Longitude'])\n    row_col_info = pd.DataFrame(row_col_info, columns=['Row', 'Column'])\n\n    final_df = pd.concat((coords_info, row_col_info), axis=1)\n\n    return final_df\n\ndwd_grid_df = json_to_table(dwd_json)\nprint(dwd_grid_df.shape)\ndwd_grid_df.head()\n</code></pre> <pre><code>(358303, 4)\n</code></pre> Latitude Longitude Row Column 0 55.054328 8.402969 0 181 1 55.054404 8.418616 0 182 2 55.045268 8.387459 1 180 3 55.045346 8.403102 1 181 4 55.036286 8.387596 2 180 <pre><code># Convert the data into geodataframe\ndwd_grid_gdf = gpd.GeoDataFrame(\n    dwd_grid_df, \n    geometry=gpd.points_from_xy(dwd_grid_df['Longitude'], dwd_grid_df['Latitude']),\n    crs=4326)\nprint(dwd_grid_gdf.shape)\ndwd_grid_gdf.head()\n</code></pre> <pre><code>(358303, 5)\n</code></pre> Latitude Longitude Row Column geometry 0 55.054328 8.402969 0 181 POINT (8.40297 55.05433) 1 55.054404 8.418616 0 182 POINT (8.41862 55.0544) 2 55.045268 8.387459 1 180 POINT (8.38746 55.04527) 3 55.045346 8.403102 1 181 POINT (8.4031 55.04535) 4 55.036286 8.387596 2 180 POINT (8.3876 55.03629) <pre><code># # Save the data\n# dwd_grid_df.to_csv(os.path.join(out_csv_dir, 'DE_DWD_UBN_Centroids.csv'), index=False)\n\n# dwd_grid_gdf.to_crs(31467).to_file(\"datasets\\shapefiles\\DE_DWD_UBN_GRIDS\\DE_DWD_UBN_Centroids_EPSG_31467.shp\")\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#filter-points-falling-on-cropland","title":"Filter Points falling on Cropland","text":"<pre><code># Load the DWD grid centroids on EE Map object\ndwd_centroids_ee = ee.FeatureCollection('projects/ee-geonextgis/assets/DE_DWD_UBN_Centroids_EPSG_31467') \ndwd_style = {\n    'color': 'red',\n    'pointSize': 1,\n    'width': 1,\n    'fillColor': '00000000'\n}\n\n# Create a 500m buffer for each feature and get bounds\ndwd_grids_ee = dwd_centroids_ee.map(lambda f: f.buffer(500).bounds())\n\nMap.addLayer(dwd_centroids_ee.style(**dwd_style), {}, 'DWD Centroids', False)\nMap.addLayer(dwd_grids_ee.style(**dwd_style), {}, 'DWD Grids', False)\n</code></pre> <pre><code># # Iterate over the columns and extract the data in the temporary folder\n# for column_id in tqdm(sorted(dwd_grid_gdf['Column'].unique())):\n\n#     filered_column_cells = dwd_grids_ee.filter(ee.Filter.eq('Column', int(column_id)))\n\n#     try:\n#         out_data_path = os.path.join(out_temp_dir, f'Col_{column_id}_DWD_LULC.csv')\n\n#         # Extract LULC info for all the DWD cells\n#         dwd_lulc_zonal_stat = geemap.zonal_statistics_by_group(\n#             esa_lulc, filered_column_cells, out_data_path, statistics_type='SUM'\n#         )\n\n#         print(f'Column ID: {column_id} | Data saved at {out_data_path}')\n\n#     except:\n#         print(f'Column ID: {column_id} | Error.')\n</code></pre> <pre><code># Merge all the data in a single file\nconcatenated_df = pd.DataFrame()\n\nlulc_class_columns = ['Class_10', 'Class_20', 'Class_30', 'Class_40', 'Class_50', 'Class_60',\n                      'Class_70', 'Class_80', 'Class_90', 'Class_95', 'Class_100']\n\ntemp_file_paths = glob(out_temp_dir + '\\\\*.csv')\n\nfor path in tqdm(temp_file_paths):\n    temp_df = pd.read_csv(path)\n\n    for col in lulc_class_columns:\n        if col not in list(temp_df.columns):\n            temp_df[col] = 0\n\n        temp_df[col] = np.round((temp_df[col] / temp_df['Class_sum']) * 100, 4)\n\n    temp_df = temp_df[['Row', 'Column', 'Longitude', 'Latitude'] + lulc_class_columns]\n\n    concatenated_df = pd.concat((concatenated_df, temp_df), axis=0)\n\n# Filter the grid cell where cropland ('Class_40') area is more than 20%\ndwd_cropland_df = concatenated_df[concatenated_df['Class_40']&gt;=20].iloc[:, :4]\ndwd_cropland_gdf = pd.merge(left=dwd_cropland_df, right=dwd_grid_gdf[['Row', 'Column', 'geometry']], on=['Row', 'Column'], how='left')\ndwd_cropland_gdf = gpd.GeoDataFrame(dwd_cropland_gdf)\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>  0%|          | 0/640 [00:00&lt;?, ?it/s]\n\n\n(190658, 5)\n</code></pre> Row Column Longitude Latitude geometry 0 444 0 5.876024 51.024361 POINT (5.87602 51.02436) 1 477 100 7.311274 50.757258 POINT (7.31127 50.75726) 2 481 100 7.312566 50.721317 POINT (7.31257 50.72132) 3 482 100 7.312888 50.712331 POINT (7.31289 50.71233) 4 511 100 7.322172 50.451747 POINT (7.32217 50.45175) <pre><code># Add the NUTS information\ndwd_cropland_gdf = dwd_cropland_gdf.to_crs(crs='epsg:3857')\ndwd_cropland_gdf = dwd_cropland_gdf.sjoin(de_nuts3_gdf[['NUTS_ID', 'NUTS_NAME', 'geometry']], how='left', predicate='intersects')\ndwd_cropland_gdf.dropna(inplace=True)\ndwd_cropland_gdf.sort_values(by=['NUTS_ID'], inplace=True)\ndwd_cropland_gdf.reset_index(drop=True, inplace=True)\ndwd_cropland_gdf['Cell_ID'] = dwd_cropland_gdf.index\ndwd_cropland_gdf = dwd_cropland_gdf[['Cell_ID', 'Row', 'Column', 'Latitude', 'Longitude', 'NUTS_ID', 'NUTS_NAME', 'geometry']]\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>(190364, 8)\n</code></pre> Cell_ID Row Column Latitude Longitude NUTS_ID NUTS_NAME geometry 0 0 703 234 48.737371 9.201742 DE111 Stuttgart, Stadtkreis POINT (1024333.233 6230415.593) 1 1 691 236 48.845227 9.229425 DE111 Stuttgart, Stadtkreis POINT (1027414.872 6248640.293) 2 2 707 233 48.701424 9.188012 DE111 Stuttgart, Stadtkreis POINT (1022804.862 6224350.32) 3 3 706 233 48.710416 9.188046 DE111 Stuttgart, Stadtkreis POINT (1022808.604 6225867.209) 4 4 705 233 48.719409 9.188080 DE111 Stuttgart, Stadtkreis POINT (1022812.348 6227384.366) <pre><code># Save the data\n# dwd_cropland_gdf.to_csv(os.path.join(out_master_dir, 'DE_DWD_UBN_Crop.csv'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#find-k-nearest-soil-points-for-each-climate-grid-cell","title":"Find k-Nearest Soil Points for Each Climate Grid Cell","text":"<pre><code># Convert the soil data in the same coordinate system\nsoil_attributes_gdf.to_crs(crs='epsg:3857', inplace=True)\nsoil_attributes_gdf.crs == dwd_cropland_gdf.crs\n</code></pre> <pre><code>True\n</code></pre> <pre><code># Extract coordinates\ndwd_cropland_coords = np.array(list(zip(dwd_cropland_gdf.geometry.x, dwd_cropland_gdf.geometry.y)))\nsoil_coords = np.array(list(zip(soil_attributes_gdf.geometry.x, soil_attributes_gdf.geometry.y)))\n\n# Build BallTree for fast nearest neighbor search\ntree = BallTree(soil_coords, metric='euclidean')\n\n# Define number of neighbors (k)\nk = 5  # Adjust based on data density\n\n# Query k-nearest neighbors for each climate point\ndistances, indices = tree.query(dwd_cropland_coords, k=k)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/01_data_preparation_for_pbms/#compute-inverse-distance-weighted-idw-average","title":"Compute Inverse Distance Weighted (IDW) Average","text":"<pre><code># Define soil properties to interpolate\nsoil_properties = soil_attributes_gdf.columns[12:]  # Add all required properties\npower = 2  # IDW power parameter\n\n# Dictionary to store weighted values for each property\nweighted_results = {prop: [] for prop in soil_properties}\n\n# Compute IDW for each climate grid cell\nfor i, climate_point in tqdm(enumerate(dwd_cropland_gdf.geometry)):\n    nearest_soil_points = soil_attributes_gdf.iloc[indices[i]]  # Get k nearest neighbors\n    nearest_distances = distances[i]\n\n    # Avoid division by zero\n    nearest_distances[nearest_distances == 0] = 1e-6\n\n    # Compute weights (w = 1/d^p)\n    weights = 1 / (nearest_distances ** power)\n\n    # Compute weighted average for each soil property\n    for prop in soil_properties:\n        weighted_avg = np.round(np.sum(weights * nearest_soil_points[prop].values) / np.sum(weights), 4)\n        weighted_results[prop].append(weighted_avg)\n\n# Assign weighted values to climate GeoDataFrame\nfor prop in soil_properties:\n    dwd_cropland_gdf[f\"{prop}\"] = weighted_results[prop]\n</code></pre> <pre><code>0it [00:00, ?it/s]\n</code></pre> <pre><code># Add soil layer depth columns (needed for PBMs)\ndwd_cropland_gdf['SoilLayerDepth_10cm'] = 0.1\ndwd_cropland_gdf['SoilLayerDepth_30cm'] = 0.3\ndwd_cropland_gdf['SoilLayerDepth_50cm'] = 0.5\ndwd_cropland_gdf['SoilLayerDepth_70cm'] = 0.7\ndwd_cropland_gdf['SoilLayerDepth_1m'] = 1\ndwd_cropland_gdf['SoilLayerDepth_2m'] = 2\n\n# Reorder the columns\ndwd_cropland_gdf = dwd_cropland_gdf[list(dwd_cropland_gdf.columns[:7]) + list(soil_attributes_gdf.columns[6:12]) + list(soil_properties) + ['geometry']]\nprint(dwd_cropland_gdf.shape)\ndwd_cropland_gdf.head()\n</code></pre> <pre><code>(190364, 38)\n</code></pre> Cell_ID Row Column Latitude Longitude NUTS_ID NUTS_NAME SoilLayerDepth_10cm SoilLayerDepth_30cm SoilLayerDepth_50cm ... BD_70cm BD_1m BD_2m OC_10cm OC_30cm OC_50cm OC_70cm OC_1m OC_2m geometry 0 0 703 234 48.737371 9.201742 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5407 1.5394 1.5394 1.2970 1.1749 0.3660 0.3316 0.1847 0.1847 POINT (1024333.233 6230415.593) 1 1 691 236 48.845227 9.229425 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.4462 1.4607 1.4607 2.2463 1.8119 0.6600 0.5122 0.2873 0.2873 POINT (1027414.872 6248640.293) 2 2 707 233 48.701424 9.188012 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5569 1.4867 1.4867 1.3170 1.1219 0.3438 0.3209 0.1713 0.1713 POINT (1022804.862 6224350.32) 3 3 706 233 48.710416 9.188046 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5501 1.5007 1.5007 1.2979 1.1741 0.3449 0.3250 0.1761 0.1761 POINT (1022808.604 6225867.209) 4 4 705 233 48.719409 9.188080 DE111 Stuttgart, Stadtkreis 0.1 0.3 0.5 ... 1.5484 1.5136 1.5136 1.3000 1.1741 0.3491 0.3254 0.1780 0.1780 POINT (1022812.348 6227384.366) <p>5 rows \u00d7 38 columns</p> <pre><code># Save the data\n# dwd_cropland_gdf.drop(columns='geometry').to_csv(os.path.join(out_master_dir, 'DE_DWD_UBN_Crop_Soil.csv'), index=False)\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/","title":"Data Preparation for PBMs - 2","text":""},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#import-dependencies","title":"Import Dependencies","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport os\nfrom glob import glob\nimport json\nfrom tqdm.auto import tqdm\n\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = 'Times New Roman'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nout_master_dir = r'datasets\\master'\nout_temp_dir = r'temp_data'\n</code></pre>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#read-the-datasets","title":"Read the Datasets","text":"<pre><code># Read the Soil Hydraulic Property dataset\npbm_data = pd.read_csv('datasets\\csvs\\soilhydraulic_property_Germany_Points_Amit.csv', delimiter=';')\npbm_data = pbm_data.iloc[:, :-2]\nprint(pbm_data.shape)\npbm_data.head()\n</code></pre> <pre><code>(190364, 144)\n</code></pre> location dampingdepth soilwater_fc_global soilwater_sat_global drainage_rate deltatheta DZF depth_1 depth_2 depth_3 ... InitialFixedPConcentration_6 slimalfa_1 slimalfa_2 slimalfa_3 slimalfa_4 slimalfa_5 slimalfa_6 Nitrogen Phosphorous Potassium 0 0 6 0.305742 0.41441 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 1 1 6 0.352146 0.447852 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 2 2 6 0.292897 0.422044 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 3 3 6 0.300213 0.420899 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 4 4 6 0.30233 0.418748 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 120.45755 7.1606 16.997152 <p>5 rows \u00d7 144 columns</p>"},{"location":"projects/Crop-Yield-Forecasting-Germany/02_data_preparation_for_pbms/#data-processing","title":"Data Processing","text":"<pre><code># Divide the Nitrogen, Phosphorous, and Potassium data by 10\npbm_data[['Nitrogen', 'Phosphorous', 'Potassium']] = pbm_data[['Nitrogen', 'Phosphorous', 'Potassium']] / 10\nprint(pbm_data.shape)\npbm_data.head()\n</code></pre> <pre><code>(190364, 144)\n</code></pre> location dampingdepth soilwater_fc_global soilwater_sat_global drainage_rate deltatheta DZF depth_1 depth_2 depth_3 ... InitialFixedPConcentration_6 slimalfa_1 slimalfa_2 slimalfa_3 slimalfa_4 slimalfa_5 slimalfa_6 Nitrogen Phosphorous Potassium 0 0 6 0.305742 0.41441 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 1 1 6 0.352146 0.447852 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 2 2 6 0.292897 0.422044 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 3 3 6 0.300213 0.420899 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 4 4 6 0.30233 0.418748 50 0.01 0.1 0.1 0.3 0.5 ... 57 0.95 0.95 0.95 0.95 0.95 0.95 12.045755 0.71606 1.699715 <p>5 rows \u00d7 144 columns</p> <pre><code># Define the scenarios\ndef process_data(row, fertilizer_scenario=2, crop_cyle_count=0, crop='winter wheat'):\n\n    location_values = [row['location']] * 6 \n    fertilizer_scenarios = [fertilizer_scenario] * 6\n    crop_cyle_counts = [crop_cyle_count] * 6\n    crop_values = [crop] * 6\n    type_values = ['PTotal', 'KTotal', 'NTotal'] * 2\n    dvs_values = [0.001, 0.001, 0.25, 0.4, 0.4, 0.9]\n    event_values = [1, 2, 3, 4, 5, 6]\n    fertilizer_value = [round(float(v), 6) for v in [row['Phosphorous'], row['Potassium'], row['Nitrogen']]]\n    fertilizer_values =  [round((v/2), 6) for v  in (fertilizer_value * 2)]\n\n    final_df = pd.DataFrame({\n        'location': location_values,\n        'FertilizerScenario': fertilizer_scenarios,\n        'CropCycleCount': crop_cyle_counts,\n        'crop': crop_values,\n        'Event': event_values,\n        'vType': type_values,\n        'DVS': dvs_values,\n        'Amount': fertilizer_values\n    })\n\n    return final_df\n</code></pre> <pre><code># Apply the algorithm on each rows\npbm_data_processed = pbm_data.apply(process_data, axis=1)\npbm_data_processed = pd.concat(pbm_data_processed.tolist(), ignore_index=True)\nprint(pbm_data_processed.shape)\npbm_data_processed.head()\n</code></pre> <pre><code>(1142184, 8)\n</code></pre> location FertilizerScenario CropCycleCount crop Event vType DVS Amount 0 0 2 0 winter wheat 1 PTotal 0.001 0.358030 1 0 2 0 winter wheat 2 KTotal 0.001 0.849858 2 0 2 0 winter wheat 3 NTotal 0.250 6.022877 3 0 2 0 winter wheat 4 PTotal 0.400 0.358030 4 0 2 0 winter wheat 5 KTotal 0.400 0.849858 <pre><code># Save the data \n# pbm_data_processed.to_csv(os.path.join(out_master_dir, 'fertilizer_Soil3_AllKreis_Krishna.csv'), index=False)\n</code></pre>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/","title":"Flood Susceptibility Zonation of Malda District","text":"<p>This project focuses on identifying flood-prone areas in the Malda District using various machine learning algorithms.</p>"},{"location":"projects/Flood-Susceptibility-Zonation-of-Malda-District/#analysis-notebooks","title":"Analysis Notebooks","text":"<ul> <li>Training and Testing Data Preparation</li> <li>Random Forest Application</li> <li>XGBoost Application</li> <li>Logistic Regression Application</li> <li>SVM Application</li> <li>Convert Image to CSV</li> <li>Classify the Image</li> <li>Assessment of Validation Metrics</li> </ul>"},{"location":"projects/INDI-Res/","title":"INDI-Res","text":"<p>A Time Series of Reservoir Area, Water Level, and Storage in India Derived from High-Resolution Multi-Satellite Observations</p>"},{"location":"projects/INDI-Res/#overview","title":"Overview","text":"<p>INDI-Res is a geospatial data and analysis project that provides a consistent, high-resolution time series of reservoir surface area, water level, and storage dynamics across India. The dataset is derived from multi-satellite Earth observation data, enabling long-term monitoring of surface water resources at national and sub-national scales.</p> <p>This project is designed to support: - Hydrological and water resource assessments - Climate variability and drought studies - Agricultural water management and irrigation planning - Reservoir operation and policy-relevant analysis  </p>"},{"location":"projects/INDI-Res/#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udce1 Multi-satellite integration (optical and/or radar-based observations)</li> <li>\ud83d\uddfa\ufe0f High spatial resolution reservoir surface area mapping</li> <li>\ud83d\udcc8 Time series of water level and storage estimates</li> <li>\ud83c\uddee\ud83c\uddf3 Nationwide coverage across India</li> <li>\ud83d\udd01 Reproducible research pipeline using Python and Jupyter notebooks</li> </ul>"},{"location":"projects/INDI-Res/#repository-structure","title":"Repository Structure","text":"<p>```text INDI-Res/ \u2502 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 environment.yml \u251c\u2500\u2500 .gitignore \u2502 \u251c\u2500\u2500 data/ \u2502   \u251c\u2500\u2500 raw/              # Original satellite and ancillary datasets \u2502   \u251c\u2500\u2500 interim/          # Preprocessed but non-final data \u2502   \u251c\u2500\u2500 processed/        # Final reservoir area, level, and storage products \u2502   \u2514\u2500\u2500 external/         # Third-party datasets (e.g., DEM, reservoir boundaries) \u2502 \u251c\u2500\u2500 notebooks/ \u2502   \u251c\u2500\u2500 00_exploration/           # Initial data inspection and QA \u2502   \u251c\u2500\u2500 01_preprocessing/         # Satellite data preprocessing \u2502   \u251c\u2500\u2500 02_feature_engineering/   # Area\u2013elevation\u2013storage relationships \u2502   \u251c\u2500\u2500 03_modeling/              # Water level and storage estimation \u2502   \u251c\u2500\u2500 04_evaluation/            # Validation and uncertainty analysis \u2502   \u2514\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 src/ \u2502   \u2514\u2500\u2500 indi_res/ \u2502       \u251c\u2500\u2500 init.py \u2502       \u251c\u2500\u2500 config.py \u2502       \u251c\u2500\u2500 data/ \u2502       \u2502   \u251c\u2500\u2500 load.py \u2502       \u2502   \u251c\u2500\u2500 preprocess.py \u2502       \u2502   \u2514\u2500\u2500 utils.py \u2502       \u251c\u2500\u2500 features/ \u2502       \u2502   \u251c\u2500\u2500 build_features.py \u2502       \u2502   \u2514\u2500\u2500 scaling.py \u2502       \u251c\u2500\u2500 models/ \u2502       \u2502   \u251c\u2500\u2500 train.py \u2502       \u2502   \u251c\u2500\u2500 predict.py \u2502       \u2502   \u2514\u2500\u2500 evaluate.py \u2502       \u251c\u2500\u2500 visualization/ \u2502       \u2502   \u2514\u2500\u2500 plots.py \u2502       \u2514\u2500\u2500 utils/ \u2502           \u2514\u2500\u2500 io.py \u2502 \u251c\u2500\u2500 scripts/ \u2502   \u251c\u2500\u2500 run_preprocessing.py \u2502   \u251c\u2500\u2500 train_model.py \u2502   \u2514\u2500\u2500 evaluate_model.py \u2502 \u251c\u2500\u2500 experiments/ \u2502   \u251c\u2500\u2500 exp_001_baseline/ \u2502   \u2502   \u251c\u2500\u2500 config.yaml \u2502   \u2502   \u2514\u2500\u2500 results.json \u2502   \u2514\u2500\u2500 exp_002_multisatellite/ \u2502 \u251c\u2500\u2500 results/ \u2502   \u251c\u2500\u2500 figures/ \u2502   \u251c\u2500\u2500 tables/ \u2502   \u2514\u2500\u2500 metrics/ \u2502 \u251c\u2500\u2500 docs/ \u2502   \u251c\u2500\u2500 methodology.md \u2502   \u251c\u2500\u2500 data_description.md \u2502   \u2514\u2500\u2500 model_details.md \u2502 \u2514\u2500\u2500 tests/     \u251c\u2500\u2500 test_data.py     \u251c\u2500\u2500 test_models.py     \u2514\u2500\u2500 test_features.py</p>"},{"location":"projects/PyImgProc-Image-Processing-using-Python/","title":"PyImgProc-Image-Processing-using-Python","text":"<p>The \"PyImgProc-Image-Processing-using-Python\" GitHub repository is a comprehensive resource for image processing tasks implemented in Python. </p>"}]}