{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input sequence: \"Dream big and work for it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.72, 0.45, 0.31], # Dream    (x^1)\n",
    "     [0.75, 0.20, 0.55], # big      (x^2)\n",
    "     [0.30, 0.80, 0.40], # and      (x^3)\n",
    "     [0.85, 0.35, 0.60], # work     (x^4)\n",
    "     [0.55, 0.15, 0.75], # for      (x^5)\n",
    "     [0.25, 0.20, 0.85]] # it       (x^6)\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Dream', 'big', 'and', 'work', 'for', 'it']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for implementing causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ values.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "d_in = inputs.shape[-1]\n",
    "d_out = 2\n",
    "print(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs,), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for implementing multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5762, -0.1627,  0.5569,  0.3635],\n",
      "         [-0.5650, -0.0622,  0.5600,  0.2991],\n",
      "         [-0.5474, -0.1223,  0.5293,  0.3414],\n",
      "         [-0.5786, -0.0938,  0.5637,  0.3375],\n",
      "         [-0.5586, -0.0418,  0.5521,  0.3015],\n",
      "         [-0.5271, -0.0006,  0.5282,  0.2698]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print('context_vecs.shape:', context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multi-head attention with weight splits\n",
    "\n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class.\n",
    "\n",
    "* Step 1: Reduce the projection dim to match desired output dim\n",
    "* Step 2: Use a linear layer to combine head outputs\n",
    "* Step 3: Tensor shape: (b, num_tokens, d_out)\n",
    "* Step 4: We implicitly split the matrix by adding a num_heads dimension. Then we unroll last dim: (b, num_tokens, head_dim)\n",
    "* Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "* Step 6: Compute dot product of each head\n",
    "* Step 7: Mask truncated to the number of tokens\n",
    "* Step 8: Use the mask to fill attention scores\n",
    "* Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "* Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "* Step 11: Add an optional linear projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_in) # Linear layer to combine\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(\n",
    "                context_length, context_length\n",
    "            ), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a 'num_heads' dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the maks to fill attention scores\n",
    "        attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]]])\n",
      "tensor([[[-0.3464,  0.3428,  0.3846, -0.1505,  0.1512,  0.0491],\n",
      "         [-0.3462,  0.3426,  0.3844, -0.1505,  0.1516,  0.0493],\n",
      "         [-0.3465,  0.3427,  0.3842, -0.1503,  0.1512,  0.0489]],\n",
      "\n",
      "        [[-0.3464,  0.3428,  0.3846, -0.1505,  0.1512,  0.0491],\n",
      "         [-0.3462,  0.3426,  0.3844, -0.1505,  0.1516,  0.0493],\n",
      "         [-0.3465,  0.3427,  0.3842, -0.1503,  0.1512,  0.0489]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89, 0.55, 0.87, 0.66], # Row 1\n",
    "    [0.57, 0.85, 0.64, 0.22, 0.58, 0.33], # Row 2\n",
    "    [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]  # Row 3\n",
    "])\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "print(batch)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "context_length = inputs.shape[0]\n",
    "dropout = 0.0\n",
    "num_heads = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads, True)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print('context_vecs.shape:', context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise implementation of multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input: (batch=1, seq_len=3, d_model=6)\n",
    "x = torch.tensor([[\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], # The \n",
    "    [6.0, 5.0, 4.0, 3.0, 2.0, 1.0], # Kid\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # Smiles\n",
    "]])\n",
    "\n",
    "batch_size, seq_len, d_model = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[ -9.0244, -11.7287,  15.5360,  -1.4474,  -4.5326,   9.4674],\n",
      "         [ -8.0564, -13.2309,   8.2228,  -8.9680,   3.1995,   4.8321],\n",
      "         [ -2.4401,  -3.5657,   3.3941,  -1.4879,  -0.1904,   2.0428]]])\n",
      "K:\n",
      " tensor([[[  8.2602,  14.1116,  -5.0345, -16.4865,  -2.9948,   8.3139],\n",
      "         [ -6.1188,  -0.1587,  -5.0885, -14.3014,   4.9540,   5.6093],\n",
      "         [  0.3059,   1.9933,  -1.4461,  -4.3983,   0.2799,   1.9890]]])\n",
      "V:\n",
      " tensor([[[ 0.5076, -3.4353,  1.8576,  2.8041,  8.9427, 13.1841],\n",
      "         [-1.9113, -3.6934,  1.8502,  1.7622,  1.6981,  3.0978],\n",
      "         [-0.2005, -1.0184,  0.5297,  0.6523,  1.5201,  2.3260]]])\n",
      "X shape: torch.Size([1, 3, 6])\n",
      "Wq shape: torch.Size([6, 6])\n",
      "Wk shape: torch.Size([6, 6])\n",
      "Wv shape: torch.Size([6, 6])\n",
      "Q shape: torch.Size([1, 3, 6])\n",
      "K shape: torch.Size([1, 3, 6])\n",
      "V shape: torch.Size([1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Define 6x6 projection matrices (d_model x d_model)\n",
    "torch.manual_seed(0) # for reprducibility\n",
    "Wq = torch.randn(d_model, d_model)\n",
    "Wk = torch.randn(d_model, d_model)\n",
    "Wv = torch.randn(d_model, d_model)\n",
    "\n",
    "# Compute Q, K, V\n",
    "# Shape logic: (B, T, d_model) @ (d_model, d_model) -> (B, T, d_model)\n",
    "Q = x @ Wq\n",
    "K = x @ Wk\n",
    "V = x @ Wv\n",
    "\n",
    "# Print Q, K, and V\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"K:\\n\", K)\n",
    "print(\"V:\\n\", V)\n",
    "\n",
    "# Print dimensionalities\n",
    "print('X shape:', x.shape)      # (1, 3, 6)\n",
    "print('Wq shape:', Wq.shape)    # (6, 6)\n",
    "print('Wk shape:', Wk.shape)    # (6, 6)\n",
    "print('Wv shape:', Wv.shape)    # (6, 6)\n",
    "print('Q shape:', Q.shape)      # (1, 3, 6)\n",
    "print('K shape:', K.shape)      # (1, 3, 6)\n",
    "print('V shape:', V.shape)      # (1, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wq:\n",
      " tensor([[-1.13, -1.15, -0.25, -0.43,  0.85,  0.69],\n",
      "        [-0.32, -2.12,  0.32, -1.26,  0.35,  0.31],\n",
      "        [ 0.12,  1.24,  1.12, -0.25, -1.35, -1.70],\n",
      "        [ 0.57,  0.79,  0.44,  0.11,  0.64,  0.44],\n",
      "        [-0.22, -0.74,  0.56,  0.26,  0.52,  2.30],\n",
      "        [-1.47, -1.59,  1.20,  0.08, -1.20, -0.00]])\n",
      "\n",
      "Wk:\n",
      " tensor([[-0.23, -0.39,  0.54, -0.40,  0.21, -0.45],\n",
      "        [-0.57, -0.56, -1.53, -1.23,  1.82, -0.55],\n",
      "        [-1.33,  0.19, -0.07, -0.49, -1.48,  2.57],\n",
      "        [-0.47,  0.34, -0.00, -0.53,  1.17,  0.39],\n",
      "        [ 1.94,  0.79, -0.02, -0.44, -1.54, -0.41],\n",
      "        [ 0.97,  1.62, -0.37, -1.30,  0.10,  0.44]])\n",
      "\n",
      "Wv:\n",
      " tensor([[ 0.07,  1.11,  0.28,  0.43, -0.80, -1.30],\n",
      "        [-0.75, -1.31,  0.21, -0.33, -0.43,  0.23],\n",
      "        [ 0.80, -0.18, -0.37, -1.21, -0.70,  1.04],\n",
      "        [-0.60, -1.28, -0.03,  1.37,  2.66,  0.99],\n",
      "        [-0.26,  0.12,  0.24,  1.16,  2.70,  1.24],\n",
      "        [ 0.54,  0.53,  0.19, -0.77, -1.90,  0.13]])\n",
      "\n",
      "Q:\n",
      " tensor([[[ -9.02, -11.73,  15.54,  -1.45,  -4.53,   9.47],\n",
      "         [ -8.06, -13.23,   8.22,  -8.97,   3.20,   4.83],\n",
      "         [ -2.44,  -3.57,   3.39,  -1.49,  -0.19,   2.04]]])\n",
      "\n",
      "K:\n",
      " tensor([[[  8.26,  14.11,  -5.03, -16.49,  -2.99,   8.31],\n",
      "         [ -6.12,  -0.16,  -5.09, -14.30,   4.95,   5.61],\n",
      "         [  0.31,   1.99,  -1.45,  -4.40,   0.28,   1.99]]])\n",
      "\n",
      "V:\n",
      " tensor([[[ 0.51, -3.44,  1.86,  2.80,  8.94, 13.18],\n",
      "         [-1.91, -3.69,  1.85,  1.76,  1.70,  3.10],\n",
      "         [-0.20, -1.02,  0.53,  0.65,  1.52,  2.33]]])\n"
     ]
    }
   ],
   "source": [
    "# Print values\n",
    "torch.set_printoptions(precision=2)\n",
    "print(\"\\nWq:\\n\", Wq)\n",
    "print(\"\\nWk:\\n\", Wk)\n",
    "print(\"\\nWv:\\n\", Wv)\n",
    "\n",
    "print(\"\\nQ:\\n\", Q)\n",
    "print(\"\\nK:\\n\", K)\n",
    "print(\"\\nV:\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q after unrolling: tensor([[[[ -9.02, -11.73,  15.54],\n",
      "          [ -1.45,  -4.53,   9.47]],\n",
      "\n",
      "         [[ -8.06, -13.23,   8.22],\n",
      "          [ -8.97,   3.20,   4.83]],\n",
      "\n",
      "         [[ -2.44,  -3.57,   3.39],\n",
      "          [ -1.49,  -0.19,   2.04]]]])\n",
      "K after unrolling: tensor([[[[  8.26,  14.11,  -5.03],\n",
      "          [-16.49,  -2.99,   8.31]],\n",
      "\n",
      "         [[ -6.12,  -0.16,  -5.09],\n",
      "          [-14.30,   4.95,   5.61]],\n",
      "\n",
      "         [[  0.31,   1.99,  -1.45],\n",
      "          [ -4.40,   0.28,   1.99]]]])\n",
      "V after unrolling: tensor([[[[ 0.51, -3.44,  1.86],\n",
      "          [ 2.80,  8.94, 13.18]],\n",
      "\n",
      "         [[-1.91, -3.69,  1.85],\n",
      "          [ 1.76,  1.70,  3.10]],\n",
      "\n",
      "         [[-0.20, -1.02,  0.53],\n",
      "          [ 0.65,  1.52,  2.33]]]])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 2\n",
    "head_dim = 3\n",
    "\n",
    "Q = Q.view(1, 3, num_heads, head_dim)\n",
    "K = K.view(1, 3, num_heads, head_dim)\n",
    "V = V.view(1, 3, num_heads, head_dim)\n",
    "\n",
    "print('Q after unrolling:', Q)\n",
    "print('K after unrolling:', K)\n",
    "print('V after unrolling:', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q after grouping by heads: tensor([[[[ -9.02, -11.73,  15.54],\n",
      "          [ -8.06, -13.23,   8.22],\n",
      "          [ -2.44,  -3.57,   3.39]],\n",
      "\n",
      "         [[ -1.45,  -4.53,   9.47],\n",
      "          [ -8.97,   3.20,   4.83],\n",
      "          [ -1.49,  -0.19,   2.04]]]])\n",
      "K after grouping by heads: tensor([[[[  8.26,  14.11,  -5.03],\n",
      "          [ -6.12,  -0.16,  -5.09],\n",
      "          [  0.31,   1.99,  -1.45]],\n",
      "\n",
      "         [[-16.49,  -2.99,   8.31],\n",
      "          [-14.30,   4.95,   5.61],\n",
      "          [ -4.40,   0.28,   1.99]]]])\n",
      "V after grouping by heads: tensor([[[[ 0.51, -3.44,  1.86],\n",
      "          [-1.91, -3.69,  1.85],\n",
      "          [-0.20, -1.02,  0.53]],\n",
      "\n",
      "         [[ 2.80,  8.94, 13.18],\n",
      "          [ 1.76,  1.70,  3.10],\n",
      "          [ 0.65,  1.52,  2.33]]]])\n"
     ]
    }
   ],
   "source": [
    "Q = Q.transpose(1, 2)\n",
    "K = K.transpose(1, 2)\n",
    "V = V.transpose(1, 2)\n",
    "\n",
    "print('Q after grouping by heads:', Q)\n",
    "print('K after grouping by heads:', K)\n",
    "print('V after grouping by heads:', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_T shape: tensor([[[[  8.26,  -6.12,   0.31],\n",
      "          [ 14.11,  -0.16,   1.99],\n",
      "          [ -5.03,  -5.09,  -1.45]],\n",
      "\n",
      "         [[-16.49, -14.30,  -4.40],\n",
      "          [ -2.99,   4.95,   0.28],\n",
      "          [  8.31,   5.61,   1.99]]]])\n"
     ]
    }
   ],
   "source": [
    "K_T = K.transpose(2, 3)\n",
    "print('K_T shape:', K_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: torch.Size([1, 2, 3, 3])\n",
      "Attention scores:\n",
      " tensor([[[[-318.27,  -21.97,  -48.61],\n",
      "          [-294.65,    9.55,  -40.73],\n",
      "          [ -87.56,   -1.77,  -12.76]],\n",
      "\n",
      "         [[ 116.15,   51.35,   23.93],\n",
      "          [ 178.44,  171.21,   49.95],\n",
      "          [  42.08,   31.79,   10.55]]]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = Q @ K_T\n",
    "print('Attention scores shape:', attn_scores.shape)\n",
    "print('Attention scores:\\n', attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask:\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "Attention scores after masking:\n",
      " tensor([[[[-318.27,    -inf,    -inf],\n",
      "          [-294.65,    9.55,    -inf],\n",
      "          [ -87.56,   -1.77,  -12.76]],\n",
      "\n",
      "         [[ 116.15,    -inf,    -inf],\n",
      "          [ 178.44,  171.21,    -inf],\n",
      "          [  42.08,   31.79,   10.55]]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print('Causal mask:\\n', mask)\n",
    "\n",
    "attn_scores.masked_fill_(mask, -torch.inf)\n",
    "print('Attention scores after masking:\\n', attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([1, 2, 3, 3])\n",
      "Attention weights:\n",
      " tensor([[[[    1.000,     0.000,     0.000],\n",
      "          [    0.000,     1.000,     0.000],\n",
      "          [    0.000,     0.998,     0.002]],\n",
      "\n",
      "         [[    1.000,     0.000,     0.000],\n",
      "          [    0.985,     0.015,     0.000],\n",
      "          [    0.997,     0.003,     0.000]]]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "head_dim = 3\n",
    "attn_weights = torch.softmax(attn_scores / head_dim**0.5, dim=-1)\n",
    "print('Attention weights shape:', attn_weights.shape)\n",
    "print('Attention weights:\\n', attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights after dropout:\n",
      " tensor([[[[    1.111,     0.000,     0.000],\n",
      "          [    0.000,     1.111,     0.000],\n",
      "          [    0.000,     1.109,     0.000]],\n",
      "\n",
      "         [[    1.111,     0.000,     0.000],\n",
      "          [    1.094,     0.000,     0.000],\n",
      "          [    1.108,     0.003,     0.000]]]])\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(0.1)\n",
    "attn_weights = dropout(attn_weights)\n",
    "print('Attention weights after dropout:\\n', attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector shape: torch.Size([1, 2, 3, 3])\n",
      "Context vec:\n",
      " tensor([[[[ 0.564, -3.817,  2.064],\n",
      "          [-2.124, -4.104,  2.056],\n",
      "          [-2.120, -4.097,  2.052]],\n",
      "\n",
      "         [[ 3.116,  9.936, 14.649],\n",
      "          [ 3.068,  9.786, 14.427],\n",
      "          [ 3.113,  9.915, 14.620]]]])\n"
     ]
    }
   ],
   "source": [
    "context_vec = attn_weights @ V\n",
    "print('Context vector shape:', context_vec.shape)\n",
    "print('Context vec:\\n', context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Reformat and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector after swapping dimensions 1 and 2: torch.Size([1, 3, 2, 3])\n",
      "Context vector:\n",
      " tensor([[[[ 0.564, -3.817,  2.064],\n",
      "          [ 3.116,  9.936, 14.649]],\n",
      "\n",
      "         [[-2.124, -4.104,  2.056],\n",
      "          [ 3.068,  9.786, 14.427]],\n",
      "\n",
      "         [[-2.120, -4.097,  2.052],\n",
      "          [ 3.113,  9.915, 14.620]]]])\n"
     ]
    }
   ],
   "source": [
    "context_vec = context_vec.transpose(1, 2)\n",
    "print('Context vector after swapping dimensions 1 and 2:', context_vec.shape)\n",
    "print('Context vector:\\n', context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector after concatenating heads: torch.Size([1, 3, 6])\n",
      "Context vector:\n",
      " tensor([[[ 0.564, -3.817,  2.064,  3.116,  9.936, 14.649],\n",
      "         [-2.124, -4.104,  2.056,  3.068,  9.786, 14.427],\n",
      "         [-2.120, -4.097,  2.052,  3.113,  9.915, 14.620]]])\n"
     ]
    }
   ],
   "source": [
    "context_vec = context_vec.reshape(batch_size, seq_len, num_heads * head_dim)\n",
    "print('Context vector after concatenating heads:', context_vec.shape)\n",
    "print('Context vector:\\n', context_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch CUDA 12.1",
   "language": "python",
   "name": "pytorchcu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
