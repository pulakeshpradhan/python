{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa956aed",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485b837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03f547",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071d10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "    def __initStates(self, size, device):\n",
    "        return torch.zeros(size).to(device), torch.zeros(size).to(device)\n",
    "    \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        if cur_state == None:\n",
    "            h_cur, c_cur = self.__initStates([input_tensor.shape[0], self.hidden_dim, input_tensor.shape[2], input_tensor.shape[3]], device=input_tensor.device)\n",
    "        else:\n",
    "            h_cur, c_cur = cur_state\n",
    "            \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a18e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_next shape: torch.Size([2, 16, 32, 32])\n",
      "c_next shape: torch.Size([2, 16, 32, 32])\n",
      "h2 shape: torch.Size([2, 16, 32, 32])\n",
      "c2 shape: torch.Size([2, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "input_dim = 3\n",
    "hidden_dim = 16\n",
    "height, width = 32, 32\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "# Create dummy input (one time step)\n",
    "x = torch.randn(batch_size, input_dim, height, width).to(device)\n",
    "\n",
    "# Initialize ConvLSTMCell\n",
    "cell = ConvLSTMCell(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    kernel_size=kernel_size,\n",
    "    bias=True,\n",
    ").to(device)\n",
    "\n",
    "# Forward pass (first time step)\n",
    "h_next, c_next = cell(x, cur_state=None)\n",
    "\n",
    "print(\"h_next shape:\", h_next.shape)\n",
    "print(\"c_next shape:\", c_next.shape)\n",
    "\n",
    "# Second time step\n",
    "x2 = torch.randn(batch_size, input_dim, height, width).to(device)\n",
    "\n",
    "h2, c2 = cell(x2, cur_state=(h_next, c_next))\n",
    "\n",
    "print(\"h2 shape:\", h2.shape)\n",
    "print(\"c2 shape:\", c2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f65fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        \n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError(\"Inconsistent list length.\")\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            \n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "            \n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        \n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "            \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            \n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "                \n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            \n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "            \n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            layer_state_list = last_state_list[-1:]\n",
    "            \n",
    "        return layer_output_list, layer_state_list\n",
    "                \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "            \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError(\"'kernel_size' must be tuple or list of tuples.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b94030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer outputs length: 1\n",
      "Last states length: 1\n",
      "Output shape: torch.Size([2, 5, 16, 32, 32])\n",
      "Hidden state shape: torch.Size([2, 16, 32, 32])\n",
      "Cell state shape: torch.Size([2, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "time_steps = 5\n",
    "input_dim = 3\n",
    "hidden_dim = 16\n",
    "height, width = 32, 32\n",
    "kernel_size = (3, 3)\n",
    "num_layers = 1\n",
    "\n",
    "# Dummy input (B, T, C, H, W)\n",
    "x = torch.randn(batch_size, time_steps, input_dim, height, width).to(device)\n",
    "\n",
    "# Model\n",
    "model = ConvLSTM(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    kernel_size=kernel_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True,\n",
    "    bias=True,\n",
    "    return_all_layers=False\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs, last_states = model(x)\n",
    "\n",
    "print(\"Layer outputs length:\", len(layer_outputs))\n",
    "print(\"Last states length:\", len(last_states))\n",
    "print(\"Output shape:\", layer_outputs[0].shape)\n",
    "print(\"Hidden state shape:\", last_states[0][0].shape)\n",
    "print(\"Cell state shape:\", last_states[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271292c",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3488d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=3, stride=2, padding=1)    \n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv_lstm_e1 = ConvLSTMCell(hidden_dim, hidden_dim, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e2 = ConvLSTMCell(hidden_dim * 2, hidden_dim * 2, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e3 = ConvLSTMCell(hidden_dim * 4, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e4 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e5 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e6 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_e7 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)    \n",
    "\n",
    "        self.conv_lstm_d1 = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d2 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d3 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d4 = ConvLSTMCell(hidden_dim * 8 * 2, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d5 = ConvLSTMCell(hidden_dim * 4 * 2, hidden_dim * 2, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d6 = ConvLSTMCell(hidden_dim * 2 * 2, hidden_dim, kernel_size=(3, 3), bias=False)\n",
    "        self.conv_lstm_d7 = ConvLSTMCell(hidden_dim * 2, hidden_dim, kernel_size=(3, 3), bias=False)\n",
    "        \n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.slope = 0.2\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self.modules():\n",
    "            normal_init(m, mean, std)\n",
    "            \n",
    "    def forward_step(self, input, states_encoder, states_decoder):\n",
    "        e1 = self.conv1(input)\n",
    "        states_e1 = self.conv_lstm_e1(e1, states_encoder[0])\n",
    "        e2 = self.conv2(F.leaky_relu(states_e1[0], self.slope))\n",
    "        states_e2 = self.conv_lstm_e2(e2, states_encoder[1])\n",
    "        e3 = self.conv3(F.leaky_relu(states_e2[0], self.slope))\n",
    "        states_e3 = self.conv_lstm_e3(e3, states_encoder[2])\n",
    "        e4 = self.conv4(F.leaky_relu(states_e3[0], self.slope))\n",
    "        states_e4 = self.conv_lstm_e4(e4, states_encoder[3])\n",
    "        e5 = self.conv5(F.leaky_relu(states_e4[0], self.slope))\n",
    "        states_e5 = self.conv_lstm_e5(e5, states_encoder[4])\n",
    "        e6 = self.conv6(F.leaky_relu(states_e5[0], self.slope))\n",
    "        states_e6 = self.conv_lstm_e6(e6, states_encoder[5])\n",
    "        e7 = self.conv7(F.leaky_relu(states_e6[0], self.slope))\n",
    "        states_e7 = self.conv_lstm_e7(e7, states_encoder[6])\n",
    "        \n",
    "        states_d1 = self.conv_lstm_d1(F.relu(states_e7[0]), states_decoder[0])\n",
    "        d1 = self.up(states_d1[0])\n",
    "        d1 = torch.cat([d1, e6], 1)\n",
    "        \n",
    "        states_d2 = self.conv_lstm_d2(F.relu(d1), states_decoder[1])\n",
    "        d2 = self.up(states_d2[0])\n",
    "        d2 = torch.cat([d2, e5], 1)\n",
    "        \n",
    "        states_d3 = self.conv_lstm_d3(F.relu(d2), states_decoder[2])\n",
    "        d3 = self.up(states_d3[0])\n",
    "        d3 = torch.cat([d3, e4], 1) \n",
    "        \n",
    "        states_d4 = self.conv_lstm_d4(F.relu(d3), states_decoder[3])\n",
    "        d4 = self.up(states_d4[0])\n",
    "        d4 = torch.cat([d4, e3], 1) \n",
    "        \n",
    "        states_d5 = self.conv_lstm_d5(F.relu(d4), states_decoder[4])\n",
    "        d5 = self.up(states_d5[0])\n",
    "        d5 = torch.cat([d5, e2], 1)\n",
    "        \n",
    "        states_d6 = self.conv_lstm_d6(F.relu(d5), states_decoder[5])\n",
    "        d6 = self.up(states_d6[0])\n",
    "        d6 = torch.cat([d6, e1], 1) \n",
    "        \n",
    "        states_d7 = self.conv_lstm_d7(F.relu(d6), states_decoder[6])\n",
    "        d7 = self.up(states_d7[0])\n",
    "        \n",
    "        out = torch.tanh(self.conv_out(d7))\n",
    "        \n",
    "        states_e = [states_e1, states_e2, states_e3, states_e4, states_e5, states_e6, states_e7]\n",
    "        states_d = [states_d1, states_d2, states_d3, states_d4, states_d5, states_d6, states_d7]\n",
    "        \n",
    "        return out, (states_e, states_d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        states_encoder = [None] * 7\n",
    "        states_decoder = [None] * 7\n",
    "        batch, _, h, w, t = x.shape\n",
    "        output = torch.zeros((batch, self.conv_out.out_channels, h, w, t), device=x.device)\n",
    "\n",
    "        for t in range(x.shape[4]):\n",
    "            output[..., t], states = self.forward_step(x[..., t], states_encoder, states_decoder)\n",
    "            states_encoder, states_decoder = states[0], states[1]\n",
    "        return output, states\n",
    "    \n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f09c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 128, 128, 10])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE\n",
    "states_encoder = [None] * 7\n",
    "states_decoder = [None] * 7\n",
    "x = torch.randn((2, 4, 128, 128, 10), dtype=torch.float32) \n",
    "model = Generator(in_channels=4, out_channels=4)\n",
    "y, states = model(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57567fb",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eec6a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(hidden_dim * 2, track_running_stats=False)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(hidden_dim * 4, track_running_stats=False)\n",
    "        self.conv4_lstm = ConvLSTMCell(hidden_dim * 4, hidden_dim * 4, kernel_size=(3, 3), bias=False)\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=4, stride=1, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(hidden_dim * 8, track_running_stats=False)\n",
    "        self.conv5_lstm = ConvLSTMCell(hidden_dim * 8, hidden_dim * 8, kernel_size=(3, 3), bias=False)\n",
    "        self.conv5 = nn.Conv2d(hidden_dim * 8, 1, kernel_size=4, stride=1, padding=1)\n",
    "                \n",
    "        self.slope = 0.2\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self.modules():\n",
    "            normal_init(m, mean, std)\n",
    "            \n",
    "    def forward_step(self, input, states):\n",
    "        x = F.leaky_relu(self.conv1(input), self.slope)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), self.slope)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), self.slope)\n",
    "        states1 = self.conv4_lstm(x, states[0])\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(states1[0])), self.slope)\n",
    "        states2 = self.conv5_lstm(x, states[1])\n",
    "        x = F.leaky_relu(self.conv5(states2[0]), self.slope)\n",
    "        \n",
    "        return x.squeeze(dim=1), (states1, states2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, _, h, w, t = x.shape\n",
    "        output = torch.zeros((batch, (h//8)-2, (w//8)-2, t), device=x.device)\n",
    "        states = (None, None)\n",
    "        for timestep in range(t):\n",
    "            output[..., timestep], states = self.forward_step(x[..., timestep], states)\n",
    "        return F.sigmoid(output)\n",
    "    \n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c17d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE\n",
    "x = torch.randn((16, 3, 32, 32, 4), dtype=torch.float32) \n",
    "model = Discriminator(in_channels=3)\n",
    "y = model(x)\n",
    "print(y.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
